---
title: "Lab 04"
subtitle: "Ordinary Least Squares"
output: 
  html_document:
    toc: true
    toc_depth: 2
    toc_float:
      smooth_scroll: false
    self_contained: false
    css: custom_style.css
    includes:
      in_header: toggle.html
    theme: 
      bootswatch: sandstone
---

```{r, include = FALSE, code=xfun::read_utf8(here::here("exercises", "before_chunk.R"))}
```


## Outline {#Outline}

__tl;dr__ working with data tables, visualizing densities, calculating bivariate statistics, and fitting linear models. 


### Objectives {.objectives}

This lab will guide you through the process of  

1. working with data tables (data frames in R parlance)
    - creating them with `data.frame()`
    - getting information about them with `nrow()`, `ncol()`, and `names()`
    - extracting variables with `<table>[[<column>]]` and `<table>$<column>`
2. visualizing distributions with density plots
3. calculating covariance
4. calculating correlation and evaluating with the t-test
5. conducting a $\chi^2$ test
6. building a simple linear model
    - the formula notation
    - the `lm()` function
    - the model `summary()`


### R Packages

We will be using the following packages:

- [archdata](https://cran.r-project.org/web/packages/archdata/index.html)
- [ggplot2](https://ggplot2.tidyverse.org/index.html)
- [palmerpenguins](https://allisonhorst.github.io/palmerpenguins/)
- [viridis](https://sjmgarnier.github.io/viridis/)

To install these packages, __run the following code in your console__:

```{r, eval = FALSE}

install.packages(
  c("archdata", "ggplot2", "palmerpenguins", "viridis")
)

```

__Note:__ You should not `install.packages()` in an Rmd document. Use that function in your R console instead. Then use `library()` as part of the preamble in your Rmd document to check packages out of the library and use them in that R session. This should always go at the start of your document!

```{r}

library(archdata)
library(ggplot2)
library(palmerpenguins)
library(viridis)

```


### Data

- `cars`
    - Includes measurements of car speed and stopping distance.
    - package: `datasets`
    - reference: <https://stat.ethz.ch/R-manual/R-devel/library/datasets/html/cars.html>
- `DartPoints`
    - Includes measurements of 91 Archaic dart points recovered during surface surveys at Fort Hood, Texas.
    - package: `archdata`
    - reference: <https://cran.r-project.org/web/packages/archdata/archdata.pdf>
- `OxfordPots`
    - Includes percentages of late Romano-British Oxford Pottery on 30 sites, along with their distance from Oxford.
    - reference: <https://cran.r-project.org/web/packages/archdata/archdata.pdf>
- `penguins` 
    - Includes measurements for penguin species, island in Palmer Archipelago, size (flipper length, body mass, bill dimensions), and sex. 
    - package: `palmerpenguins`
    - reference: <https://allisonhorst.github.io/palmerpenguins/reference/penguins.html>
- `titanic`
    - Provides information on the fate of passengers of the Titanic, including economic status, sex, age, and survival.
    - package: none
    - reference: <https://wilkelab.org/SDS375/>
    - note: we're going to download this one rather than get it from a package



## Data Tables {#Data_Tables}

```{r, echo = FALSE, fig.cap = "Elements of a Data Table"}

figure("r-tables.png")

```

For almost any statistical analysis you might want to conduct in R, you will work with _rectangular_ or _tabular_ data, data that comes as collections of values organized into rows and columns of equal length. To avoid possible misunderstandings, let's define some key terms for talking about data tables. Specifically, we will adopt the following conventions:

1. Each variable must have its own column.
2. Each observation must have its own row.
2. Each value must have its own cell.

Any table of data that satisfies these rules is called __tidy__ data. It is important to note, of course, that the opposite of tidy data is not necessarily messy data, for data can come in many formats (sound and video, for example). However, when we want to conduct some statistical analysis, like building a linear model of our data, we will almost certainly want our data to be tidy. To learn more about this concept of tidy data, you can read up on it in the book [R for Data Science](https://r4ds.had.co.nz/tidy-data.html), which is available for free online. 

### Creating tables

To create a tidy data table, we will use the `data.frame()` function. Here, for example, is how we would create the table above:  

```{r}

projectiles <- data.frame(
  type = c("Elko", "Rosegate", "DSN", "Elko", "Clovis"),
  length = c(2.03, 1.4, 1.9, 2.1, 3.3),
  width = c(0.8, 0.4, 0.3, 0.7, 0.95),
  height = c(3.23, 2.4, 1.29, 2.7, 4.15)
)

projectiles

```

Note that the values (or measurements) contained in each variable are wrapped in the `c()` function (short for _concatenate_). This variable tells R that all these values belong to this one variable. Each variable in turn is an argument to `data.frame()` having the form `<variable> = c(<value-1>, <value-2>, ..., <value-n>)`.

### Getting basic meta-data from tables 

When you want to know what variables a table includes, you can use the `name()` function.

```{r}

names(projectiles)

```

If you want to know how many variables or observations the table has, you can use `nrow()` and `ncol()` respectively.

```{r}

# number of observations
nrow(projectiles)

# number of rows
ncol(projectiles)

```

### Indexing tables

Sometimes, you will want to extract a variable from a table. To do that, you can use double brackets, `<table>[[<variable>]]`, or the dollar-sign operator, `<table>$<variable>`. 

```{r}

projectiles[["type"]]

projectiles$length

```

Note that you can and often will want to assign these to their own objects, so you can use them again later.

```{r}

p_type <- projectiles[["type"]]

p_length <- projectiles$length

```


### Exercises

1. Get the names of the variables in the `penguins` table with `names()`.
2. How many observations and variables are in this dataset? Hint: use `nrow()` and `ncol()`.
3. Extract the `bill_length_mm` variable from this table and assign it to an object called `bill_length`. Do the same for `bill_depth_mm` and call it `bill_depth`. You can use either `<table>[[<variable>]]` or `<table>$<variable>`. 


## Density Plots {#Density_Plots}

__Note!__ Before you work through this section, add the following line of code to your Rmd document for this lab and run it. If you have a stable internet connection, you can actually use `read.csv()` to download tables that are stored online, and we're using that here to get data on the Titanic.

```{r}

titanic <- read.csv("https://raw.githubusercontent.com/wilkelab/SDS375/master/datasets/titanic.csv")

```

Here you will learn how to create a kernel density estimate (KDE) plot using `ggplot()` and `geom_density()`. A KDE or "density" plot is similar to a histogram in that it seeks to show the distribution of a continuous random variable. It differs, however, in the way it goes about doing that. To see how they differ, have a look at these two examples. The one on the left is a histogram, the one on the right a density plot. Both, however, represent the same data. 

```{r, echo = FALSE, out.width = "50%", fig.align = "default"}

ggplot(titanic, aes(age)) + geom_histogram() + ggtitle("Histogram")

ggplot(titanic, aes(age)) + geom_density() + ggtitle("Kernel density estimate")

```

There are two big differences here. First, a histogram shows raw counts, a KDE the proportions (or "density"), with total area under the curve summing to one. Second, a histogram discretizes the data using bins. A KDE is smooth and continuous. In effect, the KDE approach draws a normal distribution (called a __kernel__) around each data point the variable contains with mean equal to the data point's value and standard deviation (called the __bandwidth__) set to some arbitrary value. The kernels are then summed to produce a curve like the one above. The *band*width is similar in spirit to the *bin* width.

As with the histogram, we specify a density geometry for ggplot using `geom_density()`.

```{r, echo = FALSE}

ggplot2::theme_set(ggplot2::theme_gray())

```

```{r}

ggplot(titanic, aes(age)) + 
  geom_density()

```

Again, we can specify different aesthetics like `fill` and `color` and update the labels with `labs()`.

```{r}

ggplot(titanic, aes(age)) + 
  geom_density(
    fill = "#A8BFF0", # <--- HEX color code
    color = "#183C8C"
  ) + 
  labs(
    x = "Age",
    y = "Density",
    title = "Age Distribution",
    subtitle = "Passengers of the Titanic"
  )

```

We can also map these aesthetics to other variables like the sex of the passenger.

```{r}

ggplot(titanic, aes(age, fill = sex)) + 
  geom_density() + 
  labs(
    x = "Age",
    y = "Density",
    title = "Age Distribution",
    subtitle = "Passengers of the Titanic"
  )

```

And, we can change the default fill colors using `scale_fill_manual()`, too.

```{r}

ggplot(titanic, aes(age, fill = sex)) + 
  geom_density() + 
  scale_fill_manual(
    values = c("#A8BFF0", "#FFE66D")
  ) +
  labs(
    x = "Age",
    y = "Density",
    title = "Age Distribution",
    subtitle = "Passengers of the Titanic"
  )

```

In this case, however, it's hard to see where the two distributions overlap, which makes it hard to compare them. One way to fix this is to change the opacity of the fill color using the `alpha()` function (note that the alpha or transparency of a color can range from 0 to 1.). 

```{r}

ggplot(titanic, aes(age, fill = sex)) + 
  geom_density() + 
  scale_fill_manual(
    values = alpha(c("#A8BFF0", "#FFE66D"), 0.5)
  ) +
  labs(
    x = "Age",
    y = "Density",
    title = "Age Distribution",
    subtitle = "Passengers of the Titanic"
  )

```

This is still a little hard to read, so let's try faceting instead of alpha. Let's also drop the background vertical grid lines using a theme. At the same time, we'll go ahead and drop the label "sex" from the legend as that should be obvious from the key. We do that by setting `name = NULL` in `scale_fill_manual()`.

```{r}

ggplot(titanic, aes(age, fill = sex)) + 
  geom_density() + 
  scale_fill_manual(
    name = NULL,
    values = c("#A8BFF0", "#FFE66D")
  ) +
  labs(
    x = "Age",
    y = "Density",
    title = "Age Distribution",
    subtitle = "Passengers of the Titanic"
  ) +
  facet_wrap(~sex) +
  theme_minimal() +
  theme(
    panel.grid.major.x = element_blank(),
    panel.grid.minor.x = element_blank(),
  )

```

Notice that the facet labels are redundant with the legend key here, so let's remove those, too. We do that by setting the theme arguments `strip.background` and `strip.text` to `element_blank()`. Finally, we can move the legend to the bottom of the plot and make it horizontal with `legend.position` and `legend.direction` respectively.

```{r}

ggplot(titanic, aes(age, fill = sex)) + 
  geom_density() + 
  scale_fill_manual(
    name = NULL,
    values = c("#A8BFF0", "#FFE66D")
  ) +
  labs(
    x = "Age",
    y = "Density",
    title = "Age Distribution",
    subtitle = "Passengers of the Titanic"
  ) +
  facet_wrap(~sex) +
  theme_minimal() +
  theme(
    legend.position = "bottom",
    legend.direction = "horizontal",
    panel.grid.major.x = element_blank(),
    panel.grid.minor.x = element_blank(),
    strip.background = element_blank(),
    strip.text = element_blank()
  )

```


### Exercises

1. Make a kernel density plot of penguin bill length using `ggplot()` and `geom_density()`. Then make all of the following changes:
    - Map penguin `species` to the `fill` aesthetic.
    - Update the axis labels and plot title using `labs()`.
    - Use `scale_fill_viridis` to use colorblind safe colors for the fill. Note! Species is a discrete or categorical variable, so make sure to set `discrete = TRUE`!
    - Use `facet_wrap()` to facet by `species`.
    - Choose a suitable theme, like `theme_minimal()`.
    - Remove vertical grid lines.
    - Change the legend position to bottom and make it horizontal.
    - Remove strip text and background.
    


## Bivariate Statistics {#Bivariate_Statistics}

Bivariate statistics provide simple measures of the relationship between two variables. Here we will learn how to calculate two such statistics in R: covariance and correlation. These allow us to describe the direction of the relationship (_is it positive or negative?_) and the strength of the relationship (_is it strong or weak?_). In this case, we'll investigate the relationship between penguin body mass and bill length. We'll be asking this:

__Question__: Is there a relationship between bill length and body mass? Is it positive or negative?

Before we do that, however, it is useful to visualize our data.

```{r}

ggplot(penguins, aes(x = body_mass_g, y = bill_length_mm)) + 
  geom_point(
    shape = 21,
    fill = "#A8BFF0",
    color = "#15357A",
    size = 2
  ) +
  labs(
    x = "Body Mass (g)",
    y = "Bill Length (mm)",
    title = "Palmer Penguins"
  ) +
  theme_bw()

```

What does this tell you about the relationship between these variables?

### Covariance

Covariance provides a measure of the extent to which two variables vary together. The sign of the covariance reflects a positive or negative trend, but not magnitude. To calculate this value in R, use the `cov()` function.

```{r}

bill_length <- penguins$bill_length_mm
body_mass_g <- penguins$body_mass_g

cov(bill_length, body_mass_g, use = "complete.obs") # complete.obs means ignore NA values

```

This is a positive number, meaning the relationship between bill length and body mass is positive (the one tends to increase as the other increases). The size of the number by itself is unhelpful, however, and cannot be used to infer anything about the strength of the relationship. That is because covariance is sensitive to the unit of measure. If, for example, we convert `body_mass` from grams to kilograms, we will get a different covariance statistic.

```{r}

# convert to kilograms by dividing by 1000
body_mass_kg <- body_mass_g/1000

cov(bill_length, body_mass_kg, use = "complete.obs")

```



### Correlation

To remove units of measure and prevent resulting changes in the magnitude of the covariance, we can scale the covariance by the standard deviations of the samples. The resulting value is known as Pearson's Correlation Coefficient, which ranges from -1 to 1.

```{r}

cor(bill_length, body_mass_g, use = "complete.obs")

```

Just to demonstrate that this isn't sensitive to units of measure, let's see what happens when use body mass measures in kilograms.

```{r}

cor(bill_length, body_mass_kg, use = "complete.obs")

```

There's no change! In either case, the resulting coefficient is greater than zero, suggesting a positive trend, but is this value significantly different than zero? To answer that question, we can convert this coefficient to a t-statistic and compare it to a t-distribution. This is done with the `cor.test()` function. For this test, we have the following hypotheses:

- $H_0$: the coefficient is equal to zero
- $H_1$: the coefficient is _not_ equal to zero

And, of course, we must stipulate a critical value. In this case, we will stick with tradition:  

$\alpha = 0.05$  

So, now, here is our test:

```{r}

cor.test(bill_length, body_mass_g, use = "complete.obs")

```

In this case, you see that $p < \alpha$, hence we reject the null hypothesis, meaning our coefficient estimate is significantly different than zero. There is, in other words, a positive relationship between body mass and bill length among the Palmer penguins.  

### Exercises
1. Using the `penguins` dataset, do all of the following:
    - calculate the covariance between bill length and bill depth,
    - calculate Pearson's Correlation Coefficient for bill length and bill depth,
    - do a correlation test to determine whether the coefficient is significantly different than zero, and
    - be sure to state your null and alternative hypotheses, as well as the critical value!
2. What does the correlation test tell you about the relationship between bill length and bill depth?



## Linear Models {#Linear_Models}

In this section, we will learn how to fit a linear model to our data. We will look, specifically, at a scenario involving an experiment with cars recorded in the `cars` dataset. We want to know what kind of relationship there is between the distance (in feet) a car travels after the brakes are applied and the speed (in miles per hour) the car was going when the brakes were applied. We will doing this by fitting a linear model with the `lm()` function. Here are our hypotheses:

- $H_0$: there is no relationship between speed and distance.
- $H_1$: there is a relationship between speed and distance.


### Model formula

Before doing that, however, let's discuss the formula syntax that the `lm()` function uses. To fit a model, we must first specify a formula. This involves three components: a predictor variable, the tilde `~`, and a response variable. The syntax is this:

`<response> ~ <predictor>`

In the case of the cars data, that's:

`dist ~ speed`

This can be read as saying, in effect, "distance as a function of speed." Note that you do not have to put the variables in quotes or anything like that. It's just the names of the variables separated by a tilde.


### Model fitting

In addition to specifyfing the formula, we must also tell the `lm()` what dataset our observations are coming from. We do this by specifying the `data` argument. The whole function call looks like this:

```{r}

cars_model <- lm(dist ~ speed, data = cars)

cars_model

```

Here, the model estimates both the intercept and a coefficient of relationship between speed and distance.

### Model summary

A more informative report of the model is provided by the `summary()` function. In addition to reporting on the model coefficients, this will also conduct a t-test on each coefficient, evaluating whether they are significantly different than zero.

```{r}

summary(cars_model)

```

We'll go over this `summary()` in more detail later. For now, note that it reports the coefficient "Estimate", the t-statistic (or "t value") for each coefficient estimate, and the p-value for the respective t-tests. In each case, the null hypothesis is that the coefficient is zero. A small p-value then gives us reason to reject the null and accept the coefficient estimate as significant. In this case, the p-value is very small, so we can accept both the intercept and speed coefficients. This tells us (as you might expect) that there is a significant positive relationship between the speed the car was going when it applied the brakes and the distance it traveled after applying the brakes.


### Exercises

1. Using the `penguins` dataset, build a linear model of the relationship between bill length and bill depth.
2. What are the coefficients reported by this model? Specifically, the intercept and the coefficient of relationship between bill length and bill depth.
3. Apply the `summary()` function to your model. Are the coefficients significant?

    
## Homework {#Homework}

1. Load the following datasets from the `archdata` package using `data()`.
    - `DartPoints`
    - `OxfordPots`
2. Practice extracting variables from these tables. 
    - From each, remove one variable and assign it to an object with an informative name.
    - Calculate the mean and variance for each variable. 
2. Using the `DartPoints` dataset, make a kernel density plot of dart `Length` to visualize its distribution. Make sure to do all of the following:
    - Map the dart `Name` (or type) to the `fill` aesthetic.
    - Update the axis labels and plot title using `labs()`.
    - Use `scale_fill_viridis` to use colorblind safe colors for the fill. Note! Species is a discrete or categorical variable, so make sure to set `discrete = TRUE`!
    - Use `facet_wrap()` to facet by `Name` (or type).
    - Choose a suitable theme, like `theme_minimal()`.
    - Remove vertical grid lines.
    - Change the legend position to bottom and make it horizontal.
    - Remove strip text and background.
2. Using the `DartPoints` dataset, calculate the covariance and correlation between dart length and width. 
    - Then conduct a correlation test to evaluate the significance of Pearson's Correlation Coefficient.
    - Be sure to state the null and alternative hypotheses, as well as the critical value.
    - Is the coefficient significant?
    - What does this mean about the relationship between dart length and width?
2. Using the `DartPoints` dataset, build a linear model showing the relationship (if any) between the length and width of dart points. Be sure to do all of the following:
    - To visualize the relationship, make a scatter plot of dart `Length` and `Width` using `ggplot()` and `geom_point()`. Hint: your aesthetic mapping should be `aes(x = Width, y = Length)`.
    - Use the correct formula syntax. In this case, the dependent variable is `Length` and the independent variable is `Width`. 
    - Use `summary()` to report the model. 
    - Are the coefficient estimates significant?
    - What does this mean about the relationship between the length and width of dart points? Hint: it's called allometry.
2. Using the `OxfordPots` dataset, build a linear model showing the relationship (if any) between the percentage of Oxford Pots found on an archaeological site and the distance of that site from the city of Oxford. Be sure to do all of the following:
    - To visualize the relationship, make a scatter plot of the proportion of Oxford pots and distance to Oxford using `ggplot()` and `geom_point()`. Hint: your aesthetic mapping should be `aes(x = OxfordDst, y = OxfordPct)`.
    - Use the correct formula syntax. In this case, the dependent variable is `OxfordPct` and the independent variable is `OxfordDst`. 
    - Use `summary()` to report the model. 
    - Are the coefficient estimates significant?
    - What does this mean about the relationship between the proportion of Oxford pots on an archaeological site and distance from Oxford?