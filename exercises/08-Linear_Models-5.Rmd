---
title: "Lab 07"
subtitle: "Multiple Linear Regression"
output: 
  html_document:
    toc: true
    toc_depth: 2
    toc_float:
      smooth_scroll: false
    self_contained: false
    css: custom_style.css
    includes:
      in_header: toggle.html
    theme: 
      bootswatch: sandstone
---

```{r, include = FALSE, code=xfun::read_utf8(here::here("exercises", "before_chunk.R"))}
```


## Outline {#Outline}

__TL;DR__ modifying tables, multiple linear models, multicollinearity, ANOVA, prediction. 

__Caution!__ Please note that all labs assume that you are working in an RStudio Project directory!


### Objectives

This lab will guide you through the process of  

1. Modifying tables by
    - Adding or changing variables with `mutate()`
    - Grouping variables and aggregating with `group_by()` and `summary()`
2. Creating multiple linear models
    - histogram of residuals
    - diagnostic plots
2. Checking for correlation in the predictors
    - Scatterplot matrix with `plot.data.frame()`
    - Pearson's Correlation with `cor()`
    - Variance Inflation Factor with `vif()`
2. Marginal response plots


### R Packages

We will be using the following packages:

- [archdata](https://cran.r-project.org/web/packages/archdata/index.html)
- [car](https://socialsciences.mcmaster.ca/jfox/Books/Companion/index.html)
- [dplyr](https://dplyr.tidyverse.org/)
- [ggplot2](https://ggplot2.tidyverse.org/index.html)
- [palmerpenguins](https://allisonhorst.github.io/palmerpenguins/)
- [performance](https://easystats.github.io/performance/)
- [tibble](https://tibble.tidyverse.org/)

To install these packages, __run the following code in your console__:

```{r, eval = FALSE}

install.packages(
  c("archdata", "car", "dplyr", "ggplot2", "palmerpenguins", "performance", "tibble")
)

```

__Note:__ You should not `install.packages()` in an Rmd document. Use that function in your R console instead. Then use `library()` as part of the preamble in your Rmd document to check packages out of the library and use them in that R session. This should always go at the start of your document!

```{r}

library(archdata)
library(car)
library(dplyr)
library(ggplot2)
library(palmerpenguins)
library(performance)
library(tibble)

```


### Data

- `DartPoints`
    - Includes measurements of 91 Archaic dart points recovered during surface surveys at Fort Hood, Texas.
    - package: `archdata`
    - reference: <https://cran.r-project.org/web/packages/archdata/archdata.pdf>
- `OxfordPots`
    - Includes percentages of late Romano-British Oxford Pottery on 30 sites, along with their distance from Oxford.
    - reference: <https://cran.r-project.org/web/packages/archdata/archdata.pdf>
- `penguins` 
    - Includes measurements for penguin species, island in Palmer Archipelago, size (flipper length, body mass, bill dimensions), and sex. 
    - package: `palmerpenguins`
    - reference: <https://allisonhorst.github.io/palmerpenguins/reference/penguins.html>


## Mutating Data {#Mutating_Data}

It is often the case that you will want to change a variable in your data or add new variables. This can be done with the `mutate()` function from the `dplyr` package. It is inspired by the base R `transform()` function, which works in much the same way. Here, we will learn how to use `mutate()` to change and add variables. The syntax is similar to other `dplyr` verbs we've learned already. The first argument is always the data. After that, you can specify `variable = value` or `key = value` pairs like you would when constructing a regular old data.frame.

For instance, making a table looks like this:

```
data.frame(
  x = 1:5,
  y = 6:10
)
```

And mutating a table looks like this:

```
mutate(
  penguins,
  id = 1:n(),
  bill_length_cm = bill_length_mm/10
)
```

Here we change the `penguins` table in two ways. First, we add a new variable `id`. The values it takes are each integer from 1 to the number of penguins in the table (`r nrow(penguins)` in this case). Notice that we refer to the total number using the function `n()`. This is a convenience function that works only inside `dplyr` verbs. Second, we add a variable `bill_length_cm` by adding a conversion to the original variable `bill_length_mm`, specifically we divide by 10, thus converting from millimeters to centimeters.



## Grouping Data {#Grouping_Data}


## The Pipe {#The_Piping}

```{r}

knitr::include_graphics("https://upload.wikimedia.org/wikipedia/en/b/b9/MagrittePipe.jpg")

```

Up to now, if we wanted to make a series of changes to our data, we had to save to intermediate objects or overwrite the object we were working with. For example,

```
bob <- select(penguins, species, island, bill_length_mm, body_mass_g)
tom <- filter(bob, island == "Torgersen")
sue <- mutate(tom, bill_length_squared = bill_length_mm^2)

remove(bob, tom)
```

Sometimes, you might see people write the same series of function calls this way:

```
sue <- mutate(filter(select(penguins, species, island, bill_length_mm, body_mass_g), island == "Torgersen"), bill_length_squared = bill_length_mm^2)
```

This is called nesting. You should avoid this like the plague! It makes reading code very, very difficult, as you must follow results from the inside out, as opposed to left to right, which is more natural. To avoid creating


## Visualize Model {#Visualize_Model}

To aid in the interpretation of your model, it is useful to visualize the relationship or trend it suggests (if it does suggest one!). Before building that model, however, you should, as always, make sure to visualize your data! 

```{r}

# clean up data, remove rows with missing data
penguins <- filter(
  penguins, 
  !is.na(body_mass_g), # !is.na(x) == exclude rows with NA values for x
  !is.na(bill_length_mm)
)

ggplot(penguins, aes(body_mass_g, bill_length_mm)) +
  geom_point(size = 2) +
  theme_minimal() +
  labs(
    x = "Body Mass (g)",
    y = "Bill Length (mm)"
  )

```

What is the relationship here? Let's see if a linear model can help us out.

```{r}

penguins_model <- lm(bill_length_mm ~ body_mass_g, data = penguins)

summary(penguins_model)

```

There are two ways to visualize this model:  
1. With __abline__. Use the estimated coefficients (the slope and intercept) to construct a formula that will calculate values of y across the range of x. The formula has the form: $y \sim a + bx$, where is $a$ is the intercept and $b$ is the slope, hence _abline_.
2. With __predict__. Use the model to estimate values of $y$ for specified values of $x$ and construct a line from those values.

There's actually a third much more direct way that `ggplot()` offers that uses `geom_smooth()`, but we'll save that one for another time. 

Anyway, let's try an example of each, so you can get a feel for how to do this.

### ABline

`ggplot()` has a geometry for this. As you might have guessed, it's `geom_abline()`. All we need to do is extract the values of the coefficients from the model and feed these to the `slope` and `intercept` parameters, respectively. To do that, we will use the `coefficients()` function. This provides a named vector that we can use to get our estimates. Notice that we use `<vector>[[<variable]]` like we do with tables, only this time we are extracting a single value.

```{r}

betas <- coefficients(penguins_model)

betas

intercept <- betas[["(Intercept)"]]
slope <- betas[["body_mass_g"]]

```

Now, we can plot our model over the data. This is always useful, as you can see how the model compares to the actual observations.

```{r}

ggplot(penguins, aes(body_mass_g, bill_length_mm)) +
  geom_point(size = 2) +
  theme_minimal() +
  labs(
    x = "Body Mass (g)",
    y = "Bill Length (mm)"
  ) +
  geom_abline(
    slope = slope,
    intercept = intercept,
    color = "darkred",
    size = 1 # increase line thickness to make it more visible
  )

```

With this method, we simply supply the coefficients. `ggplot()` then uses those to estimate values of y for each value of x shown within the range of x shown in the plot. Notice that the line continues across the full range of the graph. This shows that the model assumes the relationship is _linear_, meaning in this case that it will always increase to the right (to infinity) and always decrease to the left (to negative infinity)

### Predict

We can also generate values of y manually with the `predict()` function. The key here is to supply it with our model, which it will then use to make predictions.

```{r}

estimates <- predict(penguins_model)

# add to the penguins table
penguins$estimates <- estimates

ggplot(penguins) +
  geom_point(
    aes(body_mass_g, bill_length_mm),
    size = 2
  ) +
  theme_minimal() +
  labs(
    x = "Body Mass (g)",
    y = "Bill Length (mm)"
  ) +
  geom_line(
    aes(body_mass_g, estimates),
    color = "darkred",
    size = 1
  )

```

This is very similar to our abline graph above. Note, however, that the trend line or modeled relationship does not extend across the entire graph, suggesting that this linear model is not continuous from negative to positive infinity, which is misleading. There are ways to correct for this in `ggplot()`. Unfortunately, they are beyond the scope of this lab. 

In the meantime, note that you can use `predict()` to estimate the value of the response at specific values of the independent variable. To do that, you simply feed the `predict()` function, specifically its `newdata` parameter, a table with the values of the independent variable that interest you. For example, suppose you wanted to know what bill length this model would expect for a penguin having a body mass of, say, 4,500 grams. We can figure that out this way:

```{r}

new_data <- tibble(
  body_mass_g = 4500
)

predict(penguins_model, newdata = new_data)

```

If you like, you can also do that for multiple values like so:

```{r}

new_data <- tibble(
  body_mass_g = c(3000, 4500)
)

predict(penguins_model, newdata = new_data)

```

### Intervals

You can use `predict()` to calculate the prediction interval for these estimates by specifying `interval = "prediction"`. Note, too, that we ask it to provide that interval at `level = 0.95` to ensure the prediction interval is estimated at the 95% tolerance level.  

```{r}

predict(
  penguins_model, 
  newdata = new_data,
  interval = "prediction",
  level = 0.95
)

```

If we set `interval = "confidence"`, we can get the standard errors instead.

```{r}

predict(
  penguins_model, 
  newdata = new_data,
  interval = "confidence",
  level = 0.95
)

```

Here, the prediction interval refers to uncertainty around the expected value, and the confidence interval refers to uncertainty around the estimated coefficients.

We can actually add these to our model using the function `geom_ribbon()` like so.

```{r}

confidence <- predict(
  penguins_model, 
  interval = "confidence",
  level = 0.95
)

# coerce to a table
confidence <- as_tibble(confidence)
confidence$body_mass_g <- penguins$body_mass_g

ggplot() +
  geom_ribbon(
    data = confidence,
    aes(x = body_mass_g, ymin = lwr, ymax = upr),
    fill = "gray95"
  ) +
  geom_line(
    data = confidence, 
    aes(body_mass_g, fit),
    color = "darkred",
    size = 1
  ) +
  geom_point(
    data = penguins,
    aes(body_mass_g, bill_length_mm),
    size = 2
  ) +
  theme_minimal() +
  labs(
    x = "Body Mass (g)",
    y = "Bill Length (mm)"
  )

```

The ribbon geometry is, in effect, a polygon defined by an upper and lower line (`ymax` and `ymin`, respectively).



### Exercises

1. Build a model of penguin flipper length by body mass.
    - Make sure to visualize your data first! Make a scatter plot!
2. Now plot the modeled relationship between flipper length and body mass.
    - Use `coefficients()` and `geom_abline()`.
    - Use `predict()` and `geom_line()`.
    - Add the confidence interval to the second plot using `geom_ribbon()`.



## Visualize Assumptions {#Visualize_Assumptions}

Whenever you build a model, it is critically important to visualize the model and its assumptions as this will give you some indication about whether those assumptions have been met. Here, we'll visualize our penguins model, starting with the residuals.

### Residuals

One important assumption of OLS regression is that the errors are normally distributed. A simple histogram of the residuals will give us some indication of that. To get the residuals in our model, we can use the `residuals()` function.

```{r}

penguin_residuals <- residuals(penguins_model)

penguin_fit <- tibble(
  residuals = penguin_residuals
)

ggplot(penguin_fit, aes(residuals)) +
  geom_histogram() +
  labs(
    x = "Residual Bill Length (mm)",
    y = "Count"
  )

```

Do these look normally distributed to you? Do they have the approximate shape of a bell curve? If a visual check does not suffice, you can always try a Shapiro-Wilk test for normality. To do that in R, you can use the `shapiro.test()` function. Note that the null hypothesis for this test is that the variable is _not_ normally distributed.

```{r}

shapiro.test(penguin_residuals)

```


### Diagnostic Plots

Base R provides a really useful `plot()` method for linear models. You just feed this function your linear model and tell it `which` kind of plot you want to make. Here is a list of options, which you can supply to the `which` parameter:

1. Residuals vs Fitted plot
2. Normal Q-Q
3. Scale-Location
4. Cook's Distance
5. Residuals vs Leverage
6. Cook's Distance vs Leverage

Here is an example of the Q-Q Plot for our penguins model.

```{r}

plot(penguins_model, which = 2)

```

The `plot()` function in this case is extremely utilitarian. If you just want a quick visual diagnostic for your own edification, I recommend using this. However, if you want to present all these plots together in a clean way that easily communicates the assumptions being tested by each, I recommend using the `check_model()` function from the `performance` package.

```{r, fig.asp = 1.3}

check_model(penguins_model)

```

With this, we can hazard all the following conclusions:

1. The relationship is linear.
2. The model departs from homoscedasticity, but only slightly.
3. No observations have an undue influence on the model. 
4. The residuals are more or less normally distributed.

Note that none of the assumptions are met perfectly. This will never be the case, not with real world data. It should also clue you in to the fact that model evaluation is never certain. It always involves some risks that you might be wrong about your model.

### Exercises

1. Extract the residuals from your model of flipper length by body mass and visualize their distribution with a histogram.
    - Do the residuals look normally distributed?
    - Use the Shapiro Wilk test to verify this.
3. Explore the model with base `plot()`.
4. Now, use `check_model()`.
    - What do these plots tell you about the model?

    
## Homework {#Homework}

1. Load the following datasets from the `archdata` package using `data()`.
    - `DartPoints`
    - `OxfordPots`
2. Using the `DartPoints` dataset, build a linear model showing the relationship (if any) between the length and width of dart points. Be sure to do all of the following:
    - Visualize the data with a scatter plot!
    - Use `summary()` to report the model. 
    - Use `coefficients()` and `geom_abline()` to visualize the modeled relationship. Be sure to plot this over the data!
    - Use `check_model()` to visually inspect the model.
    - Does the model satisfy the assumptions of linear regression?
2. Using the `OxfordPots` dataset, build a linear model showing the relationship (if any) between the percentage of Oxford Pots found on an archaeological site and the distance of that site from the city of Oxford. Be sure to do all of the following:
    - Visualize the data with a scatter plot!
    - Use `summary()` to report the model. 
    - Use `predict()` and `geom_line()` to visualize the modeled relationship. Be sure to plot this over the data!
    - Add a confidence interval with `geom_ribbon()`.
    - Use `check_model()` to visually inspect the model.
    - Does the model satisfy the assumptions of linear regression?
    
