---
title: "Lab 11"
subtitle: "Generalized Linear Models"
output: 
  html_document:
    toc: true
    toc_depth: 2
    toc_float:
      smooth_scroll: false
    self_contained: false
    css: custom_style.css
    includes:
      in_header: toggle.html
    theme: 
      bootswatch: sandstone
---

```{r}
#| include = FALSE,
#| code = xfun::read_utf8(here::here("exercises", "before_chunk.R"))
```

## Outline {#Outline}

**TL;DR** Extensions to generalized linear models.

**Caution!** Please note that all labs assume that you are working in an RStudio Project directory!

### Objectives

This lab will guide you through the process of

1. Predicting with a GLM
2. Plotting a bivariate GLM with standard errors
2. Fitting a rate model with constant offset
2. Testing for dispersion
2. Fitting a quasi-Poisson GLM

### R Packages

We will be using the following packages:

- [AER](https://cran.r-project.org/web/packages/AER/AER.pdf)
- [archdata](https://cran.r-project.org/web/packages/archdata/index.html)
- [dplyr](https://dplyr.tidyverse.org/)
- [ggplot2](https://ggplot2.tidyverse.org/index.html)
- [here](https://here.r-lib.org/)

To install these packages, **run the following code in your console**:

```{r, eval = FALSE}

install.packages(
  c("AER", "archdata", "dplyr", "ggplot2", "here")
)

```

**Note:** You should not `install.packages()` in an Rmd document. Use that function in your R console instead. Then use `library()` as part of the preamble in your Rmd document to check packages out of the library and use them in that R session. This should always go at the start of your document!

```{r}

library(AER)
library(archdata)
library(dplyr)
library(ggplot2)
library(here)

```

### Data

- `DartPoints`
    - Includes measurements of 91 Archaic dart points recovered during surface surveys at Fort Hood, Texas.
    - package: `archdata`
    - reference: <https://cran.r-project.org/web/packages/archdata/archdata.pdf>
- `surveys`
    - A hypothetical dataset including counts of grave goods and measures of distance (in meters) from a great house in the American Southwest.
    - package: NA
    - reference: <https://github.com/kbvernon/qaad/tree/master/datasets>
- `Snodgrass`
    - Includes measurements of size, location, and contents of 91 pit houses at the Snodgrass site in Butler County, Missouri.
    - package: archdata
    - reference: <https://cran.r-project.org/web/packages/archdata/archdata.pdf>
- `surveys`
    - A hypothetical dataset including site counts per survey block along with measures of area (in km2) and fit of elevation (in meters) for each survey block.
    - package: NA
    - reference: <https://github.com/kbvernon/qaad/tree/master/datasets>

## Prediction {#Prediction}

In this section, we'll learn how to predict with a GLM and how to plot a trend line along with the standard errors. We'll be using the `Snodgrass` data to answer the following

**Question** Does the size of a house structure (measured in square feet) make it more or less likely that the structure is found inside the inner walls of the site?

We've already saved this data to disk, so we'll load it in again and prepare it for analysis using the same code as in the previous lab.

```{r}
#| echo = FALSE

data("Snodgrass")

snodgrass <- Snodgrass %>% 
  as_tibble() %>% 
  rename_with(tolower) %>% 
  select(inside, area) %>% 
  mutate(
    inside = ifelse(inside == "Inside", 1, 0)
  )

remove(Snodgrass)

```

```{r}
#| eval = FALSE

snodgrass <- here("darts", "snodgrass.csv") %>% 
  read.csv() %>% 
  as_tibble() %>% 
  rename_with(tolower) %>% 
  select(inside, area) %>% 
  mutate(
    inside = ifelse(inside == "Inside", 1, 0) # read as "if Inside, set to 1, else 0"
  )

```

As before, we'll plot these data using a scatterplot.

```{r}

ggplot(snodgrass, aes(area, inside)) + 
  geom_point(
    size = 3,
    alpha = 0.6
  ) +
  labs(
    x = "Area (sq ft)",
    y = "Inside inner wall"
  )

```

And, now we'll fit a GLM with a binomial error distribution and a logit link function:

```{r}

glm_snodgrass <- glm(
  inside ~ area,
  family = binomial(link = "logit"),
  data = snodgrass
)

```

Here's the summary again.

```{r}

summary(glm_snodgrass)

```

To predict with this, we use the `predict()` function, just as we did with linear models. 

```{r}

fit <- predict(glm_snodgrass)

# showing the first five fit
fit[1:5]

```

Note that these are on the logit scale, to get these back onto the scale of the response (i.e., probability), we need to apply the inverse link function to these data. To do that, we extract the function from the error distribution of the model with the `family()` function and the `$` operator.

```{r}

inverse_link <- family(glm_snodgrass)$linkinv

```

This is a function, so we can now apply it to our fit.

```{r}

inverse_link(fit[1:5])

```

We can use this now to plot the estimated response over our observations.

```{r}

snodgrass <- snodgrass %>% 
  mutate(
    logit = predict(glm_snodgrass),
    probability = inverse_link(logit)
  )

# plot
ggplot(snodgrass) + 
  geom_point(
    aes(area, inside),
    size = 3,
    alpha = 0.6
  ) +
  geom_line(
    aes(area, probability),
    size = 1,
    color = "#A20000"
  ) +
  labs(
    x = "Area (sq ft)",
    y = "Inside inner wall"
  )

```

To get the standard errors, we can run `predict()` with the argument `se.fit = TRUE`.

```{r}

estimates <- predict(glm_snodgrass, se.fit = TRUE)

str(estimates)

```

TWo things to note here. First, this is a list with three items: `fit`, `se.fit`, and `residual.scale`. The `fit` is just the estimated response, `se.fit` the standard error of predicted means, and `residual.scale` the residual standard deviations. The second thing to note is that, again, the fit and standard error are both on the logit scale, so we need to apply the inverse link function. Doing that, though, is a little trickier because we're working with a list. Here, just to make it as obvious as possible what we're doing, I'm going to give you one very crude, but also very simple method.

```{r}

snodgrass <- snodgrass %>% 
  mutate(
    logit = estimates$fit,
    se = estimates$se.fit,
    probability = inverse_link(logit),
    conf_hi = inverse_link(logit + 2*se),
    conf_lo = inverse_link(logit - 2*se)
  )

```

Here, we add the `logit` response and `se` or standard errors to the `snodgrass` table by extracting the `fit` and `se.fit` items from the `estimates` list. We then convert the `logit` response to a `probability` by applying the `inverse_link()` function. Next, we estimate the upper confidence line `conf_hi` by applying the `inverse_link()` to the sum of the `logit` response and 2 times the standard error (`se`). To get the lower confidence line `conf_lo`, we do the same, but taking the difference.  

Now we have everything we need to plot the confidence ribbon with `geom_ribbon()`. Notice that I add the ribbon to the plot before adding the observed points and the estimated trend line! This ensures that points and line are not obscured by the confidence ribbon.

```{r}

ggplot(snodgrass) +
  geom_ribbon(
    aes(area, ymin = conf_lo, ymax = conf_hi),
    alpha = 0.5,
    fill = "gray75"
  ) + 
  geom_point(
    aes(area, inside),
    size = 3,
    alpha = 0.6
  ) +
  geom_line(
    aes(area, probability),
    size = 1,
    color = "#A20000"
  ) +
  labs(
    x = "Area (sq ft)",
    y = "Inside inner wall"
  )

```


### Exercises

For these exercises, we'll use the modified version of the `DartPoints` dataset from the `archdata` package that we created in the previous lab. We are going to use length to see if we can discriminate Pedernales dart points from the other dart points.

1.  First, load the data in with `read.csv()` and `here()`. Be sure to call this table `darts`.
2. Make a scatter plot of the data.  
2.  Build a GLM of the Pedernales type as a function of dart length using a Binomial distribution and the logit link.
2.  Use `predict()` with `se.fit = TRUE` and assign this to an object called `estimates`. 
2. Extract the inverse link function from the model with `family()$linkinv` and give it the name `inverse_link`.
2. Now, add the logit, probability, standard errors, and inverse-transformed standard errors to the `darts` table, applying the inverse link function where necessary.
2. Plot the confidence ribbon. Make sure to add the ribbon to the plot before adding observations or the trend line!

## Rate Model {#Rate_Model}

In this section, we'll learn how to create a rate model using a Poisson GLM with a a log offset to account for differences in the size of the sampling interval. Here, we'll be using the `surveys` data to answer the following

**Question** Does elevation drive variation in the number of archaeological sites per survey block?

So, first, we'll load in the data. This time, we'll have to download the data, then load it into R. 

```{r}
#| echo = FALSE

surveys <- read_csv("https://raw.githubusercontent.com/kbvernon/qaad/master/datasets/surveys.csv")
```

```{r}
#| eval = FALSE

download.file(
  "https://raw.githubusercontent.com/kbvernon/qaad/master/datasets/surveys.csv",
  destfile = here("data", "surveys.csv")
)

surveys <- here("data", "surveys.csv") %>% 
  read.csv() %>% 
  as_tibble()

surveys

```

```{r}
#| echo = FALSE
surveys
```

As before, we'll plot these data using a scatterplot. 

```{r}

ggplot(surveys, aes(elevation, sites)) + 
  geom_point(
    size = 3,
    alpha = 0.6
  ) +
  labs(
    x = "Elevation (km)",
    y = "Site count"
  )

```

Now, let's look at differences in the area of each survey block.

```{r}

ggplot(surveys, aes(area)) + 
  geom_histogram(bins = 15) +
  labs(
    x = "Area (km2)",
    y = "Number of survey blocks"
  )

```

As you can see, the size of each survey block is not the same. This is not good! For the size biases the count: bigger areas should just by being bigger have more sites and smaller areas less sites _just as a matter of chance_. To account for this, we need to weight the response by the area.

```{r}

ggplot(surveys, aes(elevation, sites/area)) + 
  geom_point(
    size = 3,
    alpha = 0.6
  ) +
  labs(
    x = "Elevation (km)",
    y = "Site density (n/km2)"
  )

```



### Exercises

For these exercises, we'll use the `site_counts` dataset. We are going to use elevation to predict site counts per kilometer on an east-west transect through Utah.

1.  First, download the `site_counts` data with

```{r}
#| eval = FALSE
download.file(
  "https://raw.githubusercontent.com/kbvernon/qaad/master/datasets/site_counts.csv",
  destfile = here("data", "site_counts.csv")
)
```

2. Make a scatter plot of the data.  
2. Build a GLM of site counts per kilometer as a function of elevation using a Poisson distribution and a log link.
5. Build an intercept-only GLM of site counts using the same distribution and link.
6. Compare the AIC of these two models.
    - Is the AIC of the proposed model less than or greater than the AIC of the intercept-only model?
7.  Now compare these models using a Likelihood Ratio Test with `anova()` and `test = "LRT"`.
    - What is the result? Is there a significant improvement?

## Homework {#Homework}

No homework this week!
