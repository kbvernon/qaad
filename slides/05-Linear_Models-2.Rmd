---
title: "Lecture 05: Linear Models 2"  
date: 'Last updated: `r Sys.Date()`'
output:
  xaringan::moon_reader:
    lib_dir: libs
    css: [xaringan-themer.css, custom_style.css]
    seal: true
    nature:
      highlightStyle: magula
      highlightLines: true
      countIncrementalSlides: false
      ratio: '16:9'
---

```{r, include = FALSE, code=xfun::read_utf8(here::here("slides", "before_chunk.R"))}
```

```{r}

data(cars)

set.seed(12345)

cars <- cars %>% 
  as_tibble() %>%
  group_by(speed) %>% 
  slice(1) %>% 
  ungroup() %>% 
  # slice_sample(n = 10) %>%
  rename("distance" = dist) %>% 
  mutate(
    id = 1:n(),
    speed = speed * 0.44704,
    distance = distance * 0.3048,
    distance = round(distance, digits = 1)
  )

```

## &#x1F4CB; Lecture Outline

- &#x1F9EA; A simple experiment
- Competing Models
- A simple formula
- ANOVA for models
- R-Squared
- t-test for coefficients
- Diagnostic plots


---

## &#x1F9EA; A simple experiment

.pull-left[

```{r}

gg_cars <-
  ggplot(cars) +
  geom_hline(yintercept = 0, size = 1) +
  geom_vline(xintercept = 0, size = 1) +
  theme(
    panel.border = element_rect(color = "gray75", fill = "transparent")
  )

n <- nrow(cars)

gg_cars + 
  scale_x_continuous(breaks = seq(0, n+1, by = 5)) + 
  labs(
    x = "Car ID", 
    y = "Distance (m)", 
    title = "Distance to complete stop"
  ) +
  geom_segment(
    aes(x = id, xend = id, y = 0, yend = distance),
    linetype = "dashed",
    color = "gray35",
    size = 0.5
  ) +
  geom_point(
    aes(x = id, y = distance),
    size = 3.5
    ) +
  annotate(
    geom = "segment", 
    x = n + 1, y = 0,
    xend = n + 1, yend = 5,
    size = 0.5,
    linetype = "dashed",
    color = qaad_colors("rufous_red")
  ) +
  annotate(
    geom = "text", 
    x = n + 1, y = 6.5,
    label = "?",
    size = 10,
    color = qaad_colors("rufous_red")
  ) +
  coord_cartesian(
    ylim = c(-2.5, max(cars$distance) + 2),
    xlim = c(0, n+1)
  )

```

]

.pull-right[

<br>

We take _n_ cars, send each down a track, have them brake at the same point, and measure the distance it takes them to stop.  

__Question:__ how far do you think it will take the _next_ car to stop?

]


---

## Competing Models

.pull-left[

```{r}

barx <- mean(cars$distance)
mu_sigma <- sd(cars$distance)

gg_cars + 
  scale_x_continuous(breaks = seq(0, n+1, by = 5)) + 
  labs(
    x = "Car ID", 
    y = "Distance (m)", 
    title = "Simple Model"
  ) +
  geom_segment(
    aes(x = id, xend = id, y = distance, yend = barx),
    linetype = "dashed",
    color = "gray35",
    size = 0.5
  ) +
  geom_point(
    aes(x = id, y = distance),
    size = 3.5
    ) +
  geom_hline(
    aes(yintercept = barx),
    color = qaad_colors("maize_crayola"),
    size = 1
  ) +
  annotate(
    "segment",
    x = n + 1,
    y = 0,
    xend = n + 1,
    yend = barx - 0.5,
    color = qaad_colors("maize_crayola"),
    arrow = arrow(
      length = unit(0.18, "inches"), 
      type = "closed"
    )
  ) +
  coord_cartesian(
    ylim = c(-2.5, max(cars$distance) + 2),
    xlim = c(0, n+1)
  )

```

.right[Expectation: mean distance &nbsp;&nbsp;]

]


---

## Competing Models

.pull-left[

```{r}

gg_cars + 
  scale_x_continuous(breaks = seq(0, n+1, by = 5)) + 
  labs(
    x = "Car ID", 
    y = "Distance (m)", 
    title = "Simple Model"
  ) +
  geom_segment(
    aes(x = id, xend = id, y = distance, yend = barx),
    linetype = "dashed",
    color = "gray35",
    size = 0.5
  ) +
  geom_point(
    aes(x = id, y = distance),
    size = 3.5
    ) +
  geom_hline(
    aes(yintercept = barx),
    color = qaad_colors("maize_crayola"),
    size = 1
  ) +
  annotate(
    "segment",
    x = n + 1,
    y = 0,
    xend = n + 1,
    yend = barx - 0.5,
    color = qaad_colors("maize_crayola"),
    arrow = arrow(
      length = unit(0.18, "inches"), 
      type = "closed"
    )
  ) +
  coord_cartesian(
    ylim = c(-2.5, max(cars$distance) + 2),
    xlim = c(0, n+1)
  )

```

.right[Expectation: mean distance &nbsp;&nbsp;]

]

.pull-right[

```{r}

cars_lm <- lm(distance ~ speed, data = cars)

estimate <- predict(cars_lm, cars) %>% unname()

beta0 <- cars_lm$coefficients[[1]]
beta1 <- cars_lm$coefficients[["speed"]]

lm_sigma <- summary(cars_lm)$sigma

max_speed <- max(cars$speed)

lm_cars <- gg_cars + 
  scale_x_continuous(breaks = seq(0, 2 * round(max_speed/2), by = 2)) + 
  labs(
    x = "Speed (m/s)", 
    y = "Distance (m)"
  ) +
  geom_segment(
    aes(x = speed, xend = speed, y = distance, yend = estimate),
    linetype = "dashed",
    color = "gray35",
    size = 0.5
  ) +
  geom_point(
    aes(x = speed, y = distance),
    size = 3.5
  ) +
  geom_abline(
    slope = beta1,
    intercept = beta0,
    color = qaad_colors("celadon_blue"),
    size = 1
  ) +
  coord_cartesian(
    ylim = c(-2.5, max(cars$distance) + 2),
    xlim = c(0, max_speed + 1)
  )

lm_cars + ggtitle("Complex Model")

```

.right[Expectation: some function of speed &nbsp;&nbsp;]

]





---

## A simple formula

.pull-left[

.w-100.bg-near-white.ba.bw1.br2.shadow-4.ph3.mb3.h6rem[

$$y_i = E(Y) + \epsilon_i$$
$$\epsilon \sim N(0, \sigma)$$

]

.center[

.pull-left.bg-maize_crayola_o_50.ba.bw1.br2.shadow-4.pt4.pa3.h5[

.f4.fw6[Simple Model]

$$
\begin{align}
E(Y) &= \mu  \\\\
\bar{y} &=`r barx`  \\\\
\sigma &= `r mu_sigma` \\
\end{align}
$$   

]

.pull-right.bg-celadon_blue_o_70.ba.bw1.br2.shadow-4.pt4.pa3.h5[.white[

.f4.fw6[Complex Model]

$$
\begin{align}
E(Y) &= \beta X \\\\ 
\hat\beta &= `r beta1` \\\\
\sigma &= `r lm_sigma` \\
\end{align}
$$   

]]

]

]



---
count: false

## A simple formula

.pull-left[

.w-100.bg-near-white.ba.bw1.br2.shadow-4.ph3.mb3.h6rem[

$$y_i = E(Y) + \epsilon_i$$
$$\epsilon \sim N(0, \sigma)$$

]

.center[

.pull-left.bg-maize_crayola_o_50.ba.bw1.br2.shadow-4.pt4.pa3.h5[

.f4.fw6[Simple Model]

$$
\begin{align}
E(Y) &= \mu  \\\\
\bar{y} &=`r barx`  \\\\
\sigma &= `r mu_sigma` \\
\end{align}
$$    

]

.pull-right.bg-celadon_blue_o_70.ba.bw1.br2.shadow-4.pt4.pa3.h5[.white[

.f4.fw6[Complex Model]

$$
\begin{align}
E(Y) &= \beta X \\\\ 
\hat\beta &= `r beta1` \\\\
\sigma &= `r lm_sigma` \\
\end{align}
$$   

]]

]

]

.pull-right[

&#x2696;&#xFE0F; __The error is smaller__ for the more complex model. This is a good thing, __but what did it cost us__? Need to weigh this against the increased complexity!  

```{r, out.width = "100%", out.extra = "style='margin-top: 40px;'"}

dists <- tibble(
  x = seq(-20, 20, length = 500),
  Simple = dnorm(x, sd = mu_sigma),
  Complex = dnorm(x, sd = lm_sigma)
) %>% 
  pivot_longer(
    -x, 
    names_to = "models", 
    values_to = "y"
  ) %>% 
  arrange(x, models) %>% 
  mutate(
    models = as_factor(models),
    models = fct_relevel(models, "Simple", "Complex")
  )

ggplot(dists, aes(x, y, fill = models, color = models)) +
  geom_polygon(size = 1) +
  scale_color_manual(
    name = NULL,
    values = qaad_colors("maize_crayola", "celadon_blue"),
    guide = "none"
  ) +
  scale_fill_manual(
    name = NULL,
    values = alpha(qaad_colors("maize_crayola", "celadon_blue"), 0.5),
    guide = "none"
  ) +
  scale_y_continuous(
    breaks = c(0.05, 0.10)
  ) +
  labs(
    x = "Distance (m)",
    y = NULL,
    title = "Distribution of Errors"
  ) +
  theme(
    panel.grid.major.y = element_line(color = "gray92"),
    axis.text.y = element_blank(),
    axis.ticks.y = element_blank(),
    axis.title.y = element_blank(),
    plot.margin = margin()
  )

```

]




---

## ANOVA for model

.panelset[

.panel[.panel-name[Problem]

.pull-left[

```{r}

tmp_x <- 9
tmp_y <- predict(cars_lm, newdata = tibble(speed = tmp_x))

gg_cars + 
  scale_x_continuous(breaks = seq(0, 2 * round(max_speed/2), by = 2)) + 
  labs(
    x = "Speed (m/s)", 
    y = "Distance (m)"
  ) +
  geom_abline(
    slope = beta1,
    intercept = beta0,
    color = qaad_colors("celadon_blue"),
    size = 1
  ) +
  geom_hline(
    yintercept = barx,
    color = qaad_colors("maize_crayola"),
    size = 1
  ) +
  geom_point(
    aes(x = speed, y = distance),
    size = 3.5
  ) +
  annotate(
    "point",
    x = tmp_x, 
    y = tmp_y,
    shape = 21,
    fill = "transparent",
    color = "gray35",
    size = 3,
    stroke = 1
  ) +
  annotate(
    "segment",
    x = tmp_x - 0.1,
    y = tmp_y,
    xend = tmp_x - 2,
    yend = tmp_y,
    linetype = "dashed",
    color = "gray35"
  ) +
  annotate(
    "text",
    x = tmp_x - 2.1,
    y = tmp_y,
    label = "Complex",
    color = qaad_colors("celadon_blue"),
    hjust = 1.0,
    vjust = 0.4,
    size = 7
  ) +
  annotate(
    "point",
    x = tmp_x + 1, 
    y = barx,
    shape = 21,
    fill = "transparent",
    color = "gray35",
    size = 3,
    stroke = 1
  ) +
  annotate(
    "segment",
    x = tmp_x + 1,
    y = barx-0.2,
    xend = tmp_x + 1,
    yend = 5,
    linetype = "dashed",
    color = "gray35"
  ) +
  annotate(
    "text",
    x = tmp_x + 1,
    y = 4.9,
    label = "Simple",
    color = qaad_colors("maize_crayola"),
    hjust = 0.5,
    vjust = 1.0,
    size = 7
  ) +
  coord_cartesian(
    ylim = c(-2.5, max(cars$distance) + 2),
    xlim = c(0, max_speed + 1)
  )

```

]

.pull-right[

We have two models of our data, one simple, the other complex.  

__Question:__ Does the complex (bivariate) model explain more variance than the simple (intercept) model? Does the difference arise by chance?

]

]

.panel[.panel-name[Hypotheses]

The __null__ hypothesis:
- $H_0:$ no difference in variance explained.

The __alternate__ hypothesis:
- $H_1:$ difference in variance explained.

]

.panel[.panel-name[Strategy]

__Variance Decomposition__. Total variance in the dependent variable can be decomposed into the variance captured by the more complex model and the remaining (or residual) variance:  

<br>

.pull-left[

Decompose the differences:

$$(y_{i} - \bar{y}) = (\hat{y}_{i} - \bar{y}) + (y_{i} - \hat{y}_{i})$$

where

- $(y_{i} - \bar{y})$ is the total error,  
- $(\hat{y}_{i} - \bar{y})$ is the model error, and
- $(y_{i} - \hat{y}_{i})$ is the residual error.  

]

.pull-right[

Sum and square the differences:

$$SS_{T} = SS_{M} + SS_{R}$$

where 

- $SS_{T}$: Total Sum of Squares
- $SS_{M}$: Model Sum of Squares
- $SS_{R}$: Residual Sum of Squares 

]

]

.panel[.panel-name[Sum of Squares]

```{r, fig.asp = 0.3, fig.width = 1000/72}

ablines <- tibble(
  beta0 = c(barx, barx, beta0, beta0),
  beta1 = c(0, 0, beta1, beta1),
  color = qaad_colors("maize_crayola", "maize_crayola", "celadon_blue", "celadon_blue"),
  grp = c("Total Sum of Squares", rep("Model Sum of Squares", 2), "Residual Sum of Squares")
) %>% 
  mutate(
    grp = factor(grp),
    grp = fct_relevel(grp, c("Total Sum of Squares", "Model Sum of Squares"))
  )

segments <- bind_rows(
  tibble(x = cars$speed, y = barx, yend = cars$distance, grp = "Total Sum of Squares"),
  tibble(x = cars$speed, y = barx, yend = estimate, grp = "Model Sum of Squares"),
  tibble(x = cars$speed, y = estimate, yend = cars$distance, grp = "Residual Sum of Squares")
) %>% 
  mutate(
    grp = factor(grp),
    grp = fct_relevel(grp, c("Total Sum of Squares", "Model Sum of Squares"))
  )

pnts <- bind_rows(
  cars %>% mutate(grp = "Total Sum of Squares"),
  cars %>% mutate(grp = "Residual Sum of Squares")
) %>% 
  mutate(
    grp = factor(grp),
    grp = fct_relevel(grp, c("Total Sum of Squares", "Model Sum of Squares"))
  )

gg_cars + 
  scale_x_continuous(breaks = seq(0, 2 * round(max_speed/2), by = 2)) + 
  labs(
    x = "Speed (m/s)", 
    y = "Distance (m)"
  ) +
  geom_segment(
    data = segments,
    aes(x = x, xend = x, y = y, yend = yend),
    linetype = "dashed",
    color = "gray35",
    size = 0.5
  ) +
  geom_abline(
    data = ablines,
    aes(intercept = beta0, slope = beta1, color = color),
    size = 1
  ) +
  scale_color_manual(
    name = NULL,
    guide = "none",
    values = c(qaad_colors("celadon_blue", "maize_crayola"), "transparent")
  ) +
  geom_point(
    data = pnts,
    aes(x = speed, y = distance),
    size = 3.5
  ) +
  facet_wrap(~grp, ncol = 3) +
  theme(
    strip.background = element_blank(),
    strip.text = element_text(hjust = 0, size = 18)
  ) +
  coord_cartesian(
    ylim = c(-2.5, max(cars$distance) + 2),
    xlim = c(0, max_speed + 1)
  )

```

]

.panel[.panel-name[F-statistic]

```{r}

cars_anova <- aov(cars_lm)

f.value <- summary(cars_anova)[[1]][1,4]
f.p <- summary(cars_anova)[[1]][1,5]

model_df <- cars_anova$assign %>% sum()
residual_df <- cars_anova$df.residual %>% unname()

```

.pull-left[

Ratio of variances:

$$F = \frac{\text{between-group variance}}{\text{within-group variance}}$$
where

- Model variance = $\frac{SS_{M}}{k}$

- Residual variance = $\frac{SS_{R}}{n-k-1}$

Here, the denominators are the degrees of freedom, with $n$ observations and $k$ model parameters.

]

.pull-right[

For this test, $F=$ `r f.value`.

__Question:__ How probable is this estimate?  

We can answer this question by comparing the F-statistic to the F-distribution. 

]

]

.panel[.panel-name[F-distribution]

.pull-left-38[

<br>

Summary:
- $\alpha = 0.05$
- $H_0:$ no difference between groups.
- $p=$ `r f.p`

__Translation:__ the null hypothesis is really, really unlikely. So, there must be some difference between groups!

]

.pull-right-60[

```{r}

r <- 110

distribution <- tibble(
  x = c(0, seq(0, r, length = 300)),
  y = c(0, df(x[-1], df1 = model_df, df2 = residual_df))
)

a <- qf(
  0.05,
  df1 = model_df,
  df2 = residual_df,
  lower.tail = FALSE
)

n <- 100
x <- seq(a, r, length = n)
y <- df(x, df1 = model_df, df2 = residual_df)

tails <- tibble(
  x = c(a, x),
  y = c(0, y)
)

ggplot() +
  geom_polygon(
    data = distribution,
    aes(x, y),
    fill = qaad_colors("maize_crayola"),
    color = "#F15A29",
    size = 1
  ) +
  geom_polygon(
    data = tails,
    aes(x,y),
    fill = alpha(qaad_colors("rufous_red"), 0.65),
    color = qaad_colors("rufous_red"),
    size = 1
  ) +
  annotate(
    "segment",
    x = f.value,
    y = 0, 
    xend = f.value,
    yend = 0.4,
    linetype = "dashed",
    size = 1
  ) +
  annotate(
    "text",
    x = f.value,
    y = 0.4 + 0.01,
    label = paste0("F = ", round(f.value, 2)),
    size = 8,
    hjust= 1, 
    vjust = 0
  ) +
  labs(
    x = "F-statistic",
    y = "Density",
    title = paste0("F-distribution (df = ", model_df, ", ", residual_df, ")")
  )

```

]

]

]




---

## R-Squared

.pull-left[

Coefficient of Determination

$$R^2 = \frac{SS_{M}}{SS_{T}}$$  

- Proportion of variance in $y$ that can be explained by the model, $M$.
- Scale is 0 to 1. Closer to 1 means more variance explained.
- Model evaluation relative to a simple, intercept-only (or mean-only) model, $SS_T$.

]

.pull-right[

```{r, fig.align = "left", fig.width = 430/72, fig.asp = 1.15}

squares <- tibble(
  x = 0.5,
  y = max(cars$distance),
  label = c("SST", "SSM", "SSR"),
  grp = c("Total Sum of Squares", "Model Sum of Squares", "Residual Sum of Squares")
) %>% 
  mutate(
    grp = factor(grp),
    grp = fct_relevel(grp, c("Total Sum of Squares", "Model Sum of Squares"))
  )

gg_cars + 
  scale_x_continuous(breaks = seq(0, 2 * round(max_speed/2), by = 2)) + 
  labs(
    x = "Speed (m/s)", 
    y = "Distance (m)"
  ) +
  geom_segment(
    data = segments %>% filter(grp != "Residual Sum of Squares"),
    aes(x = x, xend = x, y = y, yend = yend),
    linetype = "dashed",
    color = "gray35",
    size = 0.5
  ) +
  geom_abline(
    data = ablines %>% filter(grp != "Residual Sum of Squares"),
    aes(intercept = beta0, slope = beta1, color = color),
    size = 1
  ) +
  geom_point(
    data = pnts %>% filter(grp != "Residual Sum of Squares"),
    aes(x = speed, y = distance),
    size = 3.5
  ) +
  scale_color_manual(
    name = NULL,
    guide = "none",
    values = qaad_colors("celadon_blue", "maize_crayola")
  ) +
  geom_text(
    data = squares %>% filter(grp != "Residual Sum of Squares"),
    aes(x, y, label = label),
    hjust = 0,
    vjust = 1,
    size = 8
  ) +
  facet_wrap(~grp, ncol = 1) +
  theme(
    strip.background = element_blank(),
    strip.text = element_blank()
  ) +
  coord_cartesian(
    ylim = c(-2.5, max(cars$distance) + 2),
    xlim = c(0, max_speed + 1)
  )

```

]



---

## t-test for coefficients

.panelset[

.panel[.panel-name[Problem]

.pull-left[

```{r}

betas <- summary(cars_lm)$coefficients

se <- betas[, 2, drop = TRUE]

xmin <- round(pmin(beta0 - se[[1]], beta1 - se[[2]]))
xmax <- round(pmax(beta0 + se[[1]], beta1 + se[[2]]))

cars_coefficients <- tibble(
  x = c(beta0, beta1),
  y = c("Intercept", "Slope"),
  se = se
)

gg_coefficients <- ggplot(cars_coefficients, aes(x, y)) +
  scale_x_continuous(
    breaks = c(xmin, 0, xmax),
    limits = c(xmin, xmax)
  ) +
  geom_vline(xintercept = 0, size = 1) +
  labs(
    x = "Estimate",
    y = NULL,
    title = "Model Coefficients"
  ) +
  theme(
    panel.border = element_rect(color = "gray75", fill = "transparent")
  )

gg_coefficients +
  geom_segment(
    aes(x = 0, y = y, xend = x, yend = y),
    color = "gray35",
    linetype = "dashed"
  ) +
  geom_point(size = 3)

```

]

.pull-right[

<br>

__Question:__ Are the coefficient estimates significantly different than zero?

To answer this question, we need some measure of uncertainty for these estimates.

]


]

.panel[.panel-name[Hypotheses]

The __null__ hypothesis:
- $H_0:$ coefficient estimate is _not_ different than zero.

The __alternate__ hypothesis:
- $H_1:$ coefficient estimate is different than zero.

]

.panel[.panel-name[Standard Errors]

.pull-left[

```{r}

gg_coefficients +
  geom_errorbarh(
    aes(xmin = x - se, xmax = x + se),
    height = 0.33
  ) +
  geom_point(size = 3)

```

]

.pull-right[

<br>

For simple linear regression,

- the __standard error of the slope__, $se(\hat\beta_1)$, is the ratio of the average squared error of the model to the total squared error of the predictor.

- the __standard error of the intercept__, $se(\hat\beta_0)$, is $se(\hat\beta_1)$ weighted by the average squared values of the predictor.

]

]

.panel[.panel-name[t-statistic]

```{r}

t_int <- betas[1, 3, drop = TRUE]
t_slp <- betas[2, 3, drop = TRUE]

```

.pull-left[

The t-statistic is the coefficient estimate divided by its standard error

$$t = \frac{\hat\beta}{se(\hat\beta)}$$ 

<br>

This can be compared to a t-distribution with $n-k-1$ degrees of freedom (\\(n\\) observations and \\(k\\) independent predictors). 

]

]


.panel[.panel-name[t-test]

```{r}

p_int <- pmax(0.001, betas[1, 4, drop = TRUE])
p_slp <- pmax(0.001, betas[2, 4, drop = TRUE])

```

.pull-left[

<br>

- $\alpha = 0.05$
- $H_0$: coefficient is not different than zero
- p-values:
    - intercept = `r p_int`
    - slope < `r p_slp`

]

.pull-right[

```{r}

r <- c(
  floor(pmin(t_int, t_slp)) - 1,
  ceiling(pmax(t_int, t_slp)) + 1
)

t.df <- cars_lm$df.residual

a <- qt(0.025, df = t.df, lower.tail = FALSE)

distribution <- tibble(
  x = seq(-max(r), max(r), length = 300),
  y = dt(x, df = t.df)
)

n <- 100
x <- seq(a, max(r), length = n)
y <- dt(x, df = t.df)

tails <- tibble(
  x = c(rev(-x), -a, a, x),
  y = c(rev(y), 0, 0, y),
  z = rep(c("lower", "upper"), each = n+1)
)

ggplot() +
  geom_polygon(
    data = distribution,
    aes(x, y),
    fill = qaad_colors("maize_crayola"),
    color = "#F15A29",
    size = 1
  ) +
  geom_polygon(
    data = tails,
    aes(x,y, group = z),
    fill = alpha(qaad_colors("rufous_red"), 0.65),
    color = qaad_colors("rufous_red"),
    size = 1
  ) +
  scale_x_continuous(limits = c(min(r), max(r))) +
  annotate(
    "segment",
    x = t_int,
    y = 0,
    xend = t_int,
    yend = 0.2,
    linetype = "dashed",
    size = 1
  ) +
  annotate(
    "text",
    x = t_int,
    y = 0.2 + 0.02,
    label = paste0("intercept\n", "t = ", round(t_int, 2)),
    hjust = 0.4,
    vjust = 0,
    size = 8
  ) +
  annotate(
    "segment",
    x = t_slp,
    y = 0,
    xend = t_slp,
    yend = 0.2,
    linetype = "dashed",
    size = 1
  ) +
  annotate(
    "text",
    x = t_slp,
    y = 0.2 + 0.02,
    label = paste0("slope\n", "t = ", round(t_slp, 2)),
    hjust = 0.9,
    vjust = 0,
    size = 8
  ) +
  labs(
    x = "t-statistic",
    y = "Density",
    title = paste0("t-distribution (df = ", round(t.df, 0), ")")
  )

```

]

]

.panel[.panel-name[Confidence ribbon]

.pull-left[

<br>

The standard errors can be visualized in a graph with the regression line.

- Common to show $2 \cdot se(\hat\beta)$, as this encompasses roughly 95% of the observations.
- Notice that the errors expand as they move away from the center of the data. This reflects an increase in model uncertainty.

]

.pull-right[

```{r}

ribbons <- tibble(speed = seq(-5, max(cars$speed) +5, length = 300))

se_fit <- predict(
  cars_lm, 
  newdata = ribbons,
  se.fit = TRUE
)

ribbons <- ribbons %>% 
  mutate(
    distance = se_fit$fit,
    se = 2 * se_fit$se.fit
  )

gg_cars +
  scale_x_continuous(breaks = seq(0, 2 * round(max_speed/2), by = 2)) + 
  labs(
    x = "Speed (m/s)", 
    y = "Distance (m)",
    title = "Cars Model"
  ) +
  # geom_smooth(
  #   aes(x = speed, y = distance),
  #   method = "lm",
  #   color = qaad_colors("celadon_blue"),
  #   fill = alpha(qaad_colors("celadon_blue"), 0.5),
  #   fullrange = TRUE
  # ) +
  geom_ribbon(
    data = ribbons,
    aes(x = speed, ymin = distance - se, ymax = distance + se),
    color = "transparent",
    fill = alpha(qaad_colors("celadon_blue"), 0.5),
    size = 0.6
  ) +
  geom_abline(
    slope = beta1,
    intercept = beta0,
    color = qaad_colors("celadon_blue"),
    size = 1
  ) +
  geom_point(
    aes(x = speed, y = distance),
    size = 2,
    color = "gray35"
  ) +
  coord_cartesian(
    ylim = c(-2.5, max(cars$distance) + 2),
    xlim = c(0, max_speed + 1)
  )
  

```

]

]



]


---

## Diagnostic Plots

<!--- 
https://stats.stackexchange.com/questions/58141/interpreting-plot-lm
--->

.panelset.sideways[

.panel[.panel-name[Regression assumptions]

1. __Weak Exogeneity__: the predictor variables have fixed values and are known. 

2. __Linearity__: the relationship between the predictor variables and the response variable is linear.  

2. __Constant Variance__: the variance of the errors does not depend on the values of the predictor variables. Also known as _homoscedasticity_.  

2. __Independence of errors__: the errors are uncorrelated with each other.  

2. __No perfect collinearity__: the predictors are not linearly correlated with each other.  

]

.panel[.panel-name[Residual v Fitted]

```{r, fig.width = 750/72, fig.asp = 0.35}

set.seed(12345)

n <- 50

x <- seq(0, 1, length = n)

models <- tibble(
  x = x,
  Good = 0,
  Heteroschedastic = 0,
  Nonlinear = 0.5 - 5*(x - 0.5)^2 
) %>% 
  pivot_longer(-x, names_to = "model", values_to = "y")

outcomes <- tibble(
    x = x,
    Good = rnorm(n),
    Heteroschedastic = rnorm(n, sd = x),
    Nonlinear = with(models %>% filter(model == "Nonlinear"), y + rnorm(n, sd = 0.4))
) %>% 
  pivot_longer(-x, names_to = "model", values_to = "y") %>% 
  mutate(
    y = ifelse(y > 1.5, 1, y),
    y = ifelse(y < -1.5, -1, y)
  )

errors <- models %>% 
  mutate(
    ymax = case_when(
      model == "Good" ~ y + 1,
      model == "Heteroschedastic" ~ y + x,
      TRUE ~ y + 0.4
    ),
    ymin = case_when(
      model == "Good" ~ y - 1,
      model == "Heteroschedastic" ~ y - x,
      TRUE ~ y - 0.4
    )
  )

ggplot() +
  geom_hline(yintercept = 0, color = "gray45", linetype = "dashed") +
  geom_ribbon(
    data = errors,
    aes(x, ymin = ymin, ymax = ymax),
    color = "transparent",
    fill = alpha(qaad_colors("celadon_blue"), 0.2)
  ) +
  geom_line(
    data = models,
    aes(x, y),
    color = qaad_colors("flame_orange"),
    size = 1
  ) +
  geom_point(
    data = outcomes,
    aes(x, y),
    size = 2
  ) +
  facet_wrap(~model, ncol = 3) +
  labs(
    x = "Fitted",
    y = "Residual"
  ) +
  theme(
    axis.text = element_blank(),
    axis.ticks = element_blank(),
    strip.background = element_blank(),
    strip.text = element_text(size = 18, hjust = 0)
  )

```

]

.panel[.panel-name[Normal Q-Q]

```{r, fig.width = 470/72, fig.asp = 1}

set.seed(42)

n <- 50
x <- seq(-3, 3, length = n)
g <- rnorm(n)
ht <- local({
  
  z <- rcauchy(n)
  
  for (i in seq_along(z)) {
    
    q <- z[[i]]
    
    if (q > 3) z[[i]] <- runif(1, 3.5, 5)
    
    if (q < -3) z[[i]] <- runif(1, -5, -3.5)
    
  }

  z
  
})

lt <- local({
  
  z <- c(runif(n, -5, 5), runif(n/2, -3, 3))
  
  sample(z, n)
  
})



dists <- list(
  "Right Skew"  = sample(c((g[g > -0.3] + 0.3) * 3, g), size = n),
  "Left Skew"   = sample(c(g[g < 0] * 3, g), size = n),
  "Heavy Tails" = ht,
  "Light Tails" = lt
)

max_length <- lapply(dists, length) %>% unlist() %>% max()

dists <- lapply(dists, function(x){ length(x) <- max_length; x })

dists <- dists %>% 
  as_tibble() %>% 
  pivot_longer(
    everything(),
    names_to = "distribution",
    values_to = "x"
  ) %>% 
  filter(!is.na(x)) %>% 
  arrange(distribution) %>% 
  mutate(
    distribution = factor(distribution),
    distribution = fct_relevel(distribution, "Light Tails", "Heavy Tails", "Left Skew")
  )

ggplot(dists, aes(sample = x)) +
  geom_qq(
    size = 3,
    shape = 21,
    fill = alpha("gray50", 0.3)
  ) +
  geom_qq_line(
    color = qaad_colors("rufous_red"),
    size = 1
  ) +
  facet_wrap(~distribution, ncol = 2) +
  labs(
    x = "Expected Distribution",
    y = "Empirical Distribution"
  ) + 
  theme(
    axis.text = element_blank(),
    axis.ticks = element_blank(),
    strip.background = element_blank(),
    strip.text = element_text(size = 18, hjust = 0)
  )

```

]

.panel[.panel-name[Cook's Distance]

.pull-left[

```{r}

set.seed(20)

x <- rnorm(20, mean = 20, sd = 1)
x <- c(x[1:5], -1, x[6:20], 35)
y <- 5 + .5*x + rnorm(length(x))

fit <- lm(y ~ x)

cook <- tibble(
  x = 1:length(x),
  cook = cooks.distance(fit)
)

ggplot(cook, aes(x, y = cook)) + 
  geom_segment(aes(xend = x, yend = 0)) +
  geom_point(
    size = 3
  ) +
  labs(
    x = "Observation", 
    y = expression(paste(italic("What-If?"), " Distance"))
  ) + 
  theme(
    axis.text = element_blank(),
    axis.ticks = element_blank()
  )

```

]

.pull-right[

```{r, fig.align = "left", out.width = "100%"}

figure("fulcrum.png")

```

]

]





]








---

## &#x1F52D; Looking Ahead

```{r}

schedule <- here("_misc", "course_outline.xlsx") %>% 
  readxl::read_excel(1) %>% 
  mutate(
    date = lubridate::ymd(date),
    description = glue::glue(
    "<b>{topic}</b> <span style = 'font-size: 80%; color: #9C9C9C;'>({date})</span><br>
    <div style = 'font-size: 80%; font-style: italic; color: #9C9C9C;'>{details}</div>"
  )) %>% 
  select(description)

```

.pull-left[

```{r}

schedule %>% 
  slice(5:8) %>% 
  kbl(escape = FALSE, col.names = NULL) %>% 
  kable_paper(c("striped", "hover"))

```

]