---
title: "Lecture 07: Evaluating Linear Models"  
date: 'Last updated: `r Sys.Date()`'
---

```{r setup}
#| include: false

here::here("slides", "_setup.R") |> source()

library(archdata)

data(PitHouses)

pithouses <- PitHouses |> 
  as_tibble() |> 
  rename_with(tolower) |> 
  mutate(id = 1:n()) |> 
  select(id, size, depth)

remove(PitHouses)

data(cars)

set.seed(12345)

cars <- cars |> 
  as_tibble() |> 
  slice_sample(n = 10) |> 
  rename("distance" = dist) |> 
  mutate(
    id = 1:n(),
    speed = speed * 0.44704,
    distance = distance * 0.3048,
    distance = round(distance, digits = 1)
  )

```

## üìã Lecture Outline

- üß™ A simple experiment
- Competing models
- Model interpretation
    - Formula
    - Prediction
    - Standard error vs prediction interval
- Model evaluation
    - Model complexity
    - ANOVA for models
    - R-Squared
    - t-test for coefficients
    - Diagnostic plots

## üß™ A simple experiment

:::::: {.columns}
::: {.column}

```{r}

# default settings for cars plots
defaults <- list(
  ylim(-2, max(cars$distance) + 2),
  theme(panel.border = element_rect(color = "gray65", fill = "transparent")),
  geom_hline(yintercept = 0),
  geom_vline(xintercept = 0)
)

ggplot(cars) +
  geom_segment(
    aes(x = id, xend = id, y = 0, yend = distance),
    linetype = "dashed",
    color = "gray60",
    linewidth = 1
  ) +
  geom_point(
    aes(x = id, y = distance),
    size = 4
  ) +
  geom_text(
    aes(x = id, y = distance, label = distance),
    vjust = 0,
    nudge_y = 1,
    color = "gray50"
  ) +
  annotate(
    geom = "segment", 
    x = 11, y = 0,
    xend = 11, yend = 5,
    linewidth = 0.5,
    linetype = "dashed",
    color = qcolors("kobe")
  ) +
  annotate(
    geom = "text", 
    x = 11, y = 6.5,
    label = "?",
    size = 10,
    color = qcolors("kobe")
  ) +
  labs(
    x = "Car ID", 
    y = "Distance (m)", 
    title = "Distance to complete stop"
  ) +
  scale_x_continuous(
    breaks = 0:11, 
    limits = c(0, 11.2)
  ) +
  defaults

```

:::
::: {.column}

<br>
We take ten cars, send each down a track, have them brake at the same point, and measure the distance it takes them to stop.  

__Question:__ how far do you think it will take the next car to stop?  
:::
:::::: 

## Competing Models

:::::: {.columns}
::: {.column}

```{r}

ggplot(cars) +
  geom_segment(
    aes(x = id, xend = id, y = mean(distance), yend = distance),
    linetype = "dashed",
    color = "gray60",
    linewidth = 1
  ) +
  geom_point(
    aes(x = id, y = distance),
    size = 4
  ) +
  geom_hline(
    aes(yintercept = mean(distance)),
    color = qcolors("gold"),
    linewidth = 1
  ) +
  labs(
    x = "Car ID", 
    y = "Distance (m)", 
    title = "Simple Model"
  ) +
  scale_x_continuous(
    breaks = 0:11, 
    limits = c(0, 11.2)
  ) +
  defaults

```

:::{style="text-align:center;"}
E[Y]: mean distance
:::

:::
::: {.column}

```{r}

cars_lm <- lm(distance ~ speed, data = cars)

estimate <- predict(cars_lm, cars) |> unname()

beta0 <- cars_lm$coefficients[[1]]
beta1 <- cars_lm$coefficients[["speed"]]

sd_lm <- summary(cars_lm)$sigma

sd_y <- sd(cars$distance)

bar_y <- mean(cars$distance)

ggplot(cars) +
  geom_segment(
    aes(x = speed, xend = speed, y = distance, yend = estimate),
    linetype = "dashed",
    color = "gray60",
    linewidth = 1
  ) +
  geom_point(
    aes(x = speed, y = distance),
    size = 4
    ) +
  geom_abline(
    slope = beta1,
    intercept = beta0,
    color = qcolors("sapphire"),
    linewidth = 1
  ) +
  labs(
    x = "Speed (m/s)", 
    y = "Distance (m)", 
    title = "Complex Model"
  ) +
  scale_x_continuous(
    breaks = seq(0, 10, by = 2), 
    limits = c(0, 11.2)
  ) +
  defaults

```

:::{style="text-align:center;"}
E[Y]: some function of speed]
:::

:::
::::::

# Model Interpretation

## Model Formula

:::::: {.columns}
::::: {.column}

```{r}

max_speed <- max(cars$speed)

x <- 8
ey <- beta0 + beta1 * x

deltas <- tibble(
  x = c(5, 5, 8),
  y = predict(cars_lm, newdata = tibble(speed = c(5, 8, 8))),
)

ggplot(cars) +
  scale_x_continuous(breaks = seq(0, 2 * ceiling(max_speed/2), by = 2)) + 
  labs(
    x = "Speed (m/s)", 
    y = "Distance (m)",
    title = "Complex Model"
  ) +
  coord_cartesian(
    ylim = c(floor(beta0), max(cars$distance) + 2),
    xlim = c(0, max_speed + 1)
  ) + 
  geom_abline(
    slope = beta1,
    intercept = beta0,
    color = qcolors("sapphire"),
    size = 1
  ) + 
  geom_line(
    data = deltas,
    aes(x, y),
    color = "gray30",
    linetype = "dashed"
  ) +
  annotate(
    "segment",
    x = 0, xend = 0,
    y = beta0 - 5, yend = beta0,
    color = "gray30",
    linetype = "dashed"
  ) +
  annotate(
    "text",
    x = 4.8,
    y = beta0 + beta1 * 6.5,
    label = "Delta*Y",
    parse = TRUE,
    hjust = 1,
    size = 13/.pt
  ) +
  annotate(
    "text",
    x = 6.5,
    y = ey + 1,
    label = "Delta*X",
    parse = TRUE,
    vjust = 0,
    size = 13/.pt
  ) +
  annotate(
    "segment",
    x = 8.5,
    y = beta0 + beta1 * 6.5,
    xend = 6.7,
    yend = beta0 + beta1 * 6.5,
    arrow = arrow(length = unit(0.15, "inches"), type = "closed")
  ) +
  annotate(
    "text",
    x = 8.7,
    y = beta0 + beta1 * 6.5,
    label = "hat(beta)[1]",
    parse = TRUE,
    hjust = 0,
    size = 13/.pt
  ) +
  annotate(
    "segment",
    x = 2,
    y = beta0,
    xend = 0.2,
    yend = beta0,
    arrow = arrow(length = unit(0.15, "inches"), type = "closed")
  ) +
  annotate(
    "text",
    x = 2.2,
    y = beta0,
    label = "hat(beta)[0]",
    parse = TRUE,
    hjust = 0,
    size = 13/.pt
  )

```

:::::
::::: {.column}

```{css}

.blue-panel {
  background-image: linear-gradient(to bottom right, #42758D, #0C556D);
  color: white;
}

.panel {
  border: 1px solid black;
  border-radius: 5px;
  padding: 0 0.1em;
  height: 175px;
  width: 45%;
}

.panel p {
  margin: 0 auto;
}

.flex {
  display: flex;
  gap: 0.25em;
}

.align-center {
  align-items: center;
}

.align-with-plot {
  margin-top: 1.9em;
}

```

:::: {.flex .align-center .align-with-plot}
::: {.blue-panel .panel .flex .align-center}

```{r}

sigma <- sd(residuals(cars_lm))

```

$$
\begin{align}
y_i &= E[Y] + \epsilon_i \\
\epsilon &\sim N(0,\sigma) \\
\sigma &= `r round(sigma, 2)` 
\end{align}
$$   

:::
::: {.blue-panel .panel .flex .align-center }

$$
\begin{align}
E[Y] &\approx \hat\beta X \\
\hat\beta_0 &= `r round(beta0, 2)` \\
\hat\beta_1 &= `r round(beta1, 2)`
\end{align}
$$ 

:::
::::

$\hat\beta_{0}$ is the value of $Y$ where $X=0$, here the total stopping distance when the car isn't moving, which should be zero!!!  

$\hat\beta_{1}$ is change in $Y$ relative to change in $X$, here the additional distance the car will travel for each unit increase in speed. 

:::::
::::::

## Prediction

```{css}

.relative {
  position: relative
}

.z0 {
  z-index: 0;
}

.z1 {
  z-index: 1;
}

.abs-left {
  position: absolute;
}

.w100 {
  width: 100%;
}

```

:::::: {.columns}
::::: {.column}
:::: {.relative}
::: {.fragment .fade-out fragment-index=0 .abs-left .z0 .w100}

```{r}

predictions <- tibble(
  x = c(-2, rep(x, 2)),
  y = c(rep(ey, 2), beta0-5)
)

ggplot(cars) +
  scale_x_continuous(breaks = seq(0, 2 * ceiling(max_speed/2), by = 2)) + 
  labs(
    x = "Speed (m/s)", 
    y = "Distance (m)",
    title = "Complex Model"
  ) +
  coord_cartesian(
    ylim = c(floor(beta0), max(cars$distance) + 2),
    xlim = c(0, max_speed + 1)
  ) + 
  geom_abline(
    slope = beta1,
    intercept = beta0,
    color = qcolors("sapphire"),
    size = 1
  ) + 
  geom_line(
    data = predictions,
    aes(x, y),
    color = "gray30",
    linetype = "dashed"
  ) +
  annotate(
    "text",
    x = x + 0.2,
    y = beta0 + beta1 * 5,
    label = paste0("X = ", x),
    hjust = 0,
    size = 13/.pt
  ) +
  annotate(
    "text",
    x = 4,
    y = ey + 1,
    label = paste0("E[Y] = ???"),
    vjust = 0,
    size = 13/.pt
  )

```

:::
::: {.fragment .fade-in fragment-index=0 .abs-left .z1 .w100}

```{r}

ggplot(cars) +
  scale_x_continuous(breaks = seq(0, 2 * ceiling(max_speed/2), by = 2)) + 
  labs(
    x = "Speed (m/s)", 
    y = "Distance (m)",
    title = "Complex Model"
  ) +
  coord_cartesian(
    ylim = c(floor(beta0), max(cars$distance) + 2),
    xlim = c(0, max_speed + 1)
  ) + 
  geom_abline(
    slope = beta1,
    intercept = beta0,
    color = qcolors("sapphire"),
    size = 1
  ) + 
  geom_line(
    data = predictions,
    aes(x, y),
    color = "gray30",
    linetype = "dashed"
  ) +
  annotate(
    "text",
    x = x + 0.2,
    y = beta0 + beta1 * 5,
    label = paste0("X = ", x),
    hjust = 0,
    size = 13/.pt
  ) +
  annotate(
    "text",
    x = 4,
    y = ey + 1,
    label = paste0("E[Y] = ", round(ey, 2)),
    vjust = 0,
    size = 13/.pt
  )

```

:::
::::
:::::
::::: {.column}

:::: {.flex .align-center .align-with-plot}
::: {.blue-panel .panel .flex .align-center}

$$
\begin{align}
y_i &= E[Y] + \epsilon_i \\
\epsilon &\sim N(0,\sigma) \\
\sigma &= `r round(sigma, 2)` 
\end{align}
$$   

:::
::: {.blue-panel .panel .flex .align-center }

$$
\begin{align}
E[Y] &\approx \hat\beta X \\
\hat\beta_0 &= `r round(beta0, 2)` \\
\hat\beta_1 &= `r round(beta1, 2)`
\end{align}
$$ 

:::
::::

__Question:__ if a car is going 8 m/s when it applies the brakes, how long will it take it to stop?

::: {.fragment .fade-in fragment-index=0}

$$
E[Y] = `r round(beta0, 2)` + `r round(beta1, 2)` \cdot 8 = `r round(ey, 2)`
$$

:::

:::::
::::::

## Prediction Interval

:::::: {.columns}
::::: {.column}
:::: {.relative}
::: {.fragment .fade-out fragment-index=0 .abs-left .z0 .w100}

```{r}

CI <- predict(cars_lm, newdata = tibble(speed = x), interval = "prediction")

ggplot(cars) +
  scale_x_continuous(breaks = seq(0, 2 * ceiling(max_speed/2), by = 2)) + 
  labs(
    x = "Speed (m/s)", 
    y = "Distance (m)",
    title = "Complex Model"
  ) +
  coord_cartesian(
    ylim = c(floor(beta0), max(cars$distance) + 2),
    xlim = c(0, max_speed + 1)
  )  +
  geom_abline(
    slope = beta1,
    intercept = beta0,
    color = qcolors("sapphire"),
    size = 1
  ) + 
  annotate(
    "segment",
    x = -2,
    y = ey,
    xend = x,
    yend = ey,
    color = "gray30",
    linetype = "dashed"
  ) +
  annotate(
    "text",
    x = 4,
    y = ey + 1,
    label = paste0("E[Y] = ", round(ey, 2)),
    vjust = 0,
    size = 13/.pt
  ) + 
  geom_errorbar(
    data = tibble(x = x, lo = CI[[2]], hi = CI[[3]]),
    aes(x = x, ymin = lo, ymax = hi),
    size = 1,
    width = 0.65,
    color = qcolors("bronze")
  )

```    

:::
::: {.fragment .fade-in fragment-index=0 .abs-left .z1 .w100}

```{r}

new_data <- tibble(speed = seq(-5, 15, length = 300))

CI <- predict(cars_lm, newdata = new_data, interval = "prediction")

prediction_interval <- bind_cols(new_data, as_tibble(CI))

ggplot(cars) +
  scale_x_continuous(breaks = seq(0, 2 * ceiling(max_speed/2), by = 2)) + 
  labs(
    x = "Speed (m/s)", 
    y = "Distance (m)",
    title = "Complex Model"
  ) +
  coord_cartesian(
    ylim = c(floor(beta0), max(cars$distance) + 2),
    xlim = c(0, max_speed + 1)
  ) + 
  geom_ribbon(
    data = prediction_interval,
    aes(speed, ymin = lwr, ymax = upr),
    fill = alpha(qcolors("bronze"), 0.5),
    color = qcolors("bronze"), 
    linewidth = 1
  )  +
  with_outer_glow(
    geom_abline(
      slope = beta1,
      intercept = beta0,
      color = qcolors("sapphire"),
      linewidth = 1
    ),
    colour = "white",
    sigma = 2,
    expand = 3
  )

```

:::
::::
:::::
::::: {.column}
:::: {.flex .align-center .align-with-plot}
::: {.blue-panel .panel .flex .align-center}

$$
\begin{align}
y_i &= E[Y] + \epsilon_i \\
\epsilon &\sim N(0,\sigma) \\
\sigma &= `r round(sigma, 2)` 
\end{align}
$$   

:::
::: {.blue-panel .panel .flex .align-center }

$$
\begin{align}
E[Y] &\approx \hat\beta X \\
\hat\beta_0 &= `r round(beta0, 2)` \\
\hat\beta_1 &= `r round(beta1, 2)`
\end{align}
$$ 

:::
::::

__Question__: But, what about $\epsilon$?!!! 

We can visualize this with a __prediction interval__, or the range within which a __future outcome__ is expected to fall with a certain probability for some value of $X$. [Can generalize this outside the range of the model, but with increasing uncertainty.]{.fragment .fade-in fragment-index=0}

:::::
::::::

## Confidence Interval

:::::: {.columns}
::::: {.column}

```{r}

SE <- predict(cars_lm, newdata = new_data, se.fit = TRUE)

error_interval <- tibble(
  distance = SE$fit,
  se = 2 * SE$se.fit
) |>
  bind_cols(new_data)

set.seed(42)

se_slope <- summary(cars_lm)$coefficients[2,2] * 2

randos <- tibble(
  slope = beta1 + seq(-se_slope, se_slope, length = 12),
  intercept = mean(cars$distance) - slope*mean(cars$speed)
) |>
  slice(-(c(1, n())))

ggplot(cars) +
  scale_x_continuous(breaks = seq(0, 2 * ceiling(max_speed/2), by = 2)) + 
  labs(
    x = "Speed (m/s)", 
    y = "Distance (m)",
    title = "Complex Model"
  ) +
  coord_cartesian(
    ylim = c(floor(beta0), max(cars$distance) + 2),
    xlim = c(0, max_speed + 1)
  ) + 
  geom_ribbon(
    data = prediction_interval,
    aes(speed, ymin = lwr, ymax = upr),
    fill = "transparent",
    color = qcolors("bronze"), 
    linewidth = 1
  ) +
  geom_ribbon(
    data = error_interval,
    aes(x = speed, ymin = distance - se, ymax = distance + se),
    fill = alpha(qcolors("sapphire"), 0.5),
    color = qcolors("sapphire"),
    linewidth = 1
  ) +
  geom_abline(
    data = randos,
    aes(slope = slope, intercept = intercept),
    color = "white",
    linewidth = 0.5
  )

```

:::::
::::: {.column}
:::: {.flex .align-center .align-with-plot}
::: {.blue-panel .panel .flex .align-center}

$$
\begin{align}
y_i &= E[Y] + \epsilon_i \\
\epsilon &\sim N(0,\sigma) \\
\sigma &= `r round(sigma, 2)` 
\end{align}
$$   

:::
::: {.blue-panel .panel .flex .align-center }

$$
\begin{align}
E[Y] &\approx \hat\beta X \\
\hat\beta_0 &= `r round(beta0, 2)` \\
\hat\beta_1 &= `r round(beta1, 2)`
\end{align}
$$ 

:::
::::

__Question__: But, what if we're wrong about $\beta$?!!! 

We can visualize this with a __confidence interval__, or the range within which the __average outcome__ is expected to fall with a certain probability for some value of $X$. 

:::::
::::::

# Model Evaluation

## Model Complexity

:::::: {.columns}
::::: {.column}

```{r}
#| out-width: "100%"

dists <- tibble(
  x = seq(-20, 20, length = 500),
  Simple = dnorm(x, sd = sd_y),
  Complex = dnorm(x, sd = sd_lm)
) |> 
  pivot_longer(
    -x, 
    names_to = "models", 
    values_to = "y"
  ) |> 
  arrange(x, models) |> 
  mutate(
    models = as_factor(models),
    models = fct_relevel(models, "Simple", "Complex")
  )

dists_text <- dists |> 
  group_by(models) |> 
  summarize(y = max(y)) |> 
  mutate(x = c(4.5,2))

ggplot() +
  geom_polygon(
    data = dists, 
    aes(x, y, fill = models, color = models), 
    linewidth = 1
  ) +
  geom_text(
    data = dists_text,
    aes(x, y, label = models, color = models),
    hjust = 0,
    vjust = 1,
    size = 5
  ) +
  scale_color_manual(
    name = NULL,
    values = qcolors("sapphire", "gold"),
    guide = "none"
  ) +
  scale_fill_manual(
    name = NULL,
    values = alpha(qcolors("sapphire", "gold"), 0.5),
    guide = "none"
  ) +
  scale_y_continuous(
    breaks = c(0.05, 0.10)
  ) +
  labs(
    x = "Distance (m)",
    y = NULL,
    title = "Distribution of Errors"
  ) +
  theme(
    panel.grid.major.y = element_line(color = "gray92"),
    axis.text.y = element_blank(),
    axis.ticks.y = element_blank(),
    axis.title.y = element_blank(),
    plot.margin = margin()
  )

```

:::::
::::: {.column}

::: {style="margin-top:1.9em"}
‚öñÔ∏è __The error is smaller__ for the more complex model. This is a good thing, __but what did it cost us__? Need to weigh this against the increased complexity! 
:::

:::::
::::::

## ANOVA for model

::::::::: {.panel-tabset}

### Problem

:::::: {.columns}
::::: {.column}

```{r}

barx <- mean(cars$distance)

tmp_x <- 9
tmp_y <- predict(cars_lm, newdata = tibble(speed = tmp_x))

ggplot(cars) +
  scale_x_continuous(breaks = seq(0, 2 * ceiling(max_speed/2), by = 2)) + 
  coord_cartesian(
    ylim = c(floor(beta0), max(cars$distance) + 2),
    xlim = c(0, max_speed + 1)
  ) + 
  labs(
    x = "Speed (m/s)", 
    y = "Distance (m)"
  ) +
  geom_abline(
    slope = beta1,
    intercept = beta0,
    color = qcolors("sapphire"),
    linewidth = 1
  ) +
  geom_hline(
    yintercept = barx,
    color = qcolors("gold"),
    linewidth = 1
  ) +
  geom_point(
    aes(x = speed, y = distance),
    size = 3.5
  ) +
  annotate(
    "point",
    x = tmp_x, 
    y = tmp_y,
    shape = 21,
    fill = "transparent",
    color = "gray35",
    size = 4,
    stroke = 1
  ) +
  annotate(
    "segment",
    x = tmp_x - 0.1,
    y = tmp_y,
    xend = tmp_x - 2,
    yend = tmp_y,
    linetype = "dashed",
    color = "gray35"
  ) +
  annotate(
    "text",
    x = tmp_x - 2.1,
    y = tmp_y,
    label = "Complex",
    color = qcolors("sapphire"),
    hjust = 1.0,
    vjust = 0.4,
    size = 13/.pt
  ) +
  annotate(
    "point",
    x = tmp_x + 1, 
    y = barx,
    shape = 21,
    fill = "transparent",
    color = "gray35",
    size = 4,
    stroke = 1
  ) +
  annotate(
    "segment",
    x = tmp_x + 1,
    y = barx-0.2,
    xend = tmp_x + 1,
    yend = 5,
    linetype = "dashed",
    color = "gray35"
  ) +
  annotate(
    "text",
    x = tmp_x + 1,
    y = 4.9,
    label = "Simple",
    color = qcolors("gold"),
    hjust = 0.5,
    vjust = 1.0,
    size = 13/.pt
  ) +
  defaults

```

:::::
::::: {.column}

We have two models of our data, one simple, the other complex.  

__Question:__ Does the complex (bivariate) model explain more variance than the simple (intercept) model? Does the difference arise by chance?

:::::
::::::

### Hypotheses

The __null__ hypothesis:

- $H_0:$ no difference in variance explained.

The __alternate__ hypothesis:

- $H_1:$ difference in variance explained.

### Strategy

__Variance Decomposition__. Total variance in the dependent variable can be decomposed into the variance captured by the more complex model and the remaining (or residual) variance:  

:::::: {.columns}
::::: {.column}

Decompose the differences:

$(y_{i} - \bar{y}) = (\hat{y}_{i} - \bar{y}) + (y_{i} - \hat{y}_{i})$

where

- $(y_{i} - \bar{y})$ is the total error,  
- $(\hat{y}_{i} - \bar{y})$ is the model error, and
- $(y_{i} - \hat{y}_{i})$ is the residual error. 

:::::
::::: {.column}

Sum and square the differences:

$SS_{T} = SS_{M} + SS_{R}$

where 

- $SS_{T}$: Total Sum of Squares
- $SS_{M}$: Model Sum of Squares
- $SS_{R}$: Residual Sum of Squares 

:::::
::::::

### Sum of Squares

```{r}
#| fig-asp: 0.34
#| fig-width: 14
#| out-width: 100%

ablines <- tibble(
  beta0 = c(barx, barx, beta0, beta0),
  beta1 = c(0, 0, beta1, beta1),
  color = qcolors("gold", "gold", "sapphire", "sapphire"),
  grp = c("Total Sum of Squares", rep("Model Sum of Squares", 2), "Residual Sum of Squares")
) |> 
  mutate(
    grp = factor(grp),
    grp = fct_relevel(grp, c("Total Sum of Squares", "Model Sum of Squares"))
  )

segments <- bind_rows(
  tibble(x = cars$speed, y = barx, yend = cars$distance, grp = "Total Sum of Squares"),
  tibble(x = cars$speed, y = barx, yend = estimate, grp = "Model Sum of Squares"),
  tibble(x = cars$speed, y = estimate, yend = cars$distance, grp = "Residual Sum of Squares")
) |> 
  mutate(
    grp = factor(grp),
    grp = fct_relevel(grp, c("Total Sum of Squares", "Model Sum of Squares"))
  )

pnts <- bind_rows(
  cars |> mutate(grp = "Total Sum of Squares"),
  cars |> mutate(grp = "Residual Sum of Squares")
) |> 
  mutate(
    grp = factor(grp, levels = levels(ablines$grp)),
    grp = fct_relevel(grp, c("Total Sum of Squares", "Model Sum of Squares"))
  )

ggplot(cars) +
  defaults +
  scale_x_continuous(breaks = seq(0, 2 * ceiling(max_speed/2), by = 2)) +
  labs(
    x = "Speed (m/s)", 
    y = "Distance (m)"
  ) +
  geom_segment(
    data = segments,
    aes(x = x, xend = x, y = y, yend = yend),
    linetype = "dashed",
    color = "gray35",
    linewidth = 0.75
  ) +
  geom_abline(
    data = ablines,
    aes(intercept = beta0, slope = beta1, color = color),
    linewidth = 1
  ) +
  scale_color_manual(
    name = NULL,
    guide = "none",
    values = c(qcolors("sapphire", "gold"), "transparent")
  ) +
  geom_point(
    data = pnts,
    aes(x = speed, y = distance),
    size = 3.5
  ) +
  facet_wrap(vars(grp), ncol = 3) +
  theme(
    strip.background = element_blank(),
    strip.text = element_text(hjust = 0, size = 18)
  ) +
  coord_cartesian(
    ylim = c(-2.5, max(cars$distance) + 2),
    xlim = c(0, max_speed + 1)
  )

```

### F-statistic

```{r}

cars_anova <- aov(cars_lm)

f.value <- summary(cars_anova)[[1]][1,4]
f.p <- summary(cars_anova)[[1]][1,5]

model_df <- cars_anova$assign |> sum()
residual_df <- cars_anova$df.residual |> unname()

```

$$F = \frac{\text{model variance}}{\text{residual variance}}$$

where

- Model variance = $SS_{M}/k$ for $k$ model parameters

- Residual variance = $SS_{R}/(n-k-1)$ for $k$ and $n$ observations.

__Question:__ How probable is it that $F=$ `r round(f.value, 2)`?  

We can answer this by comparing the F-statistic to the F-distribution. 

### F-distribution

:::::: {.columns}
::::: {.column width="39%"}

::: {style="margin-top:1.9em;"}
Summary:

- $\alpha = 0.05$
- $H_0:$ no difference
- $p=$ `r f.p`

__Translation:__ the null hypothesis is really, really unlikely. So, there must be some difference between models!
:::

:::::
::::: {.column width="60%"}

```{r}

r <- 110

distribution <- tibble(
  x = c(0, seq(0, r, length = 300)),
  y = c(0, df(x[-1], df1 = model_df, df2 = residual_df))
)

a <- qf(
  0.05,
  df1 = model_df,
  df2 = residual_df,
  lower.tail = FALSE
)

n <- 100
x <- seq(a, r, length = n)
y <- df(x, df1 = model_df, df2 = residual_df)

tails <- tibble(
  x = c(a, x),
  y = c(0, y)
)

ggplot() +
  geom_polygon(
    data = distribution,
    aes(x, y),
    fill = qcolors("gold"),
    color = qcolors("kobe"),
    linewidth = 1
  ) +
  geom_polygon(
    data = tails,
    aes(x,y),
    fill = alpha(qcolors("kobe"), 0.65),
    color = qcolors("kobe"),
    linewidth = 1
  ) +
  annotate(
    "segment",
    x = f.value,
    y = 0, 
    xend = f.value,
    yend = 0.3,
    linetype = "dashed",
    linewidth = 1
  ) +
  annotate(
    "text",
    x = f.value,
    y = 0.3 + 0.01,
    label = paste0("F = ", round(f.value, 2)),
    size = 13/.pt,
    hjust= 0.2, 
    vjust = 0
  ) +
  labs(
    x = NULL,
    y = "Density",
    title = paste0("F-distribution (df = ", model_df, ", ", residual_df, ")")
  )

```

:::::
::::::

:::::::::

## R-Squared

:::::: {.columns}
::::: {.column}

Coefficient of Determination

$$R^2 = 1- \frac{SS_{R}}{SS_{T}}$$  

- Proportion of variance in $y$ explained by the model $M$.
- Scale is 0 to 1. A value closer to 1 means that $M$ explains more variance.
- $M$ is evaluated relative to simple, intercept-only model.

:::::
::::: {.column}

```{r}
#| fig-align: left
#| fig-height: 6
#| fig-asp: 1.2

squares <- tibble(
  x = 0.5,
  y = max(cars$distance),
  label = c("SST", "SSM", "SSR"),
  grp = c("Total Sum of Squares", "Model Sum of Squares", "Residual Sum of Squares")
)

ggplot(cars) +
  defaults +
  scale_x_continuous(breaks = seq(0, 2 * round(max_speed/2), by = 2)) + 
  labs(
    x = "Speed (m/s)", 
    y = "Distance (m)"
  ) +
  geom_segment(
    data = segments |> filter(grp != "Model Sum of Squares"),
    aes(x = x, xend = x, y = y, yend = yend),
    linetype = "dashed",
    color = "gray35",
    size = 0.5
  ) +
  geom_abline(
    data = ablines |> filter(grp != "Model Sum of Squares"),
    aes(intercept = beta0, slope = beta1, color = color),
    size = 1
  ) +
  geom_point(
    data = pnts |> filter(grp != "Model Sum of Squares"),
    aes(x = speed, y = distance),
    size = 3.5
  ) +
  scale_color_manual(
    name = NULL,
    guide = "none",
    values = qcolors("sapphire", "gold")
  ) +
  geom_text(
    data = squares |> filter(grp != "Model Sum of Squares"),
    aes(x, y, label = label),
    hjust = 0,
    vjust = 1,
    size = 8
  ) +
  facet_wrap(~grp, ncol = 1) +
  theme(
    strip.background = element_blank(),
    strip.text = element_blank()
  ) +
  coord_cartesian(
    ylim = c(-2.5, max(cars$distance) + 2),
    xlim = c(0, max_speed + 1)
  )

```

:::::
::::::

## t-test for coefficients

::: {.panel-tabset}

### Problem

:::::: {.columns}
::::: {.column}

```{r}

betas <- summary(cars_lm)$coefficients

se <- betas[, 2, drop = TRUE]

xmin <- min(c(beta0 - se[[1]], beta1 - se[[2]]))
xmax <- max(c(beta0 + se[[1]], beta1 + se[[2]]))

cars_coefficients <- tibble(
  x = c(beta0, beta1),
  y = c("Intercept", "Slope"),
  se = se
)

gg_coefficients <- ggplot(cars_coefficients, aes(x, y)) +
  scale_x_continuous(
    breaks = c(ceiling(xmin), 0, floor(xmax)),
    limits = c(xmin, xmax)
  ) +
  geom_vline(xintercept = 0, size = 1) +
  labs(
    x = "Estimate",
    y = NULL,
    title = "Model Coefficients"
  ) +
  theme(
    panel.border = element_rect(color = "gray75", fill = "transparent")
  )

gg_coefficients +
  geom_segment(
    aes(x = 0, y = y, xend = x, yend = y),
    color = "gray35",
    linetype = "dashed"
  ) +
  geom_point(size = 4)

```

:::::
::::: {.column}

__Question:__ Are the coefficient estimates significantly different than zero?

To answer this question, we need some measure of uncertainty for these estimates.

:::::
::::::

### Hypotheses

The __null__ hypothesis:

- $H_0:$ coefficient estimate is _not_ different than zero.

The __alternate__ hypothesis:

- $H_1:$ coefficient estimate is different than zero.

### Standard Errors

:::::: {.columns}
::::: {.column}

```{r}

gg_coefficients +
  geom_errorbarh(
    aes(xmin = x - se, xmax = x + se),
    height = 0.33
  ) +
  geom_point(size = 4)

```

:::::
::::: {.column}

For simple linear regression,

- the __standard error of the slope__, $se(\hat\beta_1)$, is the ratio of the average squared error of the model to the total squared error of the predictor.

- the __standard error of the intercept__, $se(\hat\beta_0)$, is $se(\hat\beta_1)$ weighted by the average squared values of the predictor.

:::::
::::::

### t-statistic

```{r}

t_int <- betas[1, 3, drop = TRUE]
t_slp <- betas[2, 3, drop = TRUE]

```

The t-statistic is the coefficient estimate divided by its standard error

$$t = \frac{\hat\beta}{se(\hat\beta)}$$ 

<br>

This can be compared to a t-distribution with $n-k-1$ degrees of freedom (\\(n\\) observations and \\(k\\) independent predictors). 

### t-test

```{r}

p_int <- pmax(0.001, betas[1, 4, drop = TRUE])
p_slp <- pmax(0.001, betas[2, 4, drop = TRUE])

```

:::::: {.columns}
::::: {.column width="39%"}

::: {style="margin-top:1.9em;"}
Summary:

- $\alpha = 0.05$
- $H_0$: $\beta=0$
- p-values:
    - intercept = `r p_int`
    - slope < `r p_slp`
:::

:::::
::::: {.column width="60%"}

```{r}

r <- ceiling(max(c(t_int, t_slp))) + 1
r <- c(-1*r, r)

t.df <- cars_lm$df.residual

a <- qt(0.025, df = t.df, lower.tail = FALSE)

distribution <- tibble(
  x = seq(-max(r), max(r), length = 300),
  y = dt(x, df = t.df)
)

n <- 100
x <- seq(a, max(r), length = n)
y <- dt(x, df = t.df)

tails <- tibble(
  x = c(rev(-x), -a, a, x),
  y = c(rev(y), 0, 0, y),
  z = rep(c("lower", "upper"), each = n+1)
)

ggplot() +
  geom_polygon(
    data = distribution,
    aes(x, y),
    fill = qcolors("gold"),
    color = qcolors("kobe"),
    linewidth = 1
  ) +
  geom_polygon(
    data = tails,
    aes(x,y, group = z),
    fill = alpha(qcolors("kobe"), 0.65),
    color = qcolors("kobe"),
    linewidth = 1
  ) +
  coord_cartesian(xlim = c(min(r), max(r))) +
  annotate(
    "segment",
    x = t_int,
    y = 0,
    xend = t_int,
    yend = 0.2,
    linetype = "dashed",
    linewidth = 1
  ) +
  annotate(
    "segment",
    x = t_int,
    y = 0.2,
    xend = t_int-1,
    yend = 0.25,
    linetype = "dashed",
    linewidth = 1
  ) +
  annotate(
    "segment",
    x = t_int-1,
    y = 0.25,
    xend = t_int-1.2,
    yend = 0.25,
    linetype = "dashed",
    linewidth = 1
  ) +
  annotate(
    "text",
    x = t_int-1.4,
    y = 0.25,
    label = paste0("intercept\n", "t = ", round(t_int, 2)),
    hjust = 1,
    vjust = 0.5,
    size = 13/.pt,
    lineheight = 1.5
  ) +
  annotate(
    "segment",
    x = t_slp,
    y = 0,
    xend = t_slp,
    yend = 0.2,
    linetype = "dashed",
    linewidth = 1
  ) +
  annotate(
    "text",
    x = t_slp,
    y = 0.2 + 0.02,
    label = paste0("slope\n", "t = ", round(t_slp, 2)),
    hjust = 0.5,
    vjust = 0,
    size = 13/.pt,
    lineheight = 1.5
  ) +
  labs(
    x = "t-statistic",
    y = "Density",
    title = paste0("t-distribution (df = ", round(t.df, 0), ")")
  )

```

:::::
::::::

:::

## Diagnostic Plots

```{css}

@media (min-width: 30em) {

  .sideways .panel-tabset {
     display: grid;
     grid-gap: 2em;
     grid-template-columns: 25% 75%;
  }
  
  .sideways .panel-tabset-tabby {
     border-bottom: none !important;
     border-right: 1px solid #bbb; 
  }
  
  .sideways .panel-tabset [role=tab][aria-selected=true] {
     background-color: transparent;
     border: 1px solid #bbb !important;
  }
  
  .sideways h3 {
     margin-top: 0;
  }
  
}

```

::::::{.sideways}
::::: {.panel-tabset}

### Model Assumptions

1. __Weak Exogeneity__: the predictor variables have fixed values and are known. 

2. __Linearity__: the relationship between the predictor variables and the response variable is linear.  

2. __Constant Variance__: the variance of the errors does not depend on the values of the predictor variables. Also known as _homoscedasticity_.  

2. __Independence of errors__: the errors are uncorrelated with each other.  

2. __No perfect collinearity__: the predictors are not linearly correlated with each other.  

### Residual vs Fitted

```{r}
#| fig-width: 11
#| fig-asp: 0.4
#| fig-align: center

set.seed(12345)

n <- 50

x <- seq(0, 1, length = n)

models <- tibble(
  x = x,
  Good = 0,
  Heteroschedastic = 0,
  Nonlinear = 0.5 - 5*(x - 0.5)^2 
) |> 
  pivot_longer(-x, names_to = "model", values_to = "y")

outcomes <- tibble(
    x = x,
    Good = rnorm(n),
    Heteroschedastic = rnorm(n, sd = x),
    Nonlinear = with(models |> filter(model == "Nonlinear"), y + rnorm(n, sd = 0.4))
) |> 
  pivot_longer(-x, names_to = "model", values_to = "y") |> 
  mutate(
    y = ifelse(y > 1.5, 1, y),
    y = ifelse(y < -1.5, -1, y)
  )

errors <- models |> 
  mutate(
    ymax = case_when(
      model == "Good" ~ y + 1,
      model == "Heteroschedastic" ~ y + x,
      TRUE ~ y + 0.4
    ),
    ymin = case_when(
      model == "Good" ~ y - 1,
      model == "Heteroschedastic" ~ y - x,
      TRUE ~ y - 0.4
    )
  )

ggplot() +
  geom_hline(yintercept = 0, color = "gray45", linetype = "dashed") +
  geom_ribbon(
    data = errors,
    aes(x, ymin = ymin, ymax = ymax),
    color = "transparent",
    fill = alpha(qcolors("sapphire"), 0.2)
  ) +
  geom_line(
    data = models,
    aes(x, y),
    color = qcolors("bronze"),
    size = 1
  ) +
  geom_point(
    data = outcomes,
    aes(x, y),
    size = 2
  ) +
  facet_wrap(~model, ncol = 3) +
  labs(
    x = "Fitted",
    y = "Residual"
  ) +
  theme(
    axis.text = element_blank(),
    axis.ticks = element_blank(),
    strip.background = element_blank(),
    strip.text = element_text(size = 18, hjust = 0)
  )

```

### Normal Q-Q

```{r}
#| fig-width: 6
#| fig-asp: 1
#| fig-align: center

set.seed(42)

n <- 50
x <- seq(-3, 3, length = n)
g <- rnorm(n)
ht <- local({
  
  z <- rcauchy(n)
  
  for (i in seq_along(z)) {
    
    q <- z[[i]]
    
    if (q > 3) z[[i]] <- runif(1, 3.5, 5)
    
    if (q < -3) z[[i]] <- runif(1, -5, -3.5)
    
  }

  z
  
})

lt <- local({
  
  z <- c(runif(n, -5, 5), runif(n/2, -3, 3))
  
  sample(z, n)
  
})



dists <- list(
  "Right Skew"  = sample(c((g[g > -0.3] + 0.3) * 3, g), size = n),
  "Left Skew"   = sample(c(g[g < 0] * 3, g), size = n),
  "Heavy Tails" = ht,
  "Light Tails" = lt
)

max_length <- lapply(dists, length) |> unlist() |> max()

dists <- lapply(dists, function(x){ length(x) <- max_length; x })

dists <- dists |> 
  as_tibble() |> 
  pivot_longer(
    everything(),
    names_to = "distribution",
    values_to = "x"
  ) |> 
  filter(!is.na(x)) |> 
  arrange(distribution) |> 
  mutate(
    distribution = factor(distribution),
    distribution = fct_relevel(distribution, "Light Tails", "Heavy Tails", "Left Skew")
  )

ggplot(dists, aes(sample = x)) +
  geom_qq(
    size = 3,
    shape = 21,
    fill = alpha("gray50", 0.3)
  ) +
  geom_qq_line(
    color = qcolors("kobe"),
    size = 1
  ) +
  facet_wrap(~distribution, ncol = 2) +
  labs(
    x = "Expected Distribution",
    y = "Empirical Distribution"
  ) + 
  theme(
    axis.text = element_blank(),
    axis.ticks = element_blank(),
    strip.background = element_blank(),
    strip.text = element_text(size = 16, hjust = 0)
  )

```

### Residual Density

```{r}
#| fig-width: 9
#| fig-asp: 0.6
#| fig-align: center

library(performance)

check_model(
  cars_lm, 
  check = "normality",
  colors = qcolors("sapphire", "blue")
)

```

### Cook's Distance

```{r}
#| fig-width: 9
#| fig-asp: 0.6
#| fig-align: center

check_model(
  cars_lm, 
  check = "outliers",
  colors = qcolors("blue", "eerie", "kobe")
)

```

:::::
::::::
