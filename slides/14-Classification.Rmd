---
title: "Quantitative Analysis of Archaeological Data"
subtitle: "Lecture 14: Classification"  
date: 'Last updated: `r Sys.Date()`'
output:
  xaringan::moon_reader:
    lib_dir: libs
    css: [xaringan-themer.css, custom_style.css]
    seal: TRUE
    nature:
      highlightStyle: magula
      highlightLines: TRUE
      countIncrementalSlides: TRUE
      ratio: '16:9'
---

```{r} 
#| include = FALSE, 
#| code = xfun::read_utf8(here::here("slides", "before_chunk.R"))
```

```{r}

darts <- here("datasets", "darts_and_arrows.csv") %>% 
  read_csv() %>% 
  mutate(
    dart = ifelse(type == "dart", 1, 0)
  )

fakes <- iris %>% 
  as_tibble() %>% 
  rename(
    "length" = Sepal.Length,
    "width" = Sepal.Width,
    "neck" = Petal.Length,
    "thickness" = Petal.Width,
    "type" = Species
  ) %>% 
  mutate(
    type = case_when(
      type == "setosa" ~ "derp",
      type == "versicolor" ~ "flerp",
      TRUE ~ "merp"
    ),
    length = case_when(
      type == "derp" ~ length + 0.3,
      type == "flerp" ~ length - 0.3,
      TRUE ~ length + 0.1
    ),
    width = case_when(
      type == "derp" ~ width + 1.3,
      type == "flerp" ~ width + 0.4,
      TRUE ~ width + 1.2
    )
  )

```


## Outline

1. Supervised classifiers (with response)
    - Logistic regression
    - Linear Discriminant Analysis (LDA)
2. Unsupervised classifiers (without response)
    - Principal Component Analysis (PCA)
    - k-means
    - hierarchical clustering


---
class: center middle

## Is it a dart?

```{r}
#| fig.asp = 0.75,
#| out.width = "60%"

darts_long <- darts %>% 
  pivot_longer(
    cols = c(length:neck),
    names_to = "measure",
    values_to = "value"
  )

ggplot(darts_long, aes(type, value)) +
  geom_boxplot() +
  facet_wrap(~measure, scale = "free_y") +
  labs(
    x = NULL,
    y = "Millimeters"
  )

```

---

## Logistic Regression

```{r}

darts_glm <- glm(
  dart ~ length + width + thickness + neck,
  family = binomial,
  data = darts
)

responses <- bind_rows(
  ggpredict(darts_glm, "length [10:90, by=0.5]") %>% mutate(measure = "length"),
  ggpredict(darts_glm, "width [5:35, by=0.5]") %>% mutate(measure = "width")
) %>% 
  as_tibble()

```

.pull-left[

```{r}
#| fig.asp = 1

ggplot() +
  geom_ribbon(
    data = responses,
    aes(x, ymin = conf.low, ymax = conf.high),
    fill = "gray90"
  ) +
  geom_point(
    data = darts_long %>% filter(measure %in% c("length", "width")),
    aes(value, dart),
    size = 3,
    alpha = 0.5
  ) +
  geom_line(
    data = responses,
    aes(x, predicted)
  ) +
  facet_wrap(
    ~measure, 
    scale = "free_x",
    nrow = 2
  ) +
  scale_y_continuous(
    breaks = c(0, 1),
    labels = c("arrow", "dart")
  ) +
  labs(
    x = "Millimeters",
    y = NULL
  )

```

]

.pull-right[

```{r}

broom::tidy(darts_glm) %>% 
  rename("z.value" = statistic) %>% 
  kbl(table.attr = "class='table-model table-fullwidth'") %>% 
  kable_paper(c("striped", "hover"))

remove(darts_glm, darts_long, responses)

```

Note that estimates are on the logit scale!

]

---

## LDA with one predictor

.pull-left[

```{r}
#| fig.asp = 0.5

length_hist <- ggplot(
  darts,
  aes(length, color = type, fill = type)
) +
  geom_histogram(
    bins = 15,
    center = 0,
    position = "identity"
  ) +
  scale_color_viridis(
    name = NULL,
    direction = -1,
    discrete = TRUE
  ) +
  scale_fill_viridis(
    name = NULL,
    direction = -1,
    alpha = 0.5,
    discrete = TRUE
  ) +
  scale_y_continuous(
    labels = scales::number_format(accuracy = 0.1)
  ) +
  theme(
    legend.position = c(0.97, 0.98),
    legend.justification = c("right", "top")
  ) +
  labs(
    x = "Length",
    y = "Count"
  )

length_hist

```

]

.pull-right[

Uses the distribution of each X to define a **discriminant function** $f_k$ for each group $k$. 

For each observation $i$, $f_k$ determines the probability that $i$ is in $k$. 

Linear assumptions for Xs! Homoscedasticity, normality, independence, no multi-collinearity.  

]



---
count: false

## LDA with one predictor

```{r}

darts_lda <- lda(
  type ~ length,
  data = darts
)

new_data <- tibble(length = with(darts, seq(min(length), max(length), length = 100)))

predictions <- predict(darts_lda, newdata = new_data) %>% lapply(as_tibble)

decision <- bind_cols(new_data, predictions$posterior) %>% 
  pivot_longer(
    cols = c("dart", "arrow"),
    names_to = "type",
    values_to = "posterior"
  )

boundary_length <- predictions$class %>% 
  mutate(length = new_data$length) %>% 
  group_by(value) %>% 
  summarize(
    max_length = max(length), 
    min_length = min(length)
  )

boundary_length <- with(boundary_length, (max_length[[1]] + min_length[[2]])/2)

```

.pull-left[

```{r}
#| fig.asp = 0.925

length_hist <- length_hist +
  geom_vline(
    xintercept = boundary_length,
    linetype = "dashed",
    color = "gray20",
    size = 1
  ) +
  theme(
    axis.text.x = element_blank(),
    axis.ticks.x = element_blank(),
    axis.title.x = element_blank()
  )

length_prob <- ggplot(
  decision,
  aes(length, color = type, fill = type)
) +
  geom_histogram(
    bins = 15,
    center = 0,
    position = "identity",
    color = "transparent",
    fill = "transparent"
  ) +
  geom_ribbon(
    aes(ymin = 0, ymax = posterior),
    color = "transparent"
  ) +
  geom_line(
    aes(y = posterior),
    size = 1.3
  ) +
  scale_color_viridis(
    name = NULL,
    direction = -1,
    discrete = TRUE
  ) +
  scale_fill_viridis(
    name = NULL,
    direction = -1,
    alpha = 0.5,
    discrete = TRUE
  ) +
  theme(
    legend.position = "none"
  ) +
  labs(
    x = "length",
    y = "Probability"
  ) +
  scale_y_continuous(
    limits = c(0, 1),
    breaks = c(0, 0.5, 1)
  ) +
  geom_vline(
    xintercept = boundary_length,
    linetype = "dashed",
    color = "gray20",
    size = 1
  )

length_hist / length_prob

remove(train, darts_lda, predictions, decision, boundary_length)

```

]

.pull-right[

Uses the distribution of each X to define a **discriminant function** $f_k$ for each group $k$. 

For each observation $i$, $f_k$ determines the probability that $i$ is in $k$. 

Linear assumptions for Xs! Homoscedasticity, normality, independence, no multi-collinearity. 

Decision boundary (dashed line) between group 1 and 2 is the point where $f_1 = f_2$.

]

---
class: center middle

## What type is it?

```{r}
#| fig.asp = 0.75,
#| out.width = "60%"

ggplot(fakes %>% pivot_longer(cols = -type), aes(type, value)) +
  geom_boxplot() +
  facet_wrap(~name, nrow = 2, scale = "free_x") +
  coord_flip() +
  theme(
    plot.margin = margin(5, 9, 5, 5)
  ) +
  labs(
    y = "Millimeters",
    x = NULL
  )

```

---

## LDA with multiple predictors

.pull-left[

```{r}

n <- 200

exp <- 0.33

new_data <- with(
  fakes,
  expand_grid(
    length = seq(min(length)-exp, max(length)+exp, length = n),
    width = seq(min(width)-exp, max(width)+exp, length = n),
    thickness = mean(thickness),
    neck = mean(neck)
  )
)

ggplot(fakes, aes(length, width, color = type)) +
  geom_point(
    size = 4,
    alpha = 0.8
  ) +
  scale_color_viridis(
    name = NULL,
    discrete = TRUE
  ) +
  labs(
    x = "Length",
    y = "Width"
  ) +
  coord_cartesian(
    xlim = with(new_data, c(min(length), max(length))),
    ylim = with(new_data, c(min(width), max(width))),
    expand = FALSE
  ) +
  theme(
    legend.background = element_blank(),
    legend.position = c(0.01, 0.99),
    legend.justification = c("left", "top"),
    plot.margin = margin(5,5,5,5)
  )

```

]

.pull-right[

Works the same way with multiple predictors, but uses a multivariate probability distribution.

Note that it still assumes linearity! In fact, LDA is equivalent to simple linear regression!

]

---

## LDA with multiple predictors

```{r}

darts_lda <- lda(
  type ~ length + width,
  data = fakes
)

```

.pull-left[

```{r}

predictions <- predict(darts_lda, newdata = new_data)

decision <- new_data %>% 
  bind_cols(class = predictions$class) %>% 
  mutate(class = class)

ggplot() +
  geom_point(
    data = decision,
    aes(length, width, color = class),
    size = 0.2,
    alpha = 0.1
  ) + 
  geom_contour(
    data = decision,
    aes(length, width, z = as.integer(class)),
    breaks = c(1.9, 2.1),
    color = qaad_colors("rufous_red"),
    size = 1
  ) +
  geom_point(
    data = fakes,
    aes(length, width, fill = type),
    shape = 21,
    color = "black",
    size = 4,
    stroke = 0.8
  ) +
  stat_ellipse(
    data = fakes,
    aes(length, width, group = type),
    color = "gray20",
    size = 1.65
  ) +
  stat_ellipse(
    data = fakes,
    aes(length, width, group = type),
    color = "white",
    size = 1.1
  ) +
  scale_color_viridis(
    name = NULL,
    discrete = TRUE
  ) +
  scale_fill_viridis(
    name = NULL,
    alpha = 0.8,
    discrete = TRUE
  ) +
  labs(
    x = "Length",
    y = "Width"
  ) +
  coord_cartesian(
    xlim = with(new_data, c(min(length), max(length))),
    ylim = with(new_data, c(min(width), max(width))),
    expand = FALSE
  ) +
  theme(
    legend.background = element_blank(),
    legend.key = element_rect(fill = "transparent"),
    legend.position = c(0.01, 0.99),
    legend.justification = c("left", "top"),
    plot.margin = margin(5,5,5,5)
  )

```

]

.pull-right[

Works the same way with multiple predictors, but uses a multivariate probability distribution.

Note that it still assumes linearity! In fact, LDA is equivalent to simple linear regression!

Decision boundary (dark red line) between 
- between derp and flerp is the point where $f_{derp} = f_{flerp}$
- between derp and merp is the point where $f_{derp} = f_{merp}$
- between merp and flerp is the point where $f_{merp} = f_{flerp}$

]

---

## Principal Component Analysis

```{r}

fakes_pca <- fakes %>% 
  select(-type, -neck) %>% 
  prcomp(scale = TRUE)

evals <- fakes_pca$sdev^2
evctr <- fakes_pca$rotation
score <- fakes_pca$x

orthogonal <- function(x, y, a = 0, b = 1){
  
  # finds endpoint for a perpendicular segment 
  # from the point (x0,y0) 
  # to the line y=a+b*x
  
  xend <- (x+b*y-a*b)/(1+b^2)
  yend <- a + b*xend
  
  tibble(
    x = x, 
    y = y, 
    xend = xend, 
    yend = yend
  )
  
}

b <- evctr[2,1]/evctr[1,1]
a <- with(fakes, mean(width) - b*mean(length))

projection <- orthogonal(
  x = fakes$length,
  y = fakes$width,
  a = a,
  b = b
) %>% 
  mutate(method = "PCA")

```


.pull-left[

```{r}
#| fig.asp = 0.9,
#| out.width = "100%"

ggplot(fakes, aes(length, width)) + 
  geom_segment(
    data = projection,
    aes(x, y, xend = xend, yend = yend),
    color = qaad_colors("flame_orange"),
    alpha = 0.3,
    size = 0.4
  ) +
  geom_abline(
    intercept = a, 
    slope = b, 
    color = qaad_colors("flame_orange"),
    size = 1
  ) +
  geom_point(
    size = 3.5, 
    alpha = 0.7
  ) + 
  annotate(
    "segment",
    x = mean(fakes$length) - 0.5,
    y = mean(fakes$width) + (1/b*0.5),
    xend = mean(fakes$length) + 0.5,
    yend = mean(fakes$width) - (1/b*0.5),
    size = 0.5,
    color = "gray50"
  ) +
  annotate(
    "point", 
    x = mean(fakes$length),
    y = mean(fakes$width),
    size = 5,
    shape = 21,
    color = qaad_colors("rufous_red"),
    fill = "white",
    stroke = 2
  ) +
  coord_equal() +
  labs(
    x = "Length",
    y = "Width"
  ) +
  theme(
    legend.background = element_blank(),
    legend.position = c(0.01, 0.99),
    legend.justification = c("left", "top"),
    plot.margin = margin(2, 2, 2, 2)
  )

```

]

.pull-right[

**What is it?** An ordination method for reducing the number of dimensions (variables) in a dataset.  

**Why is it?** PCA can (1) visualize complex datasets, (2) summarize redundant variables, (3) impute missing data, and (4) remove collinearity.  

**How is it?** Think of it like a set of nested OLS models, only the variance isn't parallel to the y-axis, but orthogonal to the principal component.

The **goal** is to maximize the projected variance (spread of points on the PC line), or equivalently, to minimize the orthogonal distance from each point (orange lines connecting points to PC line).  

]

---

## Principal Component Analysis

.pull-left[

```{r}
#| fig.asp = 0.9,
#| out.width = "100%"

w <- sqrt(nrow(fakes)-1)/5 
s <- 0.95 # shrinkage for labels

arrow_data <- lapply(
  names(fakes)[c(1,2,4)],
  function(j){
    
    xx <- fakes[, j, drop = TRUE]
    pc1 <- score[, 1, drop = TRUE]
    pc2 <- score[, 2, drop = TRUE]
    
    tibble(
      type = j,
      x = 0,
      y = 0,
      xend = w * cor(xx, pc1) * s,
      yend = w * cor(xx, pc2) * s,
      xlab = w * cor(xx, pc1),
      ylab = w * cor(xx, pc2)
    )
    
  }
) %>% 
  bind_rows()

score <- score %>% as_tibble() %>% mutate(class = fakes$type)

ggplot() +
  geom_hline(
    yintercept = 0,
    color = "gray75",
    size = 0.5
  ) +
  geom_vline(
    xintercept = 0,
    color = "gray75",
    size = 0.5
  ) +
  geom_point(
    data = score,
    aes(PC1, PC2, color = class),
    size = 4,
    alpha = 0.8
  ) +
  scale_color_viridis(
    name = NULL,
    discrete = TRUE
  ) +
  geom_segment(
    data = arrow_data,
    aes(x, y, xend = xend, yend = yend, group = type),
    arrow = arrow(length = unit(0.18, "in"), type = "closed")
  ) +
  geom_text(
    data = arrow_data,
    aes(xlab, ylab, label = type),
    size = 8,
    hjust = c(0, 0.5, 0),
    vjust = c(0.5, 1, 0)
  ) +
  scale_x_continuous(expand = expansion(add = 1.2)) +
  scale_y_continuous(expand = expansion(add = 0.5)) +
  coord_equal() +
  theme(
    legend.background = element_blank(),
    legend.position = c(0.01, 0.99),
    legend.justification = c("left", "top")
  )

```

]

.pull-right[

**Why is it?** PCA can 

(1) **visualize complex datasets** - PC1 and PC2 represent three dimensions (variables) and their correlation (positive is < 90&deg;, negative is > &deg;)

(2) **summarize redundant variables** - PC1 can be interpreted as a general measure of *shape*. 

(3) **impute missing data** - if missing one measure (like thickness), can estimate this based on measures of other variables (width and length).

(4) **remove collinearity** - the PCs are orthogonal to each other and are uncorrelated by definition.

]

---
class: center middle

## Principal Component Analysis

.w-70.ml-auto.mr-auto.tl[

```{r}
#| fig.asp = 0.5

vimp <- summary(fakes_pca)$importance %>% 
  as_tibble(rownames = "measure") %>% 
  mutate(measure = factor(
    measure, 
    levels = c("Proportion of Variance", "Cumulative Proportion", "Standard deviation")
  )
  ) %>% 
  pivot_longer(-measure)

ggplot(
  vimp %>% filter(measure != "Standard deviation"), 
  aes(name, value, group = 1)
) + 
  geom_line(
    color = qaad_colors("rufous_red"),
    size = 1
  ) +
  geom_point(
    color = qaad_colors("rufous_red"),
    size = 4
  ) +
  facet_wrap(~measure) +
  labs(
    x = NULL,
    y = "Proportion Variance"
  ) +
  theme(
    strip.background = element_blank(),
    strip.text = element_blank()
  )

```

Figure on left is a "scree" plot showing the proportion of variance explained, $R^2$, for each individual PC. Figure on right is the cumulative variance explained, the total $R^2$ when including each additional PC.

]

---

## K-means clustering

.pull-left[

```{r}
#| out.width = "90%"

figure("12_8.png")

```

.right[Figure from [James *et al* (2021) ISLR](https://www.statlearning.com/)]

]

.pull-right[

**What is it?** A clustering algorithm that minimizes differences within groups (equivalently, maximizes similarity).

Requires a **measure of difference** or similarity. Most common is squared Euclidean distance.

**Assumes** that each observation belongs to one and only one group.

]

---

## K-means clustering

.pull-left[

```{r}
#| out.width = "90%"

figure("12_8.png")

```

.right[Figure from [James *et al* (2021) ISLR](https://www.statlearning.com/)]

]

.pull-right[

**How is it?** 

- Must choose the number of groups $K$ prior to sorting.
- Algorithm randomly assigns points to each group.
- Calculates the centroids of each group (the means of each variable).
- Re-assigns points to groups based on their distance from centroids.
- Repeats until the within-group differences are minimized.

]

---

## Hierarchical clustering

.pull-left[

```{r}

figure("12_10.png")

```

.right[Figure from [James *et al* (2021) ISLR](https://www.statlearning.com/)]

]

.pull-right[

**What is it?** A clustering algorithm that minimizes differences within groups (equivalently, maximizes similarity).

Requires a **measure of difference** or similarity. Most common is squared Euclidean distance.

Does not require a decision about the number of $K$.

A **linkage** rule must be chosen.

Results in dendrograms ("trees") representing groups and degrees of similarity/difference between observations.

]

---

## Dendrograms

.pull-left[

```{r}

figure("12_11.png")

```

.right[Figure from [James *et al* (2021) ISLR](https://www.statlearning.com/)]

]

.pull-right[

A **leaf** is the lowest terminal point and represents a unique observation.

A **fusion** represents the point where two observations or groups of observations are most similar.
- Fusions lower in the tree represent greater similarity.
- Fusions higher in the tree represent greater difference.
- Cannot infer similarity based on horizontal distance in the tree!

**Hierarchy** refers to nested clusters in the tree.

**Cuts** (dashed-lines) in the tree determine the number of groups. Thus, the position of the cut acts like $K$ in K-means.

]

---

## Hierarchical clustering

.pull-left[

```{r}

figure("12_13.png")

```

.right[Figure from [James *et al* (2021) ISLR](https://www.statlearning.com/)]
]

.pull-right[

**How is it?**

- Starts by measuring all pairwise differences between individual observations (classified as clusters of one).
- The two clusters that are the least dissimilar are fused, with the height of the fusion determined by the degree of difference.
- Compute differences for the new set of clusters.
- Fuse.
- Rinse and repeat until all clusters are fused into one.

]

---
class: middle center

## There's a lot more to consider here, naturally, but...


---
class: middle center

```{r}

figure("yenn.jpg")

```



---

## &#x1F52D; Looking Ahead

```{r}

schedule <- here("_misc", "course_outline.xlsx") %>% 
  readxl::read_excel(1) %>% 
  mutate(
    date = lubridate::ymd(date),
    description = glue::glue(
    "<b>{topic}</b> <span style = 'font-size: 80%; color: #9C9C9C;'>({date})</span><br>
    <div style = 'font-size: 80%; font-style: italic; color: #9C9C9C;'>{details}</div>"
  )) %>% 
  select(description)

```

.pull-left[

```{r}

schedule %>% 
  slice(15:16) %>% 
  kbl(escape = FALSE, col.names = NULL) %>% 
  kable_paper(c("striped", "hover"))

```

]