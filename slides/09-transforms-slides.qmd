---
title: "Lecture 09: Transforming Variables"  
date: 'Last updated: `r Sys.Date()`'
---

```{r setup}
#| include: false

here::here("slides", "_setup.R") |> source()

```

## ðŸ“‹ Lecture Outline


## Polynomial transformations for non-linearity

:::::: {.r-stack}
::: {.fragment .fade-out fragment-index=0}

```{r}

set.seed(25)

n <- 100

observations <- tibble(
  x = seq(0, 1, length = n),
  y = 1 - 3.7 * (x - 0.5)^2 + (0.75 * x) + rnorm(n, sd = 0.2)
) |> 
  mutate(
    y = scales::rescale(y, c(0.1, 0.9))
  )

bad_mod <- lm(y ~ x, data = observations)

bad_b0 <- coefficients(bad_mod)[[1]]
bad_b1 <- coefficients(bad_mod)[[2]]

bad_fit <- tibble(
  observation = 1:n,
  estimate = predict(bad_mod),
  residuals = residuals(bad_mod)
)

bad_r2 <- summary(bad_mod)$r.squared

```

__Simple Linear Model:__ $y_{i} = \beta_{0} + \beta_{1}X + \epsilon_{i}$  
$R^2 = `r bad_r2`$

```{r}
#| out-width: 100%
#| layout-ncol: 2

ggplot(observations, aes(x,y)) +
  geom_abline(
    intercept = bad_b0,
    slope = bad_b1,
    color = qcolors("kobe"),
    size = 1
  ) + 
  geom_point(
    size = 3
  ) +
  labs(
    x = "X",
    y = "Y"
  ) +
  theme(
    axis.text = element_blank(),
    axis.ticks = element_blank()
  )

ggplot(bad_fit, aes(estimate, residuals)) +
  geom_segment(
    aes(xend = estimate, yend = 0),
    linetype = "dashed"
  ) +
  geom_hline(
    aes(yintercept = 0),
    color = qcolors("kobe"),
    size = 1
  ) +
  geom_point(
    size = 3
  ) +
  labs(
    x = "Estimates",
    y = "Residuals"
  ) +
  theme(
    axis.text = element_blank(),
    axis.ticks = element_blank()
  )

```

:::
::: {.fragment .fade-in fragment-index=0}

```{r}

meh_mod <- lm(y ~ poly(x, 2), data = observations)

meh_fit <- tibble(
  observation = 1:n,
  estimate = predict(meh_mod),
  residuals = residuals(meh_mod)
)

meh_r2 <- summary(meh_mod)$r.squared

```

__Quadratic Model:__ $y_{i} = \beta_{0} + \beta_{1}X + \beta_{2}X^{2} + \epsilon_{i}$  
$R^2=`r meh_r2`$

```{r}
#| out-width: 100%
#| layout-ncol: 2

ggplot(observations, aes(x,y)) +
  geom_line(
    aes(y = meh_fit$estimate),
    color = qcolors("kobe"),
    size = 1
  ) + 
  geom_point(
    size = 3
  ) +
  labs(
    x = "X",
    y = "Y"
  ) +
  theme(
    axis.text = element_blank(),
    axis.ticks = element_blank()
  )

ggplot(meh_fit, aes(estimate, residuals)) +
  geom_segment(
    aes(xend = estimate, yend = 0),
    linetype = "dashed"
  ) +
  geom_hline(
    aes(yintercept = 0),
    color = qcolors("kobe"),
    size = 1
  ) +
  geom_point(
    size = 3
  ) +
  labs(
    x = "Estimates",
    y = "Residuals"
  ) +
  theme(
    axis.text = element_blank(),
    axis.ticks = element_blank()
  )

```


:::
::::::

## Log transformations

```{r}

n <- 100

x <- seq(0.1,5,length.out = n)

set.seed(1)

e <- rnorm(n, mean = 0, sd = 0.2)

y <- exp(0.3 + 0.5 * x + e)

```

:::::: {.columns}
::::: {.column}

```{r}
#| fig-asp: 0.9

dat <- tibble(
  v = rep(c("Y", "log(Y)"), each = n),
  y = c(y, log(y))
)

txt <- tibble(
  x = c(3.2, 14),
  y = c(35, 6),
  v = c("log(Y)", "Y")
)

ggplot() +
  geom_histogram(
    data = dat,
    position = "identity",
    aes(y, color = v, fill = v),
    bins = 20
  ) +
  scale_color_manual(values = qcolors("blue", "bronze")) +
  scale_fill_manual(values = alpha(qcolors("blue", "bronze"), 0.75)
  ) +
  geom_text(
    data = txt,
    aes(x, y, color = v, label = v),
    size = 8,
    hjust = 0
  ) +
  labs(
    x = "Value",
    y = "Count"
  ) +
  theme(
    axis.text = element_blank(),
    axis.ticks = element_blank(),
    legend.position = "none"
  )

```

:::::
::::: {.column}

__Logging__ a skewed variable normalizes it, spreading out clumped data and clumping dispersed data.  

This is the inverse of __exponentiation__: 

$$Y = log(exp(Y))$$  

We can apply these transformations to both the dependent $(Y)$ and independent variables $(X)$.  

__Question__: but, whyyyyyy???

:::::
::::::

## Log transformations for non-linearity

:::::: {.r-stack}
::: {.fragment .fade-out fragment-index=0}

```{r}

linear_mod <- lm(y ~ x)

linear_b0 <- coefficients(linear_mod)[[1]]
linear_b1 <- coefficients(linear_mod)[[2]]

linear_fit <- tibble(
  observation = 1:n,
  estimate = predict(linear_mod),
  residuals = residuals(linear_mod)
)

linear_r2 <- summary(linear_mod)$r.squared

```

__Simple Linear Model:__ $y_{i} = \beta_{0} + \beta_{1}X + \epsilon_{i}$  
$R^2 = `r linear_r2`$

```{r}
#| out-width: 100%
#| layout-ncol: 2

ggplot() +
  geom_abline(
    intercept = linear_b0,
    slope = linear_b1,
    color = qcolors("kobe")
  ) +
  geom_point(
    aes(x, y),
    size = 3
  ) +
  labs(
    x = "X",
    y = "Y"
  ) +
  theme(
    axis.text = element_blank(),
    axis.ticks = element_blank()
  )

ggplot(linear_fit, aes(estimate, residuals)) +
  geom_segment(
    aes(xend = estimate, yend = 0),
    linetype = "dashed"
  ) +
  geom_hline(
    aes(yintercept = 0),
    color = qcolors("kobe"),
    size = 1
  ) +
  geom_point(
    size = 3
  ) +
  labs(
    x = "Estimates",
    y = "Residuals"
  ) +
  theme(
    axis.text = element_blank(),
    axis.ticks = element_blank()
  )

```

:::
::: {.fragment .fade-in fragment-index=0}

```{r}

log_mod <- lm(log(y) ~ x)

log_b0 <- coefficients(log_mod)[[1]]
log_b1 <- coefficients(log_mod)[[2]]

log_fit <- tibble(
  observation = 1:n,
  estimate = predict(log_mod),
  residuals = residuals(log_mod)
)

log_r2 <- summary(log_mod)$r.squared

```

__Log Linear Model:__ $log(y_{i}) = \beta_{0} + \beta_{1}X + \epsilon_{i}$  
$R^2 = `r log_r2`$

```{r}
#| out-width: 100%
#| layout-ncol: 2

ggplot() +
  geom_abline(
    intercept = log_b0,
    slope = log_b1,
    color = qcolors("kobe")
  ) +
  geom_point(
    aes(x, log(y)),
    size = 3
  ) +
  labs(
    x = "X",
    y = "log(Y)"
  ) +
  theme(
    axis.text = element_blank(),
    axis.ticks = element_blank()
  )

ggplot(log_fit, aes(estimate, residuals)) +
  geom_segment(
    aes(xend = estimate, yend = 0),
    linetype = "dashed"
  ) +
  geom_hline(
    aes(yintercept = 0),
    color = qcolors("kobe"),
    size = 1
  ) +
  geom_point(
    size = 3
  ) +
  labs(
    x = "log(Estimates)",
    y = "Residuals"
  ) +
  theme(
    axis.text = element_blank(),
    axis.ticks = element_blank()
  )

```

:::
::::::

## âš ï¸ [Careful!]{.kobe}

Have to take care how we interpret log-models!  

$\beta_1$ means...

<br>

:::::: {.columns}
::::: {.column}
__Linear Model__  
$y_{i} = \beta_{0} + \beta_{1}X + \epsilon_{i}$  
_Increase in Y for one unit increase in X._  

<br>

__Log-Linear Model__  
$log(y_{i}) = \beta_{0} + \beta_{1}X + \epsilon_{i}$  
_Percent increase in Y for one unit increase in X._  
:::::
::::: {.column}
__Linear-Log Model__  
$y_{i} = \beta_{0} + \beta_{1}log(X) + \epsilon_{i}$   
_Increase in Y for percent increase in X._  

<br>

__Log-Log Model__  
$log(y_{i}) = \beta_{0} + \beta_{1}log(X) + \epsilon_{i}$  
_Percent increase in Y for percent increase in X._  
:::::
::::::