---
title: "Lecture 04: Linear Models 1"  
date: 'Last updated: `r Sys.Date()`'
output:
  xaringan::moon_reader:
    lib_dir: libs
    css: [xaringan-themer.css, custom_style.css]
    seal: true
    nature:
      highlightStyle: magula
      highlightLines: true
      countIncrementalSlides: false
      ratio: '16:9'
---

```{r, include = FALSE, code=xfun::read_utf8(here::here("slides", "before_chunk.R"))}
```

```{r}

# data libraries
library(archdata)

```

```{r}

data(PitHouses)

pithouses <- PitHouses %>% 
  as_tibble() %>% 
  rename_with(tolower) %>% 
  mutate(id = 1:n()) %>% 
  select(id, size, depth)

remove(PitHouses)

data(cars)

set.seed(12345)

cars <- cars %>% 
  as_tibble() %>% 
  slice_sample(n = 10) %>% 
  rename("distance" = dist) %>% 
  mutate(
    id = 1:n(),
    speed = speed * 0.44704,
    distance = distance * 0.3048,
    distance = round(distance, digits = 1)
  )

```

## &#x1F4CB; Lecture Outline

- &#x1F9EA; A simple experiment
- Competing Models
- A simple formula
- Bivariate statistics
    - Covariance
    - Correlation
    - Non-linear correlation
    - Correlation between categories
    - Correlation is not causation!
- A general formula
- Simple Linear Regression
- Ordinary Least Squares (OLS)
- Multiple Linear Regression
- &#x1F697; Cars Model, again
- Regression assumptions


---

## &#x1F9EA; A simple experiment

.pull-left[

```{r}

gg_cars <-
  ggplot(cars) +
  scale_y_continuous(limits = c(-2, max(cars$distance) + 2)) +
  geom_hline(yintercept = 0, size = 1) +
  geom_vline(xintercept = 0, size = 1) +
  theme(
    panel.border = element_rect(color = "gray75", fill = "transparent")
  )

gg_cars + 
  scale_x_continuous(breaks = 0:11, limits = c(0, 11.2)) + 
  labs(
    x = "Car ID", 
    y = "Distance (m)", 
    title = "Distance to complete stop"
  ) +
  geom_segment(
    aes(x = id, xend = id, y = 0, yend = distance),
    linetype = "dashed",
    color = "gray35",
    size = 0.5
  ) +
  geom_point(
    aes(x = id, y = distance),
    size = 3.5
    ) +
  geom_text(
    aes(x = id, y = distance, label = distance),
    vjust = 0,
    nudge_y = 1,
    color = "gray35",
    size = 5
  ) +
  annotate(
    geom = "segment", 
    x = 11, y = 0,
    xend = 11, yend = 5,
    size = 0.5,
    linetype = "dashed",
    color = "#A20000"
  ) +
  annotate(
    geom = "text", 
    x = 11, y = 6.5,
    label = "?",
    size = 10,
    color = "#A20000"
  )

```

]

.pull-right[

<br>

We take ten cars, send each down a track, have them brake at the same point, and measure the distance it takes them to stop.  

__Question:__ how far do you think it will take the next car to stop?

]


---

## Competing Models

.pull-left[

```{r}

barx <- mean(cars$distance)
mu_sigma <- sd(cars$distance)

gg_cars + 
  scale_x_continuous(breaks = 0:11, limits = c(0, 11.2)) + 
  labs(
    x = "Car ID", 
    y = "Distance (m)", 
    title = "Simple Model"
  ) +
  geom_segment(
    aes(x = id, xend = id, y = distance, yend = barx),
    linetype = "dashed",
    color = "gray35",
    size = 0.5
  ) +
  geom_point(
    aes(x = id, y = distance),
    size = 3.5
    ) +
  geom_hline(
    aes(yintercept = barx),
    color = "#A20000",
    size = 1
  ) +
  annotate(
    "segment",
    x = 11,
    y = 0,
    xend = 11,
    yend = barx - 0.1,
    arrow = arrow(
      length = unit(0.18, "inches"), 
      type = "closed"
    )
  )

```

.right[Expectation: mean distance &nbsp;&nbsp;]

]


---

## Competing Models

.pull-left[

```{r}

gg_cars + 
  scale_x_continuous(breaks = 0:11, limits = c(0, 11.2)) + 
  labs(
    x = "Car ID", 
    y = "Distance (m)", 
    title = "Simple Model"
  ) +
  geom_segment(
    aes(x = id, xend = id, y = distance, yend = mean(distance)),
    linetype = "dashed",
    color = "gray35",
    size = 0.5
  ) +
  geom_point(
    aes(x = id, y = distance),
    size = 3.5
    ) +
  geom_hline(
    aes(yintercept = mean(distance)),
    color = "#A20000",
    size = 1
  ) +
  annotate(
    "segment",
    x = 11,
    y = 0,
    xend = 11,
    yend = barx - 0.1,
    arrow = arrow(
      length = unit(0.18, "inches"), 
      type = "closed"
    )
  )

```

.right[Expectation: mean distance &nbsp;&nbsp;]

]

.pull-right[

```{r}

cars_lm <- lm(distance ~ speed, data = cars)

estimate <- predict(cars_lm, cars) %>% unname()

beta0 <- cars_lm$coefficients[[1]]
beta1 <- cars_lm$coefficients[["speed"]]

lm_sigma <- summary(cars_lm)$sigma


lm_cars <- gg_cars + 
  scale_x_continuous(breaks = seq(0, 10, by = 2), limits = c(0, 11.2)) + 
  labs(
    x = "Speed (m/s)", 
    y = "Distance (m)"
  ) +
  geom_segment(
    aes(x = speed, xend = speed, y = distance, yend = estimate),
    linetype = "dashed",
    color = "gray35",
    size = 0.5
  ) +
  geom_point(
    aes(x = speed, y = distance),
    size = 3.5
    ) +
  geom_abline(
    slope = beta1,
    intercept = beta0,
    color = "#A20000",
    size = 1
  )

lm_cars + ggtitle("Complex Model")

```

.right[Expectation: some function of speed &nbsp;&nbsp;]

]





---

## A simple formula

.pull-left[

.w-100.bg-near-white.ba.bw1.br2.shadow-4.ph3.mb3.h6rem[

$$y_i = E(Y) + \epsilon_i$$
$$\epsilon \sim N(0, \sigma)$$

]

.center[

.pull-left.bg-dark_purple_o_70.ba.bw1.br2.shadow-4.pt4.pa3.h5[.white[

.f4.fw6[Simple Model]

$$
\begin{align}
E(Y) &= \mu  \\\\
\bar{y} &=`r barx`  \\\\
\sigma &= `r mu_sigma` \\
\end{align}
$$   

]]

.pull-right.bg-maize_crayola_o_50.ba.bw1.br2.shadow-4.pt4.pa3.h5[

.f4.fw6[Complex Model]

$$
\begin{align}
E(Y) &= \beta X \\\\ 
\hat\beta &= `r beta1` \\\\
\sigma &= `r lm_sigma` \\
\end{align}
$$   

]

]

]



---
count: false

## A simple formula

.pull-left[

.w-100.bg-near-white.ba.bw1.br2.shadow-4.ph3.mb3.h6rem[

$$y_i = E(Y) + \epsilon_i$$
$$\epsilon \sim N(0, \sigma)$$

]

.center[

.pull-left.bg-dark_purple_o_70.ba.bw1.br2.shadow-4.pt4.pa3.h5[.white[

.f4.fw6[Simple Model]

$$
\begin{align}
E(Y) &= \mu  \\\\
\bar{y} &=`r barx`  \\\\
\sigma &= `r mu_sigma` \\
\end{align}
$$    

]]

.pull-right.bg-maize_crayola_o_50.ba.bw1.br2.shadow-4.pt4.pa3.h5[

.f4.fw6[Complex Model]

$$
\begin{align}
E(Y) &= \beta X \\\\ 
\hat\beta &= `r beta1` \\\\
\sigma &= `r lm_sigma` \\
\end{align}
$$   

]

]

]

.pull-right[

&#x2696;&#xFE0F; __The error is smaller__ for the more complex model. This is a good thing, __but what did it cost us__? Need to weigh this against the increased complexity!  

```{r, out.width = "100%", out.extra = "style='margin-top: 40px;'"}

dists <- tibble(
  x = seq(-20, 20, length = 500),
  Simple = dnorm(x, sd = mu_sigma),
  Complex = dnorm(x, sd = lm_sigma)
) %>% 
  pivot_longer(
    -x, 
    names_to = "models", 
    values_to = "y"
  ) %>% 
  arrange(x, models) %>% 
  mutate(
    models = as_factor(models),
    models = fct_relevel(models, "Simple", "Complex")
  )

ggplot(dists, aes(x, y, fill = models, color = models)) +
  geom_polygon(size = 1) +
  scale_color_manual(
    name = NULL,
    values = c("#0F0326", "#FACC4E"),
    guide = "none"
  ) +
  scale_fill_manual(
    name = NULL,
    values = alpha(c("#0F0326", "#FACC4E"), 0.5),
    guide = "none"
  ) +
  scale_y_continuous(
    breaks = c(0.05, 0.10)
  ) +
  labs(
    x = "Distance (m)",
    y = NULL,
    title = "Distribution of Errors"
  ) +
  theme(
    panel.grid.major.y = element_line(color = "gray92"),
    axis.text.y = element_blank(),
    axis.ticks.y = element_blank(),
    axis.title.y = element_blank(),
    plot.margin = margin()
  )

```

]


---

## Bivariate statistics

.center[Explore the relationship between two variables:]

```{r, fig.asp = 0.35, out.width = "75%"}

line_data <- tibble(
  x = seq(0, 10, length = 500),
  linear = x,
  nonlinear = dnorm(x, mean = 4)/max(dnorm(x, mean = 4)) * 7.5,
  positive = 0.7 * x + 2,
  negative = -0.3 * x + 7,
  weak = 0.1 * x,
  strong = 1.3 * x
) %>% 
  pivot_longer(-x, values_to = "y") %>% 
  arrange(name, x) %>% 
  mutate(
    relationship = case_when(
      name %in% c("linear", "nonlinear") ~ "Form",
      name %in% c("positive", "negative") ~ "Direction",
      name %in% c("weak", "strong") ~ "Strength",
      TRUE ~ NA_character_
    )
  ) %>% 
  select(relationship, name, x, y) %>% 
  filter(y <= 10)

line_labels <- tibble(
  relationship = c("Direction", "Form", "Strength"),
  x = 0, 
  y = 9.85
)

ggplot() +
  geom_line(
    data = line_data, 
    aes(x, y, group = name),
    color = "#477998",
    arrow = arrow(length=unit(0.20,"cm"), type = "closed")
  ) +
  geom_text(
    data = line_labels,
    aes(x, y, label = relationship),
    hjust = 0,
    vjust = 1,
    size = 5
  ) +
  ylim(0, 10) +
  facet_wrap(~relationship) +
  coord_fixed() +
  theme(
    axis.text = element_blank(),
    axis.ticks = element_blank(),
    axis.title = element_blank(),
    plot.margin = margin(),
    strip.background = element_blank(),
    strip.text = element_blank()
  )

remove(line_data, line_labels)

```


---

## Covariance

.pull-left[

The extent to which two variables vary together. 

$$cov(x, y) = \frac{1}{n-1} \sum_{i=1}^{n}  (x_i - \bar{x})(y_i - \bar{y})$$

* Sign reflects positive or negative trend, but magnitude depends on units of measure (e.g., $cm$ vs $km$).  
* Variance is the covariance of a variable with itself.  

]

.pull-right[

```{r}

n <- 50

# average height and weight for men in the UK
mean_height <- 175.3
mean_weight <- 83.6

height <- rnorm(n, mean = mean_height) %>% sort()
weight <- (mean_weight + (4 * scale(height)) + rnorm(n, sd = 2))

body_data <- tibble(
  covariance = rep(c("Positive", "Negative"), each = n),
  height = rep(height, 2), 
  weight = c(weight, rev(weight))  
)

body_labels <- tibble(
  covariance = c("Negative", "Positive"),
  x = min(height),
  y = mean(weight) + c(-1.8, 1.8)
)

remove(n, mean_weight, mean_height, height, weight)

bob <- ggplot(body_data, aes(height, weight)) +
  geom_hline(
    aes(yintercept = mean(weight)), 
    linetype = "dashed", 
    color = "gray55"
  ) +
  geom_vline(
    aes(xintercept = mean(height)), 
    linetype = "dashed", 
    color = "gray55"
  ) +
  geom_point(
    shape = 21,
    color = "#A20000",
    fill = alpha("#A20000", 0.6),
    size = 5
  ) +
  geom_text(
    data = body_labels,
    aes(x, y, label = covariance),
    hjust = 0,
    vjust = 0.5,
    size = 7
  ) +
  facet_wrap(~covariance, nrow = 2) +
  labs(
    x = "Height (cm)",
    y = "Weight (kg)"
  ) +
  theme(
    plot.margin = margin(),
    strip.background = element_blank(),
    strip.text = element_blank()
  )

bob

```

]


---

## Correlation

.pull-left[

Pearson's Correlation Coefficient

$$r = \frac{cov(x,y)}{s_{x}s_y}$$

- Normalizes covariance (scales from -1 to 1) using standard deviations, $s$, thus making the magnitude of $r$ independent of units of measure.  

- The significance of $r$ can be tested by converting it to a _t_-statistic and comparing it to a _t_-distribution with $n-2$ degrees of freedom.

]

.pull-right[

```{r}

bob

remove(bob)

```

]



---

## Nonlinear correlation

.pull-left[

Spearman's Rank Correlation Coefficient

$$\rho = \frac{cov\left(R(x),\; R(y) \right)}{s_{R(x)}s_{R(y)}}$$

- Pearson's correlation but with ranks (R).  
- This makes it a _robust_ estimate, less sensitive to outliers.  

]

.pull-right[

```{r}

body_data <- body_data %>% 
  mutate(
    height = if_else(height > 176.5, height + 0.3*(max(height) - height), height),
    dx = if_else(height <= 176.5, 0, (height - 176.5) / (max(height) - 176.5)),
    delta_neg = abs(90 - weight) * dx + rnorm(n(), sd = 1.3),
    delta_pos = abs(75 - weight) * dx + rnorm(n(), sd = 1.3),
    weight = case_when(
      height > 176.5 & covariance == "Negative" ~ weight + delta_neg,
      height > 176.5 & covariance == "Positive" ~ weight - delta_pos,
      TRUE ~ weight 
    )
  )

ggplot(body_data, aes(height, weight)) +
  geom_hline(
    aes(yintercept = mean(weight)), 
    linetype = "dashed", 
    color = "gray55"
  ) +
  geom_vline(
    aes(xintercept = mean(height)), 
    linetype = "dashed", 
    color = "gray55"
  ) +
  geom_point(
    shape = 21,
    color = "#A20000",
    fill = alpha("#A20000", 0.6),
    size = 5
  ) +
  facet_wrap(~covariance, nrow = 2) +
  labs(
    x = "Height (cm)",
    y = "Weight (kg)"
  ) +
  theme(
    plot.margin = margin(),
    strip.background = element_blank(),
    strip.text = element_blank()
  )

```

]



---

## Correlation between categories

.pull-left[

$\chi^2$-statistic

$$\chi^2 = \sum \frac{(y_{ij}-E(y_{ij}))^2}{E(y_{ij})}$$

- Where $y_{ij}$ is the observed count in row $i$ and column $j$ and $E(y_{ij})$ is the expected count in $ij$.
- To get the significance of this statistic, compare it to a $\chi^2$-distribution with $k-1$ degrees of freedom ($k$ being the number of categories).

]

.pull-right[

```{r, fig.asp = 0.9}

ggplot(pithouses, aes(y = size, fill = depth, color = depth)) + 
  geom_bar(position = "dodge2") +
  scale_color_manual(
    name = NULL,
    values = c("#0F0326", "#FACC4E")
  ) +
  scale_fill_manual(
    name = NULL,
    values = alpha(c("#0F0326", "#FACC4E"), 0.5)
  ) +
  theme(
    legend.position = c(0.98, 0.98),
    legend.justification = c("right", "top")
  ) +
  labs(
    x = "Count",
    y = NULL,
    title = "Pithouse Size by Depth",
    subtitle = "From Late Stone Age Arctic Norway"
  )

```

]



---

## Correlation is not causation!

<br>

```{r, fig.asp = 0.4, fig.width = 720/72, out.width = "70%"}

cage <- tibble(
  year = 1999:2009,
  films = c(2, 2, 2, 3, 1, 1, 2, 3, 4, 1, 4),
  drownings = c(109, 102, 102, 98, 85, 95, 96, 98, 123, 94, 102),
  helicopters = c(59, 64, 56, 48, 79, 75, 42, 49, 47, 69, 43)
) %>% 
  mutate(
    films_sc = scales::rescale(films, to = c(min(drownings), max(drownings)))
  )

film_color <- "#0F0326"
drowning_color <- "#EFB106"

ggplot(cage) + 
  geom_smooth(
    aes(year, drownings),
    color = alpha(drowning_color, 0.1),
    span = 0.4, 
    se = FALSE,
    size = 3
  ) +
  geom_smooth(
    aes(year, drownings),
    color = drowning_color, 
    span = 0.4, 
    se = FALSE,
    size = 0.6
  ) +
  geom_point(
    aes(year, drownings),
    shape = 21,
    color = drowning_color, 
    fill = "white",
    size = 3.5
  ) +
  geom_smooth(
    aes(year, films_sc),
    color = alpha(film_color, 0.1),
    span = 0.4, 
    se = FALSE,
    size = 3
  ) +
  geom_smooth(
    aes(year, films_sc),
    color = film_color, 
    span = 0.4, 
    se = FALSE,
    size = 0.6
  ) +
  geom_point(
    aes(year, films_sc),
    shape = 21,
    color = film_color, 
    fill = "white",
    size = 3.5
  ) +
  scale_x_continuous(
    expand = expansion(add = 0.2),
    breaks = seq(1999, 2009, by = 2)
  ) +
  scale_y_continuous(
    name = "Number of people who\ndrowned falling into a pool",
    sec.axis = dup_axis(
      name = "Number of films\nNicolas Cage appeared in",
      breaks = with(cage, seq(min(drownings), max(drownings), length = 3)),
      labels = seq(1, 4, length = 3)
    )
  ) +
  annotate(
    geom = "text",
    x = 1999,
    y = 123,
    label = paste0(
      "r = ",
      with(cage, round(cor(films, drownings), digits = 2))
    ),
    hjust = 0,
    size = 6.5,
    color = "gray20"
  ) +
  labs(x = "Year") +
  theme(
    axis.text.x        = element_text(size = 10),
    axis.text.y.left   = element_text(color = drowning_color, size = 10),
    axis.text.y.right  = element_text(color = film_color, size = 10),
    axis.title.y.left  = element_text(color = drowning_color, size = 14),
    axis.title.y.right = element_text(color = film_color, size = 14)
  )
  
```

.center[Adapted from <https://www.tylervigen.com/spurious-correlations>]





---

## A general formula

```{r}

arrow_right <- fontawesome::fa(
    "arrow-circle-right", 
    fill = "gray45",
    height = "2em",
    width = "2em"
  )

```

.flex[
.ph4.mr3.bg-near-white.ba.bw1.br2.shadow-4.flex.items-center[.center[
$$y_i = E(Y) + \epsilon_i$$
$$E(Y) \approx \hat\beta X$$
]]
.flex.items-center.o-0[.center[`r arrow_right`]]
.ph4.mh3.bg-maize_crayola_o_50.ba.bw1.br2.shadow-4.flex.items-center.o-0[.center[
$$y_i = \hat\beta X + \epsilon_i$$
]]
.flex.items-center.o-0[.center[`r arrow_right`]]
.ph4.mh3.bg-dark_purple_o_70.ba.bw1.br2.shadow-4.flex.items-center.o-0[.center.white[
$$y_i = \hat\beta_0 + \hat\beta_1 x_i + \ldots +  \hat\beta_n x_i + \epsilon_i$$
]]
]

<br>

- $y_i$ is the .fw6.dark_purple[dependent variable]  
- $X$ is a set of .fw6.dark_purple[independent variables] $x_{i1}, \ldots, x_{in}$  
    - called the design matrix
    - typically includes a constant (1) for the intercept
- $\beta$ is a set of .fw6.dark_purple[coefficients of relationship]
    - $\beta_0$ is the intercept
    - $\beta_1, \ldots, \beta_n$ are the coefficients for independent variables $x_{i1}, \ldots, x_{in}$  
- $\epsilon_i$ is the .fw6.dark_purple[residuals] or errors 





---
count: false

## A general formula

.flex[
.ph4.mr3.bg-near-white.ba.bw1.br2.shadow-4.flex.items-center[.center[
$$y_i = E(Y) + \epsilon_i$$
$$E(Y) \approx \hat\beta X$$
]]
.flex.items-center[.center[`r arrow_right`]]
.ph4.mh3.bg-maize_crayola_o_50.ba.bw1.br2.shadow-4.flex.items-center[.center[
$$y_i = \hat\beta X + \epsilon_i$$
]]
.flex.items-center.o-0[.center[`r arrow_right`]]
.ph4.mh3.bg-dark_purple_o_70.ba.bw1.br2.shadow-4.flex.items-center.o-0[.center.white[
$$y_i = \hat\beta_0 + \hat\beta_1 x_i + \ldots +  \hat\beta_n x_i + \epsilon_i$$
]]
]

<br>

- $y_i$ is the .fw6.dark_purple[dependent variable]  
- $X$ is a set of .fw6.dark_purple[independent variables] $x_{i1}, \ldots, x_{in}$  
    - called the design matrix
    - typically includes a constant (1) for the intercept
- $\beta$ is a set of .fw6.dark_purple[coefficients of relationship]
    - $\beta_0$ is the intercept
    - $\beta_1, \ldots, \beta_n$ are the coefficients for independent variables $x_{i1}, \ldots, x_{in}$  
- $\epsilon_i$ is the .fw6.dark_purple[residuals] or errors 

---
count: false

## A general formula

.flex[
.ph4.mr3.bg-near-white.ba.bw1.br2.shadow-4.flex.items-center[.center[
$$y_i = E(Y) + \epsilon_i$$
$$E(Y) \approx \hat\beta X$$
]]
.flex.items-center[.center[`r arrow_right`]]
.ph4.mh3.bg-maize_crayola_o_50.ba.bw1.br2.shadow-4.flex.items-center[.center[
$$y_i = \hat\beta X + \epsilon_i$$
]]
.flex.items-center[.center[`r arrow_right`]]
.ph4.mh3.bg-dark_purple_o_70.ba.bw1.br2.shadow-4.flex.items-center[.center.white[
$$y_i = \hat\beta_0 + \hat\beta_1 x_{i1} + \ldots +  \hat\beta_n x_{in} + \epsilon_i$$
]]
]

<br>

- $y_i$ is the .fw6.dark_purple[dependent variable]  
- $X$ is a set of .fw6.dark_purple[independent variables] $x_{i1}, \ldots, x_{in}$  
    - called the design matrix
    - typically includes a constant (1) for the intercept
- $\beta$ is a set of .fw6.dark_purple[coefficients of relationship]
    - $\beta_0$ is the intercept
    - $\beta_1, \ldots, \beta_n$ are the coefficients for independent variables $x_{i1}, \ldots, x_{in}$  
- $\epsilon_i$ is the .fw6.dark_purple[residuals] or errors 




---

## Simple Linear Regression

.pull-left[

```{r}

lm_cars + ggtitle("Cars Model")

```

]

.pull-right[

<br>

Only one explanatory variable: speed (m)

Equation: $y_i = \beta_0 + \beta_1 speed_i + \epsilon_i$

- $\beta_0$ = `r beta0`
- $\beta_1$ = `r beta1`

]


---

## Simple Linear Regression

.pull-left[

```{r}

lm_cars + ggtitle("Cars Model")

```

]

.pull-right[

<br>

Only one explanatory variable: speed (m)

Equation: $y_i = \beta_0 + \beta_1 speed_i + \epsilon_i$

- $\beta_0$ = `r beta0`
- $\beta_1$ = `r beta1`

__Question:__ How did we get these values?

]



---

## Ordinary Least Squares (OLS)

.pull-left[

A method for estimating the coefficients, $\beta$, in a linear regression model by minimizing the __Residual Sum of Squares__, $SS_{R}$.  

$$SS_{R} = \sum_{i=1}^{n} (y_{i}-\hat{y}_i)^2$$

where $\hat{y}=E(Y)$. To minimize this, we take its derivative with respect to $\beta$ and set it equal to zero.

$$\frac{d\, SS_{R}}{d\, \beta} = 0$$

]

.pull-right[

```{r}

ssr <- tibble(
  x = seq(0, 4, length.out = 500),
  y = 0.3 + (x - 2)^2
)

ggplot(ssr, aes(x, y)) +
  geom_hline(yintercept = 0.3, color = "#A20000") +
  geom_line() +
  geom_hline(yintercept = 0, size = 1) +
  geom_vline(xintercept = 0, size = 1) +
  scale_y_continuous(breaks = 0, labels = 0) +
  scale_x_continuous(breaks = 0, labels = 0) +
  labs(
    x = "Estimate of \u03B21", 
    y = "Residual Sum of Squares"
  ) +
  annotate(
    "segment", 
    x = 2, 
    y = 1.3, 
    xend = 2, 
    yend = 0.33, 
    arrow = arrow(length = unit(0.1, "inches"), type = "closed")
  ) +
  annotate(
    "text",
    x = 2, y = 1.3 + 0.1, 
    label = "Value of \u03B21 that\nminimizes SSR.",
    hjust = 0.5,
    vjust = 0,
    size = 7
  ) +
  theme(
    panel.border = element_rect(color = "gray75", fill = "transparent")
  )

```

]



---

## Ordinary Least Squares (OLS)

.pull-left[

Provides simple estimators for the coefficients:  

***
__Slope__ .gray[Ratio of covariance to variance]
$$\beta_{1} = \frac{cov(x, y)}{var(x)}$$ 

***
__Intercept__ .gray[Marginal difference in sample means]
$$\beta_{0} = \bar{y} - \beta_1 \bar{x}$$

If $\beta_{1} = 0$, then $\beta_{0} = \bar{y}$. 

]

.pull-right[

```{r}

ssr <- tibble(
  x = seq(0, 4, length.out = 500),
  y = 0.3 + (x - 2)^2
)

ggplot(ssr, aes(x, y)) +
  geom_hline(yintercept = 0.3, color = "#A20000") +
  geom_line() +
  geom_hline(yintercept = 0, size = 1) +
  geom_vline(xintercept = 0, size = 1) +
  scale_y_continuous(breaks = 0, labels = 0) +
  scale_x_continuous(breaks = 0, labels = 0) +
  labs(
    x = "Estimate of \u03B21", 
    y = "Residual Sum of Squares"
  ) +
  annotate(
    "segment", 
    x = 2, 
    y = 1.3, 
    xend = 2, 
    yend = 0.33, 
    arrow = arrow(length = unit(0.1, "inches"), type = "closed")
  ) +
  annotate(
    "text",
    x = 2, y = 1.3 + 0.1, 
    label = "Value of \u03B21 that\nminimizes SSR.",
    hjust = 0.5,
    vjust = 0,
    size = 7
  ) +
  theme(
    panel.border = element_rect(color = "gray75", fill = "transparent")
  )

```

]




---

## Multiple Linear Regression

.pull-left[

OLS can be extended to models containing multiple explanatory variables using matrix algebra. Then the coefficient estimator is:

$$\hat\beta = (X^{T}X)^{-1}X^{T}y$$

where $X^T$ is the transpose of the design matrix.  

_If you squint a little_, this is analogous to the simple linear estimator: 
- $X^{T}X$ &#x21E8; variance. 
- $X^{T}y$ &#x21E8; covariance.

]

--

.pull-right[

This is important for highlighting a critical assumption of linear regression: 

<br>

__No collinearity in the explanatory variables!__

<br>

If this is violated, $(X^{T}X)^{-1}$ can't be calculated.

]



---

## &#x1F697; Cars Model, again

.pull-left[

```{r}

lm_cars + ggtitle("Cars Model")

```

]

.pull-right[

```{r}

cv <- with(cars, cov(distance, speed))
vv <- with(cars, var(speed))

beta1 <- cv/vv

mu_distance <- mean(cars$distance)
mu_speed <- mean(cars$speed)

beta0 <- mu_distance - beta1 * mu_speed

```

<br>

.center[

.pull-left-38.bg-maize_crayola_o_50.ba.bw1.br2.shadow-4.pa2.h5[

.f4.fw6[Slope]  

$$
\begin{align}
\beta_{1} &= \frac{cov(x,y)}{var(x)} \\\\ 
\beta_{1} &= \frac{`r cv`}{`r vv`} \\\\
\beta_{1} &= `r beta1`
\end{align}
$$  

]

.pull-right-60.bg-dark_purple_o_70.ba.bw1.br2.shadow-4.pa2.h5[.white[

.f4.fw6[Intercept]

$$
\begin{align}
\beta_{0} &= \bar{y} - \beta_{1}\bar{x} \\\\ 
\beta_{0} &= `r mu_distance` - `r beta1` * `r mu_speed` \\\\
\beta_{0} &= `r beta0`
\end{align}
$$

]]

]

]


---

## Regression assumptions

1. __Weak Exogeneity__: the predictor variables have fixed values and are known. 

2. __Linearity__: the relationship between the predictor variables and the response variable is linear.  

2. __Constant Variance__: the variance of the errors does not depend on the values of the predictor variables. Also known as _homoscedasticity_.  

2. __Independence of errors__: the errors are uncorrelated with each other.  

2. __No perfect collinearity__: the predictors are not linearly correlated with each other.  



---

## &#x1F52D; Looking Ahead

```{r}

schedule <- here("_misc", "course_outline.xlsx") %>% 
  readxl::read_excel(1) %>% 
  mutate(
    date = lubridate::ymd(date),
    description = glue::glue(
    "<b>{topic}</b> <span style = 'font-size: 80%; color: #9C9C9C;'>({date})</span><br>
    <div style = 'font-size: 80%; font-style: italic; color: #9C9C9C;'>{details}</div>"
  )) %>% 
  select(description)

```

.pull-left[

```{r}

schedule %>% 
  slice(4:7) %>% 
  kbl(escape = FALSE, col.names = NULL) %>% 
  kable_paper(c("striped", "hover"))

```

]