---
title: "Lecture 04: Ordinary Least Squares"  
date: 'Last updated: `r Sys.Date()`'
---

```{r setup}
#| include: false

here::here("slides", "_setup.R") |> source()

```

```{r}
#| include: false

library(archdata)

data(PitHouses)

pithouses <- PitHouses |> 
  as_tibble() |> 
  rename_with(tolower) |> 
  mutate(id = 1:n()) |> 
  select(id, size, depth)

remove(PitHouses)

data(cars)

set.seed(12345)

cars <- cars |> 
  as_tibble() |> 
  slice_sample(n = 10) |> 
  rename("distance" = dist) |> 
  mutate(
    id = 1:n(),
    speed = speed * 0.44704,
    distance = distance * 0.3048,
    distance = round(distance, digits = 1)
  )

```

## üìã Lecture Outline

- üß™ A simple experiment
- Competing models
- Model complexity
- Bivariate statistics (covariance and correlation)
- A general formula
- Simple Linear Regression
- Ordinary Least Squares (OLS)
- Multiple Linear Regression
- &#x1F697; Cars Model, again
- Regression assumptions

## üß™ A simple experiment

:::::: {.columns}
::: {.column}

```{r}

# default settings for cars plots
defaults <- list(
  ylim(-2, max(cars$distance) + 2),
  theme(panel.border = element_rect(color = "gray65", fill = "transparent")),
  geom_hline(yintercept = 0),
  geom_vline(xintercept = 0)
)

ggplot(cars) +
  geom_segment(
    aes(x = id, xend = id, y = 0, yend = distance),
    linetype = "dashed",
    color = "gray60",
    linewidth = 1
  ) +
  geom_point(
    aes(x = id, y = distance),
    size = 4
  ) +
  geom_text(
    aes(x = id, y = distance, label = distance),
    vjust = 0,
    nudge_y = 1,
    color = "gray50"
  ) +
  annotate(
    geom = "segment", 
    x = 11, y = 0,
    xend = 11, yend = 5,
    linewidth = 0.5,
    linetype = "dashed",
    color = qcolors("kobe")
  ) +
  annotate(
    geom = "text", 
    x = 11, y = 6.5,
    label = "?",
    size = 10,
    color = qcolors("kobe")
  ) +
  labs(
    x = "Car ID", 
    y = "Distance (m)", 
    title = "Distance to complete stop"
  ) +
  scale_x_continuous(
    breaks = 0:11, 
    limits = c(0, 11.2)
  ) +
  defaults

```

:::
::: {.column}

<br>
We take ten cars, send each down a track, have them brake at the same point, and measure the distance it takes them to stop.  

__Question:__ how far do you think it will take the next car to stop?  
:::
::::::

## Competing Models

:::::: {.columns style="text-align:right;"}
::: {.column}

```{r}

ggplot(cars) +
  geom_segment(
    aes(x = id, xend = id, y = mean(distance), yend = distance),
    linetype = "dashed",
    color = "gray60",
    linewidth = 1
  ) +
  geom_point(
    aes(x = id, y = distance),
    size = 4
  ) +
  geom_hline(
    aes(yintercept = mean(distance)),
    color = qcolors("kobe"),
    linewidth = 1
  ) +
  labs(
    x = "Car ID", 
    y = "Distance (m)", 
    title = "Simple Model"
  ) +
  scale_x_continuous(
    breaks = 0:11, 
    limits = c(0, 11.2)
  ) +
  defaults

```

E[Y]: mean distance

:::
::: {.column}

```{r}

cars_lm <- lm(distance ~ speed, data = cars)

estimate <- predict(cars_lm, cars) |> unname()

beta0 <- cars_lm$coefficients[[1]]
beta1 <- cars_lm$coefficients[["speed"]]

sd_lm <- summary(cars_lm)$sigma

sd_y <- sd(cars$distance)

bar_y <- mean(cars$distance)

gg_cars <- ggplot(cars) +
  geom_segment(
    aes(x = speed, xend = speed, y = distance, yend = estimate),
    linetype = "dashed",
    color = "gray60",
    linewidth = 1
  ) +
  geom_point(
    aes(x = speed, y = distance),
    size = 4
    ) +
  geom_abline(
    slope = beta1,
    intercept = beta0,
    color = qcolors("kobe"),
    linewidth = 1
  ) +
  labs(
    x = "Speed (m/s)", 
    y = "Distance (m)", 
    title = "Complex Model"
  ) +
  scale_x_continuous(
    breaks = seq(0, 10, by = 2), 
    limits = c(0, 11.2)
  ) +
  defaults

gg_cars

```

E[Y]: some function of speed

:::
::::::

## Model Complexity

::: {.columns}
::: {.column}

```{r}
#| out-width: "100%"

dists <- tibble(
  x = seq(-20, 20, length = 500),
  Simple = dnorm(x, sd = sd_y),
  Complex = dnorm(x, sd = sd_lm)
) |> 
  pivot_longer(
    -x, 
    names_to = "models", 
    values_to = "y"
  ) |> 
  arrange(x, models) |> 
  mutate(
    models = as_factor(models),
    models = fct_relevel(models, "Simple", "Complex")
  )

dists_text <- dists |> 
  group_by(models) |> 
  summarize(y = max(y)) |> 
  mutate(x = c(4.5,2))

ggplot() +
  geom_polygon(
    data = dists, 
    aes(x, y, fill = models, color = models), 
    linewidth = 1
  ) +
  geom_text(
    data = dists_text,
    aes(x, y, label = models, color = models),
    hjust = 0,
    vjust = 1,
    size = 5
  ) +
  scale_color_manual(
    name = NULL,
    values = qcolors("sapphire", "gold"),
    guide = "none"
  ) +
  scale_fill_manual(
    name = NULL,
    values = alpha(qcolors("sapphire", "gold"), 0.5),
    guide = "none"
  ) +
  scale_y_continuous(
    breaks = c(0.05, 0.10)
  ) +
  labs(
    x = "Distance (m)",
    y = NULL,
    title = "Distribution of Errors"
  ) +
  theme(
    panel.grid.major.y = element_line(color = "gray92"),
    axis.text.y = element_blank(),
    axis.ticks.y = element_blank(),
    axis.title.y = element_blank(),
    plot.margin = margin()
  )

```

:::
::: {.column}

<br>

‚öñÔ∏è __The error is smaller__ for the more complex model. This is a good thing, __but what did it cost us__? Need to weigh this against the increased complexity! 

:::
:::

## Bivariate statistics

```{css}

.no-p-margins p {
  margin: 0;
}

```

::: {.no-p-margins style="width:80%;margin-top:1em;"}

Explore the relationship between two variables:

```{r}
#| out-width: "100%"
#| fig-asp: 0.33

line_data <- tibble(
  x = seq(0, 10, length = 500),
  linear = x,
  nonlinear = dnorm(x, mean = 4)/max(dnorm(x, mean = 4)) * 7.5,
  positive = 0.7 * x + 2,
  negative = -0.3 * x + 7,
  weak = 0.1 * x,
  strong = 1.3 * x
) |> 
  pivot_longer(-x, values_to = "y") |> 
  arrange(name, x) |> 
  mutate(
    relationship = case_when(
      name %in% c("linear", "nonlinear") ~ "Form",
      name %in% c("positive", "negative") ~ "Direction",
      name %in% c("weak", "strong") ~ "Strength",
      TRUE ~ NA_character_
    )
  ) |> 
  select(relationship, name, x, y) |> 
  filter(y <= 10)

line_labels <- tibble(
  relationship = c("Direction", "Form", "Strength"),
  x = 0, 
  y = 9.85
)

ggplot() +
  geom_line(
    data = line_data, 
    aes(x, y, group = name),
    color = qcolors("sapphire"),
    arrow = arrow(length=unit(0.20,"cm"), type = "closed"),
    linewidth = 1
  ) +
  geom_text(
    data = line_labels,
    aes(x, y, label = relationship),
    hjust = 0,
    vjust = 1,
    size = 4
  ) +
  ylim(0, 10) +
  facet_wrap(~relationship) +
  coord_fixed() +
  theme(
    axis.text = element_blank(),
    axis.ticks = element_blank(),
    axis.title = element_blank(),
    plot.margin = margin(-1,0,-1,-2),
    strip.background = element_blank(),
    strip.text = element_blank()
  )

```

:::

## Covariance

:::::: {.columns}
::: {.column width="47%"}

The extent to which two variables vary together. 

$$cov(x, y) = \frac{1}{n-1} \sum_{i=1}^{n}  (x_i - \bar{x})(y_i - \bar{y})$$

* Sign reflects positive or negative trend, but magnitude depends on units (e.g., $cm$ vs $km$).  
* Variance is the covariance of a variable with itself.  

:::
::: {.column width="53%"}

```{r}
#| fig-asp: 0.85

n <- 50

# average height and weight for men in the UK
mean_height <- 175.3
mean_weight <- 83.6

height <- rnorm(n, mean = mean_height) |> sort()
weight <- (mean_weight + (4 * scale(height)) + rnorm(n, sd = 2))

body_data <- tibble(
  covariance = rep(c("Positive", "Negative"), each = n),
  height = rep(height, 2), 
  weight = c(weight, rev(weight))  
)

body_labels <- tibble(
  covariance = c("Negative", "Positive"),
  x = min(height),
  y = mean(weight) + c(-1.5, 1.5)
)

remove(n, mean_weight, mean_height, height, weight)

bob <- function(x){
  
  ggplot(x, aes(height, weight)) +
    geom_hline(
      aes(yintercept = mean(weight)), 
      linetype = "dashed", 
      color = "gray55"
    ) +
    geom_vline(
      aes(xintercept = mean(height)), 
      linetype = "dashed", 
      color = "gray55"
    ) +
    geom_point(
      shape = 21,
      color = qcolors("kobe"),
      fill = alpha(qcolors("kobe"), 0.6),
      size = 5
    ) +
    facet_wrap(vars(covariance), nrow = 2) +
    labs(
      x = "Height (cm)",
      y = "Weight (kg)"
    ) +
    theme(
      plot.margin = margin(),
      strip.background = element_blank(),
      strip.text = element_blank()
    )
  
}

bob(body_data) +
  geom_text(
    data = body_labels,
    aes(x, y, label = covariance),
    hjust = 0,
    vjust = c(1,0),
    size = 7
  )

```

:::
::::::

## Correlation

:::::: {.columns}
::: {.column width="47%"}

Pearson's Correlation Coefficient

$$r = \frac{cov(x,y)}{s_{x}s_y}$$

- Scales covariance (from -1 to 1) using standard deviations, $s$, thus making magnitude independent of units.  

- Significance can be tested by converting to a _t_-statistic and comparing to a _t_-distribution with $df=n-2$.

:::
::: {.column width="53%"}

```{r}
#| fig-asp: 0.85

bob(body_data) +
  geom_text(
    data = body_labels,
    aes(x, y, label = covariance),
    hjust = 0,
    vjust = c(1,0),
    size = 7
  )

```

:::
::::::

## Non-linear correlation

:::::: {.columns}
::: {.column width="47%"}

Spearman's Rank Correlation Coefficient

$$\rho = \frac{cov\left(R(x),\; R(y) \right)}{s_{R(x)}s_{R(y)}}$$

- Pearson's correlation but with ranks (R).  
- This makes it a _robust_ estimate, less sensitive to outliers.  

:::
::: {.column width="53%"}

```{r}
#| fig-asp: 0.85

body_data <- body_data |> 
  mutate(
    height = if_else(height > 176.5, height + 0.3*(max(height) - height), height),
    dx = if_else(height <= 176.5, 0, (height - 176.5) / (max(height) - 176.5)),
    delta_neg = abs(90 - weight) * dx + rnorm(n(), sd = 1.3),
    delta_pos = abs(75 - weight) * dx + rnorm(n(), sd = 1.3),
    weight = case_when(
      height > 176.5 & covariance == "Negative" ~ weight + delta_neg,
      height > 176.5 & covariance == "Positive" ~ weight - delta_pos,
      TRUE ~ weight 
    )
  )

bob(body_data)

```

:::
::::::

## Correlation between categories

:::::: {.columns}
::: {.column width="47%"}

For counts or frequencies

$$\chi^2 = \sum \frac{(O_{ij}-E_{ij})^2}{E_{ij}}$$

- Analysis of contingency table
- $O_{ij}$ is observed count in row $i$, column $j$ 
- $E_{ij}$ is expected count in row $i$, column $j$ 
- Significance can be tested by comparing to a $\chi^2$-distribution with $df=k-1$ ($k$ being the number of categories).

:::
::: {.column width="53%"}

```{r}
#| fig-asp: 0.9

ggplot(pithouses, aes(y = size, fill = depth, color = depth)) + 
  geom_bar(position = "dodge2") +
  scale_color_manual(
    name = NULL,
    values = qcolors("sapphire", "gold")
  ) +
  scale_fill_manual(
    name = NULL,
    values = alpha(qcolors("sapphire", "gold"), 0.5)
  ) +
  theme(
    legend.position = c(0.98, 0.98),
    legend.justification = c("right", "top")
  ) +
  labs(
    x = "Count",
    y = NULL,
    title = "Pithouse Size by Depth",
    subtitle = "From Late Stone Age Arctic Norway"
  )

```

:::
::::::

## Correlation is not causation!

::: {style="text-align:center"}

```{r}
#| fig-asp: 0.4
#| fig-width: 10
#| out-width: "75%"

cage <- tibble(
  year = 1999:2009,
  films = c(2, 2, 2, 3, 1, 1, 2, 3, 4, 1, 4),
  drownings = c(109, 102, 102, 98, 85, 95, 96, 98, 123, 94, 102),
  helicopters = c(59, 64, 56, 48, 79, 75, 42, 49, 47, 69, 43)
) |> 
  mutate(
    films_sc = scales::rescale(films, to = c(min(drownings), max(drownings)))
  )

ggplot(cage) + 
  with_outer_glow(
    geom_smooth(
      aes(year, drownings),
      color = qcolors("gold"), 
      span = 0.4, 
      se = FALSE,
      linewidth = 1
    ),
    colour = qcolors("gold")
  ) +
  geom_point(
    aes(year, drownings),
    shape = 21,
    color = qcolors("gold"), 
    fill = "white",
    size = 4,
    stroke = 1.1
  ) +
  with_outer_glow(
    geom_smooth(
      aes(year, films_sc),
      color = qcolors("sapphire"), 
      span = 0.4, 
      se = FALSE,
      linewidth = 1
    ),
    colour = qcolors("sapphire")
  ) +
  geom_point(
    aes(year, films_sc),
    shape = 21,
    color = qcolors("sapphire"), 
    fill = "white",
    size = 4,
    stroke = 1.1
  ) +
  scale_x_continuous(
    expand = expansion(add = 0.2),
    breaks = seq(1999, 2009, by = 2)
  ) +
  scale_y_continuous(
    name = "Number of people who\ndrowned falling into a pool",
    sec.axis = dup_axis(
      name = "Number of films\nNicolas Cage appeared in",
      breaks = with(cage, seq(min(drownings), max(drownings), length = 3)),
      labels = seq(1, 4, length = 3)
    )
  ) +
  annotate(
    geom = "text",
    x = 1999,
    y = Inf,
    label = paste0("r = ", with(cage, round(cor(films, drownings), digits = 2))),
    hjust = 0,
    vjust = 2,
    size = 5,
    color = "gray20"
  ) +
  labs(x = "Year") +
  theme(
    axis.text          = element_text(size = 10),
    axis.text.y.left   = element_text(color = qcolors("gold"), size = 10),
    axis.text.y.right  = element_text(color = qcolors("sapphire"), size = 10),
    axis.title.y       = element_text(size = 14, lineheight = 1.4),
    axis.title.y.left  = element_text(color = qcolors("gold"), margin = margin(r=12)),
    axis.title.y.right = element_text(color = qcolors("sapphire"), margin = margin(l=12))
  )

```


Adapted from <https://www.tylervigen.com/spurious-correlations>

:::

## A general formula

```{css}

.gray-panel {
  background-image: linear-gradient(to bottom right, #E1E1E1, #D5D5D5);
}

.blue-panel {
  background-image: linear-gradient(to bottom right, #42758D, #0C556D);
  color: white;
}

.yellow-panel {
  background-image: linear-gradient(to bottom right, #DFC638, #D1B817);
}

.panel {
  border: 1px solid black;
  border-radius: 5px;
  margin: 0.2em;
  padding: 0.3em;
}

.panel p {
  margin: 0 auto;
}

.flex {
  display: flex;
}

.align-center {
  align-items: center;
}

```


::: {.flex .align-center}

::: {.gray-panel .panel .flex .align-center style="width:23%;height:175px;"}
$$
Y = E[Y] + \epsilon \\[6pt]
E[Y] = \beta X
$$
:::

[{{< fa circle-arrow-right size=2xl >}}]{style="margin:0 0.5em;"}

::: {.yellow-panel .panel .flex .align-center style="width:23%;height:175px;"}
$$y_i = \hat\beta X + \epsilon_i$$
:::

[{{< fa circle-arrow-right size=2xl >}}]{style="margin:0 0.5em;"}

::: {.blue-panel .panel .flex .align-center style="width:50%;height:175px;"}
$$y_i = \hat\beta_0 + \hat\beta_1 x_i + \ldots +  \hat\beta_n x_i + \epsilon_i$$
:::

:::

<br>

- $y_i$ is the __dependent variable__, a sample from the population $Y$  
- $X$ is a set of __independent variables__ $x_i, \ldots, x_n$, sometimes called the design matrix
- $\hat\beta$ is a set of __coefficients of relationship__ that estimate the true relationship $\beta$
- $\epsilon_i$ is the __residuals__ or errors 

## Simple Linear Regression

:::::: {.columns}
::: {.column}

```{r}

gg_cars

```


:::
::: {.column}

<br>

'Simple' means one explanatory variable (speed)  

$$y_i = \hat\beta_0 + \hat\beta_1 speed_i + \epsilon_i$$

- $\hat\beta_0$ = `r beta0`
- $\hat\beta_1$ = `r beta1`

__Question:__ How did we get these values?

:::
::::::

## Ordinary Least Squares

:::::: {.columns}
::::: {.column}
:::: {.r-stack}
::: {.fragment .fade-out fragment-index=0}

A method for estimating the coefficients, $\beta$, in a linear regression model by minimizing the __Residual Sum of Squares__, $SS_{R}$.  

$$SS_{R} = \sum_{i=1}^{n} (y_{i}-\hat{y}_i)^2$$

where $\hat{y} \approx E[Y]$. To minimize this, we take its derivative with respect to $\beta$ and set it equal to zero.

$$\frac{d\, SS_{R}}{d\, \beta} = 0$$
:::
::: {.fragment .fade-in fragment-index=0}

Simple estimators for coefficients:  

***
__Slope__ [Ratio of covariance to variance]{style="color:#777;"}
$$\beta_{1} = \frac{cov(x, y)}{var(x)}$$ 

***
__Intercept__ [Conditional difference in means]{style="color:#777;"}
$$\beta_{0} = \bar{y} - \beta_1 \bar{x}$$

If $\beta_{1} = 0$, then $\beta_{0} = \bar{y}$. 

:::
:::: 
:::::
::::: {.column}

```{r}
#| out-width: "100%"

ssr <- tibble(
  x = seq(0, 4, length.out = 500),
  y = 0.3 + (x - 2)^2
)

ggplot(ssr, aes(x, y)) +
  geom_hline(yintercept = 0.3, color = qcolors("kobe")) +
  geom_line() +
  geom_hline(yintercept = 0, linewidth = 1) +
  geom_vline(xintercept = 0, linewidth = 1) +
  scale_y_continuous(breaks = 0, labels = 0) +
  scale_x_continuous(breaks = 0, labels = 0) +
  labs(
    x = "Estimate of \u03B21", 
    y = "Residual Sum of Squares"
  ) +
  annotate(
    "segment", 
    x = 2, 
    y = 1.3, 
    xend = 2, 
    yend = 0.33, 
    arrow = arrow(length = unit(0.1, "inches"), type = "closed")
  ) +
  annotate(
    "text",
    x = 2, y = 1.3 + 0.15, 
    label = "Value of \u03B21 that\nminimizes SSR",
    hjust = 0.5,
    vjust = 0,
    size = 5
  ) +
  theme(
    panel.border = element_rect(color = "gray75", fill = "transparent")
  )

```

:::::
::::::

## üöó Cars Model, again

:::::: {.columns}
::: {.column}

```{r}

gg_cars

```

:::
::: {.column}

```{r}

cv <- with(cars, cov(distance, speed))
vv <- with(cars, var(speed))

beta1 <- cv/vv

mu_distance <- mean(cars$distance)
mu_speed <- mean(cars$speed)

beta0 <- mu_distance - beta1 * mu_speed

```

<br>

__Slope__

$$\hat\beta_{1} = \frac{cov(x,y)}{var(x)} = \frac{`r cv`}{`r vv`} = `r beta1`$$

***

__Intercept__

$$
\begin{align}
\hat\beta_{0} &= \bar{y} - \hat\beta_{1}\bar{x} \\[6pt]
              &= `r mu_distance` - `r beta1` * `r mu_speed` \\[6pt]
              &= `r beta0`
\end{align}
$$

:::
::::::

## Multiple Linear Regression

OLS can be extended to models containing multiple explanatory variables using matrix algebra. Then the coefficient estimator is:

$$\hat\beta = (X^{T}X)^{-1}X^{T}y$$

<br>  

Where, _if you squint a little_ 

- $X^{T}X$ &#x21E8; variance. 
- $X^{T}y$ &#x21E8; covariance.

## Regression assumptions

1. __Weak Exogeneity__: the predictor variables have fixed values and are known.  
2. __Linearity__: the relationship between the predictor variables and the response variable is linear.  
2. __Constant Variance__: the variance of the errors does not depend on the values of the predictor variables. Also known as _homoskedasticity_.  
2. __Independence of errors__: the errors are uncorrelated with each other.  
2. __No perfect collinearity__: the predictors are not linearly correlated with each other.  
