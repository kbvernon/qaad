---
title: "Lecture 06: Linear Models 3"  
date: 'Last updated: `r Sys.Date()`'
output:
  xaringan::moon_reader:
    lib_dir: libs
    css: [xaringan-themer.css, custom_style.css]
    seal: true
    nature:
      highlightStyle: magula
      highlightLines: true
      countIncrementalSlides: false
      ratio: '16:9'
---

```{r, include = FALSE, code=xfun::read_utf8(here::here("slides", "before_chunk.R"))}
```

```{r}

data(cars)

set.seed(12345)

cars <- cars %>% 
  as_tibble() %>%
  group_by(speed) %>% 
  slice(1) %>% 
  ungroup() %>% 
  # slice_sample(n = 10) %>%
  rename("distance" = dist) %>% 
  mutate(
    id = 1:n(),
    speed = speed * 0.44704,
    distance = distance * 0.3048,
    distance = round(distance, digits = 1)
  )

cars_lm <- lm(distance ~ speed, data = cars)

estimate <- predict(cars_lm, cars) %>% unname()

beta0 <- coefficients(cars_lm)[["(Intercept)"]]
beta1 <- coefficients(cars_lm)[["speed"]]

max_speed <- max(cars$speed)

x <- 8
ey <- beta0 + beta1 * x

sigma <- sd(residuals(cars_lm))

```

## &#x1F4CB; Lecture Outline

- Model Interpretation
    - Slope and Intercept
    - Prediction
    - Standard error vs prediction interval
- Polynomial transformations for non-linearity
- Log transformations for non-linearity



---

## Model Interpretation


.pull-left[

```{r, fig.asp = 1}

gg_cars <-
  ggplot(cars) +
  # geom_hline(yintercept = 0, size = 1) +
  # geom_vline(xintercept = 0, size = 1) +
  # theme(
  #   panel.border = element_rect(color = "gray75", fill = "transparent")
  # ) +
  scale_x_continuous(breaks = seq(0, 2 * round(max_speed/2), by = 2)) + 
  labs(
    x = "Speed (m/s)", 
    y = "Distance (m)"
  ) +
  coord_cartesian(
    ylim = c(floor(beta0), max(cars$distance) + 2),
    xlim = c(0, max_speed + 1)
  ) + ggtitle("Cars Model")

gg_cars  +
  geom_segment(
    aes(x = speed, xend = speed, y = distance, yend = estimate),
    linetype = "dashed",
    color = "gray35",
    size = 0.5
  ) +
  geom_abline(
    slope = beta1,
    intercept = beta0,
    color = qaad_colors("celadon_blue"),
    size = 1
  ) +
  geom_point(
    aes(x = speed, y = distance),
    size = 3.5
  )

```

]

.pull-right[

<br>

.pull-left.bg-celadon_blue_o_70.ba.bw1.br2.shadow-4.pv1.ph4.h4[.white[

$$
\begin{align}
y_i &= E[Y] + \epsilon_i \\
\epsilon &\sim N(0,\sigma) \\
\sigma &= `r sigma` 
\end{align}
$$   

]]

.pull-right.bg-celadon_blue_o_70.ba.bw1.br2.shadow-4.pv1.ph4.h4[.white[

$$
\begin{align}
E(Y) &= \beta X \\
\hat\beta_0 &= `r beta0` \\
\hat\beta_1 &= `r beta1`
\end{align}
$$   

]]

]



---
count: false

## Model Interpretation

.pull-left[

```{r, fig.asp = 1}

deltas <- tibble(
  x = c(5, 5, 8),
  y = predict(cars_lm, newdata = tibble(speed = c(5, 8, 8))),
)

gg_cars  +
  geom_abline(
    slope = beta1,
    intercept = beta0,
    color = qaad_colors("celadon_blue"),
    size = 1
  ) + 
  geom_line(
    data = deltas,
    aes(x, y),
    color = "gray30",
    linetype = "dashed"
  ) +
  annotate(
    "segment",
    x = 0, xend = 0,
    y = beta0 - 5, yend = beta0,
    color = "gray30",
    linetype = "dashed"
  ) +
  annotate(
    "text",
    x = 4.8,
    y = beta0 + beta1 * 6.5,
    label = "\u0394Y",
    hjust = 1,
    size = 7
  ) +
  annotate(
    "text",
    x = 6.5,
    y = ey + 1,
    label = "\u0394X",
    vjust = 0,
    size = 7
  ) +
  annotate(
    "segment",
    x = 8.5,
    y = beta0 + beta1 * 6.5,
    xend = 6.7,
    yend = beta0 + beta1 * 6.5,
    arrow = arrow(length = unit(0.15, "inches"), type = "closed")
  ) +
  annotate(
    "text",
    x = 8.7,
    y = beta0 + beta1 * 6.5,
    label = "\u0392\u2081",
    hjust = 0,
    size = 7
  ) +
  annotate(
    "segment",
    x = 2,
    y = beta0,
    xend = 0.2,
    yend = beta0,
    arrow = arrow(length = unit(0.15, "inches"), type = "closed")
  ) +
  annotate(
    "text",
    x = 2.2,
    y = beta0,
    label = "\u0392o",
    hjust = 0,
    size = 7
  )

```

]

.pull-right[

<br>

.pull-left.bg-celadon_blue_o_70.ba.bw1.br2.shadow-4.pv1.ph4.h4[.white[

$$
\begin{align}
y_i &= E[Y] + \epsilon_i \\
\epsilon &\sim N(0,\sigma) \\
\sigma &= `r sigma` 
\end{align}
$$   

]]

.pull-right.bg-celadon_blue_o_70.ba.bw1.br2.shadow-4.pv1.ph4.h4[.white[

$$
\begin{align}
E(Y) &= \beta X \\
\hat\beta_0 &= `r beta0` \\
\hat\beta_1 &= `r beta1`
\end{align}
$$   

]]

<br>

`\\(\hat\beta_{0}\\)` is the value of Y where `\\(X=0\\)`, here the total stopping distance when the car isn't moving, which should be zero!!!  

`\\(\hat\beta_{1}\\)` is change in Y relative to change in X, here the additional distance the car will travel for each unit increase in speed. 

]



---

## Prediction

.pull-left[

```{r, fig.asp = 1}

predictions <- tibble(
  x = c(-2, rep(x, 2)),
  y = c(rep(ey, 2), beta0-5)
)

gg_cars  +
  geom_abline(
    slope = beta1,
    intercept = beta0,
    color = qaad_colors("celadon_blue"),
    size = 1
  ) + 
  geom_line(
    data = predictions,
    aes(x, y),
    color = "gray30",
    linetype = "dashed"
  ) +
  annotate(
    "text",
    x = x + 0.2,
    y = beta0 + beta1 * 5,
    label = paste0("X = ", x),
    hjust = 0,
    size = 7
  ) +
  annotate(
    "text",
    x = 4,
    y = ey + 1,
    label = paste0("E[Y] = ???"),
    vjust = 0,
    size = 7
  )

```

]

.pull-right[

<br>

.pull-left.bg-celadon_blue_o_70.ba.bw1.br2.shadow-4.pv1.ph4.h4[.white[

$$
\begin{align}
y_i &= E[Y] + \epsilon_i \\
\epsilon &\sim N(0,\sigma) \\
\sigma &= `r sigma` 
\end{align}
$$   

]]

.pull-right.bg-celadon_blue_o_70.ba.bw1.br2.shadow-4.pv1.ph4.h4[.white[

$$
\begin{align}
E(Y) &= \beta X \\
\hat\beta_0 &= `r beta0` \\
\hat\beta_1 &= `r beta1`
\end{align}
$$   

]]

<br>

__Question:__ if a car is going 8 m/s when it applies the brakes, how long will it take it to stop? 

]



---
count: false

## Prediction

.pull-left[

```{r, fig.asp = 1}

gg_cars  +
  geom_abline(
    slope = beta1,
    intercept = beta0,
    color = qaad_colors("celadon_blue"),
    size = 1
  ) + 
  geom_line(
    data = predictions,
    aes(x, y),
    color = "gray30",
    linetype = "dashed"
  ) +
  annotate(
    "text",
    x = x + 0.2,
    y = beta0 + beta1 * 5,
    label = paste0("X = ", x),
    hjust = 0,
    size = 7
  ) +
  annotate(
    "text",
    x = 4,
    y = ey + 1,
    label = paste0("E[Y] = ", round(ey, 3)),
    vjust = 0,
    size = 7
  )

```

]

.pull-right[

<br>

.pull-left.bg-celadon_blue_o_70.ba.bw1.br2.shadow-4.pv1.ph4.h4[.white[

$$
\begin{align}
y_i &= E[Y] + \epsilon_i \\
\epsilon &\sim N(0,\sigma) \\
\sigma &= `r sigma` 
\end{align}
$$   

]]

.pull-right.bg-celadon_blue_o_70.ba.bw1.br2.shadow-4.pv1.ph4.h4[.white[

$$
\begin{align}
E(Y) &= \beta X \\
\hat\beta_0 &= `r beta0` \\
\hat\beta_1 &= `r beta1`
\end{align}
$$   

]]

<br>

__Question:__ if a car is going 8 m/s when it applies the brakes, how long will it take it to stop?

$$E[Y] = `r beta0` + `r beta1` \cdot 8 = `r ey`$$

]



---

## Prediction Interval

.pull-left[

```{r, fig.asp = 1}

CI <- predict(cars_lm, newdata = tibble(speed = x), interval = "prediction")

gg_cars  +
  geom_abline(
    slope = beta1,
    intercept = beta0,
    color = qaad_colors("celadon_blue"),
    size = 1
  ) + 
  annotate(
    "segment",
    x = -2,
    y = ey,
    xend = x,
    yend = ey,
    color = "gray30",
    linetype = "dashed"
  ) +
  annotate(
    "text",
    x = 4,
    y = ey + 1,
    label = paste0("E[Y] = ", round(ey, 3)),
    vjust = 0,
    size = 7
  ) + 
  geom_errorbar(
    data = tibble(x = x, lo = CI[[2]], hi = CI[[3]]),
    aes(x = x, ymin = lo, ymax = hi),
    size = 1,
    width = 0.65,
    color = qaad_colors("flame_orange")
  )

```

]

.pull-right[

<br>

.pull-left.bg-celadon_blue_o_70.ba.bw1.br2.shadow-4.pv1.ph4.h4[.white[

$$
\begin{align}
y_i &= E[Y] + \epsilon_i \\
\epsilon &\sim N(0,\sigma) \\
\sigma &= `r sigma` 
\end{align}
$$   

]]

.pull-right.bg-celadon_blue_o_70.ba.bw1.br2.shadow-4.pv1.ph4.h4[.white[

$$
\begin{align}
E(Y) &= \beta X \\
\hat\beta_0 &= `r beta0` \\
\hat\beta_1 &= `r beta1`
\end{align}
$$   

]]

<br>

__Question__: But, what about `\\(\epsilon\\)`?!!! 

We can visualize this with a __prediction interval__, which gives the uncertainty around the __expected outcome__. 

]



---

## Prediction Interval

.pull-left[

```{r, fig.asp = 1}

new_data <- tibble(speed = seq(-5, 15, length = 300))

CI <- predict(cars_lm, newdata = new_data, interval = "prediction")

prediction_interval <- bind_cols(new_data, as_tibble(CI))

gg_cars + 
  geom_ribbon(
    data = prediction_interval,
    aes(speed, ymin = lwr, ymax = upr),
    fill = alpha(qaad_colors("maize_crayola"), 0.7),
    color = qaad_colors("flame_orange"), 
    size = 1
  )  +
  geom_abline(
    slope = beta1,
    intercept = beta0,
    color = alpha("white", 0.5),
    size = 4
  )  +
  geom_abline(
    slope = beta1,
    intercept = beta0,
    color = qaad_colors("celadon_blue"),
    size = 1
  )

```

]

.pull-right[

<br>

.pull-left.bg-celadon_blue_o_70.ba.bw1.br2.shadow-4.pv1.ph4.h4[.white[

$$
\begin{align}
y_i &= E[Y] + \epsilon_i \\
\epsilon &\sim N(0,\sigma) \\
\sigma &= `r sigma` 
\end{align}
$$   

]]

.pull-right.bg-celadon_blue_o_70.ba.bw1.br2.shadow-4.pv1.ph4.h4[.white[

$$
\begin{align}
E(Y) &= \beta X \\
\hat\beta_0 &= `r beta0` \\
\hat\beta_1 &= `r beta1`
\end{align}
$$   

]]

<br>

__Question__: But, what about `\\(\epsilon\\)`?!!! 

We can visualize this with a __prediction interval__, which gives the uncertainty around the __expected outcome__. Can generalize this across the entire range of the model.

]



---

## Prediction Interval vs Standard Error

<!--- https://stackoverflow.com/a/38110406/7705429 --->

.pull-left[

```{r, fig.asp = 1}

new_data <- tibble(speed = seq(-5, 15, length = 300))

CI <- predict(cars_lm, newdata = new_data, interval = "prediction")

prediction_interval <- bind_cols(new_data, as_tibble(CI))

SE <- predict(cars_lm, newdata = new_data, se.fit = TRUE)

error_interval <- tibble(
  distance = SE$fit,
  se = 2 * SE$se.fit
) %>% 
  bind_cols(new_data)

gg_cars + 
  geom_ribbon(
    data = prediction_interval,
    aes(speed, ymin = lwr, ymax = upr),
    fill = alpha(qaad_colors("maize_crayola"), 0.7),
    color = qaad_colors("flame_orange"), 
    size = 1
  ) +
  geom_ribbon(
    data = error_interval,
    aes(x = speed, ymin = distance - se, ymax = distance + se),
    fill = alpha(qaad_colors("rufous_red"), 0.7),
    color = qaad_colors("rufous_red"),
    size = 1,
    linetype = "dashed"
  )  +
  geom_abline(
    slope = beta1,
    intercept = beta0,
    color = alpha("white", 0.5),
    size = 2.5
  )  +
  geom_abline(
    slope = beta1,
    intercept = beta0,
    color = qaad_colors("celadon_blue"),
    size = 1
  )

```

]

.pull-right[

<br>

.pull-left.bg-celadon_blue_o_70.ba.bw1.br2.shadow-4.pv1.ph4.h4[.white[

$$
\begin{align}
y_i &= E[Y] + \epsilon_i \\
\epsilon &\sim N(0,\sigma) \\
\sigma &= `r sigma` 
\end{align}
$$   

]]

.pull-right.bg-celadon_blue_o_70.ba.bw1.br2.shadow-4.pv1.ph4.h4[.white[

$$
\begin{align}
E(Y) &= \beta X \\
\hat\beta_0 &= `r beta0` \\
\hat\beta_1 &= `r beta1`
\end{align}
$$   

]]

<br>

Prediction interval is uncertainty around the expected value. 

Standard error is uncertainty around the coefficient estimate.  


]


---

## Prediction Interval vs Standard Error

.pull-left[

```{r, fig.asp = 1}

set.seed(42)

se_slope <- summary(cars_lm)$coefficients[2,2] * 2

randos <- tibble(
  slope = beta1 + seq(-se_slope, se_slope, length = 12),
  intercept = mean(cars$distance) - slope*mean(cars$speed)
) %>% 
  slice(-(c(1, n())))

gg_cars  +
  geom_ribbon(
    data = prediction_interval,
    aes(speed, ymin = lwr, ymax = upr),
    fill = alpha(qaad_colors("maize_crayola"), 0.7),
    color = qaad_colors("flame_orange"),
    size = 1
  ) +
  geom_ribbon(
    data = error_interval,
    aes(x = speed, ymin = distance - se, ymax = distance + se),
    fill = alpha(qaad_colors("rufous_red"), 0.7),
    color = qaad_colors("rufous_red"),
    size = 1,
    linetype = "dashed"
  ) +
  geom_abline(
    data = randos,
    aes(slope = slope, intercept = intercept),
    color = "white",
    alpha = 0.9,
    size = 0.5
  )

```

]

.pull-right[

<br>

.pull-left.bg-celadon_blue_o_70.ba.bw1.br2.shadow-4.pv1.ph4.h4[.white[

$$
\begin{align}
y_i &= E[Y] + \epsilon_i \\
\epsilon &\sim N(0,\sigma) \\
\sigma &= `r sigma` 
\end{align}
$$   

]]

.pull-right.bg-celadon_blue_o_70.ba.bw1.br2.shadow-4.pv1.ph4.h4[.white[

$$
\begin{align}
E(Y) &= \beta X \\
\hat\beta_0 &= `r beta0` \\
\hat\beta_1 &= `r beta1`
\end{align}
$$   

]]

<br>

Prediction interval is uncertainty around the expected value. 

Standard error is uncertainty around the coefficient estimate.  

]



---

## Polynomial transformations for non-linearity

```{r}

set.seed(25)

n <- 100

observations <- tibble(
  x = seq(0, 1, length = n),
  y = 1 - 3.7 * (x - 0.5)^2 + (0.75 * x) + rnorm(n, sd = 0.2)
) %>% 
  mutate(
    y = scales::rescale(y, c(0.1, 0.9))
  )

bad_mod <- lm(y ~ x, data = observations)

bad_b0 <- coefficients(bad_mod)[[1]]
bad_b1 <- coefficients(bad_mod)[[2]]

bad_fit <- tibble(
  observation = 1:n,
  estimate = predict(bad_mod),
  residuals = residuals(bad_mod)
)

bad_r2 <- summary(bad_mod)$r.squared

```

__Simple Linear Model:__ $y_{i} = \beta_{0} + \beta_{1}X + \epsilon_{i}$  
$R^2 = `r bad_r2`$

```{r, out.width = "50%", fig.align = "default"}

ggplot(observations, aes(x,y)) +
  geom_abline(
    intercept = bad_b0,
    slope = bad_b1,
    color = qaad_colors("rufous_red"),
    size = 1
  ) + 
  geom_point(
    size = 3
  ) +
  labs(
    x = "X",
    y = "Y"
  ) +
  theme(
    axis.text = element_blank(),
    axis.ticks = element_blank()
  )

ggplot(bad_fit, aes(estimate, residuals)) +
  geom_segment(
    aes(xend = estimate, yend = 0),
    linetype = "dashed"
  ) +
  geom_hline(
    aes(yintercept = 0),
    color = qaad_colors("rufous_red"),
    size = 1
  ) +
  geom_point(
    size = 3
  ) +
  labs(
    x = "Estimates",
    y = "Residuals"
  ) +
  theme(
    axis.text = element_blank(),
    axis.ticks = element_blank()
  )

```



---

## Polynomial transformations for non-linearity

```{r}

meh_mod <- lm(y ~ poly(x, 2), data = observations)

meh_fit <- tibble(
  observation = 1:n,
  estimate = predict(meh_mod),
  residuals = residuals(meh_mod)
)

meh_r2 <- summary(meh_mod)$r.squared

```

__Quadratic Model:__ $y_{i} = \beta_{0} + \beta_{1}X + \beta_{2}X^{2} \epsilon_{i}$  
$R^2=`r meh_r2`$


```{r, out.width = "50%", fig.align = "default"}

ggplot(observations, aes(x,y)) +
  geom_line(
    aes(y = meh_fit$estimate),
    color = qaad_colors("rufous_red"),
    size = 1
  ) + 
  geom_point(
    size = 3
  ) +
  labs(
    x = "X",
    y = "Y"
  ) +
  theme(
    axis.text = element_blank(),
    axis.ticks = element_blank()
  )

ggplot(meh_fit, aes(estimate, residuals)) +
  geom_segment(
    aes(xend = estimate, yend = 0),
    linetype = "dashed"
  ) +
  geom_hline(
    aes(yintercept = 0),
    color = qaad_colors("rufous_red"),
    size = 1
  ) +
  geom_point(
    size = 3
  ) +
  labs(
    x = "Estimates",
    y = "Residuals"
  ) +
  theme(
    axis.text = element_blank(),
    axis.ticks = element_blank()
  )

```



---

## Log Transformations

```{r}

n <- 100

x <- seq(0.1,5,length.out = n)

set.seed(1)

e <- rnorm(n, mean = 0, sd = 0.2)

y <- exp(0.3 + 0.5 * x + e)

```


.pull-left[

```{r, fig.asp = 0.9}

dat <- tibble(
  v = rep(c("Y", "log(Y)"), each = n),
  y = c(y, log(y))
)

txt <- tibble(
  x = c(3.2, 14),
  y = c(35, 6),
  v = c("log(Y)", "Y")
)

ggplot() +
  geom_histogram(
    data = dat,
    position = "identity",
    aes(y, color = v, fill = v),
    bins = 20
  ) +
  scale_color_manual(
    values = qaad_colors("celadon_blue", "flame_orange")
  ) +
  scale_fill_manual(
    values = alpha(qaad_colors("celadon_blue", "flame_orange"), 0.75)
  ) +
  geom_text(
    data = txt,
    aes(x, y, color = v, label = v),
    size = 8,
    hjust = 0
  ) +
  labs(
    x = "Value",
    y = "Count"
  ) +
  theme(
    axis.text = element_blank(),
    axis.ticks = element_blank(),
    legend.position = "none"
  )

```

]

.pull-right[

<br>

__Logging__ a skewed variable normalizes it, spreading out clumped data and clumping dispersed data.  

This is the inverse of __exponentiation__: 

$$Y = log(exp(Y))$$  

We can apply these transformations to both the dependent $(Y)$ and independent variables $(X)$.  

__Question__: but, whyyyyyy???

]



---

## Log Transformations for non-linearity

```{r}

linear_mod <- lm(y ~ x)

linear_b0 <- coefficients(linear_mod)[[1]]
linear_b1 <- coefficients(linear_mod)[[2]]

linear_fit <- tibble(
  observation = 1:n,
  estimate = predict(linear_mod),
  residuals = residuals(linear_mod)
)

linear_r2 <- summary(linear_mod)$r.squared

```

__Simple Linear Model:__ $y_{i} = \beta_{0} + \beta_{1}X + \epsilon_{i}$  
$R^2 = `r linear_r2`$

```{r, out.width = "50%", fig.align = "default"}

ggplot() +
  geom_abline(
    intercept = linear_b0,
    slope = linear_b1,
    color = qaad_colors("rufous_red")
  ) +
  geom_point(
    aes(x, y),
    size = 3
  ) +
  labs(
    x = "X",
    y = "Y"
  ) +
  theme(
    axis.text = element_blank(),
    axis.ticks = element_blank()
  )

ggplot(linear_fit, aes(estimate, residuals)) +
  geom_segment(
    aes(xend = estimate, yend = 0),
    linetype = "dashed"
  ) +
  geom_hline(
    aes(yintercept = 0),
    color = qaad_colors("rufous_red"),
    size = 1
  ) +
  geom_point(
    size = 3
  ) +
  labs(
    x = "Estimates",
    y = "Residuals"
  ) +
  theme(
    axis.text = element_blank(),
    axis.ticks = element_blank()
  )

```



---

## Log Transformations for non-linearity

```{r}

log_mod <- lm(log(y) ~ x)

log_b0 <- coefficients(log_mod)[[1]]
log_b1 <- coefficients(log_mod)[[2]]

log_fit <- tibble(
  observation = 1:n,
  estimate = predict(log_mod),
  residuals = residuals(log_mod)
)

log_r2 <- summary(log_mod)$r.squared

```

__Log Linear Model:__ $log(y_{i}) = \beta_{0} + \beta_{1}X + \epsilon_{i}$  
$R^2 = `r log_r2`$

```{r, out.width = "50%", fig.align = "default"}

ggplot() +
  geom_abline(
    intercept = log_b0,
    slope = log_b1,
    color = qaad_colors("rufous_red")
  ) +
  geom_point(
    aes(x, log(y)),
    size = 3
  ) +
  labs(
    x = "X",
    y = "log(Y)"
  ) +
  theme(
    axis.text = element_blank(),
    axis.ticks = element_blank()
  )

ggplot(log_fit, aes(estimate, residuals)) +
  geom_segment(
    aes(xend = estimate, yend = 0),
    linetype = "dashed"
  ) +
  geom_hline(
    aes(yintercept = 0),
    color = qaad_colors("rufous_red"),
    size = 1
  ) +
  geom_point(
    size = 3
  ) +
  labs(
    x = "log(Estimates)",
    y = "Residuals"
  ) +
  theme(
    axis.text = element_blank(),
    axis.ticks = element_blank()
  )

```






---

## Log Transformations

.center[
.f1[__Careful!__]  
Have to take care how we interpret log-models!  

.f3[`\\(\beta_1\\)` means...]

]

<br>

.pull-left[

__Linear Model:__ $y_{i} = \beta_{0} + \beta_{1}X + \epsilon_{i}$  
_Increase in Y for every one unit increase in X._

__Log-Linear Model:__ $log(y_{i}) = \beta_{0} + \beta_{1}X + \epsilon_{i}$  
_Percent increase in Y for every one unit increase in X._

]

.pull-right[

__Linear-Log Model:__ $y_{i} = \beta_{0} + \beta_{1}log(X) + \epsilon_{i}$   
_Increase in Y for every percent increase in X._

__Log-Log Model:__ $log(y_{i}) = \beta_{0} + \beta_{1}log(X) + \epsilon_{i}$  
_Percent increase in Y for every percent increase in X._

]


---

## &#x1F52D; Looking Ahead

```{r}

schedule <- here("_misc", "course_outline.xlsx") %>% 
  readxl::read_excel(1) %>% 
  mutate(
    date = lubridate::ymd(date),
    description = glue::glue(
    "<b>{topic}</b> <span style = 'font-size: 80%; color: #9C9C9C;'>({date})</span><br>
    <div style = 'font-size: 80%; font-style: italic; color: #9C9C9C;'>{details}</div>"
  )) %>% 
  select(description)

```

.pull-left[

```{r}

schedule %>% 
  slice(6:9) %>% 
  kbl(escape = FALSE, col.names = NULL) %>% 
  kable_paper(c("striped", "hover"))

```

]