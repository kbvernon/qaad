---
title: "Quantitative Analysis of Archaeological Data"
subtitle: "Lecture 11: Generalized Linear Models 3"  
date: 'Last updated: `r Sys.Date()`'
output:
  xaringan::moon_reader:
    lib_dir: libs
    css: [xaringan-themer.css, custom_style.css]
    seal: TRUE
    nature:
      highlightStyle: magula
      highlightLines: TRUE
      countIncrementalSlides: TRUE
      ratio: '16:9'
---

```{r} 
#| include = FALSE, 
#| code = xfun::read_utf8(here::here("slides", "before_chunk.R"))
```

```{r}

snodgrass <- here("datasets", "snodgrass.csv") %>% 
  read_csv() %>% 
  select(inside, area)

surveys <- here("datasets", "surveys.csv") %>% read_csv()

```

## Outline

1. Binomial GLM
2. Prediction
2. Rate model
2. Offset
3. Dispersion

---

## Binomial GLM

.pull-left[

```{r}

snodgrass_gg <- ggplot(snodgrass, aes(area, inside)) +
  labs(
    x = "Area of House Structure (sq ft)",
    y = "Inside Inner Wall"
  ) +
  scale_y_continuous(
    breaks = c(0, 0.5, 1),
    labels = c(0, 0.5, 1)
  )

snodgrass_gg + 
  geom_point(size = 3, alpha = 0.4)

```

.gray.right[Data from the Snodgrass site in Butler County, MS]

]

.pull-right[

Location inside or outside of the inner wall at the Snodgrass site is a binomially distributed variable and has expectation $E(Y) = p$ where

$$p = \frac{1}{1 + exp(-\beta X)}$$
This defines a [logistic curve](https://en.wikipedia.org/wiki/Logistic_function) or sigmoid, with $p$ being the probability of success. This constrains the estimate $E(Y)$ to be in the range 0 to 1.

]

---

## Binomial GLM

.pull-left[

```{r}

snodgrass_gg + 
  geom_point(size = 3, alpha = 0.4)

```

.gray.right[Data from the Snodgrass site in Butler County, MS]

]

.pull-right[

Taking the log of $p$ gives us

$$log\left(\frac{p}{1 - p}\right) = \beta X$$

This is known as the "logit" or [log odds](https://en.wikipedia.org/wiki/Logit). 

__Question__ What is the probability that we observe these data (these admissions) given a model with parameters $\beta$? 

]

---

## Binomial GLM

```{r}

snodgrass_glm <- glm(
  inside ~ area,
  family = binomial(link = "logit"),
  data = snodgrass
)

b0 <- coefficients(snodgrass_glm)[["(Intercept)"]]
b1 <- coefficients(snodgrass_glm)[["area"]]

ll <- logLik(snodgrass_glm)

```

.pull-left[

```{r}

snodgrass_gg + 
  geom_point(size = 3, alpha = 0.4) +
  geom_line(
    data = tibble(
      x = snodgrass$area,
      y = predict(snodgrass_glm, type = "response")
    ),
    aes(x, y),
    color = qaad_colors("rufous_red"),
    size = 1
  )

```

.gray.right[Data from the Snodgrass site in Butler County, MS]

]

.pull-right[

Estimated coefficients: 

$\beta_0 = `r b0`$  
$\beta_1 = `r b1`$   

For these, the log Likelihood is

$\mathcal{l} = `r ll`$  

]

---

## Binomial GLM

.pull-left[

```{r}

snodgrass_gg + 
  geom_point(size = 3, alpha = 0.4) +
  geom_line(
    data = tibble(
      x = snodgrass$area,
      y = predict(snodgrass_glm, type = "response")
    ),
    aes(x, y),
    color = qaad_colors("rufous_red"),
    size = 1
  )

```

.gray.right[Data from the Snodgrass site in Butler County, MS]

]

.pull-right[

Estimated coefficients: 

$\beta_0 = `r b0`$  
$\beta_1 = `r b1`$   

Note that these coefficient estimates are log-odds! To get the odds, we take the exponent.   

$\beta_0 = exp(`r b0`) = `r exp(b0)`$  
$\beta_1 = exp(`r b1`) = `r exp(b1)`$  

For a one unit increase in area, the odds of being in the inside wall increase by `r exp(b1)`.

]

---

## Prediction

```{r}

est <- predict(
  snodgrass_glm, 
  newdata = tibble(area = 300), 
  type = "response"
)

```

.pull-left[

```{r}

snodgrass_gg + 
  geom_point(size = 3, alpha = 0.4) +
  geom_line(
    data = tibble(
      x = snodgrass$area,
      y = predict(snodgrass_glm, type = "response")
    ),
    aes(x, y),
    color = qaad_colors("rufous_red"),
    size = 1
  ) +
  annotate(
    "segment",
    x = 300,
    xend = 300,
    y = 0,
    yend = est,
    linetype = "dashed",
    color = "gray50"
  ) +
  annotate(
    "segment",
    x = min(snodgrass$area),
    xend = 300,
    y = est,
    yend = est,
    linetype = "dashed",
    color = "gray50"
  ) +
  annotate(
    "point", 
    x = 300,
    y = est,
    size = 5
  )

```

.gray.right[Data from the Snodgrass site in Butler County, MS]

]

.pull-right[

Estimated coefficients: 

$\beta_0 = `r b0`$  
$\beta_1 = `r b1`$   

To get the probability, we can use the mean function (also known as the inverse link):

$$p = \frac{1}{1+exp(-\beta X)}$$

For a house structure with an area of 300 square feet, the estimated probability that it occurs inside the inner wall is `r est`.

]

---

## Prediction

```{r}

inv_link <- family(snodgrass_glm)$linkinv

est <- predict(snodgrass_glm, se.fit = TRUE)

est <- tibble(
  x = snodgrass$area,
  y = inv_link(est$fit),
  ymin = inv_link(est$fit - 2*est$se.fit),
  ymax = inv_link(est$fit + 2*est$se.fit)
) %>% 
  arrange(y)

remove(inv_link)

```

.pull-left[

```{r}

snodgrass_gg +
  geom_ribbon(
    inherit.aes = FALSE,
    data = est,
    aes(x, ymin = ymin, ymax = ymax),
    fill = "gray85"
  ) + 
  geom_point(size = 3, alpha = 0.4) +
  geom_line(
    inherit.aes = FALSE,
    data = est,
    aes(x, y),
    color = qaad_colors("rufous_red")
  )

```

.gray.right[Data from the Snodgrass site in Butler County, MS]

]

.pull-right[

Estimated coefficients: 

$\beta_0 = `r b0`$  
$\beta_1 = `r b1`$   

Must also apply the inverse link to the standard error of the coefficients!

$$p = \frac{1}{1+exp(-\beta X)}$$

Otherwise, you will get a confidence ribbon that ranges outside the possible range of the data.

]

---

## Rate model

.pull-left[

```{r}

ggplot(surveys, aes(elevation, sites)) +
  labs(
    x = "Elevation (km)",
    y = "Site count"
  ) + 
  geom_point(
    size = 3,
    alpha = 0.7
  )

```

.gray.right[_Hypothetical dataset for demonstration._]

]

.pull-right[

Site counts is a Poisson distributed variable and has expectation $E(Y) = \lambda$ where

$$\lambda = exp(\beta X)$$

By taking the exponent, this constrains the expected count to be greater than zero.

]

---

## Rate model

Suppose, however, that we find out that our counts come from these survey blocks.

```{r}
#| out.width = "70%"

figure("survey-polygons.png")

```

.right[Need to account for area in our sampling strategy!]


---

## Rate model

.pull-left[

```{r}

ggplot(surveys, aes(elevation, sites/area)) +
  labs(
    x = "Elevation (km)",
    y = "Site density (n/km2)"
  ) + 
  geom_point(
    size = 3,
    alpha = 0.7
  )

```

.gray.right[_Hypothetical dataset for demonstration._]

]

.pull-right[

In a regular poisson model, we take the log of lambda (the count).

$$log(\lambda) = \beta X$$

Now we take the log of the density (or rate!).

$$log(\lambda_i/area_i) = \beta X$$
$$log(\lambda_i) - log(area_i) = \beta X$$
$$log(\lambda_i) = \beta X + log(area_i)$$

Still linear!  
Still modeling counts!  
But now we add the log of the area of each survey block as a constant offset. 

]

---

## Rate model

```{r}

surveys_glm <- glm(
  sites ~ elevation + offset(log(area)),
  family = poisson,
  data = surveys
)

b0 <- coefficients(surveys_glm)[["(Intercept)"]]
b1 <- coefficients(surveys_glm)[["elevation"]]

ll <- logLik(surveys_glm)

trend <- tibble(
  x = surveys$elevation,
  y = fitted(surveys_glm),
  area = surveys$area
)

```


.pull-left[

```{r}

ggplot() +
  labs(
    x = "Elevation (km)",
    y = "Site density (n/km2)"
  ) + 
  geom_line(
    data = trend,
    aes(x, y/area),
    color = qaad_colors("rufous_red"),
    size = 1
  ) +
  geom_point(
    data = surveys, 
    aes(elevation, sites/area),
    size = 3,
    alpha = 0.7
  )

```

.gray.right[_Hypothetical dataset for demonstration._]

]

.pull-right[

Estimated coefficients: 

$\beta_0 = `r b0`$  
$\beta_1 = `r b1`$   

For these, the log Likelihood is

$\mathcal{l} = `r ll`$  

]

---

## Rate model

.pull-left[

```{r}

ggplot() +
  labs(
    x = "Elevation (km)",
    y = "Site density (n/km2)"
  ) + 
  geom_line(
    data = trend,
    aes(x, y/area),
    color = qaad_colors("rufous_red"),
    size = 1
  ) +
  geom_point(
    data = surveys, 
    aes(elevation, sites/area),
    size = 3,
    alpha = 0.7
  )

```

.gray.right[_Hypothetical dataset for demonstration._]

]

.pull-right[

Estimated coefficients: 

$\beta_0 = `r b0`$  
$\beta_1 = `r b1`$   

Note that these coefficient estimates are on the log scale! To get back to counts, need to take the exponent.   

$\beta_0 = exp(`r b0`) = `r exp(b0)`$  
$\beta_1 = exp(`r b1`) = `r exp(b1)`$  

For a one unit increase in elevation, the count of surveys increases by `r exp(b1)`.

]

---

## Over-dispersion

- For exponential family of distributions, variance is a function of the mean:

$$Var(\epsilon) = \phi \mu$$

.ml4[
where $\phi$ is a scaling parameter, assumed to be equal to 1, meaning the variance is assumed to be equal to the mean.
]

- When $\phi > 1$, this is called _over-dispersion_. When $\phi < 1$, it's under-dispersion.



---

## Over-dispersion

.pull-left[

```{r}

dispersion <- tibble(
  lambda = log(fitted(surveys_glm)),
  variance = log((surveys$sites/surveys$area - lambda)^2)
)

ggplot(dispersion, aes(lambda, variance)) + 
  geom_point(
    size = 3,
    alpha = 0.7
  ) +
  geom_abline(
    intercept = 0,
    slope = 1,
    color = qaad_colors("rufous_red")
  ) +
  labs(
    x = "Mean",
    y = "Variance"
  )

```

]

.pull-right[

One way to check for over (or under) dispersion is by visual inspection, plotting the variance relative to the estimates, i.e., the conditional mean.

Note that the dark red line represents the expectation that $Var(\epsilon) = \mu$, so it has a slope of 1 and intercept at 0. 

The cloud of points above the line indicates over-dispersion.

]


---

## Overdispersion

.pull-left[

```{r}

dispersion <- tibble(
  lambda = log(fitted(surveys_glm)),
  variance = log((surveys$sites/surveys$area - lambda)^2)
)

ggplot(dispersion, aes(lambda, variance)) + 
  geom_point(
    size = 3,
    alpha = 0.7
  ) +
  geom_abline(
    intercept = 0,
    slope = 1,
    color = qaad_colors("rufous_red")
  ) +
  labs(
    x = "Mean",
    y = "Variance"
  )

```

]

.pull-right[

A simple rule of thumb is to compare a model's residual deviance to its degrees of freedom. The ratio of these should be 1. Values greater than one indicate over-dispersion.

```{r}

bob <- summary(surveys_glm)

```

For our site count model, that's

$D = `r bob$deviance`$  
$df = `r bob$df.residual`$  

$D/df = `r bob$deviance/bob$df.residual`$

Again, this is greater than one, indicating over-dispersion.

]

---

## Over-dispersion

.pull-left[

```{r}

dispersion <- tibble(
  lambda = log(fitted(surveys_glm)),
  variance = log((surveys$sites/surveys$area - lambda)^2)
)

ggplot(dispersion, aes(lambda, variance)) + 
  geom_point(
    size = 3,
    alpha = 0.7
  ) +
  geom_abline(
    intercept = 0,
    slope = 1,
    color = qaad_colors("rufous_red")
  ) +
  labs(
    x = "Mean",
    y = "Variance"
  )

```

]

.pull-right[

We can also test for this idea using a simple linear model where

$$Var(\epsilon) = \mu + \alpha \mu$$

If variance is equal to the mean, then $\alpha = 0$. 

<br>

```{r}

# code adopted from AER::dispersiontest()

y <- model.response(model.frame(surveys_glm))
yhat <- fitted(surveys_glm)

variance <- ((y - yhat)^2 - y)/yhat

dispersion_model <- lm(variance ~ 1)

tibble(
  estimate = as.vector(coefficients(dispersion_model)[1]),
  statistic = as.vector(summary(dispersion_model)$coef[1, 3]),
  null = 0,
  p.value = pnorm(statistic, lower.tail = FALSE)
) %>% 
  kbl(
    format = "html",
    caption = "H-test for Overdispersion<br>Alternative: &alpha; > 0"
  ) %>% 
  kable_paper(
    "striped", 
    full_width = FALSE,
    position = "left"
  ) 

remove(y, yhat, variance)
  
```

]

---

## Over-dispersion

```{r}

quasi_glm <- glm(
  sites ~ elevation + offset(log(area)),
  family = quasipoisson,
  data = surveys
)

phi_poisson <- bob$dispersion

phi_quasi <- summary(quasi_glm)$dispersion

```


Two strategies for addressing this:

1. Use a quasi-Poisson model
2. Use a negative binomial model (also for counts/rates)

A drawback of the quasi-Poisson is that it does not use MLE to estimate coefficients, so many of the statistics, like the AIC, that we would want to use in inference are not available.

---

## &#x1F52D; Looking Ahead

```{r}

schedule <- here("_misc", "course_outline.xlsx") %>% 
  readxl::read_excel(1) %>% 
  mutate(
    date = lubridate::ymd(date),
    description = glue::glue(
    "<b>{topic}</b> <span style = 'font-size: 80%; color: #9C9C9C;'>({date})</span><br>
    <div style = 'font-size: 80%; font-style: italic; color: #9C9C9C;'>{details}</div>"
  )) %>% 
  select(description)

```

.pull-left[

```{r}

schedule %>% 
  slice(12:15) %>% 
  kbl(escape = FALSE, col.names = NULL) %>% 
  kable_paper(c("striped", "hover"))

```

]
