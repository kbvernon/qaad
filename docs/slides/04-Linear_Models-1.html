<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Lecture 04: Linear Models 1</title>
    <meta charset="utf-8" />
    <script src="libs/header-attrs/header-attrs.js"></script>
    <link href="libs/tachyons/tachyons.min.css" rel="stylesheet" />
    <script src="libs/fabric/fabric.min.js"></script>
    <link href="libs/xaringanExtra-scribble/scribble.css" rel="stylesheet" />
    <script src="libs/xaringanExtra-scribble/scribble.js"></script>
    <script>document.addEventListener('DOMContentLoaded', function() { window.xeScribble = new Scribble({"pen_color":["#FF0000"],"pen_size":3,"eraser_size":30,"palette":[]}) })</script>
    <link href="libs/tile-view/tile-view.css" rel="stylesheet" />
    <script src="libs/tile-view/tile-view.js"></script>
    <link href="libs/animate.css/animate.xaringan.css" rel="stylesheet" />
    <link href="libs/panelset/panelset.css" rel="stylesheet" />
    <script src="libs/panelset/panelset.js"></script>
    <script src="libs/clipboard/clipboard.min.js"></script>
    <link href="libs/shareon/shareon.min.css" rel="stylesheet" />
    <script src="libs/shareon/shareon.min.js"></script>
    <link href="libs/xaringanExtra-shareagain/shareagain.css" rel="stylesheet" />
    <script src="libs/xaringanExtra-shareagain/shareagain.js"></script>
    <script src="libs/kePrint/kePrint.js"></script>
    <link href="libs/lightable/lightable.css" rel="stylesheet" />
    <link rel="stylesheet" href="xaringan-themer.css" type="text/css" />
    <link rel="stylesheet" href="custom_style.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Lecture 04: Linear Models 1
### Last updated: 2022-01-31

---








## &amp;#x1F4CB; Lecture Outline

- &amp;#x1F9EA; A simple experiment
- Competing Models
- A simple formula
- Bivariate statistics
    - Covariance
    - Correlation
    - Non-linear correlation
    - Correlation between categories
    - Correlation is not causation!
- A general formula
- Simple Linear Regression
- Ordinary Least Squares (OLS)
- Multiple Linear Regression
- &amp;#x1F697; Cars Model, again
- Regression assumptions


---
count: false

## &amp;#x1F9EA; A simple experiment

.pull-left[

&lt;img src="04-Linear_Models-1_files/figure-html/unnamed-chunk-4-1.png" width="504" style="display: block; margin: auto;" /&gt;

]

.pull-right[

&lt;br&gt;

We take ten cars, send each down a track, have them brake at the same point, and measure the distance it takes them to stop.  

__Question:__ how far do you think it will take the next car to stop?

]


---

## Competing Models

.pull-left[

&lt;img src="04-Linear_Models-1_files/figure-html/unnamed-chunk-5-1.png" width="504" style="display: block; margin: auto;" /&gt;

.right[Expectation: mean distance &amp;nbsp;&amp;nbsp;]

]


---

## Competing Models

.pull-left[

&lt;img src="04-Linear_Models-1_files/figure-html/unnamed-chunk-6-1.png" width="504" style="display: block; margin: auto;" /&gt;

.right[Expectation: mean distance &amp;nbsp;&amp;nbsp;]

]

.pull-right[

&lt;img src="04-Linear_Models-1_files/figure-html/unnamed-chunk-7-1.png" width="504" style="display: block; margin: auto;" /&gt;

.right[Expectation: some function of speed &amp;nbsp;&amp;nbsp;]

]





---

## A simple formula

.pull-left[

.w-100.bg-near-white.ba.bw1.br2.shadow-4.ph3.mb3.h6rem[

`$$y_i = E(Y) + \epsilon_i$$`
`$$\epsilon \sim N(0, \sigma)$$`

]

.center[

.pull-left.bg-dark_purple_o_70.ba.bw1.br2.shadow-4.pt4.pa3.h5[.white[

.f4.fw6[Simple Model]

$$
`\begin{align}
E(Y) &amp;= \mu  \\\\
\bar{y} &amp;=10.54  \\\\
\sigma &amp;= 5.353 \\
\end{align}`
$$   

]]

.pull-right.bg-maize_crayola_o_50.ba.bw1.br2.shadow-4.pt4.pa3.h5[

.f4.fw6[Complex Model]

$$
`\begin{align}
E(Y) &amp;= \beta X \\\\ 
\hat\beta &amp;= 1.936 \\\\
\sigma &amp;= 3.087 \\
\end{align}`
$$   

]

]

]



---
count: false

## A simple formula

.pull-left[

.w-100.bg-near-white.ba.bw1.br2.shadow-4.ph3.mb3.h6rem[

`$$y_i = E(Y) + \epsilon_i$$`
`$$\epsilon \sim N(0, \sigma)$$`

]

.center[

.pull-left.bg-dark_purple_o_70.ba.bw1.br2.shadow-4.pt4.pa3.h5[.white[

.f4.fw6[Simple Model]

$$
`\begin{align}
E(Y) &amp;= \mu  \\\\
\bar{y} &amp;=10.54  \\\\
\sigma &amp;= 5.353 \\
\end{align}`
$$    

]]

.pull-right.bg-maize_crayola_o_50.ba.bw1.br2.shadow-4.pt4.pa3.h5[

.f4.fw6[Complex Model]

$$
`\begin{align}
E(Y) &amp;= \beta X \\\\ 
\hat\beta &amp;= 1.936 \\\\
\sigma &amp;= 3.087 \\
\end{align}`
$$   

]

]

]

.pull-right[

&amp;#x2696;&amp;#xFE0F; __The error is smaller__ for the more complex model. This is a good thing, __but what did it cost us__? Need to weigh this against the increased complexity!  

&lt;img src="04-Linear_Models-1_files/figure-html/unnamed-chunk-8-1.png" width="100%" style='margin-top: 40px;' style="display: block; margin: auto;" /&gt;

]


---

## Bivariate statistics

.center[Explore the relationship between two variables:]

&lt;img src="04-Linear_Models-1_files/figure-html/unnamed-chunk-9-1.png" width="75%" style="display: block; margin: auto;" /&gt;


---

## Covariance

.pull-left[

The extent to which two variables vary together. 

`$$cov(x, y) = \frac{1}{n-1} \sum_{i=1}^{n}  (x_i - \bar{x})(y_i - \bar{y})$$`

* Sign reflects positive or negative trend, but magnitude depends on units of measure (e.g., `\(cm\)` vs `\(km\)`).  
* Variance is the covariance of a variable with itself.  

]

.pull-right[

&lt;img src="04-Linear_Models-1_files/figure-html/unnamed-chunk-10-1.png" width="504" style="display: block; margin: auto;" /&gt;

]


---

## Correlation

.pull-left[

Pearson's Correlation Coefficient

`$$r = \frac{cov(x,y)}{s_{x}s_y}$$`

- Normalizes covariance (scales from -1 to 1) using standard deviations, `\(s\)`, thus making the magnitude of `\(r\)` independent of units of measure.  

- The significance of `\(r\)` can be tested by converting it to a _t_-statistic and comparing it to a _t_-distribution with `\(n-2\)` degrees of freedom.

]

.pull-right[

&lt;img src="04-Linear_Models-1_files/figure-html/unnamed-chunk-11-1.png" width="504" style="display: block; margin: auto;" /&gt;

]



---

## Nonlinear correlation

.pull-left[

Spearman's Rank Correlation Coefficient

`$$\rho = \frac{cov\left(R(x),\; R(y) \right)}{s_{R(x)}s_{R(y)}}$$`

- Pearson's correlation but with ranks (R).  
- This makes it a _robust_ estimate, less sensitive to outliers.  

]

.pull-right[

&lt;img src="04-Linear_Models-1_files/figure-html/unnamed-chunk-12-1.png" width="504" style="display: block; margin: auto;" /&gt;

]



---

## Correlation between categories

.pull-left[

`\(\chi^2\)`-statistic

`$$\chi^2 = \sum \frac{(y_{ij}-E(y_{ij}))^2}{E(y_{ij})}$$`

- Where `\(y_{ij}\)` is the observed count in row `\(i\)` and column `\(j\)` and `\(E(y_{ij})\)` is the expected count in `\(ij\)`.
- To get the significance of this statistic, compare it to a `\(\chi^2\)`-distribution with `\(k-1\)` degrees of freedom ($k$ being the number of categories).

]

.pull-right[

&lt;img src="04-Linear_Models-1_files/figure-html/unnamed-chunk-13-1.png" width="504" style="display: block; margin: auto;" /&gt;

]



---

## Correlation is not causation!

&lt;br&gt;

&lt;img src="04-Linear_Models-1_files/figure-html/unnamed-chunk-14-1.png" width="70%" style="display: block; margin: auto;" /&gt;

.center[Adapted from &lt;https://www.tylervigen.com/spurious-correlations&gt;]





---

## A general formula



.flex[
.ph4.mr3.bg-near-white.ba.bw1.br2.shadow-4.flex.items-center[.center[
`$$y_i = E(Y) + \epsilon_i$$`
`$$E(Y) \approx \hat\beta X$$`
]]
.flex.items-center.o-0[.center[<svg preserveAspectRatio="none" aria-hidden="true" role="img" viewBox="0 0 512 512" style="height:2em;width:2em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:gray45;overflow:visible;position:relative;"><path d="M256 8c137 0 248 111 248 248S393 504 256 504 8 393 8 256 119 8 256 8zm-28.9 143.6l75.5 72.4H120c-13.3 0-24 10.7-24 24v16c0 13.3 10.7 24 24 24h182.6l-75.5 72.4c-9.7 9.3-9.9 24.8-.4 34.3l11 10.9c9.4 9.4 24.6 9.4 33.9 0L404.3 273c9.4-9.4 9.4-24.6 0-33.9L271.6 106.3c-9.4-9.4-24.6-9.4-33.9 0l-11 10.9c-9.5 9.6-9.3 25.1.4 34.4z"/></svg>]]
.ph4.mh3.bg-maize_crayola_o_50.ba.bw1.br2.shadow-4.flex.items-center.o-0[.center[
`$$y_i = \hat\beta X + \epsilon_i$$`
]]
.flex.items-center.o-0[.center[<svg preserveAspectRatio="none" aria-hidden="true" role="img" viewBox="0 0 512 512" style="height:2em;width:2em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:gray45;overflow:visible;position:relative;"><path d="M256 8c137 0 248 111 248 248S393 504 256 504 8 393 8 256 119 8 256 8zm-28.9 143.6l75.5 72.4H120c-13.3 0-24 10.7-24 24v16c0 13.3 10.7 24 24 24h182.6l-75.5 72.4c-9.7 9.3-9.9 24.8-.4 34.3l11 10.9c9.4 9.4 24.6 9.4 33.9 0L404.3 273c9.4-9.4 9.4-24.6 0-33.9L271.6 106.3c-9.4-9.4-24.6-9.4-33.9 0l-11 10.9c-9.5 9.6-9.3 25.1.4 34.4z"/></svg>]]
.ph4.mh3.bg-dark_purple_o_70.ba.bw1.br2.shadow-4.flex.items-center.o-0[.center.white[
`$$y_i = \hat\beta_0 + \hat\beta_1 x_i + \ldots +  \hat\beta_n x_i + \epsilon_i$$`
]]
]

&lt;br&gt;

- `\(y_i\)` is the .fw6.dark_purple[dependent variable]  
- `\(X\)` is a set of .fw6.dark_purple[independent variables] `\(x_{i1}, \ldots, x_{in}\)`  
    - called the design matrix
    - typically includes a constant (1) for the intercept
- `\(\beta\)` is a set of .fw6.dark_purple[coefficients of relationship]
    - `\(\beta_0\)` is the intercept
    - `\(\beta_1, \ldots, \beta_n\)` are the coefficients for independent variables `\(x_{i1}, \ldots, x_{in}\)`  
- `\(\epsilon_i\)` is the .fw6.dark_purple[residuals] or errors 





---
count: false

## A general formula

.flex[
.ph4.mr3.bg-near-white.ba.bw1.br2.shadow-4.flex.items-center[.center[
`$$y_i = E(Y) + \epsilon_i$$`
`$$E(Y) \approx \hat\beta X$$`
]]
.flex.items-center[.center[<svg preserveAspectRatio="none" aria-hidden="true" role="img" viewBox="0 0 512 512" style="height:2em;width:2em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:gray45;overflow:visible;position:relative;"><path d="M256 8c137 0 248 111 248 248S393 504 256 504 8 393 8 256 119 8 256 8zm-28.9 143.6l75.5 72.4H120c-13.3 0-24 10.7-24 24v16c0 13.3 10.7 24 24 24h182.6l-75.5 72.4c-9.7 9.3-9.9 24.8-.4 34.3l11 10.9c9.4 9.4 24.6 9.4 33.9 0L404.3 273c9.4-9.4 9.4-24.6 0-33.9L271.6 106.3c-9.4-9.4-24.6-9.4-33.9 0l-11 10.9c-9.5 9.6-9.3 25.1.4 34.4z"/></svg>]]
.ph4.mh3.bg-maize_crayola_o_50.ba.bw1.br2.shadow-4.flex.items-center[.center[
`$$y_i = \hat\beta X + \epsilon_i$$`
]]
.flex.items-center.o-0[.center[<svg preserveAspectRatio="none" aria-hidden="true" role="img" viewBox="0 0 512 512" style="height:2em;width:2em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:gray45;overflow:visible;position:relative;"><path d="M256 8c137 0 248 111 248 248S393 504 256 504 8 393 8 256 119 8 256 8zm-28.9 143.6l75.5 72.4H120c-13.3 0-24 10.7-24 24v16c0 13.3 10.7 24 24 24h182.6l-75.5 72.4c-9.7 9.3-9.9 24.8-.4 34.3l11 10.9c9.4 9.4 24.6 9.4 33.9 0L404.3 273c9.4-9.4 9.4-24.6 0-33.9L271.6 106.3c-9.4-9.4-24.6-9.4-33.9 0l-11 10.9c-9.5 9.6-9.3 25.1.4 34.4z"/></svg>]]
.ph4.mh3.bg-dark_purple_o_70.ba.bw1.br2.shadow-4.flex.items-center.o-0[.center.white[
`$$y_i = \hat\beta_0 + \hat\beta_1 x_i + \ldots +  \hat\beta_n x_i + \epsilon_i$$`
]]
]

&lt;br&gt;

- `\(y_i\)` is the .fw6.dark_purple[dependent variable]  
- `\(X\)` is a set of .fw6.dark_purple[independent variables] `\(x_{i1}, \ldots, x_{in}\)`  
    - called the design matrix
    - typically includes a constant (1) for the intercept
- `\(\beta\)` is a set of .fw6.dark_purple[coefficients of relationship]
    - `\(\beta_0\)` is the intercept
    - `\(\beta_1, \ldots, \beta_n\)` are the coefficients for independent variables `\(x_{i1}, \ldots, x_{in}\)`  
- `\(\epsilon_i\)` is the .fw6.dark_purple[residuals] or errors 

---
count: false

## A general formula

.flex[
.ph4.mr3.bg-near-white.ba.bw1.br2.shadow-4.flex.items-center[.center[
`$$y_i = E(Y) + \epsilon_i$$`
`$$E(Y) \approx \hat\beta X$$`
]]
.flex.items-center[.center[<svg preserveAspectRatio="none" aria-hidden="true" role="img" viewBox="0 0 512 512" style="height:2em;width:2em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:gray45;overflow:visible;position:relative;"><path d="M256 8c137 0 248 111 248 248S393 504 256 504 8 393 8 256 119 8 256 8zm-28.9 143.6l75.5 72.4H120c-13.3 0-24 10.7-24 24v16c0 13.3 10.7 24 24 24h182.6l-75.5 72.4c-9.7 9.3-9.9 24.8-.4 34.3l11 10.9c9.4 9.4 24.6 9.4 33.9 0L404.3 273c9.4-9.4 9.4-24.6 0-33.9L271.6 106.3c-9.4-9.4-24.6-9.4-33.9 0l-11 10.9c-9.5 9.6-9.3 25.1.4 34.4z"/></svg>]]
.ph4.mh3.bg-maize_crayola_o_50.ba.bw1.br2.shadow-4.flex.items-center[.center[
`$$y_i = \hat\beta X + \epsilon_i$$`
]]
.flex.items-center[.center[<svg preserveAspectRatio="none" aria-hidden="true" role="img" viewBox="0 0 512 512" style="height:2em;width:2em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:gray45;overflow:visible;position:relative;"><path d="M256 8c137 0 248 111 248 248S393 504 256 504 8 393 8 256 119 8 256 8zm-28.9 143.6l75.5 72.4H120c-13.3 0-24 10.7-24 24v16c0 13.3 10.7 24 24 24h182.6l-75.5 72.4c-9.7 9.3-9.9 24.8-.4 34.3l11 10.9c9.4 9.4 24.6 9.4 33.9 0L404.3 273c9.4-9.4 9.4-24.6 0-33.9L271.6 106.3c-9.4-9.4-24.6-9.4-33.9 0l-11 10.9c-9.5 9.6-9.3 25.1.4 34.4z"/></svg>]]
.ph4.mh3.bg-dark_purple_o_70.ba.bw1.br2.shadow-4.flex.items-center[.center.white[
`$$y_i = \hat\beta_0 + \hat\beta_1 x_{i1} + \ldots +  \hat\beta_n x_{in} + \epsilon_i$$`
]]
]

&lt;br&gt;

- `\(y_i\)` is the .fw6.dark_purple[dependent variable]  
- `\(X\)` is a set of .fw6.dark_purple[independent variables] `\(x_{i1}, \ldots, x_{in}\)`  
    - called the design matrix
    - typically includes a constant (1) for the intercept
- `\(\beta\)` is a set of .fw6.dark_purple[coefficients of relationship]
    - `\(\beta_0\)` is the intercept
    - `\(\beta_1, \ldots, \beta_n\)` are the coefficients for independent variables `\(x_{i1}, \ldots, x_{in}\)`  
- `\(\epsilon_i\)` is the .fw6.dark_purple[residuals] or errors 




---

## Simple Linear Regression

.pull-left[

&lt;img src="04-Linear_Models-1_files/figure-html/unnamed-chunk-16-1.png" width="504" style="display: block; margin: auto;" /&gt;

]

.pull-right[

&lt;br&gt;

Only one explanatory variable: speed (m)

Equation: `\(y_i = \beta_0 + \beta_1 speed_i + \epsilon_i\)`

- `\(\beta_0\)` = -2.011
- `\(\beta_1\)` = 1.936

]


---

## Simple Linear Regression

.pull-left[

&lt;img src="04-Linear_Models-1_files/figure-html/unnamed-chunk-17-1.png" width="504" style="display: block; margin: auto;" /&gt;

]

.pull-right[

&lt;br&gt;

Only one explanatory variable: speed (m)

Equation: `\(y_i = \beta_0 + \beta_1 speed_i + \epsilon_i\)`

- `\(\beta_0\)` = -2.011
- `\(\beta_1\)` = 1.936

__Question:__ How did we get these values?

]



---

## Ordinary Least Squares (OLS)

.pull-left[

A method for estimating the coefficients, `\(\beta\)`, in a linear regression model by minimizing the __Residual Sum of Squares__, `\(SS_{R}\)`.  

`$$SS_{R} = \sum_{i=1}^{n} (y_{i}-\hat{y}_i)^2$$`

where `\(\hat{y}=E(Y)\)`. To minimize this, we take its derivative with respect to `\(\beta\)` and set it equal to zero.

`$$\frac{d\, SS_{R}}{d\, \beta} = 0$$`

]

.pull-right[

&lt;img src="04-Linear_Models-1_files/figure-html/unnamed-chunk-18-1.png" width="504" style="display: block; margin: auto;" /&gt;

]



---

## Ordinary Least Squares (OLS)

.pull-left[

Provides simple estimators for the coefficients:  

***
__Slope__ .gray[Ratio of covariance to variance]
`$$\beta_{1} = \frac{cov(x, y)}{var(x)}$$` 

***
__Intercept__ .gray[Marginal difference in sample means]
`$$\beta_{0} = \bar{y} - \beta_1 \bar{x}$$`

If `\(\beta_{1} = 0\)`, then `\(\beta_{0} = \bar{y}\)`. 

]

.pull-right[

&lt;img src="04-Linear_Models-1_files/figure-html/unnamed-chunk-19-1.png" width="504" style="display: block; margin: auto;" /&gt;

]




---

## Multiple Linear Regression

.pull-left[

OLS can be extended to models containing multiple explanatory variables using matrix algebra. Then the coefficient estimator is:

`$$\hat\beta = (X^{T}X)^{-1}X^{T}y$$`

where `\(X^T\)` is the transpose of the design matrix.  

_If you squint a little_, this is analogous to the simple linear estimator: 
- `\(X^{T}X\)` &amp;#x21E8; variance. 
- `\(X^{T}y\)` &amp;#x21E8; covariance.

]

--

.pull-right[

This is important for highlighting a critical assumption of linear regression: 

&lt;br&gt;

__No collinearity in the explanatory variables!__

&lt;br&gt;

If this is violated, `\((X^{T}X)^{-1}\)` can't be calculated.

]



---

## &amp;#x1F697; Cars Model, again

.pull-left[

&lt;img src="04-Linear_Models-1_files/figure-html/unnamed-chunk-20-1.png" width="504" style="display: block; margin: auto;" /&gt;

]

.pull-right[



&lt;br&gt;

.center[

.pull-left-38.bg-maize_crayola_o_50.ba.bw1.br2.shadow-4.pa2.h5[

.f4.fw6[Slope]  

$$
`\begin{align}
\beta_{1} &amp;= \frac{cov(x,y)}{var(x)} \\\\ 
\beta_{1} &amp;= \frac{10.426}{5.385} \\\\
\beta_{1} &amp;= 1.936
\end{align}`
$$  

]

.pull-right-60.bg-dark_purple_o_70.ba.bw1.br2.shadow-4.pa2.h5[.white[

.f4.fw6[Intercept]

$$
`\begin{align}
\beta_{0} &amp;= \bar{y} - \beta_{1}\bar{x} \\\\ 
\beta_{0} &amp;= 10.54 - 1.936 * 6.482 \\\\
\beta_{0} &amp;= -2.011
\end{align}`
$$

]]

]

]


---

## Regression assumptions

1. __Weak Exogeneity__: the predictor variables have fixed values and are known. 

2. __Linearity__: the relationship between the predictor variables and the response variable is linear.  

2. __Constant Variance__: the variance of the errors does not depend on the values of the predictor variables. Also known as _homoscedasticity_.  

2. __Independence of errors__: the errors are uncorrelated with each other.  

2. __No perfect collinearity__: the predictors are not linearly correlated with each other.  



---

## &amp;#x1F52D; Looking Ahead



.pull-left[

&lt;table class=" lightable-paper lightable-striped lightable-hover" style='font-family: "Arial Narrow", arial, helvetica, sans-serif; margin-left: auto; margin-right: auto;'&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; &lt;b&gt;Linear Models 1&lt;/b&gt; &lt;span style="font-size: 80%; color: #9C9C9C;"&gt;(2022-02-01)&lt;/span&gt;&lt;br&gt;
&lt;div style="font-size: 80%; font-style: italic; color: #9C9C9C;"&gt;Covariance. Correlation. Ordinary Least Squares. Regression Assumptions.&lt;/div&gt; &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; &lt;b&gt;Linear Models 2&lt;/b&gt; &lt;span style="font-size: 80%; color: #9C9C9C;"&gt;(2022-02-08)&lt;/span&gt;&lt;br&gt;
&lt;div style="font-size: 80%; font-style: italic; color: #9C9C9C;"&gt;R-squared. ANOVA. Coefficient Standard Errors. Residuals and Diagnostic Plots.&lt;/div&gt; &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; &lt;b&gt;Linear Models 3&lt;/b&gt; &lt;span style="font-size: 80%; color: #9C9C9C;"&gt;(2022-02-15)&lt;/span&gt;&lt;br&gt;
&lt;div style="font-size: 80%; font-style: italic; color: #9C9C9C;"&gt;Prediction. Interpretation. Polynomial Regression. Log-transformations. Heteroschedasticity.&lt;/div&gt; &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; &lt;b&gt;Linear Models 4&lt;/b&gt; &lt;span style="font-size: 80%; color: #9C9C9C;"&gt;(2022-02-22)&lt;/span&gt;&lt;br&gt;
&lt;div style="font-size: 80%; font-style: italic; color: #9C9C9C;"&gt;Multiple Linear Regression, Multicollinearity, Variance Inflation Factor, ANOVA&lt;/div&gt; &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

]
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "magula",
"highlightLines": true,
"countIncrementalSlides": false,
"ratio": "16:9"
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
