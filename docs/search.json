[
  {
    "objectID": "classes.html",
    "href": "classes.html",
    "title": "Classes",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nDate\n\n\nTitle\n\n\nDescription\n\n\n\n\n\n\nJan 10, 2023\n\n\nWeek 01: Introduction\n\n\nWhy statistics? Why R? Why Quarto? Making a webpage and a plot in just seconds!\n\n\n\n\nJan 17, 2023\n\n\nWeek 02: Probability as a Model\n\n\nA simple experiment. Random variables. Probability distributions. Probability as a model.\n\n\n\n\nJan 24, 2023\n\n\nWeek 03: Statistical Inference\n\n\nTesting hypotheses, visualizing distributions, reading and writing tabular data, and learning how to work with it in R.\n\n\n\n\nJan 31, 2023\n\n\nWeek 04: Ordinary Least Squares\n\n\nCalculating and testing bivariate statistics, including correlation and covariance. Visualizing probability densities. Fitting linear models using ordinary least squares. And some simple table indexing in R.\n\n\n\n\nFeb 7, 2023\n\n\nWeek 05: Visualizing Distributions\n\n\nA review and a deep dive into visualizing distributions in R, including bar chart, histogram, kernel density, cumulative density, and boxplot. Examples are given using base R {graphics} and {ggplot2}.\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "classes/01-intro.html#lab-exercises",
    "href": "classes/01-intro.html#lab-exercises",
    "title": "Week 01: Introduction",
    "section": "🔬 Lab Exercises",
    "text": "🔬 Lab Exercises\nWeek 01 Lab"
  },
  {
    "objectID": "classes/02-probability.html#lab-exercises",
    "href": "classes/02-probability.html#lab-exercises",
    "title": "Week 02: Probability as a Model",
    "section": "🔬 Lab Exercises",
    "text": "🔬 Lab Exercises\nWeek 02 Lab"
  },
  {
    "objectID": "classes/03-inference.html#lab-exercises",
    "href": "classes/03-inference.html#lab-exercises",
    "title": "Week 03: Statistical Inference",
    "section": "🔬 Lab Exercises",
    "text": "🔬 Lab Exercises\nWeek 03 Lab"
  },
  {
    "objectID": "classes/04-ols.html#lab-exercises",
    "href": "classes/04-ols.html#lab-exercises",
    "title": "Week 04: Ordinary Least Squares",
    "section": "🔬 Lab Exercises",
    "text": "🔬 Lab Exercises\nWeek 04 Lab"
  },
  {
    "objectID": "classes/05-distributions.html#lab-exercises",
    "href": "classes/05-distributions.html#lab-exercises",
    "title": "Week 05: Visualizing Distributions",
    "section": "🔬 Lab Exercises",
    "text": "🔬 Lab Exercises\nWeek 05 Lab"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Quantitative Analysis of Archaeological Data",
    "section": "",
    "text": "This is a Github page setup to host lectures and other content for the University of Utah course ANTH 5850: Quantitative Analysis of Archaeological Data (affectionately referred to as “quad”). As its name suggests, this class offers students quantitative tools and techniques for working with archaeological data. Those tools include, first and foremost, the language of statistics, but also importantly the statistical programming language R, and finally the mark-up language Markdown (via Quarto), which aids in literate programming (think science communication). Obviously, no one can become fluent in a language - much less three languages! - with just four months of exposure. For that, there is no substitute for immersion, for living and working with these languages and the people who speak them, meaning scientists. This course is merely designed to get you started on that process and to hopefully make it smoother for you as you go. I think the word for it is a “survey” course.\nOn this website, you’ll find course lecture slides and labs. These are organized by class meetings, which you can find a link to in the navbar. The site was built using the open-source scientific and technical publishing system, Quarto, which you’ll also learn about in this course! The source code for the website, along with the lecture slides and lab exercises, can be found at the associated Github repository."
  },
  {
    "objectID": "index.html#inspiration",
    "href": "index.html#inspiration",
    "title": "Quantitative Analysis of Archaeological Data",
    "section": "Inspiration?",
    "text": "Inspiration?\nI can’t take credit for all of the content in this course. The lecture slides, in particular, are adapted from the lectures of Dr. Simon Brewer in the Department of Geography at the University of Utah. The R labs, at least the parts of them concerned with data science rather than statistics, draw heavily on the very popular book R for Data Science (2e) by Hadley Wickham and Garrett Grolemund.\nIt probably goes without saying, of course, but those folks are way smarter than I could ever hope to be, so any errors or confusions that occur here are definitely, one-hundred percent, without a doubt my own."
  },
  {
    "objectID": "index.html#reuse",
    "href": "index.html#reuse",
    "title": "Quantitative Analysis of Archaeological Data",
    "section": "Reuse",
    "text": "Reuse\n\nText and figures are licensed under Creative Commons Attribution CC BY 4.0. Any computer code (R, HTML, CSS, etc.) in slides and worksheets, including in slide and worksheet sources, is also licensed under MIT. Note that figures in slides may be pulled in from external sources and may be licensed under different terms. For such images, image credits are available in the slide notes, accessible via pressing the letter ‘p’."
  },
  {
    "objectID": "labs/01-intro-lab.html",
    "href": "labs/01-intro-lab.html",
    "title": "Lab 01: Introduction",
    "section": "",
    "text": "In this lab, you will learn\n\nhow to use RStudio\nhow to make a plot with R\nhow to do math in R, create objects, use functions, etc.,\nhow to create an R Project folder\nhow to make a website (what?!) with Quarto\nand, you’ll also learn the ends and outs of a typical workflow in R\n\nNo additional packages required this week.\n\n\n\ncars\n\nIncludes measurements of car speed and stopping distance.\npackage: datasets\n\nreference: https://stat.ethz.ch/R-manual/R-devel/library/datasets/html/cars.html"
  },
  {
    "objectID": "labs/01-intro-lab.html#working-in-rstudio",
    "href": "labs/01-intro-lab.html#working-in-rstudio",
    "title": "Lab 01: Introduction",
    "section": "Working in RStudio",
    "text": "Working in RStudio\n\n\n\n\nIf you are going to do anything with R, RStudio is hands-down the best place to do it. RStudio is an open-source integrated development environment (or IDE) that makes programming in R simpler, more efficient, and most importantly, more reproducible. Some of its more user-friendly features are syntax highlighting (it displays code in different colors depending on what it is or does, which makes it easier for you to navigate the code that you’ve written), code completion (it will try to guess what code you are attempting to write and write it for you), and keyboard shortcuts for the more repetitive tasks.\nPane layout\nWhen you first open RStudio, you should see three window panes: the Console, the Environment, and the Viewer. If you open an R script, a fourth Source pane will also open. The default layout of these panes is shown in the figure above.\n\n\nSource. The Source pane provides basic text editing functionality, allowing you to create and edit R scripts. Importantly, you cannot execute the code in these scripts directly, but you can save the scripts that you write as simple text files. A dead give away that you have an R script living on your computer is the .R extension, for example, my_script.R.\n\n\nConsole. The Console pane, as its name suggests, provides an interface to the R console, which is where your code actually gets run. While you can type R code directly into the console, you can’t save the R code you write there into an R script like you can with the Source editor. That means you should reserve the console for non-essential tasks, meaning tasks that are not required to replicate your results.\n\nEnvironment. The Environment pane is sort of like a census of your digital zoo, providing a list of its denizens, i.e., the objects that you have created during your session. This pane also has the History tab, which shows the R code you have sent to the console in the order that you sent it.\n\n\nViewer. The Viewer pane is a bit of a catch-all, including a Files tab, a Plots tab, a Help tab, and a Viewer tab.\n\nThe Files tab works like a file explorer. You can use it to navigate through folders and directories. By default, it is set to your working directory.\nThe Plots tab displays any figures you make with R.\nThe Help tab is where you can go to find helpful R documentation, including function pages and vignettes.\nThe actual Viewer tab provides a window to visualize R Markdown.\n\n\n\nLet’s try out a few bits of code just to give you a sense of the difference between Source and Console.\n\nAs you work through this lab, you can practice running code in the Console, but make sure to do the actual exercises in an R script.\n\nExercises\n\nFirst, let’s open a new R script. To open an R script in RStudio, just click File > New File > R Script (or hit Ctrl + Shift + N, Cmd + Shift + N on Mac OS).\nCopy this code into the console and hit Enter.\n\n\nrep(\"Boba Fett\", 5)\n\n\nNow, copy that code into the R script you just opened and hit Enter again. As you see, the code does not run. Instead, the cursor moves down to the next line. To actually run the code, put the cursor back on the line with the code, and hit Ctrl + Enter (CMD + Enter on Mac OS)."
  },
  {
    "objectID": "labs/01-intro-lab.html#make-your-first-plot",
    "href": "labs/01-intro-lab.html#make-your-first-plot",
    "title": "Lab 01: Introduction",
    "section": "Make Your First Plot!",
    "text": "Make Your First Plot!\nTo ease you into working with R, let’s visualize some data to answer a simple question: Do fast moving objects take longer to slow down than slow moving objects? Don’t worry about understanding all of this! It’s just to give you a feel for the sort of graphics you can make with R. We’ll actually spend all of the next lab learning how to make even better graphics.\nThe data\nTo answer that question, we’ll use the cars data.frame that comes pre-loaded with R. A data.frame is simply an R object that stores tabular data, with rows for each observation and columns for each variable. Let’s have a look at the first n rows of this table, specifically the first 5 rows. We can do this using the function head().\n\nhead(cars, n = 5)\n\n  speed dist\n1     4    2\n2     4   10\n3     7    4\n4     7   22\n5     8   16\n\n\nNote that, in this case, each row represents a car. The first column, or variable, records the speed (in miles per hour) each car was traveling when it applied its brakes, and the second column provides measures of the distances (in feet) that each took to stop.\nThe plot() function\nThe base R graphics package provides a generic function for plotting, which - as you might have guessed - is called plot(). To see how it works, try running this code:\n\nplot(cars)\n\n\n\n\nCustomizing your plot\nWith the plot() function, you can do a lot of customization to the resulting graphic. For instance, you can modify all of the following:\n\n\npch will change the point type,\n\nmain will change the main plot title,\n\nxlab and ylab will change the x and y axis labels,\n\ncex will change the size of shapes within the plot region,\n\npch will change the type of point used (you can use triangles, squares, or diamonds, among others),\n\ncol changes the color of the point (or its border), and\n\nbg changes the color of the point fill (depending on the type of point it is)\n\nFor instance, try running this code:\n\nplot(\n  cars,\n  pch = 21,\n  bg = \"darkorange\",\n  col = \"darkred\",\n  cex = 2\n)\n\n\n\n\nExercises\n\nComplete the following line of code to preview only the first three rows of the cars table.\n\n\nhead(cars, n = )\n\n\nModify the code below to change the size (cex) of the points from 2 to 1.5.\n\n\nplot(\n  cars,\n  pch = 21,\n  bg = \"darkorange\",\n  col = \"darkred\",\n  cex = 2\n)\n\n\nWhat does this plot tell us about the relationship between car speed and stopping distance? Is it positive or negative? Or is there no relationship at all? If there is a relationship, what might explain it?\nComplete the code below to add “Stopping distance for cars” as the main title.\n\n\nplot(\n  cars,\n  pch = 21,\n  bg = \"darkorange\",\n  col = \"darkred\",\n  cex = 1,\n  main = \n)\n\n\nComplete the code below to add “Speed (mph)” as the x-axis label and “Distance (ft)” as the y-axis label.\n\n\nplot(\n  cars,\n  pch = 21,\n  bg = \"darkorange\",\n  col = \"darkred\",\n  cex = 2,\n  main = \"Stopping distance for cars\",\n  xlab = ,\n  ylab = \n)"
  },
  {
    "objectID": "labs/01-intro-lab.html#r-basics",
    "href": "labs/01-intro-lab.html#r-basics",
    "title": "Lab 01: Introduction",
    "section": "R Basics",
    "text": "R Basics\n\n\n\n\nR is a calculator\nYou can just do math with it:\n\n300 * (2/25)\n\n[1] 24\n\n3^2 + 42\n\n[1] 51\n\nsin(17)\n\n[1] -0.961\n\n\nObjects and Functions\nBut, R is more than just a calculator. There are a lot of things you can make with R, and a lot of things you can do with it. The things that you make are called objects and the things that you do things with are called functions. Any complex statistical operation you want to conduct in R will almost certainly involve the use of one or more functions.\nCalling functions\nTo use a function, we call it like this:\n\nfunction_name(arg1 = value1, arg2 = value2, ...)\n\nTry calling the seq() function.\n\nseq(from = 1, to = 5)\n\n[1] 1 2 3 4 5\n\n\nAs you can see, this generates a sequence of numbers starting at 1 and ending at 5. There are two things to note about this. First, we do not have to specify the arguments explicitly, but they must be in the correct order:\n\nseq(1, 5) \n\n[1] 1 2 3 4 5\n\nseq(5, 1)\n\n[1] 5 4 3 2 1\n\n\nSecond, the seq() function has additional arguments you can specify, like by and length. While we do not have to specify these because they have default values, you can change one or the other (but not at the same time!):\n\nseq(1, 5, by = 2)\n\n[1] 1 3 5\n\nseq(1, 5, length = 3)\n\n[1] 1 3 5\n\n\nCreating objects\nTo make an object in R, you use the arrow, <-, like so:\n\nobject_name <- value\n\nTry creating an object with value 5.137 and assigning it to the name bob, like this:\n\nbob <- 5.137\n\nThere are three things to note here. First, names in R must start with a letter and can only contain letters, numbers, underscores, and periods.\n\n# Good\nwinter_solder <- \"Buckey\"\nobject4 <- 23.2\n\n# Bad\nwinter soldier <- \"Buckey\" # spaces not allowed\n4object <- 23.2            # cannot start with a number\n\nSecond, when you create an object with <-, it ends up in your workspace or environment (you can see it in the RStudio environment pane). Finally, it is worth noting that the advantage of creating objects is that we can take the output of one function and pass it to another.\n\nx <- seq(1, 5, length = 3)\n\nlogx <- log(x)\n\nexp(logx)\n\n[1] 1 3 5\n\n\nExercises\n\nUse seq() to generate a sequence of numbers from 3 to 12.\nUse seq() to generate a sequence of numbers from 3 to 12 with length 25.\nWhy doesn’t this code work?\n\n\nseq(1, 5, by = 2, length = 10)\n\n\nUse <- to create an object with value 25 and assign it to a name of your choice.\nNow try to create another object with a different value and name.\nWhat is wrong with this code?\n\n\n2bob <- 10"
  },
  {
    "objectID": "labs/01-intro-lab.html#workflow",
    "href": "labs/01-intro-lab.html#workflow",
    "title": "Lab 01: Introduction",
    "section": "Workflow",
    "text": "Workflow\n\n\n\n\nAs you work more and more in R, you will learn that statistical analyses tend to involve the same basic set of tasks:\n\n\nimporting data,\n\nwrangling data to get it into a format necessary for analysis,\n\nexploring data with some simple descriptive statistics,\n\n\nanalyzing data with models to investigate potential trends or relationships, and\n\nsummarizing the results.\n\nAt various stages, you will also spend considerable time\n\n\nvisualizing the data and the results, either to explore the data further or to help communicate the results to others.\n\nA lot of the output of this process, we will also want to save for later, perhaps to include in a publication (like a figure or model summary), but maybe also to avoid repetition of difficult and time-consuming tasks, so the workflow will also involve\n\n\nexporting refined data and models.\n\nTo make this more concrete, let’s try out an example, working with the cars data again. As we go through this, try running all the code in the console.\nAn Example\nSuppose we return to the question we asked in the plotting section: Does the speed a car is going when it applies its brakes determine the distance it takes the car to stop? Obviously, the answer is Yes, but let’s pretend we don’t know the answer, so we can walk through the process of answering the question anyway.\nImport\nFirst, we need some data. In this case, we do not actually need to import the cars dataset because it is already available to us in R, so let’s just pretend like we did.\nExplore\nNow, let’s explore the data. Always, always, always, the best way to explore data is to visualize data! We already did this once, but it can’t hurt to try it again!\n\nplot(\n  cars,\n  pch = 21,\n  bg = adjustcolor(\"gray\", 0.75),\n  col = \"gray25\",\n  cex = 1.5\n)\n\n\n\n\nThen, we can do things like calculate the mean stopping distance.\n\nmean(cars$dist)\n\n[1] 43\n\n\nNote that we use the $ operator to pull the distance (dist) values from the cars table and supply it to the mean() function. Don’t worry too much about wrapping your head around that idea as we will talk about it more in another lab. We can also make a histogram to explore the distribution of stopping distances:\n\nhist(cars$dist)\n\n\n\n\nWhat does this tell you about car stopping distances? Is it clustered? Random?\nWrangle\nMaybe we think that one really long distance is exceptional, perhaps owing to measurement error, and we want to remove it from our analysis. In that case, we want to subset the data, including only distance values less than some amount, say 100 ft.\n\ncars <- subset(cars, dist < 100)\n\nThis is data wrangling, preparing the data for analysis.\nAnalyze\nNow, finally, we might want to answer our question directly by modeling the relationship between car speeds and stopping distances. Here, our hypothesis is that there is no relationship. This is called the null hypothesis. If we can show that this hypothesis is very likely false, then we can with some confidence accept the alternative hypothesis, namely, that there is a relationship. To test the null hypothesis, we can construct a simple linear model. In R, we do this:\n\ndistance_model <- lm(dist ~ speed, data = cars)\n\nsummary(distance_model)\n\n\nCall:\nlm(formula = dist ~ speed, data = cars)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-26.79  -9.15  -1.67   8.01  43.05 \n\nCoefficients:\n            Estimate Std. Error t value        Pr(>|t|)    \n(Intercept)  -14.002      6.295   -2.22           0.031 *  \nspeed          3.640      0.392    9.29 0.0000000000033 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 14.1 on 47 degrees of freedom\nMultiple R-squared:  0.647, Adjusted R-squared:  0.64 \nF-statistic: 86.3 on 1 and 47 DF,  p-value: 0.00000000000326\n\n\nWoah! That’s a lot to digest. For now, just note that the asterisks (*) imply that that there is very likely a relationship between speed and distance. But, what is that relationship? Or, what does it look like? Well, let’s try to visualize that in R, too.\n\nplot(\n  cars,\n  pch = 21,\n  bg = adjustcolor(\"gray\", 0.75),\n  col = \"gray25\",\n  cex = 1.5,\n  xlab = \"Speed (mph)\",\n  ylab = \"Distance (ft)\"\n)\n\nabline(\n  distance_model, \n  col = \"#A20000\",\n  lwd = 2\n)\n\ntitle(\n  \"Distance Model\",\n  line = 0.3, \n  adj = 0, \n  cex = 1.5\n)\n\n\n\n\nExport\nNow, if we feel it necessary, we can save our model, so we can inspect it again later.\n\nsave(distance_model, file = \"distance_model.Rds\")\n\nAnd that’s it! Now, all we have to do is write this up and publish it! Easy peasy."
  },
  {
    "objectID": "labs/01-intro-lab.html#r-projects",
    "href": "labs/01-intro-lab.html#r-projects",
    "title": "Lab 01: Introduction",
    "section": "R Projects",
    "text": "R Projects\n\n\n\n\nYOU CANNOT EAT R CODE. Believe me. You can’t. Eventually, you’ll have to close out of R, turn off your computer, walk away, and do whatever it is that you do to maintain your existence. That means you need some way to save your progress and you need some place to save it. R has a few built-in tools for this, and they are really convenient, at least early on. However, you will be much better off if you get into the habit of using RStudio Projects. What is an R Project? Basically, it’s a folder on your computer that you can use to organize all the data, code, figures, texts, and analyses associated with a single scientific research project.\nWhen you open your project in RStudio, it will establish your project folder as your working directory by default. The advantage of this is that you can access R scripts and data using relative file paths rather than specifying the full path from your computer’s root directory. Why is this advantageous? Because you can copy the project folder to any computer you want and your relative file paths will just work!\nExercises\n\nBefore we setup your project, let’s turn off some of R’s default settings.\n\nIn RStudio, go to Tools > Global Options….\nIn the dialog box that appears, navigate to the General section, and under Workspace, make sure “Restore .RData into workspace at startup” is unchecked.\nThen, for “Save workspace to .Rdata on exit”, select Never.\nHit “Apply”, then hit “OK.”\n\n\n\nNow, we are going to create a new project for you for this class. You will use this folder to save all your lab and homework exercises, required datasets, and figures. To do that, follow these steps:\n\nIn RStudio, go to File > New Project….\nIn the dialog box that appears, select New Directory, then New Project.\nPut “qaad” as the Directory name.\nThen Browse to a location on your computer where you would like to keep this project and hit “OK.”\nMake sure “Create a git repository” and “Use renv with this project” are unchecked.\n\nThen click “Create Project.” This will restart RStudio with your project loaded. You can confirm this by looking at the top left of the RStudio window. It should say “qaad - RStudio” now. If you look in the File pane (bottom-right), you will also see a file called “qaad.Rproj.”\n\n\nOnce you have your project folder setup, have a look at the Files pane again. You should see a button that says “New Folder.” Click that, and in the dialog box that appears, enter “R” and hit “OK.” You should now see a folder in your project directory called “R.” This is where you will keep all the files with your R code in it. Repeat this process to add “data”, “figures”, and “_misc” folders to your project. The “_misc” folder is short for miscellaneous. This folder is not strictly necessary but I find it helpful. It’s like that drawer in the kitchen where random stuff goes. It might not be clean or orderly, but at least your kitchen is!\nJust to check that everything is working, minimize RStudio and navigate to the location of your R Project on your computer. Do you see the folders you have created and the “qaad.Rproj” file?"
  },
  {
    "objectID": "labs/01-intro-lab.html#quarto",
    "href": "labs/01-intro-lab.html#quarto",
    "title": "Lab 01: Introduction",
    "section": "Quarto",
    "text": "Quarto\n\n\n\n\n\nFigure 1: Artwork from “Hello, Quarto” keynote by Julia Lowndes and Mine Çetinkaya-Rundel, presented at RStudio Conference 2022. Illustrated by Allison Horst.\n\n\nQuarto offers a unified framework for statistical programming and science communication by letting you write and run R code alongside text to explain your methods to others. The text you write is formatted using Markdown syntax (the same syntax you would use on, for example, a Reddit post). The basis for creating documents using Quarto is a test-based file format with the extension “.qmd”, short for Quarto Markdown. In just about every one of these documents you come across, you will find three major components:\n\na YAML header surrounded at the top and bottom by three hyphens, ---,\nR code chunks surrounded at the top and bottom by three back ticks, ```, and\ntext formatted with markdown syntax like # heading 1 and _italics_.\n\nHere is an example:\n---\ntitle: \"Week 01\"\nauthor: \"Kenneth Blake Vernon\"\ndate: \"1/10/2023\"\nformat: html\nexecute:\n  echo: true\n---\n\n## Lab Exercises\n\n### Plot\n\n1. Complete the following line of code to preview only the first three rows of the `cars` table.\n\n```{r}\n\nhead(cars, n = )\n\n```\n\n***\n\n## Homework Exercises\n\n1. \nLet’s start with some simple markdown formatting and work our way back to the YAML.\nMarkdown formatting\nMarkdown is a lightweight markup language for formatting plain text and is designed to be easy to read and write. The markdown you will use most often includes all of the following (borrowed from here:\nText formatting \n------------------------------------------------------------\n\n*italic*  or _italic_\n**bold**   __bold__\n`code`\nsuperscript^2^ and subscript~2~\n\nHeadings\n------------------------------------------------------------\n\n# 1st Level Header\n\n## 2nd Level Header\n\n### 3rd Level Header\n\nLists\n------------------------------------------------------------\n\n*   Bulleted list item 1\n\n*   Bulleted list item 2\n\n    * Nested list item 2a\n\n    * Nested list item 2b\n\n1.  Numbered list item 1\n\n1.  Numbered list item 2. The numbers are incremented automatically in the output.\n\n1.  Numbered list item 3. \n\nLinks and images\n------------------------------------------------------------\n\n<http://example.com>\n\n[linked phrase](http://example.com)\n\n![optional caption text](path/to/img.png)\nYAML\n‘YAML’ is a recursive acronym that means “YAML Ain’t Markup Language.” You don’t actually need to know that. I just think it’s funny. The YAML controls features of the whole document, specifying, for instance, the title and author. It looks like this:\n---\ntitle: \"Week 01\"\nauthor: \"Kenneth Blake Vernon\"\ndate: \"1/11/2022\"\nformat: html\n---\nNotice the three dashes --- at the top and bottom. These must be there in order for Quarto to recognize it as the YAML. You should always include this at the beginning of the document. There’s A LOT you can specify in the YAML. In fact, you can specify basically anything you want, but being quite new to Quarto, I don’t think that would be helpful. For now, let me draw your attention to the format field. Quarto (with the power of a utility known as Pandoc) can generate a wide variety of output formats, including Word documents, PDFs, revealjs slides (presentations, what the slides in tihs class are built with), and even Powerpoint (if you really insist on it). In this class, we’ll stick with the default HTML output, so the only thing you will need to specify in the YAML is the title, author, and date.\nBy the way, the HTML output is the same stuff that a website is built on. In fact, when you open the resulting HTML file, it will open in your browser.\nR code chunks\nAll R code that you want to run needs to be “fenced” by three back ticks ```. You also need to tell Quarto that it’s R code and not, say, Python or SQL. To do that, you add {r} after the first set of back ticks. Altogether, it should look like this:\n```{r}\n\n1+1\n\n```\nInstead of typing this every time, you can use Ctrl + Alt + I (or CMD) in RStudio, and it will automatically generate a code chunk in your qmd document. You can run the code in these chunks like you would code in an R script, by placing the cursor over it and hitting Ctrl + Enter. You can specify options for code chunks in R Markdown that will affect the way that they behave or are displayed. You can find a complete list of chunk options at http://yihui.name/knitr/options/. Here are a few examples:\n\n\neval: false prevents code from being evaluated.\n\necho: false hides the code but not the results of running the code.\n\nwarning: false prevents R messages and warnings from appearing in the knitted document.\n\nHere is how it would look to specify these in a code chunk:\n```{r}\n#| echo: false\n#| warning: false\n\n1+1\n\n```\nYou can also set these globally, applying them to all code chunks, by specifying them in the execute field (for code execution) in the YAML at the top of your qmd document. It would look like this:\n---\ntitle: \"Week 01\"\nauthor: \"Kenneth Blake Vernon\"\ndate: \"1/11/2022\"\nformat: html\nexecute:\n  echo: false\n  warning: false\n---\nThere are loads more of these options, some of the more important ones involve figures you generate with these documents. Rather than overload you with all of those now, we’ll try to go over some of those here or there in future labs and homework exercises.\nExercises\n\nLet’s create a new qmd document in RStudio. To do that, follow these steps:\n\nGo to File > New File > Quarto Document….\nIn the dialog box that appears, put “ANTH 5580 (QAAD) Week 01” as the Title.\nPut your name as Author.\nHit “OK”.\n\n\n\nRStudio will open a new qmd document for you. Notice that it sets up the YAML for you. Let’s copy the template we will use for these course assignments. To do that, follow these steps:\n\nScroll up to the section above with the example of a qmd document. You can now copy and paste this into your qmd document (I recommend typing it out by hand, so you can get a feel for it, but that’s not necessary).\n\n\nNotice that this template has two level two headers, “Lab Exercises” and “Homework Exercises.” These are the two major assignments you will have to complete each week. You will enter all your answers in a qmd document with this format and submit it via Canvas. To make sure your code is actually working, you can “render” the document and see if it completes without error. This is partly what I will do each week when grading your assignments. To keep these things organized, each exercise section in the lab should have its own level three header, like ### Plot for this week. Since there is no R related homework assignment for this week, you can just delete that section from this qmd document. Before continuing, save your qmd document to the R folder in your course project directory.\nNow, go back through this lab and re-do the exercises by adding them to this qmd document. Make sure to save that again, then submit it on the Canvas course page. Again, go ahead and render the document, too, just to make sure everything is working. This is the process that you will go through each week!"
  },
  {
    "objectID": "labs/01-intro-lab.html#homework",
    "href": "labs/01-intro-lab.html#homework",
    "title": "Lab 01: Introduction",
    "section": "Homework",
    "text": "Homework\nThere is no R related homework assignment for this week. Please fill out the pre-course self-assessment survey on Canvas."
  },
  {
    "objectID": "labs/02-probability-lab.html",
    "href": "labs/02-probability-lab.html",
    "title": "Lab 02: Statistical Graphics",
    "section": "",
    "text": "This lab will guide you through the process of\n\nloading (or “attaching”) R packages with library()\n\ngenerating summary statistics for your data\nvisualizing data with the grammar of graphics and ggplot()\n\naesthetic mappings\ngeometric objects\nfacets\nscales\nthemes\n\n\n\nWe will be using the following packages:\n\narchdata\nggplot2\npalmerpenguins\nskimr\nviridis\n\n\n\npenguins\n\nIncludes measurements for penguin species, island in Palmer Archipelago, size (flipper length, body mass, bill dimensions), and sex.\npackage: palmerpenguins\n\nreference: https://allisonhorst.github.io/palmerpenguins/reference/penguins.html\n\n\n\n\nDartPoints\n\nIncludes measurements of 91 Archaic dart points recovered during surface surveys at Fort Hood, Texas.\npackage: archdata\n\nreference: https://cran.r-project.org/web/packages/archdata/archdata.pdf"
  },
  {
    "objectID": "labs/02-probability-lab.html#the-library",
    "href": "labs/02-probability-lab.html#the-library",
    "title": "Lab 02: Statistical Graphics",
    "section": "The Library",
    "text": "The Library\nR is an extensible programming language, meaning you can write R code to extend the functionality of base R. To share that code, R users will often bundle it into a package, a collection of functions, data, and documentation. You can think of packages as apps, but apps specifically designed for R. To make the functionality a package offers available in R, you have to load them in with the library() function (the technical term is attach).\nYou should always, always, always load all the packages you use at the beginning of a document. That way, people who read your code know exactly what packages you are using all at once and right away. To make this really, really explicit, I prefer to set this off with its own section that I call the “R Preamble.” In a Quarto document, it looks like this:\n## R Preamble\n\n```{r packages}\n#| warning: false\n\nlibrary(archdata)\nlibrary(ggplot2)\nlibrary(palmerpenguins)\nlibrary(skimr)\nlibrary(viridis)\n\n```\nOf course, these aren’t just automatically on your computer, so you have to install the packages first. Then you can open them in R. To do that, you use the function install.packages(). For the packages used today, you can use this call just once like so:\ninstall.packages(\n  c(\"archdata\", \"ggplot2\", \"palmerpenguins\", \"skimr\", \"viridis\")\n)\nNote that you only need to run this once, so don’t put this as a line in your Quarto document, which you might render multiple times. Just run it in the console.\nExercises\n\nOpen a new Quarto document and add the R Preamble with an R code chunk with the library() calls that load the R packages required for this lab.\nNow actually run each library() call. You can do that by either highlighting them and hitting Ctrl + Enter (Cmd + Enter) or by clicking the green arrow that appears in the top right of the code chunk."
  },
  {
    "objectID": "labs/02-probability-lab.html#summary-statistics",
    "href": "labs/02-probability-lab.html#summary-statistics",
    "title": "Lab 02: Statistical Graphics",
    "section": "Summary Statistics",
    "text": "Summary Statistics\n\n\n\n\n\nFigure 1: Artwork by Allison Horst.\n\n\nLet’s use R to describe some properties of a sample of penguins from Palmer Station in Antarctica. These data became available in R when you loaded the palmerpenguins package. They aren’t currently visible in your environment (for complicated reasons), but trust me, they’re there. The name of the dataset is penguins, so you can call it that way.\n\nhead(penguins, n = 5)\n\n# A tibble: 5 × 8\n  species island    bill_length_mm bill_depth_mm flipper_l…¹ body_…² sex    year\n  <fct>   <fct>              <dbl>         <dbl>       <int>   <int> <fct> <int>\n1 Adelie  Torgersen           39.1          18.7         181    3750 male   2007\n2 Adelie  Torgersen           39.5          17.4         186    3800 fema…  2007\n3 Adelie  Torgersen           40.3          18           195    3250 fema…  2007\n4 Adelie  Torgersen           NA            NA            NA      NA <NA>   2007\n5 Adelie  Torgersen           36.7          19.3         193    3450 fema…  2007\n# … with abbreviated variable names ¹​flipper_length_mm, ²​body_mass_g\n\n\n\n\nCentral tendency\nThe central tendency is, as its name suggests, a value around which other values tend to cluster. There are two primary measures of central tendency: the mean and the median. As you may recall, the mean or average of a sample is simply the sum of a finite set of values, \\(x_i\\), divided by the number of values, \\(n\\).\n\\[\\bar{x} = \\frac{1}{n} \\sum_{i=1}^{n} x_i\\]\nRemember that this is only an estimate of the central tendency of a population, \\(\\mu\\)! In R, you can calculate the mean by hand if you like, but it’s probably easier to use the built-in R function, mean(). Let’s use this to calculate the mean bill length of penguins.\n\nmean(penguins$bill_length_mm)\n\n[1] NA\n\n\nWhoops! Need to set na.rm = TRUE to ignore missing or NA values.\n\nmean(penguins$bill_length_mm, na.rm = TRUE)\n\n[1] 43.9\n\n\nAn important limitation of the mean is its sensitivity to outliers - you know, like rich people. If you calculate the mean household income in the United States, for example, the incomes of obscenely wealthy individuals like Jeff Bezos and Elon Musk will pull that measure up, thus painting a much rosier picture of the US than the reality the rest of us live in. When there are extreme outliers, it is often advisable to use the median because it is less sensitive to outliers as it is the “middle” number or value that evenly divides the sample in half.\n\nmedian(penguins$bill_length_mm, na.rm = TRUE)\n\n[1] 44.5\n\n\n\n\n\n\n\n\nNote\n\n\n\n\n\nBy the way, did you notice the weird use of the dollar sign $? This is used to index data and pull out a specific element by its name. For example, when we run the code snippet penguins$bill_length_mm, we are asking R to pull the variable bill_length_mm from the penguins table and give us its values. One important implication of this sort of indexing is that we can assign the variable to its own object outside of the table, e.g.\nbill_length <- penguins$bill_length_mm\nYou’ll actually learn more about this, in particular how to work with tabular data, in the next lab.\n\n\n\nDispersion\nDispersion describes the spread of data around its central tendency. Are the values tightly clustered around the central tendency or highly dispersed? Is there, in other words, a lot of variability? This is what dispersion seeks to characterize. As with the central tendency, it has two primary measures: variance and standard deviation. The variance of a sample is the mean squared error.\n\\[s^{2} = \\frac{1}{n-1} \\sum_{i=1}^{n} (x_i - \\bar{x})^{2}\\]\nThis is an estimate of the population variance, \\(\\sigma^2\\). To calculate the variance of a sample with R, use var().\n\nvar(penguins$body_mass_g, na.rm = TRUE)\n\n[1] 643131\n\n\nWhoa! That’s a really big number. Do penguins get that big? The answer, of course, is No. The number is large because variance is squared error, so this is in units of squared-grams, \\(g^2\\), not grams, \\(g\\). Why do we square it? Well, if you think about it, some of the errors will be negative and some will be positive (some penguins will be larger than average and some smaller), but on balance - on average - the sum of the errors will tend towards zero, which is not terribly informative of the spread of the data. Or, put that another way, what we want with a measure of dispersion is not just difference but distance from the mean, and distance is always positive. Squaring the errors is one way of ensuring that the values are positive (similar to taking the absolute value).\nThere is one small catch to squaring though. It makes it a really weird measure to think about - like, what is a square gram? Hard to say. That’s why it is common to take the square root of the variance, to get the measure back into units of the data. This value is known as the standard deviation, \\(s\\). You can calculate it with the sd() function.\n\nsd(penguins$body_mass_g, na.rm = TRUE)\n\n[1] 802\n\n\nThat’s about 1.8 pounds (if you prefer Imperial units).\nTable summaries\nTo generate summary statistics for all the variables in your data, base R provides a really nice summary() function that you can apply to a table like so:\n\nsummary(penguins)\n\n      species          island    bill_length_mm bill_depth_mm \n Adelie   :152   Biscoe   :168   Min.   :32.1   Min.   :13.1  \n Chinstrap: 68   Dream    :124   1st Qu.:39.2   1st Qu.:15.6  \n Gentoo   :124   Torgersen: 52   Median :44.5   Median :17.3  \n                                 Mean   :43.9   Mean   :17.1  \n                                 3rd Qu.:48.5   3rd Qu.:18.7  \n                                 Max.   :59.6   Max.   :21.5  \n                                 NA's   :2      NA's   :2     \n flipper_length_mm  body_mass_g       sex           year     \n Min.   :172       Min.   :2700   female:165   Min.   :2007  \n 1st Qu.:190       1st Qu.:3550   male  :168   1st Qu.:2007  \n Median :197       Median :4050   NA's  : 11   Median :2008  \n Mean   :201       Mean   :4202                Mean   :2008  \n 3rd Qu.:213       3rd Qu.:4750                3rd Qu.:2009  \n Max.   :231       Max.   :6300                Max.   :2009  \n NA's   :2         NA's   :2                                 \n\n\nAs you can see, this prints out summary statistics for the variables in your data. However, the printout is not easy to read and it provides a somewhat limited set of summary statistics. As an alternative, you might try the skim() function from the skimr package.\n\nNote that I have applied css styling to this table output to make it more compact and fit on the screen. Yours will look slightly different.\n\nskim(penguins)\n\n\nData summary\n\n\nName\npenguins\n\n\nNumber of rows\n344\n\n\nNumber of columns\n8\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n3\n\n\nnumeric\n5\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\nspecies\n0\n1.00\nFALSE\n3\nAde: 152, Gen: 124, Chi: 68\n\n\nisland\n0\n1.00\nFALSE\n3\nBis: 168, Dre: 124, Tor: 52\n\n\nsex\n11\n0.97\nFALSE\n2\nmal: 168, fem: 165\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\nbill_length_mm\n2\n0.99\n43.9\n5.46\n32.1\n39.2\n44.5\n48.5\n59.6\n▃▇▇▆▁\n\n\nbill_depth_mm\n2\n0.99\n17.1\n1.97\n13.1\n15.6\n17.3\n18.7\n21.5\n▅▅▇▇▂\n\n\nflipper_length_mm\n2\n0.99\n200.9\n14.06\n172.0\n190.0\n197.0\n213.0\n231.0\n▂▇▃▅▂\n\n\nbody_mass_g\n2\n0.99\n4201.8\n801.95\n2700.0\n3550.0\n4050.0\n4750.0\n6300.0\n▃▇▆▃▂\n\n\nyear\n0\n1.00\n2008.0\n0.82\n2007.0\n2007.0\n2008.0\n2009.0\n2009.0\n▇▁▇▁▇\n\n\n\n\n\n\n\nAs you can see, there are three major sections of this printout: (i) Data Summary, (ii) Factor variables, and (iii) Numeric variables. The Data Summary gives you an overview of your table, with counts of the number of rows and columns, as well as counts of the different types of variables (factor, numeric, etc). The section on factor variables gives you counts for each level of the factor (for example, counts of the different species of penguins), as well as information about missing data. Finally, the section on numeric variables gives you information on missing data, as well as measures of dispersion and central tendency, including the mean, median (p50), and standard deviation (sd), the range or minimum and maximum values (p0 and p100), and the inner quartiles (p25 and p75).11 You’ll learn more about quartiles and how to visualize distributions in the next lab.\nExercises\n\nHave a look at the penguins data again. Use head() to preview the first 15 rows.\nWith the penguins data, calculate all of the following:\n\nmedian body mass\nmean bill depth\nvariance in bill depth\nstandard deviation in bill length"
  },
  {
    "objectID": "labs/02-probability-lab.html#the-grammar-of-graphics",
    "href": "labs/02-probability-lab.html#the-grammar-of-graphics",
    "title": "Lab 02: Statistical Graphics",
    "section": "The Grammar of Graphics",
    "text": "The Grammar of Graphics\nIt’s easy to imagine how you would go about with pen and paper drawing a bar chart of, say, the number of penguins in each species in the penguins dataset. But, what if you had to dictate the steps to make that graph to another person, one you can’t see or physically interact with? All you can do is use words to communicate the graphic you want. How would you do it? The challenge here is that you and your illustrator must share a coherent vocabulary for describing graphics. That way you can unambiguously communicate your intent. That’s essentially what the grammar of graphics is, a language with a set of rules (a grammar) for specifying each component of a graphic.\nNow, if you squint just right, you can see that R has a sort of grammar built-in with the base graphics package. To visualize data, it provides the default plot() function, which you learned about in the last lab. This is a workhorse function in R that will give you a decent visualization of your data fast, with minimal effort. It does have its limitations though. For starters, the default settings are, shall we say, less than appealing. I mean, they’re fine if late-nineties styles are your thing, but less than satisfying if a more modern look is what you’re after.2 Second, taking fine-grained control over graphics generated with plot() can be quite frustrating, especially when you want to have a faceted figure (a figure with multiple plot panels).2 But, you know, opinions, everyone has them, and there’s no accounting for taste.\nThat’s where the ggplot2 package comes in. It provides an elegant implementation of the grammar of graphics, one with more modern aesthetics and with a more standardized framework for fine-tuning figures, so that’s what we’ll be using here. From time to time, I’ll try to give you examples of how to do things with the plot() function, too, so you can speak sensibly to the die-hard holdouts, but we’re going to focus on learning ggplot.\n\n\n\n\n\n\nTip\n\n\n\n\n\nThere are several excellent sources of additional information on statistical graphics in R and statistical graphics in general that I would recommend.\n\nThe website for the ggplot2 package: https://ggplot2.tidyverse.org/. This has loads of articles and references that will answer just about any question you might have.\n\nThe R graph gallery website: https://r-graph-gallery.com/. This has straightforward examples of how to make all sorts of different plot visualizations, both with base R and ggplot.\n\nClaus Wilke’s free, online book Fundamentals of Data Visualization, which provides high-level rules or guidelines for generating statistical graphics in a way that clearly communicates its meaning or intent and is visually appealing.\n\nThe free, online book ggplot2: Elegant Graphics for Data Analysis (3ed) by Hadley Wickham, Danielle Navarro, and Thomas Lin Pedersen. This is a more a deep dive into the grammar of graphics than a cookbook, but it also has lots of examples of making figures with ggplot2.\n\n\n\n\nSo, to continue our analogy above, we’re going to treat R like our illustrator, and ggplot2 is the language we are going to speak to R to visualize our data. So, how do we do that? Well, let’s start with the basics. Suppose we want to know if there’s some kind of relationship (an allometric relationship) among the Palmer Station penguins between their body mass and bill length. Here’s how we would visualize that.\n\nggplot(data = penguins) + \n  geom_point(\n    mapping = aes(x = body_mass_g, y = bill_length_mm)\n  )\n\n\n\n\nHere, we have created what is known as a scatterplot, a representation of the raw data as points on a Cartesian grid. There are several things to note about the code used to generate this plot.\n\nFirst, it begins with a call to the ggplot() function. This takes a data argument. In this case, we say that we want to make a plot to visualize the penguins data.\nThe next function call is geom_point(). This is a way of specifying the geometry we want to plot. Here we chose points, but we could have used another choice (lines, for example, or polygons).\nNote that the geom_point() call takes a mapping argument. You use this to specify how variables in your data are mapped to properties of the graphic. Here, we chose to map the body_mass_g variable to the x-coordinates and the bill_length_mm variable to the y-coordinates. Importantly, we use the aes() function to supply an aesthetic to the mapping parameter. This is always the case.\nThe final thing to point out here is that we combined or connected these arguments using the plus-sign, +. You should read this literally as addition, as in “make this ggplot of the penguins data and add a point geometry to it.” Be aware that the use of the plus-sign in this way is unique to the ggplot2 package and won’t work with other graphical tools in R.\n\nWe can summarize these ideas with a simple template. All that is required to make a graph in R is to replace the elements in the bracketed sections with a dataset, a geometry function, and an aesthetic mapping.\nggplot(data = <DATA>) + \n  <GEOM_FUNCTION>(mapping = aes(<MAPPINGS>))\nOne of the great things about ggplot, something that makes it stand out compared to alternative graphics engines in R, is that you can assign plots to a variable and call it in different places, or modify it as needed.\npenguins_plot <- ggplot(data = penguins) + \n  geom_point(\n    mapping = aes(x = body_mass_g, y = bill_length_mm)\n  )\n\npenguins_plot\nExercises\n\nRecreate the scatterplot above, but switch the axes. Put bill length on the x-axis and body mass on the y-axis.\nNow create a scatterplot of bill length (on the y-axis) by bill depth (on the x-axis)."
  },
  {
    "objectID": "labs/02-probability-lab.html#aesthetics",
    "href": "labs/02-probability-lab.html#aesthetics",
    "title": "Lab 02: Statistical Graphics",
    "section": "Aesthetics",
    "text": "Aesthetics\nIn the plot above, we only specified the position of the points (the x- and y-coordinates) in the aesthetic mapping, but there are many aesthetics (see the figure below), and we can map the same or other variables to those.\n\n\nFigure 2: Commonly used aesthetics. Figure from Claus O. Wilke. Fundamentals of Data Visualization. O’Reilly, 2019.\n\n\nConsider, for example, the fact that there are three penguin species in our dataset: Adelie, Gentoo, and Chinstrap. Do we think the relationship between body mass and bill length holds for all of them? Let’s add penguin species to our aesthetic mapping (specifically to the color parameter) and see what happens.\n\nggplot(data = penguins) + \n  geom_point(\n    mapping = aes(x = body_mass_g, y = bill_length_mm, color = species)\n  )\n\n\n\n\nNotice that ggplot2 automatically assigns a unique color to each species and adds a legend to the right that explains each color. In this way, the color doesn’t just change the look of the figure. It conveys information about the data. Rather than mapping a variable in the data to a specific aesthetic, though, we can also define an aesthetic manually for the geometry as a whole. In this case, the aesthetics do not convey information about the data. They merely change the look of the figure. The key to doing this is to move the specification outside the aes(), but still inside the geom_point() function.\n\nggplot(data = penguins) + \n  geom_point(\n    mapping = aes(x = body_mass_g, y = bill_length_mm),\n    shape = 21,\n    size = 4,\n    color = \"darkred\",\n    fill = \"darkgoldenrod1\"\n  )\n\n\n\n\nNotice that we specified the shape with a number. R has 25 built-in shapes that you can specify with a number, as shown in the figure below. Some important differences in these shapes concern the border and fill colors. The hollow shapes (0-14) have a border that you specify with color, the solid shapes (15-20) have a border and fill, both specified with color, and the filled shapes (21-24) have separate border and fill colors, specified with color and fill respectively.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\n\n\nNote that you can use hexadecimal codes like #004F2D instead of “forestgreen” to specify a color. This also allows you to specify a much wider range of colors. See https://htmlcolorcodes.com/ for one way of exploring colors.\n\n\n\nExercises\n\nChange the code below to map the species variable to the x-axis (in addition to the color).\n\nggplot(data = penguins) + \n  geom_point(\n    mapping = aes(x = body_mass_g, y = bill_length_mm, color = species)\n  )\n\nWhat does this do to the position of the points?\nChange the code below to map the species variable to the shape aesthetic (in addition to the color).\n\n# hint: use shape = ...\nggplot(data = penguins) + \n  geom_point(\n    mapping = aes(x = body_mass_g, y = bill_length_mm, color = species)\n  )\n\nChange the code below to map the species variable to the size aesthetic (replacing color).\n\n# hint: use size = ...\nggplot(data = penguins) + \n  geom_point(\n    mapping = aes(x = body_mass_g, y = bill_length_mm, color = species)\n  )\n\nFor the following code, change the color, size, and shape aesthetics for the entire geometry (do not map them to the data).\n\nggplot(data = penguins) + \n  geom_point(\n    mapping = aes(x = body_mass_g, y = bill_length_mm),\n    color = , # <------- insert value here\n    size = ,  # <------- \n    shape =   # <------- \n  )"
  },
  {
    "objectID": "labs/02-probability-lab.html#geometries",
    "href": "labs/02-probability-lab.html#geometries",
    "title": "Lab 02: Statistical Graphics",
    "section": "Geometries",
    "text": "Geometries\nHave a look at these two plots.\n\n\n\n\n\n\n\n\n\n\nBoth represent the same data and the same x and y variables, but they do so in very different ways. That difference concerns their different geometries. As their name suggests, these are geometrical objects used to represent the data. To change the geometry, simply change the geom_*() function. For example, to create the plots above, use the geom_point() and geom_smooth() functions.\n# left\nggplot(data = penguins) + \n  geom_point(\n    mapping = aes(x = body_mass_g, y = bill_length_mm)\n  )\n\n# right\nggplot(data = penguins) + \n  geom_smooth(\n    mapping = aes(x = body_mass_g, y = bill_length_mm)\n  )\nWhile every geometry function takes a mapping argument, not every aesthetic works (or is needed) for every geometry. For example, there’s no shape aesthetic for lines, but there is a linetype. Conversely, points have a shape, but not a linetype.\n\nggplot(data = penguins) + \n  geom_smooth(\n    mapping = aes(x = body_mass_g, y = bill_length_mm, linetype = species),\n  )\n\n\n\n\nOne really important thing to note here is that you can add multiple geometries to the same plot to represent the same data. Simply add them together with +.\n\nggplot(data = penguins) + \n  geom_smooth(\n    mapping = aes(x = body_mass_g, y = bill_length_mm, linetype = species),\n  ) + \n  geom_point(\n    mapping = aes(x = body_mass_g, y = bill_length_mm, color = species)\n  )\n\n\n\n\nThat’s a hideous figure, though it should get the point across. While layering in this way is a really powerful tool for visualizing data, it does have one important drawback. Namely, it violates the DRY principle (Don’t Repeat Yourself), as it specifies the x and y variables twice. This makes it harder to make changes, forcing you to edit the same aesthetic parameters in multiple locations. To avoid this, ggplot2 allows you to specify a common set of aesthetic mappings in the ggplot() function itself. These will then apply globally to all the geometries in the figure.\nggplot(\n  data = penguins,\n  mapping = aes(x = body_mass_g, y = bill_length_mm)\n) +\n  geom_smooth(mapping = aes(linetype = species)) +\n  geom_point(mapping = aes(color = species))\nNotice that you can still specify specific aesthetic mappings in each geometry function. These will apply only locally to that specific geometry rather than globally to all geometries in the plot. In the same way, you can specify different data for each geometry.\n\nggplot(\n  data = penguins,\n  mapping = aes(x = body_mass_g, y = bill_length_mm)\n) +\n  geom_smooth(data = filter(penguins, species == \"Adelie\")) +\n  geom_point(mapping = aes(color = species))\n\n\n\n\nSome of the more important geometries you are likely to use include:\n\ngeom_point()\ngeom_line()\ngeom_segment()\ngeom_polygon()\ngeom_boxplot()\ngeom_histogram()\ngeom_density()\n\nWe’ll actually cover those last three in the section on plotting distributions. For a complete list of available geometries, see the layers section of the ggplot2 website reference page."
  },
  {
    "objectID": "labs/02-probability-lab.html#facets",
    "href": "labs/02-probability-lab.html#facets",
    "title": "Lab 02: Statistical Graphics",
    "section": "Facets",
    "text": "Facets\nSometimes mapping variables to aesthetics can generate a lot of noise and clutter, making it hard to read or interpret a figure. One way to handle this is to split your plot into multiple plots or facets based on levels of a categorical variable like species. To do this for one categorical variable, you use the facet_wrap() function.\n\nggplot(data = penguins) +\n  geom_point(\n    mapping = aes(x = body_mass_g, y = bill_length_mm)\n  ) +\n  facet_wrap(vars(species))\n\n\n\n\nPlacing species in the vars() function tells facet_wrap() to “split the plot by species.” If you want to split the plot by two categorical variables, like species and sex, use the facet_grid() function.\n\nggplot(data = penguins) + \n  geom_point(\n    mapping = aes(x = body_mass_g, y = bill_length_mm)\n  ) +\n  facet_grid(rows = vars(species), cols = vars(sex))\n\n\n\n\nEvidently, there are some penguins for whom the sex is unknown. To remove these penguins from the dataset, you can use the na.omit() function.\n\nggplot(data = na.omit(penguins)) + \n  geom_point(\n    mapping = aes(x = body_mass_g, y = bill_length_mm)\n  ) +\n  facet_grid(rows = vars(species), cols = vars(sex))\n\n\n\n\nExercises\n\nUse facet_wrap() to split the following scatterplot of the penguins data by sex.\n\nggplot(data = na.omit(penguins)) + \n  geom_point(\n    mapping = aes(x = body_mass_g, y = bill_length_mm)\n  ) + \n  facet_wrap() # <------- insert value here\n\nNow, map the species to the color aesthetic for the point geometry.\nUse facet_grid() to split the following scatterplot of the penguins data by species and island.\n\nggplot(data = na.omit(penguins)) + \n  geom_point(\n    mapping = aes(x = body_mass_g, y = bill_length_mm)\n  ) + \n  facet_grid() # <------- insert value here\n\nWhat does this tell you about how species are distributed across islands?"
  },
  {
    "objectID": "labs/02-probability-lab.html#scales",
    "href": "labs/02-probability-lab.html#scales",
    "title": "Lab 02: Statistical Graphics",
    "section": "Scales",
    "text": "Scales\nScales provide the basic structure that determines how data values get mapped to visual properties in a graph. The most obvious example is the axes because these determine where things will be located in the graph, but color scales are also important if you want your figure to provide additional information about your data. Here, we will briefly cover two aspects of scales that you will often want to change: axis labels and color palettes, in particular palettes that are colorblind safe.\nLabels\nBy default, ggplot2 uses the names of the variables in the data to label the axes. This, however, can lead to poor graphics as naming conventions in R are not the same as those you might want to use to visualize your data. Fortunately, ggplot2 provides tools for renaming the axis and plot titles. The one you are likely to use most often is probably the labs() function. Here is a standard usage:\n\nggplot(data = penguins) + \n  geom_point(\n    mapping = aes(x = body_mass_g, y = bill_length_mm)\n  ) +\n  labs(\n    x = \"Body mass (g)\",\n    y = \"Bill length (mm)\",\n    title = \"Palmer Penguins\"\n  )\n\n\n\n\nColor Palettes\nWhen you map a variable to an aesthetic property, ggplot2 will supply a default color palette. This is fine if you are just wanting to explore the data yourself, but when it comes to publication-ready graphics, you should be a little more thoughtful. The main reason for this is that you want to make sure your graphics are accessible. For instance, the default ggplot2 color palette is not actually colorblind safe. To address this shortcoming, you can specify colorblind safe color palettes using the scale_color_viridis() function from the viridis package.3 It works like this:3 When it comes to colors in R, the sky is the limit. As far as I am aware, the most comprehensive list of palettes for use in R is provided by the paletteer package, which attempts to collect all of the palettes scattered across the R ecosystem into one place. See also this beautiful website for creating custom color palettes: https://coolors.co/.\nggplot(data = penguins) + \n  geom_point(\n    mapping = aes(x = body_mass_g, y = bill_length_mm, color = species)\n  ) +\n  labs(\n    x = \"Body mass (g)\",\n    y = \"Bill length (mm)\",\n    title = \"Palmer Penguins\"\n  ) +\n  scale_color_viridis(\n    option = \"viridis\", \n    discrete = TRUE\n  )\n\n\n\n\n\n\n\n\n\n\nFor comparison, I’m showing the viridis scale next to the default ggplot2 scale, so you can see the difference. Two things to note about scale_color_viridis(). First, you choose a specific colorblind safe palette with the option parameter. In this case, I chose viridis, but there are others, including magma, cividis, and inferno, to name a few. Second, if the variable is continuous rather than discrete, you will have to set discrete = FALSE in the function, otherwise it will throw an error.\nExercises\n\nUsing the penguins dataset, plot body mass (y variable) by bill length (x variable) and change the axis labels to reflect this.\nUsing the penguins dataset, plot bill length (y variable) by bill depth (x variable) and change the axis labels to reflect this.\n\nUsing the code below, try out these different colorblind safe palettes from the viridis package:\n\nmagma\ncividis\ninferno\n\n\n\nggplot(data = penguins) + \n  geom_point(\n    mapping = aes(x = body_mass_g, y = bill_length_mm, color = species)\n  ) +\n  labs(\n    x = \"Body mass (g)\",\n    y = \"Bill length (mm)\",\n    title = \"Palmer Penguins\"\n  ) +\n  scale_color_viridis(\n    option = , # <------- insert value here\n    discrete = TRUE\n  )"
  },
  {
    "objectID": "labs/02-probability-lab.html#themes",
    "href": "labs/02-probability-lab.html#themes",
    "title": "Lab 02: Statistical Graphics",
    "section": "Themes",
    "text": "Themes\nTo control the display of non-data elements in a figure, you can specify a theme. This is done with the theme() function. Using this can get pretty complicated, pretty quick, as there are many many elements of a figure that can be modified, so rather than elaborate on it in detail, I want to draw your attention to pre-defined themes that you can use to modify your plots in a consistent way.\nHere is an example of the black and white theme, which removes filled background grid squares, leaving only the grid lines.\n\nggplot(data = penguins) + \n  geom_point(\n    mapping = aes(x = body_mass_g, y = bill_length_mm, color = species)\n  ) +\n  labs(\n    x = \"Body mass (g)\",\n    y = \"Bill length (mm)\",\n    title = \"Palmer Penguins\"\n  ) +\n  theme_bw()\n\n\n\n\nExercises\n\nComplete the code below, trying out each separate theme:\n\ntheme_minimal()\ntheme_classic()\ntheme_void()\n\n\n\nggplot(data = penguins) + \n  geom_point(\n    mapping = aes(x = body_mass_g, y = bill_length_mm, color = species)\n  ) +\n  labs(\n    x = \"Body mass (g)\",\n    y = \"Bill length (mm)\",\n    title = \"Palmer Penguins\"\n  ) +\n  theme_    # <------- complete function call to change theme"
  },
  {
    "objectID": "labs/02-probability-lab.html#homework",
    "href": "labs/02-probability-lab.html#homework",
    "title": "Lab 02: Statistical Graphics",
    "section": "Homework",
    "text": "Homework\n\n\nLoad data. For this homework exercise, we’ll work with the DartPoints dataset from the archdata package. To load that dataset, use data(DartPoints).\n\nSummary statistics. Let’s summarize these data now.\n\nUse head() to print out the first 10 rows of the table.\nUse mean() and median() on the Length, Width, Thickness, and Weight variables. (Hint: use DartPoints$<VARIABLE> as in DartPoints$Length.)\nUse var() and sd() on the same.\nNow use skim() to summarize the DartPoints data.\n\n\n\nGraphics. And now to visualize them.\n\nUse ggplot() to make a scatterplot showing dart point length as a function of weight. (Hint: use geom_point().)\nIs there a trend?\nMap the dart point Name (this is the dart point type) to the color aesthetic. (Hint: this should go inside the aes() mapping!)\nDo you see any meaningful differences between dart point types?\nChange the size of all points to 2.5. (Hint: this should go outside the aes() mapping but inside geom_point()!)\nUse scale_color_viridis() to make the color scale colorblind safe. Feel free to use whichever palette you prefer. (Hint: dart point type is a categorical variable, so you need to set discrete = TRUE!)\nTry out facet_wrap() on the dart point Name variable.\nDoes this make it easier or harder to see differences between types?"
  },
  {
    "objectID": "labs/03-inference-lab.html",
    "href": "labs/03-inference-lab.html",
    "title": "Lab 03: Statistical Inference",
    "section": "",
    "text": "This lab will guide you through the process of\n\nmaking a tidy data.frame\nvisualizing distributions with histograms\nrunning a t-test\nperforming an ANOVA\n\n\narchdata\nggplot2\npalmerpenguins\nskimr\n\nMake sure to load these into your R session with library(). This should always go at the start of your document!\n\nlibrary(archdata)\nlibrary(ggplot2)\nlibrary(palmerpenguins)\nlibrary(skimr)\n\n\n\n\npenguins\n\nIncludes measurements for penguin species, island in Palmer Archipelago, size (flipper length, body mass, bill dimensions), and sex.\npackage: palmerpenguins\n\nreference: https://allisonhorst.github.io/palmerpenguins/reference/penguins.html\n\n\n\n\nDartPoints\n\nIncludes measurements of 91 Archaic dart points recovered during surface surveys at Fort Hood, Texas.\npackage: archdata\n\nreference: https://cran.r-project.org/web/packages/archdata/archdata.pdf\n\n\n\n\ntitanic\n\nProvides information on the fate of passengers of the Titanic, including economic status, sex, age, and survival.\npackage: none\nreference: https://wilkelab.org/SDS375/\n\nnote: we’re going to download this one rather than get it from a package\n\n\n\nSnodgrass\n\nIncludes measurements of size, location, and contents of 91 pit houses at the Snodgrass site in Butler County, Missouri.\nreference: https://cran.r-project.org/web/packages/archdata/archdata.pdf"
  },
  {
    "objectID": "labs/03-inference-lab.html#data-frames",
    "href": "labs/03-inference-lab.html#data-frames",
    "title": "Lab 03: Statistical Inference",
    "section": "Data Frames",
    "text": "Data Frames\n\n\n\n\n\nFigure 1: Elements of a Data Frame.\n\n\nConsider this research scenario: you have a museum collection of projectile points that you want to use for an analysis, maybe you want to know whether East Gate and Rose Spring points are actually the same type of point, commonly referred to using the portmanteau Rosegate. Following tradition, you think maybe it’s the size and shape that will you help finally put this old question to rest, so for each individual point, you set about measuring its length, width, and height.\nIn this course, we’ll refer to each individual measure that you make as a value, each set of such measures for an individual point we will call (somewhat awkwardly) an observation, and each individual type of measurement will be a variable. It is almost certainly the case that you will be collecting this data and storing it in something like a spreadsheet or table, like that shown in Figure 1. You will sometimes hear data scientists refer to data stored in this way as rectangular data. All they mean by that is that the data come as collections of values organized into rows and columns of equal length. While data may admit of many different ways of being organized into rows and columns, here we will focus on a rectangular format known as tidy data. As defined by Hadley Wickham in R for Data Science (2e), tidy data must follow three simple rules:\n\nEach variable must have its own column,\nEach observation must have its own row, and\nEach value must have its own cell.\n\nIt is important to note, as Wickham cautions, that the opposite of tidy data is not necessarily messy data, for data can come in many formats (sound and video, for example). However, when we want to conduct some statistical analysis, and especially when we want to conduct such an analysis in R, we will almost certainly want our data to be tidy.\nCreating tables\nIn R, tabular datasets are known as data frames. To create a data frame, we use the eponymous data.frame() function. Here, for example, is how we would create the table in Figure 1 above:\n\nprojectiles <- data.frame(\n  type = c(\"Elko\", \"Rosegate\", \"DSN\", \"Elko\", \"Clovis\"),\n  length = c(2.03, 1.4, 1.9, 2.1, 3.3),\n  width = c(0.8, 0.4, 0.3, 0.7, 0.95),\n  height = c(3.23, 2.4, 1.29, 2.7, 4.15)\n)\n\nprojectiles\n\n      type length width height\n1     Elko   2.03  0.80   3.23\n2 Rosegate   1.40  0.40   2.40\n3      DSN   1.90  0.30   1.29\n4     Elko   2.10  0.70   2.70\n5   Clovis   3.30  0.95   4.15\n\n\nNote that the values (or measurements) contained in each variable are wrapped in the c() function (short for concatenate). This variable tells R that all these values belong to this one variable. Each variable in turn is an argument to data.frame() having the form <variable> = c(<value-1>, <value-2>, ..., <value-n>).\nGetting basic meta-data from tables\nWhen you want to know what variables a table includes, you can use the names() function.\n\nnames(projectiles)\n\n[1] \"type\"   \"length\" \"width\"  \"height\"\n\n\nIf you want to know how many variables or observations the table has, you can use nrow() and ncol() respectively.\n\n# number of observations\nnrow(projectiles)\n\n[1] 5\n\n# number of variables\nncol(projectiles)\n\n[1] 4\n\n\nExercises\n\nGet the names of the variables in the penguins table with names().\nHow many observations and variables are in this dataset? Hint: use nrow() and ncol().\nExtract the bill_length_mm variable from this table and assign it to an object called bill_length. Do the same for bill_depth_mm and call it bill_depth. You can use either <table>[[<variable>]] or <table>$<variable>."
  },
  {
    "objectID": "labs/03-inference-lab.html#histograms",
    "href": "labs/03-inference-lab.html#histograms",
    "title": "Lab 03: Statistical Inference",
    "section": "Histograms",
    "text": "Histograms\nA histogram is an excellent aid for visualizing the distribution of numerical data. Making one involves “binning” a continuous variable, counting the number of its values that fall into each bin, then drawing a rectangle for each bin whose height is proportional to the count for that bin. A good example of this is the distribution of a population over age ranges, like the age distribution of the passengers on the ill-fated voyage of the Titanic. Before we get to what the histogram looks like for this macabre example, let’s first have a look at the raw data:11 These data come from Claus Wilke’s Fundamentals of Data Visualization.\n\n\n\n\n\n\nWarning\n\n\n\n\n\nTo be able to work through the examples in this section, you need to run the code below. We’ll explain what this code does in another lab.\n\ntitanic <- read.csv(\"https://raw.githubusercontent.com/wilkelab/SDS375/master/datasets/titanic.csv\")\n\n\n\n\n\nhead(titanic)\n\n  class   age    sex survived\n1   1st 29.00 female survived\n2   1st  2.00 female     died\n3   1st 30.00   male     died\n4   1st 25.00 female     died\n5   1st  0.92   male survived\n6   1st 47.00   male survived\n\n\nYou see that our data has an age for each individual passenger. If we bin those ages into 5 year intervals (0-5, 5-10, 10-15, and so on) and count the number of passengers that fall into each bin or age range, we get a summary table that looks like this.\n\n\n\n\n  \n  \n\nAge Range\n      Count\n       \n      Age Range\n      Count\n    \n\n\n0–5\n36\n   \n41–45\n54\n\n\n6–10\n19\n   \n46–50\n50\n\n\n11–15\n18\n   \n51–55\n26\n\n\n16–20\n99\n   \n56–60\n22\n\n\n21–25\n139\n   \n61–65\n16\n\n\n26–30\n121\n   \n66–70\n3\n\n\n31–35\n76\n   \n71–75\n3\n\n\n36–40\n74\n   \n76–80\n0\n\n\n\n\n\n\nWe can actually visualize this distribution with a histogram using ggplot() and geom_histogram(). Importantly, we pass ggplot() the raw tidy data, not the summary table. Here is how that looks:\n\nggplot(titanic) + \n  geom_histogram(\n    aes(age)\n  )\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\nNotice that ggplot2 provides an informative message that the default number of bins used in this plot is 30, which makes the age interval for each bin about 2.5 years. We can (and SHOULD!) change this by specifying a different number of bins with bins, as the number of bins can dramatically change the interpretation of the distribution.\n\nggplot(titanic) +\n  geom_histogram(\n    aes(age),\n    bins = 10\n  )\n\n\n\n\n\n\n\nUnfortunately, the default histogram produced by ggplot2 doesn’t do a great job of conveying the relationship between the axis text and the ranges represented by the widths of the rectangles. For instance, zero falls at the center of the first histogram, which would seem to suggest that the range of values represented by that rectangle includes negative ages. What it is really saying is that zero falls in the range of that bin, but that is not obvious. One solution to this issue is to set the binwidth (or bin width, as in the range of values for that bin) and the bin boundary (or where the left side of the rectangle is positioned relative to the bin range). That’s a tad confusing, I know, but the basic idea is that if you set the width to 5 and the boundary to 0, then the bin for the range 0-5 will start at 0, and the bin for the range 5-10 will start at 5, and the bin for 10-15 will start at 10, and so on. Finding the best options for visualizing your data will, of course, involve some trial and error.\n\nggplot(titanic) +\n  geom_histogram(\n    aes(age),\n    binwidth = 5,\n    boundary = 0\n  )\n\n\n\n\n\n\n\nAs always, you can change the fill and outline color for these plots by supplying those parameters to the geometry. We can also clean up the labels with labs() and change the theme if we like (and we do). And since the heights of these bars are what we care about, we can turn off the light vertical grid lines that ggplot2 adds by default by specifying this as a theme() option.\n\nggplot(titanic) +\n  geom_histogram(\n    aes(age),\n    binwidth = 5,\n    boundary = 0,\n    fill = \"#56B4E9\",\n    color = \"#01587A\"\n  ) +\n  labs(\n    x = \"Age (years)\",\n    y = \"Count\",\n    title = \"Age Distribution\",\n    subtitle = \"Passengers of the Titanic\"\n  ) +\n  theme_minimal(14) +\n  theme(\n    panel.grid.major.x = element_blank(),\n    panel.grid.minor.x = element_blank()\n  )\n\n\n\n\n\n\n\nOne last point before moving on. You may have noticed that the data contains a binary variable survived. Do you think maybe the age distribution of those that survived differs from the distribution for those that did not survive? Do you think maybe they were older? Younger? Or is there no difference? Let’s explore this a little by visualizing the different distributions using facet_wrap().\n\nggplot(titanic) +\n  geom_histogram(\n    aes(age),\n    binwidth = 5,\n    boundary = 0,\n    fill = \"#56B4E9\",\n    color = \"#01587A\"\n  ) +\n  labs(\n    x = \"Age (years)\",\n    y = \"Count\",\n    title = \"Age Distribution\",\n    subtitle = \"Passengers of the Titanic\"\n  ) +\n  facet_wrap(vars(survived)) + # <----- adding facets here\n  theme_minimal(14) +\n  theme(\n    panel.grid.major.x = element_blank(),\n    panel.grid.minor.x = element_blank()\n  )\n\n\n\n\n\n\n\nHmmmm 🤔. What do you think this means? And how might it help to compare this to the total distribution?\nExercises\n\nCreate a histogram of penguin bill length using the penguins dataset. Then do all of the following:\n\nChange the number of bins (try two different options).\nTry specifying the bin width and boundary.\nChange the fill and outline color.\nReset the labels to be more informative.\nChange the theme and remove the vertical grid lines.\nFacet the distributions by penguin species.\n\n\nRepeat (1), but use the DartPoints dataset from the archdata package, creating a histogram of dart length (Length in the table) and facet by dart type (Name).\n\nDoes it look like the dart types might differ in length? Or maybe they’re all basically the same?"
  },
  {
    "objectID": "labs/03-inference-lab.html#t-test",
    "href": "labs/03-inference-lab.html#t-test",
    "title": "Lab 03: Statistical Inference",
    "section": "t-test",
    "text": "t-test\nWe use a t-test (technically, Welch’s two-sample t-test) to evaluate whether two samples come from the same population. This test starts by computing the \\(t\\) statistic, or the standardized difference in sample means:\n\\[t = \\frac{\\bar{x}_1 - \\bar{x}_2}{s_{\\bar{x}}}\\]\nwhere \\(s_{\\bar{x}}\\) is the standard error of the sample mean. This is then compared to a t-distribution to determine the probability of that difference - the more improbable, the less likely the samples come from the same distribution. Why is that? Well, if you think about it, we are starting from the assumption that the null hypothesis is true. Then we are asking, if the null hypothesis is true, how likely is that we would get this difference in sample means? And, if it’s extremely unlikely, that would presumably count against the null model being true.\nTo perform this test in R, we use the t.test() function, providing it with two samples. Suppose, for example, that we have two samples of projectile points, namely their lengths in millimeters, and we want to answer this question: Are these samples of the same point type (the same population)? Here are our two samples:\n\nsample1 <- c(59.10, 61.97, 56.23, 53.83, 60.24, 44.27, 61.41, 55.07, 55.01, 50.56)\nsample2 <- c(58.42, 68.09, 60.85, 61.60, 57.25, 63.08, 58.57, 57.34, 64.60, 60.49)\n\nmean(sample1) - mean(sample2)\n\n[1] -5.26\n\n\nBefore we run the test, let’s make sure we have our hypotheses clear.\n\n\n\\(H_0\\): There is NO difference in mean length (\\(\\bar{x}_1 = \\bar{x}_2\\))\n\n\\(H_1\\): There is a difference in mean length (\\(\\bar{x}_1 \\neq \\bar{x}_2\\))\n\nWe also need to specify our critical value. Here, we’ll stick with a common standard of 0.05:\n\n\\(\\alpha = 0.05\\)\n\nNow, we can run our test!\n\nt.test(sample1, sample2)\n\n\n    Welch Two Sample t-test\n\ndata:  sample1 and sample2\nt = -3, df = 15, p-value = 0.02\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -9.598 -0.922\nsample estimates:\nmean of x mean of y \n     55.8      61.0 \n\n\nNotice the output that R has provided here. You have the name of the test: Welch Two Sample t-test (this is a version of Student’s t-test for two independent samples). It gives you the t-statistic, the degrees of freedom (df), and the p-value. It also states the alternative hypothesis and gives you the mean of each sample. In this case, \\(p < \\alpha\\). Hence, we reject the null. A significant difference exists between the means of these two samples.\nIf your samples are in a data.frame, you can also call t.test() using R’s formula syntax. So, assume your data above are in a table like so:\n\nn <- length(sample1)\n\nsamples <- data.frame(\n  sample = c(rep(1, n), rep(2, n)),\n  length = c(sample1, sample2)\n)\n\nhead(samples)\n\n  sample length\n1      1   59.1\n2      1   62.0\n3      1   56.2\n4      1   53.8\n5      1   60.2\n6      1   44.3\n\n\nThen you would run the t.test() this way:\n\nt.test(length ~ sample, data = samples)\n\n\n    Welch Two Sample t-test\n\ndata:  length by sample\nt = -3, df = 15, p-value = 0.02\nalternative hypothesis: true difference in means between group 1 and group 2 is not equal to 0\n95 percent confidence interval:\n -9.598 -0.922\nsample estimates:\nmean in group 1 mean in group 2 \n           55.8            61.0 \n\n\nThe formula in this example is length ~ sample. The tilde expresses a relation of dependence. In this case, we want to know whether the length of a point differs in some meaningful way between samples. So, here we are, in effect, asking R to run a t-test comparing the mean length in each sample using the samples dataset. You will notice that the results are the same, we just called the function in a slightly different way because of how we had the data stored.\nExercises\n\nPerform a t-test on these two samples of dart length (drawn from the DartPoints dataset).\n\nCalculate the mean for each sample.\n\nBe sure to specify the null and alternate hypotheses.\n\nState the critical value.\n\nRun the test and print the results.\n\n\n\n\ndarl <- c(42.8,40.5,37.5,40.3,30.6,41.8,40.3,48.5,47.7,33.6,32.4,42.2,33.5,\n          41.8,38,35.5,31.2,34.5,33.1,32,38.1,47.6,42.3,38.3,50.6,54.2,44.2,40)\ntravis <- c(56.5,54.6,46.3,57.6,49.1,64.6,69,40.1,41.5,46.3,39.6)\n\n\nBased on the test, do you accept or reject the null hypothesis? Why?"
  },
  {
    "objectID": "labs/03-inference-lab.html#anova",
    "href": "labs/03-inference-lab.html#anova",
    "title": "Lab 03: Statistical Inference",
    "section": "ANOVA",
    "text": "ANOVA\nLike the t-test, the ANOVA is used to test for a difference between samples, to see whether they come from the same population. Unlike the t-test, however, the ANOVA can be used on more than two samples. It does that by decomposing the total variance into within and between group variance. The ratio of those standardized variances defines the F-statistic:\n\\[F = \\frac{\\text{between-group variance}}{\\text{within-group variance}}\\]\nBy comparing the value of \\(F\\) to the F-distribution, we can evaluate the probability of getting that value. A highly improbable F-statistic means that at least one group or sample comes from a different population. To perform this test in R, we use the aov() function.\nTo illustrate this method, let’s return to the DartPoints dataset, which contains various measures for five different dart point types (Darl, Ensor, Pedernales, Travis, and Wells). We’ll try to answer the question: Are these samples from the same dart point type (the same population)? Less formally, we want to know whether the types we have lumped these darts into are really all that different. As always, we first specify our null and alternate hypotheses and the critical value we will use to determine whether to reject the null.\n\n\n\\(H_0\\): There is no difference in length between groups.\n\n\\(H_1\\): At least one group differs in length.\n\nAnd our critical value, again, will be 0.05.\n\n\\(\\alpha = 0.05\\)\n\nNow, we can conduct our test.\n\ndata(DartPoints)\n\naov_test <- aov(Length ~ Name, data = DartPoints)\n\nThere are two things to note here. First, we are assigning the output of this ANOVA test to an object. Second, we call the test using a formula, in this case Length ~ Name. The full call to aov() you can read as saying, “Perform an analysis of variance comparing the lengths for each sample in this dataset.”\nANOVA Table\nIf we print out the result of the aov() call, it looks like this:\n\naov_test\n\nCall:\n   aov(formula = Length ~ Name, data = DartPoints)\n\nTerms:\n                Name Residuals\nSum of Squares  5532      9067\nDeg. of Freedom    4        86\n\nResidual standard error: 10.3\nEstimated effects may be unbalanced\n\n\nWhile this printout offers some important information, a much more useful ummary of the test is provided by an ANOVA table. We can generate one of these by running the summary() function on our test.\n\nsummary(aov_test)\n\n            Df Sum Sq Mean Sq F value      Pr(>F)    \nName         4   5532    1383    13.1 0.000000022 ***\nResiduals   86   9067     105                        \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThis table has two rows, one for the between-group variance and one for the within-group variance respectively. As you can see, R refers to these, somewhat cryptically, as the Name (the grouping variable in the data) and Residuals. The reason for this concerns the fact that the aov() function is actually fitting a linear model, but let’s leave that detail to the side. For now, just note that the columns are, in order,\n\n\nDf = the degrees of freedom,\n\n\nSum Sq = the sum of squares,\n\n\nMean Sq = the mean sum of squares (the sum of squares divided by the degrees of freedom),\n\n\nF value = the F-statistic (the ratio of the mean sum of squares), and\n\n\nPr(>F) = the p-value, formally the probability of getting a value of the statistic greater than observed (determined by comparing the F-statistic to the F-distribution).\n\nIn this case, \\(p < \\alpha\\), so we reject the null hypothesis, meaning that at least one of these groups is different.\nExercises\n\nPerform an ANOVA on the DartPoints dataset to see if at least one dart type differs signficantly in its Width.\n\nBe sure to specify the null and alternate hypotheses.\nState the critical value.\nRun the test and print the results.\nBe sure to assign the test to an object.\nProvide a summary() of the test.\n\n\nBased on the test, do you accept or reject the null hypothesis? Why?"
  },
  {
    "objectID": "labs/03-inference-lab.html#homework",
    "href": "labs/03-inference-lab.html#homework",
    "title": "Lab 03: Statistical Inference",
    "section": "Homework",
    "text": "Homework\n\nLoad the Snodgrass dataset from the archdata package using data(Snodgrass). This dataset includes measurements of pithouses found in a small village affiliated with maize farmers in Missouri about 650 years ago.\nGenerate a summary table using the skim() function from skimr.\nFor practice, let’s get some of those summary statistics manually. Calculate the mean and standard deviation of the inside floor area of each pithouse using mean() and sd(). You’ll need to pull the Area variable out of the table with Snodgrass$Area. Hint: if you were to calculate the variance, you would use var(Snodgrass$Area).\nUse ggplot() and geom_histogram() to plot a histogram showing the distribution of floor area. Make sure to do all of the following (only need to make one graph):\n\nChange the default number of bins (or optionally, you can specify the breaks).\nChange the fill and outline color.\nUpdate the labels and the plot title.\nChoose an appropriate theme and remove the vertical grid lines.\nFacet by the Inside variable. Then try faceting using the Segment variable.\nDo the distributions look different?\n\n\nSome of these pithouses occur inside a walled-in portion of the village. This information is provided by the Inside variable in the dataset. Run a t-test to determine whether the mean floor area of pithouses inside the walled-in area differs significantly from the mean floor area of pithouses outside the wall.\n\nThese data are in a table, so make sure to use the formula notation.\nState the null and alternative hypotheses.\nSpecify the critical value.\nRun the test and print the result.\nDoes the test confirm or refute the null hypothesis? Why?\nHow would you interpret this result?\n\n\nThe pithouses were initially divided into three groups or “segments” (Segment in the table) based on their location: segment one includes those inside the wall, segment two those to the north and west of the wall, and segment three those to the east and south of the wall. Run an ANOVA test to determine whether mean floor area for these groups is significantly different.\n\nState the null and alternative hypotheses.\nSpecify the critical value.\nRun the test and print the result.\nBe sure to assign the result to an object.\nProvide a summary() of the test.\nDoes the test confirm or refute the null hypothesis? Why?\nHow would you interpret this result?"
  },
  {
    "objectID": "labs/04-ols-lab.html",
    "href": "labs/04-ols-lab.html",
    "title": "Lab 04: Ordinary Least Squares",
    "section": "",
    "text": "This lab will guide you through the process of\n\nindexing data.frames with base R\nvisualizing distributions with density plots\ncalculating covariance\ncalculating correlation and evaluating with the t-test\nbuilding a simple linear model\n\nthe formula notation\nthe lm() function\nthe model summary()\n\n\n\n\nWe will be using the following packages:\n\narchdata\nggplot2\npalmerpenguins\nviridis\n\n\nlibrary(archdata)\nlibrary(ggplot2)\nlibrary(palmerpenguins)\nlibrary(viridis)\n\n\n\n\ncars\n\nIncludes measurements of car speed and stopping distance.\npackage: datasets\n\nreference: https://stat.ethz.ch/R-manual/R-devel/library/datasets/html/cars.html\n\n\n\n\nDartPoints\n\nIncludes measurements of 91 Archaic dart points recovered during surface surveys at Fort Hood, Texas.\npackage: archdata\n\nreference: https://cran.r-project.org/web/packages/archdata/archdata.pdf\n\n\n\n\nOxfordPots\n\nIncludes percentages of late Romano-British Oxford Pottery on 30 sites, along with their distance from Oxford.\nreference: https://cran.r-project.org/web/packages/archdata/archdata.pdf\n\n\n\n\npenguins\n\nIncludes measurements for penguin species, island in Palmer Archipelago, size (flipper length, body mass, bill dimensions), and sex.\npackage: palmerpenguins\n\nreference: https://allisonhorst.github.io/palmerpenguins/reference/penguins.html\n\n\n\n\ntitanic\n\nProvides information on the fate of passengers of the Titanic, including economic status, sex, age, and survival.\npackage: none\nreference: https://wilkelab.org/SDS375/\n\nnote: we’re going to download this one rather than get it from a package"
  },
  {
    "objectID": "labs/04-ols-lab.html#indexing-tables-with-base-r",
    "href": "labs/04-ols-lab.html#indexing-tables-with-base-r",
    "title": "Lab 04: Ordinary Least Squares",
    "section": "Indexing tables with base R",
    "text": "Indexing tables with base R\n\n\n\n\n\nFigure 1: Elements of a Data Frame.\n\n\nIt will often be the case that you do not need all the data in a table, but only a subset of rows and columns. To get the rows and columns you want, though, you need to be able to, as it were, point to them or point R to them. Another word for this is indexing.\nLet’s start with the use of square brackets, [,]. The basic idea here is that you can take a table and index it by row and column by appending the square brackets to it. The basic syntax is this:\ntable[row,column]\nAs an example, let’s say we are working with our simple projectile point table:\n\nprojectiles <- data.frame(\n  type = c(\"Elko\", \"Rosegate\", \"DSN\", \"Elko\", \"Clovis\"),\n  length = c(2.03, 1.4, 1.9, 2.1, 3.3),\n  width = c(0.8, 0.4, 0.3, 0.7, 0.95),\n  height = c(3.23, 2.4, 1.29, 2.7, 4.15)\n)\n\nprojectiles\n\n      type length width height\n1     Elko   2.03  0.80   3.23\n2 Rosegate   1.40  0.40   2.40\n3      DSN   1.90  0.30   1.29\n4     Elko   2.10  0.70   2.70\n5   Clovis   3.30  0.95   4.15\n\n\nAnd maybe we want the value at the 3rd row and 2nd column, so we’re wanting the length of that particular desert side-notched (or DSN). Here is one way to do that with just the numeric position (or coordinates) of that value:\n\nprojectiles[3,2]\n\n[1] 1.9\n\n\nWhile we did specify both a row and a column in this example, that is not required.\n\nprojectiles[3,]\n\n  type length width height\n3  DSN    1.9   0.3   1.29\n\nprojectiles[,2]\n\n[1] 2.03 1.40 1.90 2.10 3.30\n\n\n\n\n\n\n\n\nDanger\n\n\n\n\n\nDid you notice the different outputs? projectiles[3,] returns a data.frame, but projectiles[,2] returns a vector. This is a “gotcha” in R, a little bit of unexpected behavior. The most common situation in which indexing returns a vector from a data.frame is when a single variable is selected. Sometimes getting just the variable is intentional (see below), but other times it is not, so it’s worth being aware of.\n\n\n\nWe can also subset multiple rows and columns, though this requires that we use vectors of data, and not just single values. A useful tool in this regard is the colon, :, which allows you to create a sequence of integers, starting with the number on the left and proceeding by one to the number on the right.\n\n1:3\n\n[1] 1 2 3\n\n\nNow, we can use this syntax to index the last four rows of our table and the first three columns.\n\nprojectiles[2:5, 1:3]\n\n      type length width\n2 Rosegate    1.4  0.40\n3      DSN    1.9  0.30\n4     Elko    2.1  0.70\n5   Clovis    3.3  0.95\n\n\nIf we want to get rows or columns that are not next to each other in the table, we can use the c() function, as in concatenate.\n\nc(1,2,4)\n\n[1] 1 2 4\n\n\nWhen applied to the projectiles table, we get the following.\n\nprojectiles[c(2,4), c(1,2,4)]\n\n      type length height\n2 Rosegate    1.4    2.4\n4     Elko    2.1    2.7\n\n\nImportantly, you can also index columns by name.\n\nprojectiles[1:3, c(\"type\", \"length\")]\n\n      type length\n1     Elko   2.03\n2 Rosegate   1.40\n3      DSN   1.90\n\n\nOne advantage of using names rather than numbers is that it is much more readable as it is not immediately obvious with numbers what columns you are actually selecting. More importantly, though, using names is more robust. Were the length column for whatever reason to move to the right of the height column, its numeric position in the table would be 4, not 2. So, using projectiles[,2] will work to index the length variable only if length is at that position. Using projectiles[,\"length\"] to index it will work either way, though, regardless of the numeric position of that variable.\nSo, that’s pretty much the basics of indexing rectangular data with base R. Before moving on, though, let’s talk about one additional thing you might want to do with a data.frame, and that’s extract an entire variable or column. There are two primary ways to achieve this. You can use double brackets, <table>[[<variable>]], or you can use the dollar-sign operator, <table>$<variable>.\n\nprojectiles[[\"type\"]]\n\n[1] \"Elko\"     \"Rosegate\" \"DSN\"      \"Elko\"     \"Clovis\"  \n\nprojectiles$length\n\n[1] 2.03 1.40 1.90 2.10 3.30\n\n\nNote that you can and often will want to assign these to their own objects, so you can use them again later.\n\np_type <- projectiles[[\"type\"]]\n\np_length <- projectiles$length\n\nAnd, if you want, you can index specific values in the vector as you would rows in the table.\n\nprojectiles$length\n\n[1] 2.03 1.40 1.90 2.10 3.30\n\nprojectiles$length[c(1,2,4)]\n\n[1] 2.03 1.40 2.10\n\n\nExercises\n\nGet the names of the variables in the penguins table with names().\nHow many observations and variables are in this dataset? Hint: use nrow() and ncol().\nTry indexing multiple rows and columns of the penguins data using the square brackets with row numbers and column names, for example, penguins[1:25, c(\"species\", \"island\", \"body_mass_g\")]. Try doing this a couple of different ways.\nExtract the bill_length_mm variable from this table and assign it to an object called bill_length. Do the same for bill_depth_mm and call it bill_depth. You can use either <table>[[<variable>]] or <table>$<variable>."
  },
  {
    "objectID": "labs/04-ols-lab.html#density-plots",
    "href": "labs/04-ols-lab.html#density-plots",
    "title": "Lab 04: Ordinary Least Squares",
    "section": "Density Plots",
    "text": "Density Plots\nHere you will learn how to generate a “density” plot or a plot of the underlying probability density of a variable using ggplot() and geom_density(). This is similar to a histogram in that it seeks to show the distribution of a continuous random variable. It differs, however, in the way it goes about doing that. To see how they differ, let’s first have a look at these two plots of the titanic data. The one on the left is a histogram, the one on the right a density plot.\n\n\n\n\n\n\nWarning\n\n\n\n\n\nTo be able to work through the examples in this section, you need to run the code below. We’ll explain what this code does in another lab.\n\ntitanic <- read.csv(\"https://raw.githubusercontent.com/wilkelab/SDS375/master/datasets/titanic.csv\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe big difference here is that histograms discretize a sample using bins, counts the number of observations that fall into each bin, and then represents those counts using the heights of the rectangles. A density plot uses a method known as kernel density estimate (or KDE) to estimate the density of each observation and represents it using the height of a smooth and continuous curve. The KDE approach works like this. First, you set each data point as the mean of a distribution, typically the normal or Gaussian distribution (also called the kernel). Each distribution is assumed to have the same varianc eor standard deviation (called the bandwidth), which is set to some arbitrary value. The heights of the kernels are then summed to produce a curve like the one above.\nAs with the histogram, we specify a density geometry for ggplot using geom_density().\n\nggplot(titanic, aes(age)) + \n  geom_density()\n\n\n\n\nAgain, we can specify different aesthetics like fill and color and update the labels with labs().\n\nggplot(titanic, aes(age)) + \n  geom_density(\n    fill = \"#A8BFF0\", \n    color = \"#183C8C\"\n  ) + \n  labs(\n    x = \"Age\",\n    y = \"Density\",\n    title = \"Age Distribution\",\n    subtitle = \"Passengers of the Titanic\"\n  )\n\n\n\n\nWe can also map these aesthetics to other variables like the sex of the passenger.\n\nggplot(titanic, aes(age, fill = sex)) + \n  geom_density() + \n  labs(\n    x = \"Age\",\n    y = \"Density\",\n    title = \"Age Distribution\",\n    subtitle = \"Passengers of the Titanic\"\n  )\n\n\n\n\nAnd, we can change the default fill colors using scale_fill_manual(), too.\n\nggplot(titanic, aes(age, fill = sex)) + \n  geom_density() + \n  scale_fill_manual(\n    values = c(\"#A8BFF0\", \"#FFE66D\")\n  ) +\n  labs(\n    x = \"Age\",\n    y = \"Density\",\n    title = \"Age Distribution\",\n    subtitle = \"Passengers of the Titanic\"\n  )\n\n\n\n\nIn this case, however, it’s hard to see where the two distributions overlap, which makes it hard to compare them. One way to fix this is to change the opacity of the fill color using the alpha() function (note that the alpha or transparency of a color can range from 0 to 1.).\n\nggplot(titanic, aes(age, fill = sex)) + \n  geom_density() + \n  scale_fill_manual(\n    values = alpha(c(\"#A8BFF0\", \"#FFE66D\"), 0.5)\n  ) +\n  labs(\n    x = \"Age\",\n    y = \"Density\",\n    title = \"Age Distribution\",\n    subtitle = \"Passengers of the Titanic\"\n  )\n\n\n\n\nThis is still a little hard to read, so let’s try faceting instead of alpha. Let’s also drop the background vertical grid lines using the theme() function. At the same time, we’ll go ahead and drop the label “sex” from the legend as that should be obvious from the key. We do that by setting name = NULL in scale_fill_manual().\n\nggplot(titanic, aes(age, fill = sex)) + \n  geom_density() + \n  scale_fill_manual(\n    name = NULL,\n    values = c(\"#A8BFF0\", \"#FFE66D\")\n  ) +\n  labs(\n    x = \"Age\",\n    y = \"Density\",\n    title = \"Age Distribution\",\n    subtitle = \"Passengers of the Titanic\"\n  ) +\n  facet_wrap(~sex) +\n  theme_minimal() +\n  theme(\n    panel.grid.major.x = element_blank(),\n    panel.grid.minor.x = element_blank(),\n  )\n\n\n\n\nNotice that the facet labels are redundant with the legend key here, so let’s remove those, too. We do that by setting the theme arguments strip.background and strip.text to element_blank(). Finally, we can move the legend to the bottom of the plot and make it horizontal with legend.position and legend.direction respectively.\n\nggplot(titanic, aes(age, fill = sex)) + \n  geom_density() + \n  scale_fill_manual(\n    name = NULL,\n    values = c(\"#A8BFF0\", \"#FFE66D\")\n  ) +\n  labs(\n    x = \"Age\",\n    y = \"Density\",\n    title = \"Age Distribution\",\n    subtitle = \"Passengers of the Titanic\"\n  ) +\n  facet_wrap(~sex) +\n  theme_minimal() +\n  theme(\n    legend.position = \"bottom\",\n    legend.direction = \"horizontal\",\n    panel.grid.major.x = element_blank(),\n    panel.grid.minor.x = element_blank(),\n    strip.background = element_blank(),\n    strip.text = element_blank()\n  )\n\n\n\n\nExercises\n\nMake a kernel density plot of penguin bill length using ggplot() and geom_density(). Then make all of the following changes:\n\nMap penguin species to the fill aesthetic.\nUpdate the axis labels and plot title using labs().\nUse scale_fill_viridis to use colorblind safe colors for the fill. Note! Species is a discrete or categorical variable, so make sure to set discrete = TRUE!\nUse facet_wrap() to facet by species.\nChoose a suitable theme, like theme_minimal().\nRemove vertical grid lines.\nChange the legend position to bottom and make it horizontal.\nRemove strip text and background."
  },
  {
    "objectID": "labs/04-ols-lab.html#bivariate-statistics",
    "href": "labs/04-ols-lab.html#bivariate-statistics",
    "title": "Lab 04: Ordinary Least Squares",
    "section": "Bivariate Statistics",
    "text": "Bivariate Statistics\nBivariate statistics provide simple measures of the relationship between two variables. Here we will learn how to calculate two such statistics in R: covariance and correlation. These allow us to describe the direction of the relationship (is it positive or negative?) and the strength of the relationship (is it strong or weak?). In this case, we’ll investigate the relationship between penguin body mass and bill length. We’ll be asking this question: Is there a relationship between bill length and body mass? Is it positive or negative?\nBefore we do that, however, it is useful to visualize our data. Since we are concerned with a potential relationship, we will use a scatterplot, or a cloud of points arrayed along the dimensions of two variables, in this case body mass and bill length.\n\nggplot(penguins, aes(x = body_mass_g, y = bill_length_mm)) + \n  geom_point(\n    shape = 21,\n    fill = \"#A8BFF0\",\n    color = \"#15357A\",\n    size = 2\n  ) +\n  labs(\n    x = \"Body Mass (g)\",\n    y = \"Bill Length (mm)\",\n    title = \"Palmer Penguins\"\n  ) +\n  theme_bw()\n\n\n\n\nWhat does this tell you about the relationship between these variables?\nCovariance\nCovariance provides a measure of the extent to which two variables vary together. The sign of the covariance reflects a positive or negative trend, but not magnitude. To calculate this value in R, use the cov() function.\n\nbill_length <- penguins$bill_length_mm\nbody_mass_g <- penguins$body_mass_g\n\ncov(bill_length, body_mass_g, use = \"complete.obs\") # complete.obs means ignore NA values\n\n[1] 2606\n\n\nThis is a positive number, meaning the relationship between bill length and body mass is positive (the one tends to increase as the other increases). The size of the number by itself is unhelpful, however, and cannot be used to infer anything about the strength of the relationship. That is because covariance is sensitive to the unit of measure. If, for example, we convert body_mass from grams to kilograms, we will get a different covariance statistic.\n\n# convert to kilograms by dividing by 1000\nbody_mass_kg <- body_mass_g/1000\n\ncov(bill_length, body_mass_kg, use = \"complete.obs\")\n\n[1] 2.61\n\n\nCorrelation\nTo remove units of measure and prevent resulting changes in the magnitude of the covariance, we can scale the covariance by the standard deviations of the samples. The resulting value is known as Pearson’s Correlation Coefficient, which ranges from -1 to 1.\n\ncor(bill_length, body_mass_g, use = \"complete.obs\")\n\n[1] 0.595\n\n\nJust to demonstrate that this isn’t sensitive to units of measure, let’s see what happens when use body mass measures in kilograms.\n\ncor(bill_length, body_mass_kg, use = \"complete.obs\")\n\n[1] 0.595\n\n\nThere’s no change! In either case, the resulting coefficient is greater than zero, suggesting a positive trend, but is this value significantly different than zero? To answer that question, we can convert this coefficient to a t-statistic and compare it to a t-distribution. This is done with the cor.test() function. For this test, we have the following hypotheses:\n\n\n\\(H_0\\): the coefficient is equal to zero\n\n\\(H_1\\): the coefficient is not equal to zero\n\nAnd, of course, we must stipulate a critical value. In this case, we will stick with tradition:\n\\(\\alpha = 0.05\\)\nSo, now, here is our test:\n\ncor.test(bill_length, body_mass_g, use = \"complete.obs\")\n\n\n    Pearson's product-moment correlation\n\ndata:  bill_length and body_mass_g\nt = 14, df = 340, p-value <0.0000000000000002\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.522 0.660\nsample estimates:\n  cor \n0.595 \n\n\nIn this case, you see that \\(p < \\alpha\\), hence we reject the null hypothesis, meaning our coefficient estimate is significantly different than zero. There is, in other words, a significant positive relationship between body mass and bill length among the Palmer penguins.\nExercises\n\nUsing the penguins dataset, do all of the following:\n\ncalculate the covariance between bill length and bill depth,\ncalculate Pearson’s Correlation Coefficient for bill length and bill depth,\ndo a correlation test to determine whether the coefficient is significantly different than zero, and\nbe sure to state your null and alternative hypotheses, as well as the critical value!\n\n\nWhat does the correlation test tell you about the relationship between bill length and bill depth?"
  },
  {
    "objectID": "labs/04-ols-lab.html#linear-models",
    "href": "labs/04-ols-lab.html#linear-models",
    "title": "Lab 04: Ordinary Least Squares",
    "section": "Linear Models",
    "text": "Linear Models\nIn this section, we will learn how to fit a linear model to our data. We will look, specifically, at a scenario involving an experiment with cars recorded in the cars dataset. We want to know what kind of relationship there is between the distance (in feet) a car travels after the brakes are applied and the speed (in miles per hour) the car was going when the brakes were applied. We will be doing this by fitting a linear model with the lm() function. Here are our hypotheses:\n\n\n\\(H_0\\): there is no relationship between speed and distance.\n\n\\(H_1\\): there is a relationship between speed and distance.\n\nModel formula\nFirst, however, let’s discuss the formula syntax that the lm() function uses. You were already introduced to this with the t.test(), but let’s go into a little more detail now. To fit a model, we must first specify a formula. This involves three components: a predictor variable, the tilde ~, and a response variable. The syntax is this:\n<response> ~ <predictor> or <dependent> ~ <independent>\nIn the case of the cars data, that’s:\ndist ~ speed\nThis can be read as saying, in effect, “distance as a function of speed.” Note that you do not have to put the variables in quotes or anything like that. It’s just the names of the variables separated by a tilde.\nModel fitting\nIn addition to specifyfing the formula, we must also tell the lm() function what data set our observations are coming from. We do this by specifying the data argument. The whole function call looks like this:\n\ncars_model <- lm(dist ~ speed, data = cars)\n\ncars_model\n\n\nCall:\nlm(formula = dist ~ speed, data = cars)\n\nCoefficients:\n(Intercept)        speed  \n     -17.58         3.93  \n\n\nHere, the model estimates a coefficient for both the intercept and the relationship between speed and distance.\nModel summary\nA more informative report of the model is provided by the summary() function. In addition to reporting on the model coefficients, this will also conduct a t-test on each coefficient, evaluating whether they are significantly different than zero.\n\nsummary(cars_model)\n\n\nCall:\nlm(formula = dist ~ speed, data = cars)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-29.07  -9.53  -2.27   9.21  43.20 \n\nCoefficients:\n            Estimate Std. Error t value        Pr(>|t|)    \n(Intercept)  -17.579      6.758   -2.60           0.012 *  \nspeed          3.932      0.416    9.46 0.0000000000015 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 15.4 on 48 degrees of freedom\nMultiple R-squared:  0.651, Adjusted R-squared:  0.644 \nF-statistic: 89.6 on 1 and 48 DF,  p-value: 0.00000000000149\n\n\nWe’ll go over this summary() in more detail later. For now, note that it reports the coefficient “Estimate”, the t-statistic (or “t value”) for each coefficient estimate, and the p-value for the respective t-tests. In each case, the null hypothesis is that the coefficient is zero. A small p-value then gives us reason to reject the null and accept the coefficient estimate as significant. In this case, the p-value is very small, so we can accept both the intercept and speed coefficients. This tells us (as you might expect) that there is a significant positive relationship between the speed the car was going when it applied the brakes and the distance it traveled after applying the brakes.\nExercises\n\nUsing the penguins dataset, build a linear model of the relationship between bill length and bill depth.\nWhat are the coefficients reported by this model? Specifically, the intercept and the coefficient of relationship between bill length and bill depth.\nApply the summary() function to your model. Are the coefficients significant?"
  },
  {
    "objectID": "labs/04-ols-lab.html#homework",
    "href": "labs/04-ols-lab.html#homework",
    "title": "Lab 04: Ordinary Least Squares",
    "section": "Homework",
    "text": "Homework\n\nLoad the following datasets from the archdata package using data().\n\nDartPoints\nOxfordPots\n\n\nPractice extracting variables from these tables.\n\nFrom each, remove one variable and assign it to an object with an informative name.\nCalculate the mean and variance for each variable.\n\n\nUsing the DartPoints dataset, make a kernel density plot of dart Length to visualize its distribution. Make sure to do all of the following:\n\nMap the dart Name (or type) to the fill aesthetic.\nUpdate the axis labels and plot title using labs().\nUse scale_fill_viridis to use colorblind safe colors for the fill. Note! Species is a discrete or categorical variable, so make sure to set discrete = TRUE!\nUse facet_wrap() to facet by Name (or type).\nChoose a suitable theme, like theme_minimal().\nRemove vertical grid lines.\nChange the legend position to bottom and make it horizontal.\nRemove strip text and background.\n\n\nUsing the DartPoints dataset, calculate the covariance and correlation between dart length and width.\n\nThen conduct a correlation test to evaluate the significance of Pearson’s Correlation Coefficient.\nBe sure to state the null and alternative hypotheses, as well as the critical value.\nIs the coefficient significant?\nWhat does this mean about the relationship between dart length and width?\n\n\nUsing the DartPoints dataset, build a linear model showing the relationship (if any) between the length and width of dart points. Be sure to do all of the following:\n\nTo visualize the relationship, make a scatter plot of dart Length and Width using ggplot() and geom_point(). Hint: your aesthetic mapping should be aes(x = Width, y = Length).\nUse the correct formula syntax. In this case, the dependent variable is Length and the independent variable is Width.\nUse summary() to report the model.\nAre the coefficient estimates significant?\nWhat does this mean about the relationship between the length and width of dart points? Hint: it’s called allometry.\n\n\nUsing the OxfordPots dataset, build a linear model showing the relationship (if any) between the percentage of Oxford Pots found on an archaeological site and the distance of that site from the city of Oxford. Be sure to do all of the following:\n\nTo visualize the relationship, make a scatter plot of the proportion of Oxford pots and distance to Oxford using ggplot() and geom_point(). Hint: your aesthetic mapping should be aes(x = OxfordDst, y = OxfordPct).\nUse the correct formula syntax. In this case, the dependent variable is OxfordPct and the independent variable is OxfordDst.\nUse summary() to report the model.\nAre the coefficient estimates significant?\nWhat does this mean about the relationship between the proportion of Oxford pots on an archaeological site and distance from Oxford?"
  },
  {
    "objectID": "labs/05-distributions-lab.html",
    "href": "labs/05-distributions-lab.html",
    "title": "Lab 05: Visualizing Distributions",
    "section": "",
    "text": "This lab will guide you through the process of\n\nvisualizing amounts with bar charts\nvisualizing distributions with\n\nhistograms\nprobability density plots\ncumulative distribution plots\nboxplots\n\n\nbase R alternatives to ggplot\n\nWe will be using the following packages:\n\narchdata\npalmerpenguins\ntidyverse\nviridis\n\n\nlibrary(archdata)\nlibrary(palmerpenguins)\nlibrary(tidyverse)\nlibrary(viridis)\n\n\n\n\n\n\n\nNote\n\n\n\nYou do not have to explicitly attach the {graphics} package, as it comes pre-loaded every time you open a new R session. That’s partly what it means for it to be a “base” R package.\n\n\n\n\n\npenguins\n\nIncludes measurements for penguin species, island in Palmer Archipelago, size (flipper length, body mass, bill dimensions), and sex.\npackage: palmerpenguins\n\nreference: https://allisonhorst.github.io/palmerpenguins/reference/penguins.html\n\n\n\n\nDartPoints\n\nIncludes measurements of 91 Archaic dart points recovered during surface surveys at Fort Hood, Texas.\npackage: archdata\n\nreference: https://cran.r-project.org/web/packages/archdata/archdata.pdf\n\n\n\n\ntitanic\n\nProvides information on the fate of passengers of the Titanic, including economic status, sex, age, and survival.\npackage: none\nreference: https://wilkelab.org/SDS375/\n\nnote: we’re going to download this one rather than get it from a package"
  },
  {
    "objectID": "labs/05-distributions-lab.html#bar-charts",
    "href": "labs/05-distributions-lab.html#bar-charts",
    "title": "Lab 05: Visualizing Distributions",
    "section": "Bar charts",
    "text": "Bar charts\nBar charts or bar plots use the length or height of bars to represent the amount of some variable across categories or groups. With ggplot2, you can create a bar chart with geom_bar(). As an example, we’ll use the penguins data set.\n\npenguins\n\n# A tibble: 344 × 8\n   species island    bill_length_mm bill_depth_mm flipper_…¹ body_…² sex    year\n   <fct>   <fct>              <dbl>         <dbl>      <int>   <int> <fct> <int>\n 1 Adelie  Torgersen           39.1          18.7        181    3750 male   2007\n 2 Adelie  Torgersen           39.5          17.4        186    3800 fema…  2007\n 3 Adelie  Torgersen           40.3          18          195    3250 fema…  2007\n 4 Adelie  Torgersen           NA            NA           NA      NA <NA>   2007\n 5 Adelie  Torgersen           36.7          19.3        193    3450 fema…  2007\n 6 Adelie  Torgersen           39.3          20.6        190    3650 male   2007\n 7 Adelie  Torgersen           38.9          17.8        181    3625 fema…  2007\n 8 Adelie  Torgersen           39.2          19.6        195    4675 male   2007\n 9 Adelie  Torgersen           34.1          18.1        193    3475 <NA>   2007\n10 Adelie  Torgersen           42            20.2        190    4250 <NA>   2007\n# … with 334 more rows, and abbreviated variable names ¹​flipper_length_mm,\n#   ²​body_mass_g\n\n\n\nggplot(penguins) +\n  geom_bar(\n    aes(x = species)\n  )\n\n\n\n\nYou can reorder the species variable based on their frequencies (with the most frequent first, the least frequent last) using fct_infreq(), short for factor infrequent. The word ‘factor’ here refers to the way that R represents categorical or grouping variables.\n\nggplot(penguins) +\n  geom_bar(\n    aes(x = fct_infreq(species))\n  )\n\n\n\n\nIt will sometimes be preferable to orient bar charts horizontally. The simplest way to do that is to pass the factor or grouping variable to the “y” argument in the aes(). Notice that you have to change the labels, too, since the count will now be on the x-axis, species on the y-axis.\n\nggplot(penguins) +\n  geom_bar(\n    aes(y = fct_infreq(species)),\n  )\n\n\n\n\nYou will notice now that the Adelie penguins, the most frequent species in our data, appear at the bottom. This is because the ordering starts at the origin, as it did when these were arrayed on the x-axis. In this case and in many other cases, it will make sense to re-order these with the most frequent category or species at the top and the least frequent on the bottom. To do that, we use fct_rev(), short for factor reverse, as in “reverse the order of this factor.”\n\nggplot(penguins) +\n  geom_bar(\n    aes(y = fct_rev(fct_infreq(species)))\n  )\n\n\n\n\nNow, let’s update the default fill color and theme. We’ll also remove most of the grid lines, as they do not contribute to interpreting the data.\n\nggplot(penguins) +\n  geom_bar(\n    aes(y = fct_rev(fct_infreq(species))),\n    fill = \"#6B8F7E\"\n  ) +\n  labs(\n    x = \"Count\", \n    y = NULL, \n    title = \"Palmer Penguins\"\n  ) +\n  theme_bw() +\n  theme(\n    panel.grid.minor = element_blank(), # remove all minor grid lines\n    panel.grid.major.y = element_blank() # remove major grid lines only on the y-axis\n  )\n\n\n\n\nExercises\n\nCreate a bar chart of the counts of dart point types using the DartPoints dataset (the types are stored in the Name variable). Remember to load that data into R with data(\"DartPoints\"). Then do all of the following:\n\nRe-orient the figure horizontally.\nOrder the types by their frequency, with the most frequent on the top, the least frequent on the bottom.\nAdd appropriate labels.\nChange the theme and remove unnecessary grid lines."
  },
  {
    "objectID": "labs/05-distributions-lab.html#histograms",
    "href": "labs/05-distributions-lab.html#histograms",
    "title": "Lab 05: Visualizing Distributions",
    "section": "Histograms",
    "text": "Histograms\nA histogram is an excellent aid for visualizing the distribution of numerical data. Making one involves “binning” a continuous variable, counting the number of its values that fall into each bin, then drawing a rectangle for each bin whose height is proportional to the count for that bin. A good example of this is the distribution of a population over age ranges, like the age distribution of the passengers on the ill-fated voyage of the Titanic. Before we get to what the histogram looks like for this macabre example, let’s first have a look at the raw data:11 These data come from Claus Wilke’s Fundamentals of Data Visualization.\n\n\n\n\n\n\nWarning\n\n\n\n\n\nTo be able to work through the examples in this section, you need to run the code below. We’ll explain what this code does in another lab.\n\ntitanic <- read.csv(\"https://raw.githubusercontent.com/wilkelab/SDS375/master/datasets/titanic.csv\")\n\n\n\n\n\nhead(titanic)\n\n  class   age    sex survived\n1   1st 29.00 female survived\n2   1st  2.00 female     died\n3   1st 30.00   male     died\n4   1st 25.00 female     died\n5   1st  0.92   male survived\n6   1st 47.00   male survived\n\n\nYou see that our data has an age for each individual passenger. If we bin those ages into 5 year intervals (0-5, 5-10, 10-15, and so on) and count the number of passengers that fall into each bin or age range, we get a summary table that looks like this.\n\n\n\n\n  \n  \n\nAge Range\n      Count\n       \n      Age Range\n      Count\n    \n\n\n0–5\n36\n   \n41–45\n54\n\n\n6–10\n19\n   \n46–50\n50\n\n\n11–15\n18\n   \n51–55\n26\n\n\n16–20\n99\n   \n56–60\n22\n\n\n21–25\n139\n   \n61–65\n16\n\n\n26–30\n121\n   \n66–70\n3\n\n\n31–35\n76\n   \n71–75\n3\n\n\n36–40\n74\n   \n76–80\n0\n\n\n\n\n\n\nHere’s how those values look as bins:\n\n\n\n\n\nTo construct the actual histogram, we use ggplot() and geom_histogram(). Importantly, we pass ggplot() the raw tidy data, not the summary table, and in this case, we are going to use the default number of bins. Here is how that looks:\n\nggplot(titanic) + \n  geom_histogram(\n    aes(age)\n  )\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nNotice that ggplot2 provides an informative message that the default number of bins used in this plot is 30, which makes the age interval for each bin about 2.5 years. We can (and SHOULD!) change this by specifying a different number of bins with bins, as the number of bins can dramatically change the interpretation of the distribution.\n\nggplot(titanic) +\n  geom_histogram(\n    aes(age),\n    bins = 10\n  )\n\n\n\n\nUnfortunately, the default histogram produced by ggplot2 doesn’t do a great job of conveying the relationship between the axis text and the ranges represented by the widths of the rectangles. For instance, zero falls at the center of the first histogram, which would seem to suggest that the range of values represented by that rectangle includes negative ages. What it is really saying is that zero falls in the range of that bin, but that is not obvious. One solution to this issue is to set the binwidth (or bin width, as in the range of values for that bin) and the bin boundary (or where the left side of the rectangle is positioned relative to the bin range). That’s a tad confusing, I know, but the basic idea is that if you set the width to 5 and the boundary to 0, then the bin for the range 0-5 will start at 0, and the bin for the range 5-10 will start at 5, and the bin for 10-15 will start at 10, and so on. Finding the best options for visualizing your data will, of course, involve some trial and error.\n\nggplot(titanic) +\n  geom_histogram(\n    aes(age),\n    binwidth = 5,\n    boundary = 0\n  )\n\n\n\n\nAs always, you can change the fill and outline color for these plots by supplying those parameters to the geometry. We can also clean up the labels with labs() and change the theme if we like (and we do). And since the heights of these bars are what we care about, we can turn off the light vertical grid lines that ggplot2 adds by default by specifying this as a theme() option.\n\nggplot(titanic) +\n  geom_histogram(\n    aes(age),\n    binwidth = 5,\n    boundary = 0,\n    fill = \"#6B8F7E\",\n    color = \"#4C6257\",\n  ) +\n  labs(\n    x = \"Age (years)\",\n    y = \"Count\",\n    title = \"Passengers of the Titanic\"\n  ) +\n  theme_minimal() +\n  theme(\n    panel.grid.major.x = element_blank(),\n    panel.grid.minor = element_blank()\n  )\n\n\n\n\nOne last point before moving on. You may have noticed that the data contains a binary variable survived. Do you think maybe the age distribution of those that survived differs from the distribution for those that did not survive? Do you think maybe they were older? Younger? Or is there no difference? Let’s explore this a little by visualizing the different distributions using facet_wrap().\n\nggplot(titanic) +\n  geom_histogram(\n    aes(age),\n    binwidth = 5,\n    boundary = 0,\n    fill = \"#6B8F7E\",\n    color = \"#4C6257\",\n  ) +\n  labs(\n    x = \"Age (years)\",\n    y = \"Count\",\n    title = \"Passengers of the Titanic\"\n  ) +\n  facet_wrap(vars(survived)) + # <----- adding facets here\n  theme_minimal() +\n  theme(\n    panel.grid.major.x = element_blank(),\n    panel.grid.minor.x = element_blank()\n  )\n\n\n\n\nHmmmm 🤔. What do you think this means? And how might it help to compare this to the total distribution?\nExercises\n\nCreate a histogram of penguin bill length using the penguins dataset from the palmerpenguins package. Then do all of the following:\n\nChange the number of bins (try two different options).\nTry specifying the bin width and boundary.\nChange the fill and outline color.\nReset the labels to be more informative.\nChange the theme and remove the vertical grid lines.\n\n\nRepeat (1), but use the DartPoints dataset from the {archdata} package, creating a histogram of dart length (Length in the table).\n\nDoes it look like the dart types might differ in length? Or maybe they’re all basically the same?"
  },
  {
    "objectID": "labs/05-distributions-lab.html#density-plots",
    "href": "labs/05-distributions-lab.html#density-plots",
    "title": "Lab 05: Visualizing Distributions",
    "section": "Density Plots",
    "text": "Density Plots\nHere you will learn how to generate a “density” plot or a plot of the underlying probability density of a variable using ggplot() and geom_density(). This is similar to a histogram in that it seeks to show the distribution of a continuous random variable. It differs, however, in the way it goes about doing that. To see how they differ, let’s first have a look at these two plots of the titanic data. The one on the left is a histogram, the one on the right a density plot.\n\n\n\n\n\n\nWarning\n\n\n\n\n\nTo be able to work through the examples in this section, you need to run the code below. We’ll explain what this code does in another lab.\n\ntitanic <- read.csv(\"https://raw.githubusercontent.com/wilkelab/SDS375/master/datasets/titanic.csv\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe big difference here is that histograms discretize a sample using bins, counts the number of observations that fall into each bin, and then represents those counts using the heights of the rectangles. A density plot uses a method known as kernel density estimate (or KDE) to estimate the density of each observation and represents it using the height of a smooth and continuous curve. The KDE approach works like this. First, you define the kernel, often the Gaussian distribution with constant variance but a mean defined by each observation. Then you define a bandwidth, which is used to scale each kernel. The heights of the kernels are then summed to produce a curve like the one above.\n\n\n\n\n\nAs with the histogram, we specify a density geometry for ggplot using geom_density(). We can also update the fill and outline colors, remove unnecessary grid lines, specify the labels, and choose a simpler theme.\n\nggplot(titanic) + \n  geom_density(\n    aes(age),\n    fill = \"#6B8F7E\",\n    color = \"#4C6257\"\n  ) +\n  theme_minimal() +\n  theme(\n    panel.grid.major.x = element_blank(),\n    panel.grid.minor = element_blank()\n  )\n\n\n\n  labs(\n    x = \"Age (years)\",\n    y = \"Density\",\n    title = \"Passengers of the Titanic\"\n  )\n\n$x\n[1] \"Age (years)\"\n\n$y\n[1] \"Density\"\n\n$title\n[1] \"Passengers of the Titanic\"\n\nattr(,\"class\")\n[1] \"labels\"\n\n\nWe can also map these aesthetics to other variables like the sex of the passenger. And, we can change the default fill colors using scale_fill_manual(), too.\n\nggplot(titanic) + \n  geom_density(\n    aes(age, fill = sex)\n  ) + \n  scale_fill_manual(\n    values = c(\"#A8BFF0\", \"#FFE66D\")\n  ) +\n  labs(\n    x = \"Age (years)\",\n    y = \"Density\",\n    title = \"Passengers of the Titanic\"\n  )\n\n\n\n\nIn this case, however, it’s hard to see where the two distributions overlap, which makes it hard to compare them. One way to fix this is to change the opacity of the fill color using the alpha() function (note that the alpha or transparency of a color can range from 0 to 1.).\n\nggplot(titanic) + \n  geom_density(\n    aes(age, fill = sex)\n  ) + \n  scale_fill_manual(\n    values = alpha(c(\"#A8BFF0\", \"#FFE66D\"), 0.5)\n  ) +\n  labs(\n    x = \"Age\",\n    y = \"Density\",\n    title = \"Passengers of the Titanic\"\n  )\n\n\n\n\nThis is still a little hard to read, so let’s try faceting instead. Let’s also drop the background vertical grid lines using the theme() function. At the same time, we’ll go ahead and drop the label “sex” from the legend as that should be obvious from the key. We do that by setting name = NULL in scale_fill_manual().\n\nggplot(titanic, aes(age, fill = sex)) + \n  geom_density() + \n  scale_fill_manual(\n    name = NULL,\n    values = c(\"#A8BFF0\", \"#FFE66D\")\n  ) +\n  labs(\n    x = \"Age\",\n    y = \"Density\",\n    title = \"Age Distribution\",\n    subtitle = \"Passengers of the Titanic\"\n  ) +\n  facet_wrap(~sex) +\n  theme_minimal() +\n  theme(\n    panel.grid.major.x = element_blank(),\n    panel.grid.minor.x = element_blank(),\n  )\n\n\n\n\nNotice that the facet labels are redundant with the legend key here, so let’s remove those, too. We do that by setting the theme arguments strip.background and strip.text to element_blank(). Finally, we can move the legend to the bottom of the plot and make it horizontal with legend.position and legend.direction respectively.\n\nggplot(titanic, aes(age, fill = sex)) + \n  geom_density() + \n  scale_fill_manual(\n    name = NULL,\n    values = c(\"#A8BFF0\", \"#FFE66D\")\n  ) +\n  labs(\n    x = \"Age\",\n    y = \"Density\",\n    title = \"Age Distribution\",\n    subtitle = \"Passengers of the Titanic\"\n  ) +\n  facet_wrap(~sex) +\n  theme_minimal() +\n  theme(\n    legend.position = \"bottom\",\n    legend.direction = \"horizontal\",\n    panel.grid.major.x = element_blank(),\n    panel.grid.minor.x = element_blank(),\n    strip.background = element_blank(),\n    strip.text = element_blank()\n  )\n\n\n\n\nExercises\n\nMake a kernel density plot of penguin bill length using ggplot() and geom_density(). Then make all of the following changes:\n\nMap penguin species to the fill aesthetic.\nUpdate the axis labels and plot title using labs().\nUse scale_fill_viridis to use colorblind safe colors for the fill. Note! Species is a discrete or categorical variable, so make sure to set discrete = TRUE!\nUse facet_wrap() to facet by species.\nChoose a suitable theme, like theme_minimal().\nRemove vertical grid lines.\nChange the legend position to bottom and make it horizontal.\nRemove strip text and background.\n\n\n\nDo the same as (1), but for dart point length, and substitute dart point type for species."
  },
  {
    "objectID": "labs/05-distributions-lab.html#cumulative-distribution-plots",
    "href": "labs/05-distributions-lab.html#cumulative-distribution-plots",
    "title": "Lab 05: Visualizing Distributions",
    "section": "Cumulative Distribution Plots",
    "text": "Cumulative Distribution Plots\nWhen constructed from a sample, the cumulative distribution is technically referred to as the empirical cumulative distribution function (or eCDF). It has one critical advantage over histograms and probability density plots, namely, that you don’t have to specify a binwidth or bandwidth. That’s because you first order the data from smallest to largest value, then count the number of observations that are equal to or less than each unique value, and increment the cumulative proportion of observations by that amount.\nAs a simple example, consider this vector or sample: (0.3, 2.0, 3.4, 1.2, 2.2, 1.9).\nRearranging it smallest to largest value, we get: (0.3, 1.2, 1.9, 2.0, 2.2, 3.4).\nNow, for each unique value, we count the number of observations that are less than or equal to it, so\n0.0 -> none of them, so 0\n0.3 -> just 0.3, so 1\n1.2 -> 0.3 and 1.2, so 2\n1.9 -> 0.3, 1.2, and 1.9, so 3\n2.0 -> 0.3, 1.2, 1.9, and 2.0, so 4\n2.2 -> 0.3, 1.2, 1.9, 2.0, and 2.2, so 5\n3.4 -> all of them, so 6\nAs proportions of the total sample, which has six observations, that’s\n0.0 -> 0/6 = 0.00\n0.3 -> 1/6 = 0.17\n1.2 -> 2/6 = 0.33 \n1.9 -> 3/6 = 0.50 \n2.0 -> 4/6 = 0.67 \n2.2 -> 5/6 = 0.83\n3.4 -> 6/6 = 1.00\nNow, we can plot that.\n\n\n\n\n\nSo, fewer assumptions here, but it’s also a smidge harder to interpret since it gives you the probability of being less than or equal to x, for example, the probability of being less than or equal to 1.2 is 0.33.\nUnfortunately, plotting the eCDF of, for example, the ages of passengers on the Titanic, is not straightforward with ggplot2 because you have to use what is known as a stat_*() function, in this case, stat_ecdf(), rather than the more familiar geometry functions. This is also an example of when it would be useful to have major grid lines along both axes, so we will only remove the minor ones.\n\nggplot(titanic) +\n  stat_ecdf(\n    aes(age),\n    geom = \"step\",\n    color = \"#4C6257\",\n    linewidth = 1.2\n  ) +\n  theme_minimal() +\n  theme(\n    panel.grid.minor = element_blank()\n  ) +\n  labs(\n    x = \"Age (years)\",\n    y = \"Probability\",\n    title = \"Passengers of the Titanic\"\n  )\n\n\n\n\nYou can facet these, too, if you so desire.\n\nggplot(titanic) +\n  stat_ecdf(\n    aes(age, color = class),\n    geom = \"step\",\n    linewidth = 1.2\n  ) +\n  scale_color_manual(\n    name = \"Class\",\n    values = c(\"#942911\", \"#37423D\", \"#0094C6\")\n  ) +\n  theme_minimal() +\n  theme(\n    panel.grid.minor = element_blank(),\n    legend.justification = c(\"right\", \"bottom\"),\n    legend.position = c(0.98, 0.05)\n  ) +\n  labs(\n    x = \"Age (years)\",\n    y = \"Probability\",\n    title = \"Passengers of the Titanic\"\n  )\n\n\n\n\nTwo things to note here. First, there’s the interpretation. Consider the age 40. If you follow that vertical grid line up to where it intersect the line for each class, you will see that only about 50% of first class passengers were 40 years old or younger, but for second and third class, that number is closer to 80 or even 85%. The implication here is that first class passengers on the Titanic were generally older than second and third class passengers.\nThe second thing to note is that I moved the legend into the plot area by specifying the legend justification and the legend position. The position may consist of character strings like “top” or “bottom” or a vector of x,y coordinates (both ranging from 0 to 1, zero for left and bottom, one for top and right). The justification determines how the legend is oriented relative to the position coordinates. In this case, the bottom right corner of the legend will be at x = 0.98 and y = 0.05. This usually requires some trial and error before finding a position you like.\nExercises\n\nMake an eCDF plot of penguin bill length using ggplot() and stat_ecdf(). Then make all of the following changes:\n\nMap penguin species to the color aesthetic.\nUpdate the axis labels and plot title using labs().\nUse scale_color_viridis to use colorblind safe colors for the fill. Note! Species is a discrete or categorical variable, so make sure to set discrete = TRUE!\nChoose a suitable theme, like theme_minimal().\nRemove minor grid lines on each axis.\nMove the legend into the plot panel.\n\n\nDo the same as (1), but for dart point length, and substitute dart point type for species."
  },
  {
    "objectID": "labs/05-distributions-lab.html#boxplots",
    "href": "labs/05-distributions-lab.html#boxplots",
    "title": "Lab 05: Visualizing Distributions",
    "section": "Boxplots",
    "text": "Boxplots\nIn this section we’ll learn how to make boxplots, which provide a simple but effective way of representing the distribution of a variable. For one or two variables, it’s often better to use a density plot, but if you’re comparing the distributions of lots of variables or lots of samples of the same variable across multiple categories, a density plot can get crowded quick. That’s when it’s useful to turn to boxplots, so we’ll focus on that here.\n\nset.seed(42)\n\ny <- rnorm(250)\n\nlabels <- tibble(\n  y = c(boxplot.stats(y)$stats, max(y)),\n  x = 0.5,\n  label = c(\"1.5 x IQR\", \"first quartile\", \"median\", \"third quartile\", \"1.5 x IQR\", \"outlier\")\n)\n\nggplot(tibble(x = 0, y), aes(x, y)) + \n  stat_boxplot(geom ='errorbar', width = 0.33) +\n  geom_boxplot(\n    fill = \"#6B8F7E\", \n    width = 0.6,\n    outlier.size = 4,\n    notch = TRUE,\n    notchwidth = 0.75\n  ) +\n  geom_text(\n    data = labels,\n    aes(x, y, label = label), \n    hjust = 0,\n    size = 11/.pt\n  ) +\n  geom_point(\n    data = tibble(x = runif(length(y), -1.2, -0.5), y = y),\n    aes(x, y),\n    size = 3,\n    color = \"#4C6257\",\n    alpha = 0.85\n  ) +\n  coord_cartesian(xlim = c(-2.2, 3)) +\n  theme_void()\n\n\n\n\nAs you can see, the boxplot shows the distribution of a variable using a five-number summary, which includes all of the following:\n\nMinimum: the lowest value excluding outliers\nMaximum: the greatest value excluding outliers\nMedian: the middle value that separates the data in half (also called the second quartile)\nFirst quartile: the middle value of the lower half of the data, meaning 75% of the data fall above it and %25 below it (also called the lower quartile)\nThird quartile: the middle value of the upper half of the data, meaning 25% of the data fall above it and 75% below it (also called the upper quartile)\n\nBy extension, this includes the\n\nInterquartile Range: the distance between the first and third quartile, which includes 50% of the data.\n\nNotice that the minimum and maximum values are connected to the first and third quartiles by lines commonly referred to as “whiskers.” These are drawn to the largest and smallest values that fall within 1.5 * IQR of the first and third quartiles, respectively. Observations that fall above or below the whiskers are considered “outliers”.\nTo make a boxplot with ggplot, we need our table of data (as always), which we pass to the ggplot() function. We then specify the boxplot geometry with geom_boxplot().\n\nggplot(penguins) +\n  geom_boxplot(\n    aes(x = species, y = bill_length_mm),\n    fill = \"#6B8F7E\"\n  )\n\n\n\n\nNotice that we specified that the distribution of bill length should range over the y-axis. This makes the boxplots display vertically in the graph. We can change them to horizontal by having the variables distribution range over the x-axis.\n\nggplot(penguins) +\n  geom_boxplot(\n    aes(x = bill_length_mm, y = species),\n    fill = \"#6B8F7E\"\n  )\n\n\n\n\nNow, let’s update the axis labels and add a title. In this case, we’ll drop the y-axis label because it should be obvious from the values there that these are different species. While we’re at it, let’s also change the theme settings to minimal and remove the horizontal grid lines. And, we’ll add perpendicular lines at the end of the whiskers with stat_boxplot(geom = \"errorbar\"). Note that we move the aes() call up to ggplot(), so that these arguments can be shared between the stat and the geometry.\n\nggplot(penguins, aes(x = bill_length_mm, y = species)) +\n  stat_boxplot(geom ='errorbar', width = 0.33) +\n  geom_boxplot(fill = \"#6B8F7E\") +\n  labs(\n    x = \"Bill Length (mm)\",\n    y = NULL,\n    title = \"Palmer Penguins\"\n  ) +\n  theme_bw() +\n  theme(\n    panel.grid.major.y = element_blank(),\n    panel.grid.minor = element_blank(),\n  )\n\n\n\n\nExercises\n\nMake a boxplot showing the distribution of penguin body mass by island. Do all of the following:\n\nPosition the boxes horizontally.\nChange the fill color to a color of your choice.\nUpdate the labels and add a title.\nChange the theme to one of your choice.\nRemove the horizontal grid lines.\nAdd perpendicular lines to the whiskers.\n\n\nDo the same as (1), but for dart point length, substituting dart type for island."
  },
  {
    "objectID": "labs/05-distributions-lab.html#base-r-graphics",
    "href": "labs/05-distributions-lab.html#base-r-graphics",
    "title": "Lab 05: Visualizing Distributions",
    "section": "Base R {graphics}\n",
    "text": "Base R {graphics}\n\nWhile we have focused on visualizing distributions using ggplot2, it’s also useful to learn how to construct them with the {graphics} package from base R. The latter is not something you would want to rely on to create publication-quality figures, but it does provide simple and powerful tools for visually exploring your data. This means we will not spend much time learning the ends and outs of manipulating base R graphics to try and make them look similar to those generated with ggplot(). Believe me when I say that would be a painful experience. You will be best served only turning to {graphics} for rough and ready exploratory visualizations and not for the sort of fine-tuning you would do with ggplot2 to get a figure ready for publication. With that, then, here are the base R alternatives.\nFair warning, we’re going to make sure we have plenty of margin space for these plots, so labels and whatnot don’t get cutoff. We set the margins using the par() function (for graphical parameters) and pass a vector of length 4 to the mar (for margins) argument, one value for the size of each margin.\nFirst, we’ll get the default margins.\n\ndefault_margins <- par()$mar\n\nbig_margins <- c(6, 8, 4, 1)\n\nBar chart\nThe base R equivalent of ggplot() + geom_bar() is the barplot() function. There are many differences between these two methods, but the primary one is that barplot() requires you to compute the counts for each category beforehand. To do that, we’ll use the handy - though misleadingly named - table() function.\n\nspecies <- table(penguins$species)\n\nspecies\n\n\n   Adelie Chinstrap    Gentoo \n      152        68       124 \n\nbarplot(\n  height = sort(species, decreasing = TRUE),\n  col = \"#6B8F7E\",\n  xlab = NULL,\n  ylab = \"Count\"\n)\n\n\n\n\nNotice that we re-ordered the species variable using the sort() function with decreasing = TRUE rather than fct_infreq(). This will work only on the summarized data generated by the table() function, not the raw data passed to geom_bar().\nIf you want to make this horizontal, just specify horiz = TRUE. You’ll also want to change the sorting to increasing, to get the most frequent species on top. And use las = 1 to rotate the axis text so that they are horizontal, too.\n\npar(mar = big_margins)\n\nbarplot(\n  height = sort(species, decreasing = FALSE),\n  horiz = TRUE,\n  las = 1,\n  col = \"#6B8F7E\",\n  xlab = \"Count\",\n  ylab = NULL\n)\n\n\n\n\nHistogram\nThe base R equivalent of ggplot() + geom_histogram() is the extremely compact hist() function. It looks like this.\n\n# reset margins\npar(mar = default_margins)\n\nhist(\n  titanic$age,\n  breaks = seq(0, 75, by = 5),\n  xlab = \"Age (years)\",\n  ylab = \"Count\",\n  main = \"Passengers of the Titanic\",\n  col = \"#6B8F7E\",\n  border = \"#4C6257\"\n)\n\n\n\n\nTwo important things to note here. First, you have to pass the histogram function the variable age, so you have to pull out of the data.frame with titanic$age. Second, you use breaks instead of bins or binwidth to define the bins.\nProbability density\nThe base R equivalent of ggplot() + geom_density() uses the density() and plot() functions. It looks like this.\n\nkde <- density(titanic$age)\n\nplot(\n  kde,\n  xlab = \"Age (years)\",\n  ylab = \"Density\",\n  main = \"Passengers of the Titanic\"\n)\n\n\n\n\nTo control the fill and outline color, it’s necessary to add the KDE as an additional layer to the plot with the polygon() function.\n\nkde <- density(titanic$age)\n\nplot(\n  kde,\n  xlab = \"Age (years)\",\n  ylab = \"Density\",\n  main = \"Passengers of the Titanic\"\n)\n\npolygon(\n  kde,\n  col = \"#6B8F7E\",\n  border = \"#4C6257\"\n)\n\n\n\n\nAnd that’s about as far as I want to take you with that.\nCumulative distribution\nThe base R equivalent of ggplot() + stat_ecdf() uses the ecdf() and plot() functions. It looks like this.\n\necdf_function <- ecdf(titanic$age)\n\nplot(\n  ecdf_function,\n  col = \"#4C6257\",\n  xlab = \"Age (years)\",\n  ylab = \"Density\",\n  main = \"Passengers of the Titanic\"\n)\n\n\n\n\nThat’s a little weird. To get this into a proper stewise line, we need to do a little work. Specifically, we need to sort the unique age values of passengers on the Titanic. Then we need to feed those to the ecdf_function() we created with ecdf(). We also need to specify that we want the stepwise plot type with type = \"s\". Ugh… 😕\n\nx <- sort(unique(titanic$age))\n\nplot(\n  x,\n  ecdf_function(x),\n  type = \"s\",\n  col = \"#4C6257\",\n  xlab = \"Age (years)\",\n  ylab = \"Density\",\n  main = \"Passengers of the Titanic\"\n)\n\n\n\n\nMother of dragons… 🐉\nBoxplot\nThe base R equivalent of ggplot() + geom_boxplot() is, thankfully, boxplot(). It looks like this.\n\nboxplot(\n  penguins$bill_length_mm ~ penguins$species,\n  col = \"#4C6257\",\n  xlab = NULL,\n  ylab = \"Bill Length (mm)\",\n  main = \"Palmer Penguins\"\n)\n\n\n\n\nNotice that I used the formula notation. This is to specify the grouping structure for the boxes, so a separate box for each penguin species.\nTo make that figure horizontal, use horizontal = TRUE.\n\npar(mar = big_margins)\n\nboxplot(\n  penguins$bill_length_mm ~ penguins$species,\n  horizontal = TRUE,\n  las = 1,\n  col = \"#4C6257\",\n  xlab = \"Bill Length (mm)\",\n  ylab = NULL,\n  main = \"Palmer Penguins\"\n)\n\n\n\n\nHey, in the base R bar chart above, didn’t we specify a horizontal orientation with horiz = TRUE?\nYeah, yeah you did… 🤷\nExercises\nFor each of the following, make sure to change the fill (col) and outline (border) colors and update the labels.\n\nCreate a bar chart of the count of dart point types using table() and the barplot() function. Give the plot a horizontal orientation.\nCreate a histogram of penguin bill length using the hist() function.\nCreate a histogram of dart point length using the hist() function.\nCreate a probability density plot of penguin bill length using the density() and plot() functions.\nCreate a probability density plot of dart point length using the density() and plot() functions.\nCreate an eCDF plot of penguin bill length using the ecdf() and plot() functions. Add a separate color for each species. Make sure the plot is stepwise.\nCreate an eCDF plot of dart point length using the ecdf() and plot() functions. Add a separate color for dart point type. Make sure the plot is stepwise.\nCreate a boxplot of penguin bill length grouped by species using the boxplot() function. Give the plot a horizontal orientation.\nCreate a boxplot of dart point length grouped by dart point type using the boxplot() function. Give the plot a horizontal orientation."
  },
  {
    "objectID": "slides/01-intro-slides.html#lecture-outline",
    "href": "slides/01-intro-slides.html#lecture-outline",
    "title": "Lecture 01: Introduction",
    "section": "📋 Lecture Outline",
    "text": "📋 Lecture Outline\n\nCourse Mechanics\n\n🧱 Structure\n\n🎯 Objectives\n\n🏋 Expectations\n\n🤝 Ethics\n\n💾 Software\n\n\nCourse Content\n\nWhy statistics?\n\nWhat is an archaeological population?\n\nA note about terminology and notation\n\nStatistical programming with \n\nLiterate programming with Quarto"
  },
  {
    "objectID": "slides/01-intro-slides.html#course-structure",
    "href": "slides/01-intro-slides.html#course-structure",
    "title": "Lecture 01: Introduction",
    "section": "🧱 Course Structure",
    "text": "🧱 Course Structure\n\nMeetings are online every Tuesday from 2:00 to 5:00 PM MST.\n\nMeeting structure:\n\nhomework review and lecture (80 minutes),\n\nbreak (10 minutes), and\n\nlab (90 minutes).\n\n\nCourse work:\n\nlab and homework exercises due every Monday before class by 9:00 PM MST, and a\n\nterm project.\n\n\nAll course materials will be made available on the course website.\n\nAll graded materials will be submitted through Canvas."
  },
  {
    "objectID": "slides/01-intro-slides.html#course-objectives",
    "href": "slides/01-intro-slides.html#course-objectives",
    "title": "Lecture 01: Introduction",
    "section": "🎯 Course Objectives",
    "text": "🎯 Course Objectives\n\nStudents will develop programming skills by learning how to:\n\nimport and export data,\nwrangle (or prepare) data for analysis,\nexplore and visualize data, and\nbuild models of data and evaluate them.\n\n\n\nAnd students will gain statistical understanding by learning how to:\n\nformulate questions and alternative hypotheses,\nidentify and explain appropriate statistical tools,\nreport the results of analysis using scientific standards, and\ncommunicate the analysis to a general audience."
  },
  {
    "objectID": "slides/01-intro-slides.html#course-expectations",
    "href": "slides/01-intro-slides.html#course-expectations",
    "title": "Lecture 01: Introduction",
    "section": "🏋 Course Expectations",
    "text": "🏋 Course Expectations\nLearning is a lot like moving to a new city. You get lost, you get frustrated, you even get embarrassed! But gradually, over time, you come to know your way around. Unfortunately, you’ll only have four months in this new city, so we need to be realistic about what we can actually achieve here.\n\n\nYou won’t become fluent in R, markdown, or statistics, but…\n\n\nyou will gain some sense of the way things tend to go with those languages."
  },
  {
    "objectID": "slides/01-intro-slides.html#course-ethics",
    "href": "slides/01-intro-slides.html#course-ethics",
    "title": "Lecture 01: Introduction",
    "section": "🤝 Course Ethics",
    "text": "🤝 Course Ethics\nAll course policies and other University requirements can be found in the course syllabus. They are very, very thorough, so rather than enumerate them all, let’s just summarize them this way:\n\n\nThere are many ways to be a bully. Don’t be any of them.\n\nAnd if you see someone getting bullied, do something about it."
  },
  {
    "objectID": "slides/01-intro-slides.html#software",
    "href": "slides/01-intro-slides.html#software",
    "title": "Lecture 01: Introduction",
    "section": "💾 Software",
    "text": "💾 Software\nThe primary statistical tools for this class are\n\n Programming Language\nRStudio\nQuarto\n\nWe will go over how to install each of these during our first lab."
  },
  {
    "objectID": "slides/01-intro-slides.html#why-statistics",
    "href": "slides/01-intro-slides.html#why-statistics",
    "title": "Lecture 01: Introduction",
    "section": "Why statistics?",
    "text": "Why statistics?\n\n\n\n\n\n\n\n\n\n We want to understand something about a population.\n\nWe can never observe the entire population, so we draw a sample.\n\n\nWe then use a model to describe the sample.\n\n\nBy comparing that model to a null model, we can infer something about the population."
  },
  {
    "objectID": "slides/01-intro-slides.html#what-population-does-archaeology-study",
    "href": "slides/01-intro-slides.html#what-population-does-archaeology-study",
    "title": "Lecture 01: Introduction",
    "section": "What population does archaeology study?",
    "text": "What population does archaeology study?"
  },
  {
    "objectID": "slides/01-intro-slides.html#a-note-on-terminology-and-notation",
    "href": "slides/01-intro-slides.html#a-note-on-terminology-and-notation",
    "title": "Lecture 01: Introduction",
    "section": "A note on terminology and notation",
    "text": "A note on terminology and notation\n\n\n\nA statistic is a property of a sample.\n“We measured the heights of 42 actors who auditioned for the role of Aragorn and took the average.”\nA parameter is a property of a population.\n“Human males have an average height of 1.74 meters (5.7 feet).”\nNote: Parameters are usually capitalized.\n\n\n\n\n\n  \n  \n  \n  \n    \n      \n      population\n      sample\n    \n  \n  \n    Size\nN\nn\n    Mean\nμ\nx̄\n    Standard Deviation\nσ\ns\n    Proportion\nP\np\n    Correlation\nρ\nr"
  },
  {
    "objectID": "slides/01-intro-slides.html#why-fa-brands-r-project-colorsteelblue",
    "href": "slides/01-intro-slides.html#why-fa-brands-r-project-colorsteelblue",
    "title": "Lecture 01: Introduction",
    "section": "Why ?",
    "text": "Why ?\nWhy R?\n\n\n\n\nIt’s free!It’s transferrable!It’s efficient!It’s extensible!It’s pretty figures!It’s reproducible!It’s a community!\n\n\nR is free software under the terms of the Free Software Foundation’s GNU General Public License.\n\n\nR will run on any system: Mac OS, Windows, or Linux.\n\n\nR lets you exploit the awesome computing powers of the modern world. It also provides an elegant and concise syntax for writing complex statistical operations.\n\n\n\n\n\nR users can write add-on packages that provide additional functionality. Here are a few of my favorites.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR offers a lot of tools to produce really, really impressive graphics. For example, here is a simple plot of a normal distribution:\n\n\n\n\n\n\n\n\n\n\n\nR facilitates reproducible research in two ways. First, it forces you to declare explicitly each step in your analysis.\n\n# take the mean\nmean(my_data)\n\n# take the standard deviation\nsd(my_data)\n\nSecond, it makes R code shareable. In the simplest case, we use R scripts, but we can also use Quarto, a much more flexible tool for writing, running, and explaining R code.\n\n\nR is also an incredibly active and growing community."
  },
  {
    "objectID": "slides/01-intro-slides.html#literate-programming-with-markdown-fa-brands-markdown",
    "href": "slides/01-intro-slides.html#literate-programming-with-markdown-fa-brands-markdown",
    "title": "Lecture 01: Introduction",
    "section": "Literate programming with markdown ",
    "text": "Literate programming with markdown \n\nMarkdown is a lightweight markup language for creating formatted text using a plain-text editor. From the Wikipedia page.\n\n\n\nINPUT\n\n\nThis is a sentence in Markdown, containing `code`, **bold text**, and *italics*.\n\n\nOUTPUT\n\nThis is a sentence in Markdown, containing code, bold text, and italics."
  },
  {
    "objectID": "slides/01-intro-slides.html#quarto-markdown-r",
    "href": "slides/01-intro-slides.html#quarto-markdown-r",
    "title": "Lecture 01: Introduction",
    "section": "Quarto = Markdown + R",
    "text": "Quarto = Markdown + R\nQuarto allows you to run code and format text in one document.\n\n\nINPUT\n\nThis is an example of Quarto with markdown __syntax__ \nand __R code__.\n\n```{r}\n#| fig-width: 4\n#| fig-asp: 1\n#| fig-align: center\n\nfit <- lm(dist ~ speed, data = cars)\n\npar(pty = \"s\")\n\nplot(cars, pch = 19, col = 'darkgray')\nabline(fit, lwd = 2)\n```\n\nOUTPUT\n\nThis is an example of Quarto with markdown syntax and R code."
  },
  {
    "objectID": "slides/02-probability-slides.html#lecture-outline",
    "href": "slides/02-probability-slides.html#lecture-outline",
    "title": "Lecture 02: Probability as a Model",
    "section": "📋 Lecture Outline",
    "text": "📋 Lecture Outline\n\nWhy statistics?\n🧪 A simple experiment\nSome terminology\n🎰 Random variables\n🎲 Probability\n📊 Probability Distribution\nProbability Distribution as a Model\nProbability Distribution as a Function\nProbability Mass Functions\nProbability Density Functions\n🚗 Cars Model\nBrief Review\nA Simple Formula\nA Note About Complexity"
  },
  {
    "objectID": "slides/02-probability-slides.html#why-statistics",
    "href": "slides/02-probability-slides.html#why-statistics",
    "title": "Lecture 02: Probability as a Model",
    "section": "Why statistics?",
    "text": "Why statistics?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe want to understand something about a population.\nWe can never observe the entire population, so we draw a sample.\nWe then use a model to describe the sample.\nBy comparing that model to a null model, we can infer something about the population.\n\n\nHere, we’re going to focus on statistical description, aka models."
  },
  {
    "objectID": "slides/02-probability-slides.html#a-simple-experiment",
    "href": "slides/02-probability-slides.html#a-simple-experiment",
    "title": "Lecture 02: Probability as a Model",
    "section": "🧪 A simple experiment",
    "text": "🧪 A simple experiment\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe take ten cars, send each down a track, have them brake at the same point, and measure the distance it takes them to stop.\n\n\nQuestion: how far do you think it will take the next car to stop?\n\n\nQuestion: what distance is the most probable?\nBut, how do we determine this?"
  },
  {
    "objectID": "slides/02-probability-slides.html#some-terminology",
    "href": "slides/02-probability-slides.html#some-terminology",
    "title": "Lecture 02: Probability as a Model",
    "section": "Some terminology",
    "text": "Some terminology"
  },
  {
    "objectID": "slides/02-probability-slides.html#random-variables",
    "href": "slides/02-probability-slides.html#random-variables",
    "title": "Lecture 02: Probability as a Model",
    "section": "🎰 Random Variables",
    "text": "🎰 Random Variables\nTwo types of random variable:\n\n\nDiscrete random variables often take only integer (non-decimal) values.\n\nExamples: number of heads in 10 tosses of a fair coin, number of victims of the Thanos snap, number of projectile points in a stratigraphic level, number of archaeological sites in a watershed.\n\nContinuous random variables take real (decimal) values.\n\nExamples: cost in property damage of a superhero fight, kilocalories per kilogram, kilocalories per hour, ratio of isotopes\n\nNote: for continuous random variables, the sample space is infinite!"
  },
  {
    "objectID": "slides/02-probability-slides.html#probability",
    "href": "slides/02-probability-slides.html#probability",
    "title": "Lecture 02: Probability as a Model",
    "section": "🎲 Probability",
    "text": "🎲 Probability\nLet \\(X\\) be the number of heads in two tosses of a fair coin. What is the probability that \\(X=1\\)?"
  },
  {
    "objectID": "slides/02-probability-slides.html#probability-distribution-as-a-model",
    "href": "slides/02-probability-slides.html#probability-distribution-as-a-model",
    "title": "Lecture 02: Probability as a Model",
    "section": "Probability Distribution as a Model",
    "text": "Probability Distribution as a Model\n\n\n\nHas two components:\n\nCentral-tendency or “first moment”\n\nPopulation mean (\\(\\mu\\)). Gives the expected value of an experiment, \\(E[X] = \\mu\\).\n\nSample mean (\\(\\bar{x}\\)). Estimate of \\(\\mu\\) based on a sample from \\(X\\) of size \\(n\\).\n\n\n\n\nDispersion or “second moment”\n\nPopulation variance (\\(\\sigma^2\\)). The expected value of the squared difference from the mean.\n\nSample variance (\\(s^2\\)). Estimate of \\(\\sigma^2\\) based on a sample from \\(X\\) of size \\(n\\).\n\nStandard deviation (\\(\\sigma\\)) or \\(s\\) is the square root of the variance."
  },
  {
    "objectID": "slides/02-probability-slides.html#probability-distribution-as-a-function",
    "href": "slides/02-probability-slides.html#probability-distribution-as-a-function",
    "title": "Lecture 02: Probability as a Model",
    "section": "Probability Distribution as a Function",
    "text": "Probability Distribution as a Function\nThese can be defined using precise mathematical functions:\n\nA probability mass function (PMF) for discrete random variables.\n\nExamples: Bernoulli, Binomial, Negative Binomial, Poisson\n\nStraightforward probability interpretation.\n\n\n\n\nA probability density function (PDF) for continuous random variables.\n\nExamples: Normal, Chi-squared, Student’s t, and F\n\nHarder to interpret probability:\n\nWhat is the probability that a car takes 10.317 m to stop? What about 10.31742 m?\n\nBetter to consider probability across an interval.\n\n\nRequires that the function integrate to one (probability is the area under the curve)."
  },
  {
    "objectID": "slides/02-probability-slides.html#bernoulli",
    "href": "slides/02-probability-slides.html#bernoulli",
    "title": "Lecture 02: Probability as a Model",
    "section": "Bernoulli",
    "text": "Bernoulli\n\n\n\nDf. distribution of a binary random variable (“Bernoulli trial”) with two possible values, 1 (success) and 0 (failure), with \\(p\\) being the probability of success. E.g., a single coin flip.\n\\[f(x,p) = p^{x}(1-p)^{1-x}\\]\nMean: \\(p\\)\nVariance: \\(p(1-p)\\)"
  },
  {
    "objectID": "slides/02-probability-slides.html#binomial",
    "href": "slides/02-probability-slides.html#binomial",
    "title": "Lecture 02: Probability as a Model",
    "section": "Binomial",
    "text": "Binomial\n\n\n\nDf. distribution of a random variable whose value is equal to the number of successes in \\(n\\) independent Bernoulli trials. E.g., number of heads in ten coin flips.\n\\[f(x,p,n) = \\binom{n}{x}p^{x}(1-p)^{1-x}\\]\nMean: \\(np\\)\nVariance: \\(np(1-p)\\)"
  },
  {
    "objectID": "slides/02-probability-slides.html#poisson",
    "href": "slides/02-probability-slides.html#poisson",
    "title": "Lecture 02: Probability as a Model",
    "section": "Poisson",
    "text": "Poisson\n\n\n\nDf. distribution of a random variable whose value is equal to the number of events occurring in a fixed interval of time or space. E.g., number of orcs passing through the Black Gates in an hour.\n\\[f(x,\\lambda) = \\frac{\\lambda^{x}e^{-\\lambda}}{x!}\\]\nMean: \\(\\lambda\\)\nVariance: \\(\\lambda\\)"
  },
  {
    "objectID": "slides/02-probability-slides.html#normal-gaussian",
    "href": "slides/02-probability-slides.html#normal-gaussian",
    "title": "Lecture 02: Probability as a Model",
    "section": "Normal (Gaussian)",
    "text": "Normal (Gaussian)\n\n\n\nDf. distribution of a continuous random variable that is symmetric from positive to negative infinity. E.g., the height of actors who auditioned for the role of Aragorn.\n\\[f(x,\\mu,\\sigma) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\;exp\\left[-\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^2\\right]\\]\nMean: \\(\\mu\\)\nVariance: \\(\\sigma^2\\)"
  },
  {
    "objectID": "slides/02-probability-slides.html#cars-model",
    "href": "slides/02-probability-slides.html#cars-model",
    "title": "Lecture 02: Probability as a Model",
    "section": "🚗 Cars Model",
    "text": "🚗 Cars Model\n\n\n\n\n\nLet’s use the Normal distribution to describe the cars data.\n\n\\(Y\\) is stopping distance for population\n\\(Y\\) is normally distributed, \\(Y \\sim N(\\mu, \\sigma)\\)\nExperiment is a random sample of size \\(n\\) from \\(Y\\) with \\(y_1, y_2, ..., y_n\\) observations.\nSample statistics (\\(\\bar{y}, s\\)) approximate population parameters (\\(\\mu, \\sigma\\)).\n\n\n\nSample statistics:\n\nMean (\\(\\bar{y}\\)) = 10.54 m\n\n\n\nSample statistics:\n\nMean (\\(\\bar{y}\\)) = 10.54 m\n\nThis is our approximate expectation\n\n\\(E[Y] = \\mu \\approx \\bar{y}\\)\n\n\n\nSample statistics:\n\nMean (\\(\\bar{y}\\)) = 10.54 m\n\nBut, there’s error, \\(\\epsilon\\), in this estimate.\n\n\\(\\epsilon_i = y_i - \\bar{y}\\)\n\n\n\nSample statistics:\n\nMean (\\(\\bar{y}\\)) = 10.54 m\n\nThe average squared error is the variance:\n\n\\(s^2 = \\frac{1}{n-1}\\sum \\epsilon_{i}^{2}\\)\n\n\n\nSample statistics:\n\nMean (\\(\\bar{y}\\)) = 10.54 m\nS.D. (\\(s\\)) = 5.353 m\n\nThis is our uncertainty, how big we think any given error will be.\n\n\nSample statistics:\n\nMean (\\(\\bar{y}\\)) = 10.54 m\nS.D. (\\(s\\)) = 5.353 m\n\nSo, here is our probability model.\n\\[Y \\sim N(\\bar{y}, s)\\] This is only an estimate of \\(N(\\mu, \\sigma)\\)!\n\n\nSample statistics:\n\nMean (\\(\\bar{y}\\)) = 10.54 m\nS.D. (\\(s\\)) = 5.353 m\n\nWith it, we can say, for example, that the probability that a random draw from this distribution falls within one standard deviation (dashed lines) of the mean (solid line) is 68.3%."
  },
  {
    "objectID": "slides/02-probability-slides.html#a-simple-formula",
    "href": "slides/02-probability-slides.html#a-simple-formula",
    "title": "Lecture 02: Probability as a Model",
    "section": "A Simple Formula",
    "text": "A Simple Formula\n\n\n\n\n\nThis gives us a simple formula\n\\[y_i = \\bar{y} + \\epsilon_i\\] where\n\n\\(y_i\\): stopping distance for car \\(i\\), data\n\\(\\bar{y} \\approx E[Y]\\): expectation, predictable\n\\(\\epsilon_i\\): error, unpredictable\n\n\n\nThis gives us a simple formula\n\\[y_i = \\bar{y} + \\epsilon_i\\]\nIf we subtract the mean, we have a model of the errors centered on zero:\n\\[\\epsilon_i = 0 + (y_i - \\bar{y})\\]\n\n\nThis gives us a simple formula\n\\[y_i = \\bar{y} + \\epsilon_i\\]\nIf we subtract the mean, we have a model of the errors centered on zero:\n\\[\\epsilon_i = 0 + (y_i - \\bar{y})\\]\nThis means we can construct a probability model of the errors centered on zero."
  },
  {
    "objectID": "slides/02-probability-slides.html#probability-model-of-errors",
    "href": "slides/02-probability-slides.html#probability-model-of-errors",
    "title": "Lecture 02: Probability as a Model",
    "section": "Probability Model of Errors",
    "text": "Probability Model of Errors\n\n\nNote that the mean changes, but the variance stays the same."
  },
  {
    "objectID": "slides/02-probability-slides.html#summary",
    "href": "slides/02-probability-slides.html#summary",
    "title": "Lecture 02: Probability as a Model",
    "section": "Summary",
    "text": "Summary\nNow our simple formula is this:\n\\[y_i = \\bar{y} + \\epsilon_i\\] \\[\\epsilon \\sim N(0, s) \\]\n\nAgain, \\(\\bar{y} \\approx E[Y] = \\mu\\).\nFor any future outcome:\n\nThe expected value is deterministic\nThe error is stochastic\n\n\nMust assume that the errors are iid!\n\nindependent = they do not affect each other\nidentically distributed = they are from the same probability distribution\n\nThe distribution is now a model of the errors!"
  },
  {
    "objectID": "slides/03-inference-slides.html#lecture-outline",
    "href": "slides/03-inference-slides.html#lecture-outline",
    "title": "Lecture 03: Statistical Inference",
    "section": "📋 Lecture Outline",
    "text": "📋 Lecture Outline\n\nWhy statistics?\nStatistical Inference\nSimple Example\nHypotheses\nTests\nRejecting the null hypothesis\nStudent’s t-test\nANOVA"
  },
  {
    "objectID": "slides/03-inference-slides.html#why-statistics",
    "href": "slides/03-inference-slides.html#why-statistics",
    "title": "Lecture 03: Statistical Inference",
    "section": "Why statistics?",
    "text": "Why statistics?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe want to understand something about a population.\nWe can never observe the entire population, so we draw a sample.\nWe then use a model to describe the sample.\nBy comparing that model to a null model, we can infer something about the population.\n\n\nHere, we’re going to focus on statistical inference."
  },
  {
    "objectID": "slides/03-inference-slides.html#simple-example",
    "href": "slides/03-inference-slides.html#simple-example",
    "title": "Lecture 03: Statistical Inference",
    "section": "Simple Example",
    "text": "Simple Example\n\n\n\n\n\n\n\n\n\nTwo samples of length (mm, n=300).\n\nQuestion: Are these samples of the same type? The same population?"
  },
  {
    "objectID": "slides/03-inference-slides.html#sample-distribution",
    "href": "slides/03-inference-slides.html#sample-distribution",
    "title": "Lecture 03: Statistical Inference",
    "section": "Sample Distribution",
    "text": "Sample Distribution\nDo these really represent different types?\n\n\n\n\n\n\n\n\nTwo models:\n\nsame type (same population)\n\ndifferent types (different populations)\n\nNote that:\n\nthese models are mutually exclusive and\n\nthe second model is more complex."
  },
  {
    "objectID": "slides/03-inference-slides.html#hypotheses",
    "href": "slides/03-inference-slides.html#hypotheses",
    "title": "Lecture 03: Statistical Inference",
    "section": "Hypotheses",
    "text": "Hypotheses\nThe two models represent our hypotheses."
  },
  {
    "objectID": "slides/03-inference-slides.html#testing-method",
    "href": "slides/03-inference-slides.html#testing-method",
    "title": "Lecture 03: Statistical Inference",
    "section": "Testing Method",
    "text": "Testing Method\n\n\n\n\n\n\nProcedure:\n\nTake sample(s).\nCalculate test statistic.\n\nCompare to test probability distribution.\nGet p-value.\nCompare to critical value.\nAccept (or reject) null hypothesis."
  },
  {
    "objectID": "slides/03-inference-slides.html#average-of-differences",
    "href": "slides/03-inference-slides.html#average-of-differences",
    "title": "Lecture 03: Statistical Inference",
    "section": "Average of differences",
    "text": "Average of differences\n\n\n\n\n\n\nSuppose we take two samples from the same population, calculate the difference in their means, and repeat this 1,000 times.\nQuestion: What will the average difference be between the sample means?"
  },
  {
    "objectID": "slides/03-inference-slides.html#probability-of-differences",
    "href": "slides/03-inference-slides.html#probability-of-differences",
    "title": "Lecture 03: Statistical Inference",
    "section": "Probability of differences",
    "text": "Probability of differences\n\n\n\nIf we convert these differences into a probability distribution, we can estimate the probability of any given difference.\nThe p-value represents how likely it is that the difference we see arises by chance."
  },
  {
    "objectID": "slides/03-inference-slides.html#model-of-differences",
    "href": "slides/03-inference-slides.html#model-of-differences",
    "title": "Lecture 03: Statistical Inference",
    "section": "Model of Differences",
    "text": "Model of Differences\n\n\n\n\n\n\nIn classical statistics, we use a model of that distribution to estimate the probability of a given difference. Here we are using \\(N(0,\\) 0.69\\()\\), where the probability of getting a difference \\(\\pm\\) 1 mm (\\(\\pm2s\\)) or greater is 0.05."
  },
  {
    "objectID": "slides/03-inference-slides.html#rejecting-the-null-hypothesis",
    "href": "slides/03-inference-slides.html#rejecting-the-null-hypothesis",
    "title": "Lecture 03: Statistical Inference",
    "section": "Rejecting the Null Hypothesis",
    "text": "Rejecting the Null Hypothesis\n\n\n\nQuestion: How do we decide?\nDefine a critical limit (\\(\\alpha\\))\n\nMust be determined prior to the test!\nIf \\(p < \\alpha\\), reject. ← THIS IS THE RULE!\nGenerally, \\(\\alpha = 0.05\\)"
  },
  {
    "objectID": "slides/03-inference-slides.html#why-the-critical-limit",
    "href": "slides/03-inference-slides.html#why-the-critical-limit",
    "title": "Lecture 03: Statistical Inference",
    "section": "Why the critical limit?",
    "text": "Why the critical limit?\nBecause we might be wrong! But what kind of wrong?\n\n\n\nWith \\(\\alpha=0.05\\), we are saying, “If we run this test 100 times, only 5 of those tests should result in a Type 1 Error.”"
  },
  {
    "objectID": "slides/03-inference-slides.html#students-t-test",
    "href": "slides/03-inference-slides.html#students-t-test",
    "title": "Lecture 03: Statistical Inference",
    "section": "Student’s t-test",
    "text": "Student’s t-test\n\nProblemHypothesesDifferencet-statisticComplexityt-distribution\n\n\n\n\n\n\n\nWe have two samples of projectile points, each consisting of 300 measurements of length (mm).\nQuestion: Are these samples of the same point type? The same population?\n\n\n\n\nThe null hypothesis:\n\\(H_0: \\mu_1 = \\mu_2\\)\nThe alternate hypothesis:\n\\(H_1: \\mu_1 \\neq \\mu_2\\)\nThis is a two-sided t-test as the difference can be positive or negative.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion: Is this difference (-1.82) big enough to reject the null?\n\n\n\n\nA t-statistic standardizes the difference in means using the standard error of the sample mean.\n\\[t = \\frac{\\bar{x}_1 - \\bar{x}_2}{s_{\\bar{x}}}\\]\n\n\n\nFor our samples, \\(t =\\) -3.61. This is a model of our data!\nQuestion: How probable is this estimate?\nWe can answer this by comparing the t-statistic to the t-distribution.\n\n\nBut first, we need to evaluate how complex the t-statistic is. We do this by estimating the degrees of freedom, or the number of values that are free to vary after calculating a statistic. In this case, we have two samples with 300 observations each, hence:\n\ndf = 530\n\nCrucially, this affects the shape of the t-distribution and, thus, determines the location of the critical value we use to evaluate the null hypothesis.\n\n\n\n\n\n\n\n\n\n\n\nSummary:\n\n\\(\\alpha = 0.05\\)\n\n\\(H_{0}: \\mu_1 = \\mu_2\\)\n\n\\(p =\\) 0.0003\n\nTranslation: the null hypothesis is really, really unlikely. So, there must be some difference in the mean!"
  },
  {
    "objectID": "slides/03-inference-slides.html#anova",
    "href": "slides/03-inference-slides.html#anova",
    "title": "Lecture 03: Statistical Inference",
    "section": "ANOVA",
    "text": "ANOVA\n\n\n\n\nProblemHypothesesStrategySum of SquaresF-statisticF-distribution\n\n\n\n\n\n\n\nWe have five samples of points, each consisting of 100 measurements of length (mm).\nQuestion: Are these samples of the same point type? The same population?\nAnalysis of Variance (ANOVA) is like a t-test but for more than two samples.\n\n\n\n\nThe null hypothesis:\n\\(H_0:\\) no difference between groups\nThe alternate hypothesis:\n\\(H_1:\\) at least one group is different\n\n\n\n\nVariance Decomposition. When group membership is known, the contribution of any value \\(x_{ij}\\) to the variance can be split into two parts:\n\\[(x_{ij} - \\bar{x}) = (\\bar{x}_{j} - \\bar{x}) + (x_{ij} - \\bar{x}_{j})\\]\nwhere\n\n\\(i\\) is the \\(i\\)th observation,\n\n\\(j\\) is the \\(j\\)th group,\n\n\\(\\bar{x}\\) is the between-group mean, and\n\n\\(\\bar{x_{j}}\\) is the within-group mean (of group \\(j\\)).\n\n\n\n\n\n\n\n\n\n\n\n\n\nSum and square the differences for all \\(n\\) observation and \\(m\\) groups gives us:\n\\[SS_{T} = SS_{B} + SS_{W}\\]\nwhere\n\n\\(SS_{T}\\): Total Sum of Squares\n\\(SS_{B}\\): Between-group Sum of Squares\n\\(SS_{W}\\): Within-group Sum of Squares\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRatio of variances:\n\\[F = \\frac{\\text{between-group variance}}{\\text{within-group variance}}\\]\nwhere\n\nBetween-group variance = \\(SS_{B}/df_{B}\\) and \\(df_{B}=m-1\\).\nWithin-group variance = \\(SS_{W}/df_{W}\\) and \\(df_{W}=m(n-1)\\).\n\nQuestion: Here, \\(F=\\) 5.63. How probable is this estimate?\nWe can answer this question by comparing the F-statistic to the F-distribution.\n\n\n\n\n\n\n\n\n\n\n\nSummary:\n\n\\(\\alpha = 0.05\\)\n\\(H_{0}:\\) no difference\n\\(p=\\) 0.003\n\nTranslation: the null hypothesis is really, really unlikely, so there must be some difference between groups!"
  },
  {
    "objectID": "slides/04-ols-slides.html#lecture-outline",
    "href": "slides/04-ols-slides.html#lecture-outline",
    "title": "Lecture 04: Ordinary Least Squares",
    "section": "📋 Lecture Outline",
    "text": "📋 Lecture Outline\n\n🧪 A simple experiment\nCompeting models\nModel complexity\nBivariate statistics (covariance and correlation)\nA general formula\nSimple Linear Regression\nOrdinary Least Squares (OLS)\nMultiple Linear Regression\n🚗 Cars Model, again\nRegression assumptions"
  },
  {
    "objectID": "slides/04-ols-slides.html#a-simple-experiment",
    "href": "slides/04-ols-slides.html#a-simple-experiment",
    "title": "Lecture 04: Ordinary Least Squares",
    "section": "🧪 A simple experiment",
    "text": "🧪 A simple experiment\n\n\n\n\n\n\n\n\n We take ten cars, send each down a track, have them brake at the same point, and measure the distance it takes them to stop.\nQuestion: how far do you think it will take the next car to stop?"
  },
  {
    "objectID": "slides/04-ols-slides.html#competing-models",
    "href": "slides/04-ols-slides.html#competing-models",
    "title": "Lecture 04: Ordinary Least Squares",
    "section": "Competing Models",
    "text": "Competing Models\n\n\n\n\n\n\n\nE[Y]: mean distance\n\n\n\n\n\n\nE[Y]: some function of speed"
  },
  {
    "objectID": "slides/04-ols-slides.html#model-complexity",
    "href": "slides/04-ols-slides.html#model-complexity",
    "title": "Lecture 04: Ordinary Least Squares",
    "section": "Model Complexity",
    "text": "Model Complexity\n\n\n\n\n\n\n\n\n\n⚖️ The error is smaller for the more complex model. This is a good thing, but what did it cost us? Need to weigh this against the increased complexity!"
  },
  {
    "objectID": "slides/04-ols-slides.html#bivariate-statistics",
    "href": "slides/04-ols-slides.html#bivariate-statistics",
    "title": "Lecture 04: Ordinary Least Squares",
    "section": "Bivariate statistics",
    "text": "Bivariate statistics\n\n\n\n\nExplore the relationship between two variables:"
  },
  {
    "objectID": "slides/04-ols-slides.html#covariance",
    "href": "slides/04-ols-slides.html#covariance",
    "title": "Lecture 04: Ordinary Least Squares",
    "section": "Covariance",
    "text": "Covariance\n\n\nThe extent to which two variables vary together.\n\\[cov(x, y) = \\frac{1}{n-1} \\sum_{i=1}^{n}  (x_i - \\bar{x})(y_i - \\bar{y})\\]\n\nSign reflects positive or negative trend, but magnitude depends on units (e.g., \\(cm\\) vs \\(km\\)).\n\nVariance is the covariance of a variable with itself."
  },
  {
    "objectID": "slides/04-ols-slides.html#correlation",
    "href": "slides/04-ols-slides.html#correlation",
    "title": "Lecture 04: Ordinary Least Squares",
    "section": "Correlation",
    "text": "Correlation\n\n\nPearson’s Correlation Coefficient\n\\[r = \\frac{cov(x,y)}{s_{x}s_y}\\]\n\nScales covariance (from -1 to 1) using standard deviations, \\(s\\), thus making magnitude independent of units.\nSignificance can be tested by converting to a t-statistic and comparing to a t-distribution with \\(df=n-2\\)."
  },
  {
    "objectID": "slides/04-ols-slides.html#non-linear-correlation",
    "href": "slides/04-ols-slides.html#non-linear-correlation",
    "title": "Lecture 04: Ordinary Least Squares",
    "section": "Non-linear correlation",
    "text": "Non-linear correlation\n\n\nSpearman’s Rank Correlation Coefficient\n\\[\\rho = \\frac{cov\\left(R(x),\\; R(y) \\right)}{s_{R(x)}s_{R(y)}}\\]\n\nPearson’s correlation but with ranks (R).\n\nThis makes it a robust estimate, less sensitive to outliers."
  },
  {
    "objectID": "slides/04-ols-slides.html#correlation-between-categories",
    "href": "slides/04-ols-slides.html#correlation-between-categories",
    "title": "Lecture 04: Ordinary Least Squares",
    "section": "Correlation between categories",
    "text": "Correlation between categories\n\n\nFor counts or frequencies\n\\[\\chi^2 = \\sum \\frac{(O_{ij}-E_{ij})^2}{E_{ij}}\\]\n\nAnalysis of contingency table\n\\(O_{ij}\\) is observed count in row \\(i\\), column \\(j\\)\n\\(E_{ij}\\) is expected count in row \\(i\\), column \\(j\\)\nSignificance can be tested by comparing to a \\(\\chi^2\\)-distribution with \\(df=k-1\\) (\\(k\\) being the number of categories)."
  },
  {
    "objectID": "slides/04-ols-slides.html#correlation-is-not-causation",
    "href": "slides/04-ols-slides.html#correlation-is-not-causation",
    "title": "Lecture 04: Ordinary Least Squares",
    "section": "Correlation is not causation!",
    "text": "Correlation is not causation!\n\n\n\n\n\n\nAdapted from https://www.tylervigen.com/spurious-correlations"
  },
  {
    "objectID": "slides/04-ols-slides.html#a-general-formula",
    "href": "slides/04-ols-slides.html#a-general-formula",
    "title": "Lecture 04: Ordinary Least Squares",
    "section": "A general formula",
    "text": "A general formula\n\n\n\n\n\n\\[\nY = E[Y] + \\epsilon \\\\[6pt]\nE[Y] = \\beta X\n\\]\n\n\n\n\\[y_i = \\hat\\beta X + \\epsilon_i\\]\n\n\n\n\\[y_i = \\hat\\beta_0 + \\hat\\beta_1 x_i + \\ldots +  \\hat\\beta_n x_i + \\epsilon_i\\]\n\n\n\n\n\\(y_i\\) is the dependent variable, a sample from the population \\(Y\\)\n\n\\(X\\) is a set of independent variables \\(x_i, \\ldots, x_n\\), sometimes called the design matrix\n\\(\\hat\\beta\\) is a set of coefficients of relationship that estimate the true relationship \\(\\beta\\)\n\\(\\epsilon_i\\) is the residuals or errors"
  },
  {
    "objectID": "slides/04-ols-slides.html#simple-linear-regression",
    "href": "slides/04-ols-slides.html#simple-linear-regression",
    "title": "Lecture 04: Ordinary Least Squares",
    "section": "Simple Linear Regression",
    "text": "Simple Linear Regression\n\n\n\n\n\n\n\n\n\n‘Simple’ means one explanatory variable (speed)\n\\[y_i = \\hat\\beta_0 + \\hat\\beta_1 speed_i + \\epsilon_i\\]\n\n\\(\\hat\\beta_0\\) = -2.0107\n\\(\\hat\\beta_1\\) = 1.9362\n\nQuestion: How did we get these values?"
  },
  {
    "objectID": "slides/04-ols-slides.html#ordinary-least-squares",
    "href": "slides/04-ols-slides.html#ordinary-least-squares",
    "title": "Lecture 04: Ordinary Least Squares",
    "section": "Ordinary Least Squares",
    "text": "Ordinary Least Squares\n\n\n\n\nA method for estimating the coefficients, \\(\\beta\\), in a linear regression model by minimizing the Residual Sum of Squares, \\(SS_{R}\\).\n\\[SS_{R} = \\sum_{i=1}^{n} (y_{i}-\\hat{y}_i)^2\\]\nwhere \\(\\hat{y} \\approx E[Y]\\). To minimize this, we take its derivative with respect to \\(\\beta\\) and set it equal to zero.\n\\[\\frac{d\\, SS_{R}}{d\\, \\beta} = 0\\]\n\n\nSimple estimators for coefficients:\n\nSlope Ratio of covariance to variance \\[\\beta_{1} = \\frac{cov(x, y)}{var(x)}\\]\n\nIntercept Conditional difference in means \\[\\beta_{0} = \\bar{y} - \\beta_1 \\bar{x}\\]\nIf \\(\\beta_{1} = 0\\), then \\(\\beta_{0} = \\bar{y}\\)."
  },
  {
    "objectID": "slides/04-ols-slides.html#cars-model-again",
    "href": "slides/04-ols-slides.html#cars-model-again",
    "title": "Lecture 04: Ordinary Least Squares",
    "section": "🚗 Cars Model, again",
    "text": "🚗 Cars Model, again\n\n\n\n\n\n\n\n\n\n\n\n\nSlope\n\\[\\hat\\beta_{1} = \\frac{cov(x,y)}{var(x)} = \\frac{10.426}{5.3847} = 1.9362\\]\n\nIntercept\n\\[\n\\begin{align}\n\\hat\\beta_{0} &= \\bar{y} - \\hat\\beta_{1}\\bar{x} \\\\[6pt]\n              &= 10.54 - 1.9362 * 6.4821 \\\\[6pt]\n              &= -2.0107\n\\end{align}\n\\]"
  },
  {
    "objectID": "slides/04-ols-slides.html#multiple-linear-regression",
    "href": "slides/04-ols-slides.html#multiple-linear-regression",
    "title": "Lecture 04: Ordinary Least Squares",
    "section": "Multiple Linear Regression",
    "text": "Multiple Linear Regression\nOLS can be extended to models containing multiple explanatory variables using matrix algebra. Then the coefficient estimator is:\n\\[\\hat\\beta = (X^{T}X)^{-1}X^{T}y\\]\n\nWhere, if you squint a little\n\n\\(X^{T}X\\) ⇨ variance.\n\\(X^{T}y\\) ⇨ covariance."
  },
  {
    "objectID": "slides/04-ols-slides.html#regression-assumptions",
    "href": "slides/04-ols-slides.html#regression-assumptions",
    "title": "Lecture 04: Ordinary Least Squares",
    "section": "Regression assumptions",
    "text": "Regression assumptions\n\nWeak Exogeneity: the predictor variables have fixed values and are known.\n\nLinearity: the relationship between the predictor variables and the response variable is linear.\n\nConstant Variance: the variance of the errors does not depend on the values of the predictor variables. Also known as homoskedasticity.\n\nIndependence of errors: the errors are uncorrelated with each other.\n\nNo perfect collinearity: the predictors are not linearly correlated with each other."
  },
  {
    "objectID": "slides/05-distributions-slides.html#lecture-outline",
    "href": "slides/05-distributions-slides.html#lecture-outline",
    "title": "Lecture 05: Visualizing Distributions",
    "section": "📋 Lecture Outline",
    "text": "📋 Lecture Outline\n\nBar Chart\nHistogram\nProbability Density\nCumulative Density\nBoxplot"
  },
  {
    "objectID": "slides/05-distributions-slides.html#whats-it-for",
    "href": "slides/05-distributions-slides.html#whats-it-for",
    "title": "Lecture 05: Visualizing Distributions",
    "section": "What’s it for?",
    "text": "What’s it for?\nVisualize the amount of some variable across categories, represented using length or height of bars."
  },
  {
    "objectID": "slides/05-distributions-slides.html#a-rule-of-thumb",
    "href": "slides/05-distributions-slides.html#a-rule-of-thumb",
    "title": "Lecture 05: Visualizing Distributions",
    "section": "A rule of thumb 👍",
    "text": "A rule of thumb 👍\nOften easier to read when oriented horizontally."
  },
  {
    "objectID": "slides/05-distributions-slides.html#grouped-data",
    "href": "slides/05-distributions-slides.html#grouped-data",
    "title": "Lecture 05: Visualizing Distributions",
    "section": "Grouped data",
    "text": "Grouped data\nGrouped bar chart can represent higher dimensional data.\n\nAlthough this graph is not terribly informative…"
  },
  {
    "objectID": "slides/05-distributions-slides.html#whats-it-for-1",
    "href": "slides/05-distributions-slides.html#whats-it-for-1",
    "title": "Lecture 05: Visualizing Distributions",
    "section": "What’s it for?",
    "text": "What’s it for?\nVisualize the approximate distribution of a continuous random variable."
  },
  {
    "objectID": "slides/05-distributions-slides.html#bin-width",
    "href": "slides/05-distributions-slides.html#bin-width",
    "title": "Lecture 05: Visualizing Distributions",
    "section": "Bin Width",
    "text": "Bin Width\nObtained by counting the number of observations that fall into each interval or “bin.”"
  },
  {
    "objectID": "slides/05-distributions-slides.html#lookout",
    "href": "slides/05-distributions-slides.html#lookout",
    "title": "Lecture 05: Visualizing Distributions",
    "section": "⚠️ Lookout!",
    "text": "⚠️ Lookout!\nThe shape of the distribution depends on the bin width."
  },
  {
    "objectID": "slides/05-distributions-slides.html#a-rule-of-thumb-1",
    "href": "slides/05-distributions-slides.html#a-rule-of-thumb-1",
    "title": "Lecture 05: Visualizing Distributions",
    "section": "A rule of thumb 👍",
    "text": "A rule of thumb 👍\n\n\nGenerally, a bad idea to use stacked or dodged groupings in a single histogram.\n\n\n\n\n\n\n\nBetter to use facets."
  },
  {
    "objectID": "slides/05-distributions-slides.html#whats-it-for-2",
    "href": "slides/05-distributions-slides.html#whats-it-for-2",
    "title": "Lecture 05: Visualizing Distributions",
    "section": "What’s it for?",
    "text": "What’s it for?\nVisualize the approximate distribution of a continuous random variable."
  },
  {
    "objectID": "slides/05-distributions-slides.html#kernel-density-estimation-kde",
    "href": "slides/05-distributions-slides.html#kernel-density-estimation-kde",
    "title": "Lecture 05: Visualizing Distributions",
    "section": "Kernel Density Estimation (KDE)",
    "text": "Kernel Density Estimation (KDE)\n\n\nProcedure:\n\nDefine a kernel, often a normal distribution with mean equal to the observation.\nDefine bandwidth for scaling the kernel.\nSum the kernels.\n\n\n\n\n\n\n\n\nThe kernels in this figure are not to scale."
  },
  {
    "objectID": "slides/05-distributions-slides.html#grouped-data-1",
    "href": "slides/05-distributions-slides.html#grouped-data-1",
    "title": "Lecture 05: Visualizing Distributions",
    "section": "Grouped data",
    "text": "Grouped data\nThere’s not a simple answer for how to plot multiple KDE’s, but facets are your friend."
  },
  {
    "objectID": "slides/05-distributions-slides.html#whats-it-for-3",
    "href": "slides/05-distributions-slides.html#whats-it-for-3",
    "title": "Lecture 05: Visualizing Distributions",
    "section": "What’s it for?",
    "text": "What’s it for?\nVisualize the approximate distribution of a continuous random variable without having to specify a bandwidth."
  },
  {
    "objectID": "slides/05-distributions-slides.html#procedure",
    "href": "slides/05-distributions-slides.html#procedure",
    "title": "Lecture 05: Visualizing Distributions",
    "section": "Procedure",
    "text": "Procedure\n\n\nConsider this sample:\n(0.3, 2.0, 3.4, 1.2, 2.2, 1.9).\nTo calculate its eCDF, we divide the number of observations that are less than or equal to each unique value by the total sample size.\n0.0 -> 0/6 = 0.00\n0.3 -> 1/6 = 0.17\n1.2 -> 2/6 = 0.33 \n1.9 -> 3/6 = 0.50 \n2.0 -> 4/6 = 0.67 \n2.2 -> 5/6 = 0.83\n3.4 -> 6/6 = 1.00"
  },
  {
    "objectID": "slides/05-distributions-slides.html#lookout-1",
    "href": "slides/05-distributions-slides.html#lookout-1",
    "title": "Lecture 05: Visualizing Distributions",
    "section": "⚠️ Lookout!",
    "text": "⚠️ Lookout!\nThese are a little bit harder to interpret. Gives the probability of being less than or equal to x. E.g., the probability of being 28 years old or younger is 0.5."
  },
  {
    "objectID": "slides/05-distributions-slides.html#whats-it-for-4",
    "href": "slides/05-distributions-slides.html#whats-it-for-4",
    "title": "Lecture 05: Visualizing Distributions",
    "section": "What’s it for?",
    "text": "What’s it for?\nVisualize the approximate distribution of a continuous random variable using its quartiles.\n\nUseful for plotting distributions across multiple groups."
  },
  {
    "objectID": "slides/05-distributions-slides.html#quartiles",
    "href": "slides/05-distributions-slides.html#quartiles",
    "title": "Lecture 05: Visualizing Distributions",
    "section": "Quartiles",
    "text": "Quartiles\nWhen the data are ordered from smallest to largest, the quartiles divide them into four sets of more-or-less equal size. The second quartile is the median!"
  },
  {
    "objectID": "slides/05-distributions-slides.html#a-rule-of-thumb-2",
    "href": "slides/05-distributions-slides.html#a-rule-of-thumb-2",
    "title": "Lecture 05: Visualizing Distributions",
    "section": "A rule of thumb 👍",
    "text": "A rule of thumb 👍\nSometimes easier to read when oriented horizontally."
  }
]