[
  {
    "objectID": "classes.html",
    "href": "classes.html",
    "title": "Classes",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nDate\n\n\nTitle\n\n\nDescription\n\n\n\n\n\n\nJan 10, 2023\n\n\nWeek 01: Introduction\n\n\nWhy statistics? Why R? Why Quarto? Making a webpage and a plot in just seconds!\n\n\n\n\nJan 17, 2023\n\n\nWeek 02: Probability as a Model\n\n\nA simple experiment. Random variables. Probability distributions. Probability as a model.\n\n\n\n\nJan 24, 2023\n\n\nWeek 03: Statistical Inference\n\n\nTesting hypotheses, visualizing distributions, reading and writing tabular data, and learning how to work with it in R.\n\n\n\n\nJan 31, 2023\n\n\nWeek 04: Ordinary Least Squares\n\n\nCalculating and testing bivariate statistics, including correlation and covariance. Visualizing probability densities. Fitting linear models using ordinary least squares. And some simple table indexing in R.\n\n\n\n\nFeb 7, 2023\n\n\nWeek 05: Visualizing Distributions\n\n\nA review and a deep dive into visualizing distributions in R, including bar chart, histogram, kernel density, cumulative density, and boxplot. Examples are given using base R {graphics} and {ggplot2}.\n\n\n\n\n\nFeb 14, 2023\n\n\nWeek 06: Working with Tables\n\n\nA review and a deep dive into working with rectangular data in R.\n\n\n\n\nFeb 21, 2023\n\n\nWeek 07: Evaluating Linear Models\n\n\n(Stats) Learn how to interpret linear models, make predictions, and use standard tests and diagnostics for evaluation, including making diagnostic plots. (R) Model summaries. Diagnostic plots. Prediction and plotting.\n\n\n\n\nFeb 28, 2023\n\n\nWeek 08: Multiple Linear Models\n\n\n(Stats) Learn how to interpret multiple linear models, make predictions, and use standard tests and diagnostics for evaluation, including making diagnostic plots. (R) Model summaries. Diagnostic plots. Prediction and plotting.\n\n\n\n\nMar 14, 2023\n\n\nWeek 10: Transforming Variables\n\n\n(Stats) Learn how to add qualitative variables and transformed variables to linear models and interpret the results. (R) Scaling and centering variables, specifying model formulas to include intercept offsets, interactions, log transforms, and polynomial terms.\n\n\n\n\nMar 21, 2023\n\n\nWeek 11: Maximum Likelihood\n\n\n(Stats) Making sense of maximum likelihood. (R) Learn how to save figures. Learn how to generate robust relative paths in project folders with here().\n\n\n\n\nMar 28, 2023\n\n\nWeek 12: Logistic Regression\n\n\n(Stats) Modeling binary outcomes and proportions with logistic regression. Evaluating GLMs with Deviance, Information Criteria, and the Likelihood Ratio Test. (R) Fitting logistic models and evaluating them with LRT.\n\n\n\n\nApr 4, 2023\n\n\nWeek 13: Poisson Regression\n\n\n(Stats) Modeling count data with Poisson regression. Testing for dispersion and using a negative binomial to account for it. Log offsets. (R) Fitting Poisson and negative binomials models, testing for dispersion, and evaluating models with LRT.\n\n\n\n\nApr 11, 2023\n\n\nWeek 14: Partial Dependence\n\n\n(Stats) Making sense of covariance in context. (R) Estimating and visualizing partial dependence.\n\n\n\n\n\nApr 18, 2023\n\n\nWeek 15: Regression Tables\n\n\n(Stats) Nothing new. (R) How to report results of regression in a table with R.\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "classes/01-intro.html#lab-exercises",
    "href": "classes/01-intro.html#lab-exercises",
    "title": "Week 01: Introduction",
    "section": "🔬 Lab Exercises",
    "text": "🔬 Lab Exercises\nWeek 01 Lab"
  },
  {
    "objectID": "classes/02-probability.html#lab-exercises",
    "href": "classes/02-probability.html#lab-exercises",
    "title": "Week 02: Probability as a Model",
    "section": "🔬 Lab Exercises",
    "text": "🔬 Lab Exercises\nWeek 02 Lab"
  },
  {
    "objectID": "classes/03-inference.html#lab-exercises",
    "href": "classes/03-inference.html#lab-exercises",
    "title": "Week 03: Statistical Inference",
    "section": "🔬 Lab Exercises",
    "text": "🔬 Lab Exercises\nWeek 03 Lab"
  },
  {
    "objectID": "classes/04-ols.html#lab-exercises",
    "href": "classes/04-ols.html#lab-exercises",
    "title": "Week 04: Ordinary Least Squares",
    "section": "🔬 Lab Exercises",
    "text": "🔬 Lab Exercises\nWeek 04 Lab"
  },
  {
    "objectID": "classes/05-distributions.html#lab-exercises",
    "href": "classes/05-distributions.html#lab-exercises",
    "title": "Week 05: Visualizing Distributions",
    "section": "🔬 Lab Exercises",
    "text": "🔬 Lab Exercises\nWeek 05 Lab"
  },
  {
    "objectID": "classes/06-dataframes.html",
    "href": "classes/06-dataframes.html",
    "title": "Week 06: Working with Tables",
    "section": "",
    "text": "No lecture this week."
  },
  {
    "objectID": "classes/06-dataframes.html#lab-exercises",
    "href": "classes/06-dataframes.html#lab-exercises",
    "title": "Week 06: Working with Tables",
    "section": "🔬 Lab Exercises",
    "text": "🔬 Lab Exercises\nWeek 06 Lab"
  },
  {
    "objectID": "classes/07-evaluation.html#lab-exercises",
    "href": "classes/07-evaluation.html#lab-exercises",
    "title": "Week 07: Evaluating Linear Models",
    "section": "🔬 Lab Exercises",
    "text": "🔬 Lab Exercises\nWeek 07 Lab"
  },
  {
    "objectID": "classes/08-multiple-lm.html#lab-exercises",
    "href": "classes/08-multiple-lm.html#lab-exercises",
    "title": "Week 08: Multiple Linear Models",
    "section": "🔬 Lab Exercises",
    "text": "🔬 Lab Exercises\nWeek 08 Lab"
  },
  {
    "objectID": "classes/10-transforms.html#lab-exercises",
    "href": "classes/10-transforms.html#lab-exercises",
    "title": "Week 10: Transforming Variables",
    "section": "🔬 Lab Exercises",
    "text": "🔬 Lab Exercises\nWeek 10 Lab"
  },
  {
    "objectID": "classes/11-maximum-likelihood.html#lab-exercises",
    "href": "classes/11-maximum-likelihood.html#lab-exercises",
    "title": "Week 11: Maximum Likelihood",
    "section": "🔬 Lab Exercises",
    "text": "🔬 Lab Exercises\nWeek 11 Lab"
  },
  {
    "objectID": "classes/12-logistic-regression.html#lab-exercises",
    "href": "classes/12-logistic-regression.html#lab-exercises",
    "title": "Week 12: Logistic Regression",
    "section": "🔬 Lab Exercises",
    "text": "🔬 Lab Exercises\nWeek 12 Lab"
  },
  {
    "objectID": "classes/13-poisson-regression.html#lab-exercises",
    "href": "classes/13-poisson-regression.html#lab-exercises",
    "title": "Week 13: Poisson Regression",
    "section": "🔬 Lab Exercises",
    "text": "🔬 Lab Exercises\nWeek 13 Lab"
  },
  {
    "objectID": "classes/14-partial-dependence.html",
    "href": "classes/14-partial-dependence.html",
    "title": "Week 14: Partial Dependence",
    "section": "",
    "text": "No lecture this week."
  },
  {
    "objectID": "classes/14-partial-dependence.html#lab-exercises",
    "href": "classes/14-partial-dependence.html#lab-exercises",
    "title": "Week 14: Partial Dependence",
    "section": "🔬 Lab Exercises",
    "text": "🔬 Lab Exercises\nWeek 14 Lab"
  },
  {
    "objectID": "classes/15-regression-tables.html",
    "href": "classes/15-regression-tables.html",
    "title": "Week 15: Regression Tables",
    "section": "",
    "text": "No lecture this week."
  },
  {
    "objectID": "classes/15-regression-tables.html#lab-exercises",
    "href": "classes/15-regression-tables.html#lab-exercises",
    "title": "Week 15: Regression Tables",
    "section": "🔬 Lab Exercises",
    "text": "🔬 Lab Exercises\nWeek 15 Lab"
  },
  {
    "objectID": "help.html",
    "href": "help.html",
    "title": "Looking for help",
    "section": "",
    "text": "Wisdom of the Ancients (xkcd 979)."
  },
  {
    "objectID": "help.html#the-google-search-paradox",
    "href": "help.html#the-google-search-paradox",
    "title": "Looking for help",
    "section": "The Google Search Paradox",
    "text": "The Google Search Paradox\nAs you make your first tentative forays into the R programming environment, you will on occasion experience the jarring dislocation of an R error, a typically bright red eruption of your R console, perhaps symbolic of your code exploding before your eyes. Here is one of the more infamous errors you are likely to encounter:\n\nobject of type 'closure' is not subsettable\nNever mind what this particular error means.1 The point is that it can be terribly frustrating when you encounter it or one of its kin.1 Though check out Jenny Bryan’s talk at the 2020 RStudio Conference: https://rstudio.com/resources/rstudioconf-2020/object-of-type-closure-is-not-subsettable/\nTroubleshooting these errors can often be an additional source of anxiety and frustration, especially early on, as you simply lack the words required to describe your problem accurately and, thus, to formulate the question whose answer you so desperately need. I like to refer to this unhappy circumstance as the Google Search Paradox because you will inevitably find yourself staring at an empty search bar, wondering what words to give to Google. It’s also a bit like Meno’s Paradox, or the Paradox of Inquiry. For if you could properly state your question, it’s probably the case that you already know the answer. So, you either know and thus don’t need to ask, or need to ask, but don’t know how.\nOf course, the situation is not nearly so dire as this. In truth, you always know at least a little about your problem - you do have the error itself after all! - and can thus Google your way through to an answer - eventually, anyway. But life is fleeting, as they say, and time is short, so you should probably avoid the brute force approach, relying instead on searching efficiently. To help you with that (and to help you get better with that), this page provides a brief annotated list of where to look for answers, starting from within R itself!"
  },
  {
    "objectID": "help.html#r-helpers",
    "href": "help.html#r-helpers",
    "title": "Looking for help",
    "section": "R helpers",
    "text": "R helpers\nTypically, though not always, R code will have lots of supporting documentation. These come in two varieties: function help pages and vignettes. If you are having trouble a single function to work properly, you may find its help page more useful. If you are having trouble getting through some analysis and you cannot pinpoint the exact reason for your trouble, the vignettes are probably where you should look. There are a couple of ways to access this documentation.\n\nFrom within R, you can use the help() and ?... functions to access short-form documentation. Examples include help(\"plot\") and ?plot. This will call up the function’s documentation page and display it in your computer’s graphical device.\nhelp.search(...) and ??... both provide means of searching through help pages to find multiple functions with the same name (and potentially the same or similar uses). Simply replace the ellipses (three dots) with a character string and these functions will return all help pages with that string. So if you want to carry out a cluster analysis, typing ??cluster will search for any functions in your installed packages that use the word cluster.\nThe rdrr.io website provides access to all function help pages online. If you Google an R function, a link to its documentation on this website is typically the first that you will see. For the best search results, I recommend Googling “R <package name> <function name>.”\nFrom within R, you can also access the vignettes using some combination of vignette(), browseVignettes(), and RShowDoc().\n\nThe function vignette() with no argument specified will bring up a list of all available vignettes, organized by package. If you want the vignettes for a particular R package, you can also type vignette(package = ...), for example, vignette(package = \"grid\") will bring up the vignettes for the grid package.\n\nbrowseVignettes() will open a locally hosted HTML page in your browser with links to all available R vignettes. This is actually quite helpful, and you should give it a try when you get a chance. Just browsing through these vignettes will give you a great feel for all that you can do in R.\n\nRShowDoc() is mostly for opening a single vignette. This is usefully paired with vignette(), which will give you the name of the vignette and package, so that you can, for example, call RShowDoc(what = \"plotExample\", package = \"grid\"). This will bring up the “plotExample” vignette from the grid package.\n\nPackage authors have lots of resources for sharing their documentation now, including websites designed specifically to present both function help pages and vignettes. Here is an example of the website for the colorspace package.\nFinally, you can access all available documentation for official R packages by navigating the Comprehensive R Archive Network (CRAN) website, here: https://cran.r-project.org/."
  },
  {
    "objectID": "help.html#rstudio-helpers",
    "href": "help.html#rstudio-helpers",
    "title": "Looking for help",
    "section": "RStudio helpers",
    "text": "RStudio helpers\nWhile RStudio provides loads of support to R users, here we mention some of the more important ones.\n\nRStudio How To Articles provide loads of how-to guides for working with R and RStudio. This is a very comprehensive suite of useful documentation.\nRStudio Cheatsheets strive to communicate package information in a single, concise poster format with lots of visual queues and simple definitions. These can be really helpful when you need a quick refresher on the use of some bit of code.\nRStudio Community is an online forum where individuals ask and answer questions about R and RStudio. They have a very strict code of conduct for their members that emphasizes mutual respect and inclusivity, so you will generally find the discussions here much more friendly and supportive. Use of this forum is highly recommended.\nRStudio Education is a very, very recent development by RStudio (it came online in 2020), and it is simply amazing as a resource for not only learning R itself, but also learning how to teach R. Please note that, with the exception of number 4, these RStudio help tools can be accessed within the RStudio IDE under the Help tab."
  },
  {
    "objectID": "help.html#r-community-helpers",
    "href": "help.html#r-community-helpers",
    "title": "Looking for help",
    "section": "R Community helpers",
    "text": "R Community helpers\nThe R community refers to R users who are actively communicating with and supporting other R users. As there are lots and lots of engaged R users these days, and more and more every day, the community is definitely thriving. There is also an expanding ethos within this community driven largely by RStudio and its code of conduct, so you will generally find R users to be a friendly bunch (if a little hoity-toity). So, let’s talk about where you can engage with this community. We have already mentioned one, RStudio Community, but here we will list some more.\n\nStack Overflow is a forum for programmers in all programming languages to ask and answer questions, much like RStudio Community. It’s just been around longer (2008 to be exact), which means its code of conduct has evolved over time to address a number of unanticipated issues. The consequence is that answers to questions will run the gamut from being respectful and clear to downright insulting. Still, it is a rich resource for addressing your R coding issues. And it has gotten a lot better.\nROpenSci is an R programming community focused on promoting open and reproducible research in science. They have a forum much like RStudio Community, a blog with helpful news and overviews of the packages in their ecosystem, and a rich suite of webpages for their supported R packages, which you can explore here.\nR-bloggers is a clearinghouse for R related content, basically an aggregator of content from individual blogs. It is worth perusing every now and then to pick up the occasional gem of R understanding.\nThe #rstats Twitter community is something. Use this if you use Twitter, I guess…\nThe rstats subreddit is a helpful community of Redditors that are pretty good about answering questions you might have."
  },
  {
    "objectID": "help.html#other-resources",
    "href": "help.html#other-resources",
    "title": "Looking for help",
    "section": "Other resources",
    "text": "Other resources\n\nThe UCLA Institute for Digital Research & Education offers Statistical Consulting geared toward R. This is a tremendous resource for both R and statistics and is highly recommended."
  },
  {
    "objectID": "help.html#reproducible-examples",
    "href": "help.html#reproducible-examples",
    "title": "Looking for help",
    "section": "Reproducible examples",
    "text": "Reproducible examples\nOthers have likely asked the same question you want to ask, so you will not always need to make a post yourself. But, in the off chance that you do find yourself confronted with a question never asked before, you need to make sure you provide R users with all the information and resources they need to help troubleshoot your code and to do so with the least effort possible. This involves providing a “reproducible example” or reprex. There are two essential ingredients to a reprex:\n\nIt needs to be reproducible, obviously. That means you need to make sure you provide everything needed to reproduce your error as is, for instance, all library() calls in your code.\nIt needs to be minimal. In other words, do not include anything extraneous or burdensome, like a 400 MB data object. A much smaller R object should suffice.\n\nA lot has been written about how to put together a reprex, so rather than belabor the point here, it is perhaps best to direct you to Jenny Bryan’s reprex package, which will walk you through the process of submitting a help request on the various forums mentioned above."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Quantitative Analysis of Archaeological Data",
    "section": "",
    "text": "This is a Github page setup to host lectures and other content for the University of Utah course ANTH 5850: Quantitative Analysis of Archaeological Data (affectionately referred to as “quad”). As its name suggests, this class offers students quantitative tools and techniques for working with archaeological data. Those tools include, first and foremost, the language of statistics, but also importantly the statistical programming language R, and finally the mark-up language Markdown (via Quarto), which aids in literate programming (think science communication). Obviously, no one can become fluent in a language - much less three languages! - with just four months of exposure. For that, there is no substitute for immersion, for living and working with these languages and the people who speak them, meaning scientists. This course is merely designed to get you started on that process and to hopefully make it smoother for you as you go. I think the word for it is a “survey” course.\nOn this website, you’ll find course lecture slides and labs. These are organized by class meetings, which you can find a link to in the navbar. The site was built using the open-source scientific and technical publishing system, Quarto, which you’ll also learn about in this course! The source code for the website, along with the lecture slides and lab exercises, can be found at the associated Github repository."
  },
  {
    "objectID": "index.html#inspiration",
    "href": "index.html#inspiration",
    "title": "Quantitative Analysis of Archaeological Data",
    "section": "Inspiration?",
    "text": "Inspiration?\nI can’t take credit for all of the content in this course. The lecture slides, in particular, are adapted from the lectures of Dr. Simon Brewer in the Department of Geography at the University of Utah. The R labs, at least the parts of them concerned with data science rather than statistics, draw heavily on the very popular book R for Data Science (2e) by Hadley Wickham and Garrett Grolemund.\nIt probably goes without saying, of course, but those folks are way smarter than I could ever hope to be, so any errors or confusions that occur here are definitely, one-hundred percent, without a doubt my own."
  },
  {
    "objectID": "index.html#reuse",
    "href": "index.html#reuse",
    "title": "Quantitative Analysis of Archaeological Data",
    "section": "Reuse",
    "text": "Reuse\n\nText and figures are licensed under Creative Commons Attribution CC BY 4.0. Any computer code (R, HTML, CSS, etc.) in slides and worksheets, including in slide and worksheet sources, is also licensed under MIT. Note that figures in slides may be pulled in from external sources and may be licensed under different terms. For such images, image credits are available in the slide notes, accessible via pressing the letter ‘p’."
  },
  {
    "objectID": "labs/01-intro-lab.html",
    "href": "labs/01-intro-lab.html",
    "title": "Lab 01: Introduction",
    "section": "",
    "text": "In this lab, you will learn\n\nhow to use RStudio\nhow to make a plot with R\nhow to do math in R, create objects, use functions, etc.,\nhow to create an R Project folder\nhow to make a website (what?!) with Quarto\nand, you’ll also learn the ends and outs of a typical workflow in R\n\nNo additional packages required this week.\n\n\n\ncars\n\nIncludes measurements of car speed and stopping distance.\npackage: datasets\n\nreference: https://stat.ethz.ch/R-manual/R-devel/library/datasets/html/cars.html"
  },
  {
    "objectID": "labs/01-intro-lab.html#working-in-rstudio",
    "href": "labs/01-intro-lab.html#working-in-rstudio",
    "title": "Lab 01: Introduction",
    "section": "Working in RStudio",
    "text": "Working in RStudio\n\n\n\n\nIf you are going to do anything with R, RStudio is hands-down the best place to do it. RStudio is an open-source integrated development environment (or IDE) that makes programming in R simpler, more efficient, and most importantly, more reproducible. Some of its more user-friendly features are syntax highlighting (it displays code in different colors depending on what it is or does, which makes it easier for you to navigate the code that you’ve written), code completion (it will try to guess what code you are attempting to write and write it for you), and keyboard shortcuts for the more repetitive tasks.\nPane layout\nWhen you first open RStudio, you should see three window panes: the Console, the Environment, and the Viewer. If you open an R script, a fourth Source pane will also open. The default layout of these panes is shown in the figure above.\n\n\nSource. The Source pane provides basic text editing functionality, allowing you to create and edit R scripts. Importantly, you cannot execute the code in these scripts directly, but you can save the scripts that you write as simple text files. A dead give away that you have an R script living on your computer is the .R extension, for example, my_script.R.\n\n\nConsole. The Console pane, as its name suggests, provides an interface to the R console, which is where your code actually gets run. While you can type R code directly into the console, you can’t save the R code you write there into an R script like you can with the Source editor. That means you should reserve the console for non-essential tasks, meaning tasks that are not required to replicate your results.\n\nEnvironment. The Environment pane is sort of like a census of your digital zoo, providing a list of its denizens, i.e., the objects that you have created during your session. This pane also has the History tab, which shows the R code you have sent to the console in the order that you sent it.\n\n\nViewer. The Viewer pane is a bit of a catch-all, including a Files tab, a Plots tab, a Help tab, and a Viewer tab.\n\nThe Files tab works like a file explorer. You can use it to navigate through folders and directories. By default, it is set to your working directory.\nThe Plots tab displays any figures you make with R.\nThe Help tab is where you can go to find helpful R documentation, including function pages and vignettes.\nThe actual Viewer tab provides a window to visualize R Markdown.\n\n\n\nLet’s try out a few bits of code just to give you a sense of the difference between Source and Console.\n\nAs you work through this lab, you can practice running code in the Console, but make sure to do the actual exercises in an R script.\n\nExercises\n\nFirst, let’s open a new R script. To open an R script in RStudio, just click File > New File > R Script (or hit Ctrl + Shift + N, Cmd + Shift + N on Mac OS).\nCopy this code into the console and hit Enter.\n\n\nrep(\"Boba Fett\", 5)\n\n\nNow, copy that code into the R script you just opened and hit Enter again. As you see, the code does not run. Instead, the cursor moves down to the next line. To actually run the code, put the cursor back on the line with the code, and hit Ctrl + Enter (CMD + Enter on Mac OS)."
  },
  {
    "objectID": "labs/01-intro-lab.html#make-your-first-plot",
    "href": "labs/01-intro-lab.html#make-your-first-plot",
    "title": "Lab 01: Introduction",
    "section": "Make Your First Plot!",
    "text": "Make Your First Plot!\nTo ease you into working with R, let’s visualize some data to answer a simple question: Do fast moving objects take longer to slow down than slow moving objects? Don’t worry about understanding all of this! It’s just to give you a feel for the sort of graphics you can make with R. We’ll actually spend all of the next lab learning how to make even better graphics.\nThe data\nTo answer that question, we’ll use the cars data.frame that comes pre-loaded with R. A data.frame is simply an R object that stores tabular data, with rows for each observation and columns for each variable. Let’s have a look at the first n rows of this table, specifically the first 5 rows. We can do this using the function head().\n\nhead(cars, n = 5)\n\n  speed dist\n1     4    2\n2     4   10\n3     7    4\n4     7   22\n5     8   16\n\n\nNote that, in this case, each row represents a car. The first column, or variable, records the speed (in miles per hour) each car was traveling when it applied its brakes, and the second column provides measures of the distances (in feet) that each took to stop.\nThe plot() function\nThe base R graphics package provides a generic function for plotting, which - as you might have guessed - is called plot(). To see how it works, try running this code:\n\nplot(cars)\n\n\n\n\nCustomizing your plot\nWith the plot() function, you can do a lot of customization to the resulting graphic. For instance, you can modify all of the following:\n\n\npch will change the point type,\n\nmain will change the main plot title,\n\nxlab and ylab will change the x and y axis labels,\n\ncex will change the size of shapes within the plot region,\n\npch will change the type of point used (you can use triangles, squares, or diamonds, among others),\n\ncol changes the color of the point (or its border), and\n\nbg changes the color of the point fill (depending on the type of point it is)\n\nFor instance, try running this code:\n\nplot(\n  cars,\n  pch = 21,\n  bg = \"darkorange\",\n  col = \"darkred\",\n  cex = 2\n)\n\n\n\n\nExercises\n\nComplete the following line of code to preview only the first three rows of the cars table.\n\n\nhead(cars, n = )\n\n\nModify the code below to change the size (cex) of the points from 2 to 1.5.\n\n\nplot(\n  cars,\n  pch = 21,\n  bg = \"darkorange\",\n  col = \"darkred\",\n  cex = 2\n)\n\n\nWhat does this plot tell us about the relationship between car speed and stopping distance? Is it positive or negative? Or is there no relationship at all? If there is a relationship, what might explain it?\nComplete the code below to add “Stopping distance for cars” as the main title.\n\n\nplot(\n  cars,\n  pch = 21,\n  bg = \"darkorange\",\n  col = \"darkred\",\n  cex = 1,\n  main = \n)\n\n\nComplete the code below to add “Speed (mph)” as the x-axis label and “Distance (ft)” as the y-axis label.\n\n\nplot(\n  cars,\n  pch = 21,\n  bg = \"darkorange\",\n  col = \"darkred\",\n  cex = 2,\n  main = \"Stopping distance for cars\",\n  xlab = ,\n  ylab = \n)"
  },
  {
    "objectID": "labs/01-intro-lab.html#r-basics",
    "href": "labs/01-intro-lab.html#r-basics",
    "title": "Lab 01: Introduction",
    "section": "R Basics",
    "text": "R Basics\n\n\n\n\nR is a calculator\nYou can just do math with it:\n\n300 * (2/25)\n\n[1] 24\n\n3^2 + 42\n\n[1] 51\n\nsin(17)\n\n[1] -0.961\n\n\nObjects and Functions\nBut, R is more than just a calculator. There are a lot of things you can make with R, and a lot of things you can do with it. The things that you make are called objects and the things that you do things with are called functions. Any complex statistical operation you want to conduct in R will almost certainly involve the use of one or more functions.\nCalling functions\nTo use a function, we call it like this:\n\nfunction_name(arg1 = value1, arg2 = value2, ...)\n\nTry calling the seq() function.\n\nseq(from = 1, to = 5)\n\n[1] 1 2 3 4 5\n\n\nAs you can see, this generates a sequence of numbers starting at 1 and ending at 5. There are two things to note about this. First, we do not have to specify the arguments explicitly, but they must be in the correct order:\n\nseq(1, 5) \n\n[1] 1 2 3 4 5\n\nseq(5, 1)\n\n[1] 5 4 3 2 1\n\n\nSecond, the seq() function has additional arguments you can specify, like by and length. While we do not have to specify these because they have default values, you can change one or the other (but not at the same time!):\n\nseq(1, 5, by = 2)\n\n[1] 1 3 5\n\nseq(1, 5, length = 3)\n\n[1] 1 3 5\n\n\nCreating objects\nTo make an object in R, you use the arrow, <-, like so:\n\nobject_name <- value\n\nTry creating an object with value 5.137 and assigning it to the name bob, like this:\n\nbob <- 5.137\n\nThere are three things to note here. First, names in R must start with a letter and can only contain letters, numbers, underscores, and periods.\n\n# Good\nwinter_solder <- \"Buckey\"\nobject4 <- 23.2\n\n# Bad\nwinter soldier <- \"Buckey\" # spaces not allowed\n4object <- 23.2            # cannot start with a number\n\nSecond, when you create an object with <-, it ends up in your workspace or environment (you can see it in the RStudio environment pane). Finally, it is worth noting that the advantage of creating objects is that we can take the output of one function and pass it to another.\n\nx <- seq(1, 5, length = 3)\n\nlogx <- log(x)\n\nexp(logx)\n\n[1] 1 3 5\n\n\nExercises\n\nUse seq() to generate a sequence of numbers from 3 to 12.\nUse seq() to generate a sequence of numbers from 3 to 12 with length 25.\nWhy doesn’t this code work?\n\n\nseq(1, 5, by = 2, length = 10)\n\n\nUse <- to create an object with value 25 and assign it to a name of your choice.\nNow try to create another object with a different value and name.\nWhat is wrong with this code?\n\n\n2bob <- 10"
  },
  {
    "objectID": "labs/01-intro-lab.html#workflow",
    "href": "labs/01-intro-lab.html#workflow",
    "title": "Lab 01: Introduction",
    "section": "Workflow",
    "text": "Workflow\n\n\n\n\nAs you work more and more in R, you will learn that statistical analyses tend to involve the same basic set of tasks:\n\n\nimporting data,\n\nwrangling data to get it into a format necessary for analysis,\n\nexploring data with some simple descriptive statistics,\n\n\nanalyzing data with models to investigate potential trends or relationships, and\n\nsummarizing the results.\n\nAt various stages, you will also spend considerable time\n\n\nvisualizing the data and the results, either to explore the data further or to help communicate the results to others.\n\nA lot of the output of this process, we will also want to save for later, perhaps to include in a publication (like a figure or model summary), but maybe also to avoid repetition of difficult and time-consuming tasks, so the workflow will also involve\n\n\nexporting refined data and models.\n\nTo make this more concrete, let’s try out an example, working with the cars data again. As we go through this, try running all the code in the console.\nAn Example\nSuppose we return to the question we asked in the plotting section: Does the speed a car is going when it applies its brakes determine the distance it takes the car to stop? Obviously, the answer is Yes, but let’s pretend we don’t know the answer, so we can walk through the process of answering the question anyway.\nImport\nFirst, we need some data. In this case, we do not actually need to import the cars dataset because it is already available to us in R, so let’s just pretend like we did.\nExplore\nNow, let’s explore the data. Always, always, always, the best way to explore data is to visualize data! We already did this once, but it can’t hurt to try it again!\n\nplot(\n  cars,\n  pch = 21,\n  bg = adjustcolor(\"gray\", 0.75),\n  col = \"gray25\",\n  cex = 1.5\n)\n\n\n\n\nThen, we can do things like calculate the mean stopping distance.\n\nmean(cars$dist)\n\n[1] 43\n\n\nNote that we use the $ operator to pull the distance (dist) values from the cars table and supply it to the mean() function. Don’t worry too much about wrapping your head around that idea as we will talk about it more in another lab. We can also make a histogram to explore the distribution of stopping distances:\n\nhist(cars$dist)\n\n\n\n\nWhat does this tell you about car stopping distances? Is it clustered? Random?\nWrangle\nMaybe we think that one really long distance is exceptional, perhaps owing to measurement error, and we want to remove it from our analysis. In that case, we want to subset the data, including only distance values less than some amount, say 100 ft.\n\ncars <- subset(cars, dist < 100)\n\nThis is data wrangling, preparing the data for analysis.\nAnalyze\nNow, finally, we might want to answer our question directly by modeling the relationship between car speeds and stopping distances. Here, our hypothesis is that there is no relationship. This is called the null hypothesis. If we can show that this hypothesis is very likely false, then we can with some confidence accept the alternative hypothesis, namely, that there is a relationship. To test the null hypothesis, we can construct a simple linear model. In R, we do this:\n\ndistance_model <- lm(dist ~ speed, data = cars)\n\nsummary(distance_model)\n\n\nCall:\nlm(formula = dist ~ speed, data = cars)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-26.79  -9.15  -1.67   8.01  43.05 \n\nCoefficients:\n            Estimate Std. Error t value        Pr(>|t|)    \n(Intercept)  -14.002      6.295   -2.22           0.031 *  \nspeed          3.640      0.392    9.29 0.0000000000033 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 14.1 on 47 degrees of freedom\nMultiple R-squared:  0.647, Adjusted R-squared:  0.64 \nF-statistic: 86.3 on 1 and 47 DF,  p-value: 0.00000000000326\n\n\nWoah! That’s a lot to digest. For now, just note that the asterisks (*) imply that that there is very likely a relationship between speed and distance. But, what is that relationship? Or, what does it look like? Well, let’s try to visualize that in R, too.\n\nplot(\n  cars,\n  pch = 21,\n  bg = adjustcolor(\"gray\", 0.75),\n  col = \"gray25\",\n  cex = 1.5,\n  xlab = \"Speed (mph)\",\n  ylab = \"Distance (ft)\"\n)\n\nabline(\n  distance_model, \n  col = \"#A20000\",\n  lwd = 2\n)\n\ntitle(\n  \"Distance Model\",\n  line = 0.3, \n  adj = 0, \n  cex = 1.5\n)\n\n\n\n\nExport\nNow, if we feel it necessary, we can save our model, so we can inspect it again later.\n\nsave(distance_model, file = \"distance_model.Rds\")\n\nAnd that’s it! Now, all we have to do is write this up and publish it! Easy peasy."
  },
  {
    "objectID": "labs/01-intro-lab.html#r-projects",
    "href": "labs/01-intro-lab.html#r-projects",
    "title": "Lab 01: Introduction",
    "section": "R Projects",
    "text": "R Projects\n\n\n\n\nYOU CANNOT EAT R CODE. Believe me. You can’t. Eventually, you’ll have to close out of R, turn off your computer, walk away, and do whatever it is that you do to maintain your existence. That means you need some way to save your progress and you need some place to save it. R has a few built-in tools for this, and they are really convenient, at least early on. However, you will be much better off if you get into the habit of using RStudio Projects. What is an R Project? Basically, it’s a folder on your computer that you can use to organize all the data, code, figures, texts, and analyses associated with a single scientific research project.\nWhen you open your project in RStudio, it will establish your project folder as your working directory by default. The advantage of this is that you can access R scripts and data using relative file paths rather than specifying the full path from your computer’s root directory. Why is this advantageous? Because you can copy the project folder to any computer you want and your relative file paths will just work!\nExercises\n\nBefore we setup your project, let’s turn off some of R’s default settings.\n\nIn RStudio, go to Tools > Global Options….\nIn the dialog box that appears, navigate to the General section, and under Workspace, make sure “Restore .RData into workspace at startup” is unchecked.\nThen, for “Save workspace to .Rdata on exit”, select Never.\nHit “Apply”, then hit “OK.”\n\n\n\nNow, we are going to create a new project for you for this class. You will use this folder to save all your lab and homework exercises, required datasets, and figures. To do that, follow these steps:\n\nIn RStudio, go to File > New Project….\nIn the dialog box that appears, select New Directory, then New Project.\nPut “qaad” as the Directory name.\nThen Browse to a location on your computer where you would like to keep this project and hit “OK.”\nMake sure “Create a git repository” and “Use renv with this project” are unchecked.\n\nThen click “Create Project.” This will restart RStudio with your project loaded. You can confirm this by looking at the top left of the RStudio window. It should say “qaad - RStudio” now. If you look in the File pane (bottom-right), you will also see a file called “qaad.Rproj.”\n\n\nOnce you have your project folder setup, have a look at the Files pane again. You should see a button that says “New Folder.” Click that, and in the dialog box that appears, enter “R” and hit “OK.” You should now see a folder in your project directory called “R.” This is where you will keep all the files with your R code in it. Repeat this process to add “data”, “figures”, and “_misc” folders to your project. The “_misc” folder is short for miscellaneous. This folder is not strictly necessary but I find it helpful. It’s like that drawer in the kitchen where random stuff goes. It might not be clean or orderly, but at least your kitchen is!\nJust to check that everything is working, minimize RStudio and navigate to the location of your R Project on your computer. Do you see the folders you have created and the “qaad.Rproj” file?"
  },
  {
    "objectID": "labs/01-intro-lab.html#quarto",
    "href": "labs/01-intro-lab.html#quarto",
    "title": "Lab 01: Introduction",
    "section": "Quarto",
    "text": "Quarto\n\n\n\n\n\nFigure 1: Artwork from “Hello, Quarto” keynote by Julia Lowndes and Mine Çetinkaya-Rundel, presented at RStudio Conference 2022. Illustrated by Allison Horst.\n\n\nQuarto offers a unified framework for statistical programming and science communication by letting you write and run R code alongside text to explain your methods to others. The text you write is formatted using Markdown syntax (the same syntax you would use on, for example, a Reddit post). The basis for creating documents using Quarto is a test-based file format with the extension “.qmd”, short for Quarto Markdown. In just about every one of these documents you come across, you will find three major components:\n\na YAML header surrounded at the top and bottom by three hyphens, ---,\nR code chunks surrounded at the top and bottom by three back ticks, ```, and\ntext formatted with markdown syntax like # heading 1 and _italics_.\n\nHere is an example:\n---\ntitle: \"Week 01\"\nauthor: \"Kenneth Blake Vernon\"\ndate: \"1/10/2023\"\nformat: html\nexecute:\n  echo: true\n---\n\n## Lab Exercises\n\n### Plot\n\n1. Complete the following line of code to preview only the first three rows of the `cars` table.\n\n```{r}\n\nhead(cars, n = )\n\n```\n\n***\n\n## Homework Exercises\n\n1. \nLet’s start with some simple markdown formatting and work our way back to the YAML.\nMarkdown formatting\nMarkdown is a lightweight markup language for formatting plain text and is designed to be easy to read and write. The markdown you will use most often includes all of the following (borrowed from here:\nText formatting \n------------------------------------------------------------\n\n*italic*  or _italic_\n**bold**   __bold__\n`code`\nsuperscript^2^ and subscript~2~\n\nHeadings\n------------------------------------------------------------\n\n# 1st Level Header\n\n## 2nd Level Header\n\n### 3rd Level Header\n\nLists\n------------------------------------------------------------\n\n*   Bulleted list item 1\n\n*   Bulleted list item 2\n\n    * Nested list item 2a\n\n    * Nested list item 2b\n\n1.  Numbered list item 1\n\n1.  Numbered list item 2. The numbers are incremented automatically in the output.\n\n1.  Numbered list item 3. \n\nLinks and images\n------------------------------------------------------------\n\n<http://example.com>\n\n[linked phrase](http://example.com)\n\n![optional caption text](path/to/img.png)\nYAML\n‘YAML’ is a recursive acronym that means “YAML Ain’t Markup Language.” You don’t actually need to know that. I just think it’s funny. The YAML controls features of the whole document, specifying, for instance, the title and author. It looks like this:\n---\ntitle: \"Week 01\"\nauthor: \"Kenneth Blake Vernon\"\ndate: \"1/11/2022\"\nformat: html\n---\nNotice the three dashes --- at the top and bottom. These must be there in order for Quarto to recognize it as the YAML. You should always include this at the beginning of the document. There’s A LOT you can specify in the YAML. In fact, you can specify basically anything you want, but being quite new to Quarto, I don’t think that would be helpful. For now, let me draw your attention to the format field. Quarto (with the power of a utility known as Pandoc) can generate a wide variety of output formats, including Word documents, PDFs, revealjs slides (presentations, what the slides in tihs class are built with), and even Powerpoint (if you really insist on it). In this class, we’ll stick with the default HTML output, so the only thing you will need to specify in the YAML is the title, author, and date.\nBy the way, the HTML output is the same stuff that a website is built on. In fact, when you open the resulting HTML file, it will open in your browser.\nR code chunks\nAll R code that you want to run needs to be “fenced” by three back ticks ```. You also need to tell Quarto that it’s R code and not, say, Python or SQL. To do that, you add {r} after the first set of back ticks. Altogether, it should look like this:\n```{r}\n\n1+1\n\n```\nInstead of typing this every time, you can use Ctrl + Alt + I (or CMD) in RStudio, and it will automatically generate a code chunk in your qmd document. You can run the code in these chunks like you would code in an R script, by placing the cursor over it and hitting Ctrl + Enter. You can specify options for code chunks in R Markdown that will affect the way that they behave or are displayed. You can find a complete list of chunk options at http://yihui.name/knitr/options/. Here are a few examples:\n\n\neval: false prevents code from being evaluated.\n\necho: false hides the code but not the results of running the code.\n\nwarning: false prevents R messages and warnings from appearing in the knitted document.\n\nHere is how it would look to specify these in a code chunk:\n```{r}\n#| echo: false\n#| warning: false\n\n1+1\n\n```\nYou can also set these globally, applying them to all code chunks, by specifying them in the execute field (for code execution) in the YAML at the top of your qmd document. It would look like this:\n---\ntitle: \"Week 01\"\nauthor: \"Kenneth Blake Vernon\"\ndate: \"1/11/2022\"\nformat: html\nexecute:\n  echo: false\n  warning: false\n---\nThere are loads more of these options, some of the more important ones involve figures you generate with these documents. Rather than overload you with all of those now, we’ll try to go over some of those here or there in future labs and homework exercises.\nExercises\n\nLet’s create a new qmd document in RStudio. To do that, follow these steps:\n\nGo to File > New File > Quarto Document….\nIn the dialog box that appears, put “ANTH 5580 (QAAD) Week 01” as the Title.\nPut your name as Author.\nHit “OK”.\n\n\n\nRStudio will open a new qmd document for you. Notice that it sets up the YAML for you. Let’s copy the template we will use for these course assignments. To do that, follow these steps:\n\nScroll up to the section above with the example of a qmd document. You can now copy and paste this into your qmd document (I recommend typing it out by hand, so you can get a feel for it, but that’s not necessary).\n\n\nNotice that this template has two level two headers, “Lab Exercises” and “Homework Exercises.” These are the two major assignments you will have to complete each week. You will enter all your answers in a qmd document with this format and submit it via Canvas. To make sure your code is actually working, you can “render” the document and see if it completes without error. This is partly what I will do each week when grading your assignments. To keep these things organized, each exercise section in the lab should have its own level three header, like ### Plot for this week. Since there is no R related homework assignment for this week, you can just delete that section from this qmd document. Before continuing, save your qmd document to the R folder in your course project directory.\nNow, go back through this lab and re-do the exercises by adding them to this qmd document. Make sure to save that again, then submit it on the Canvas course page. Again, go ahead and render the document, too, just to make sure everything is working. This is the process that you will go through each week!"
  },
  {
    "objectID": "labs/01-intro-lab.html#homework",
    "href": "labs/01-intro-lab.html#homework",
    "title": "Lab 01: Introduction",
    "section": "Homework",
    "text": "Homework\nThere is no R related homework assignment for this week. Please fill out the pre-course self-assessment survey on Canvas."
  },
  {
    "objectID": "labs/02-probability-lab.html",
    "href": "labs/02-probability-lab.html",
    "title": "Lab 02: Statistical Graphics",
    "section": "",
    "text": "This lab will guide you through the process of\n\nloading (or “attaching”) R packages with library()\n\ngenerating summary statistics for your data\nvisualizing data with the grammar of graphics and ggplot()\n\naesthetic mappings\ngeometric objects\nfacets\nscales\nthemes\n\n\n\nWe will be using the following packages:\n\narchdata\nggplot2\npalmerpenguins\nskimr\nviridis\n\n\n\npenguins\n\nIncludes measurements for penguin species, island in Palmer Archipelago, size (flipper length, body mass, bill dimensions), and sex.\npackage: palmerpenguins\n\nreference: https://allisonhorst.github.io/palmerpenguins/reference/penguins.html\n\n\n\n\nDartPoints\n\nIncludes measurements of 91 Archaic dart points recovered during surface surveys at Fort Hood, Texas.\npackage: archdata\n\nreference: https://cran.r-project.org/web/packages/archdata/archdata.pdf"
  },
  {
    "objectID": "labs/02-probability-lab.html#the-library",
    "href": "labs/02-probability-lab.html#the-library",
    "title": "Lab 02: Statistical Graphics",
    "section": "The Library",
    "text": "The Library\nR is an extensible programming language, meaning you can write R code to extend the functionality of base R. To share that code, R users will often bundle it into a package, a collection of functions, data, and documentation. You can think of packages as apps, but apps specifically designed for R. To make the functionality a package offers available in R, you have to load them in with the library() function (the technical term is attach).\nYou should always, always, always load all the packages you use at the beginning of a document. That way, people who read your code know exactly what packages you are using all at once and right away. To make this really, really explicit, I prefer to set this off with its own section that I call the “R Preamble.” In a Quarto document, it looks like this:\n## R Preamble\n\n```{r packages}\n#| warning: false\n\nlibrary(archdata)\nlibrary(ggplot2)\nlibrary(palmerpenguins)\nlibrary(skimr)\nlibrary(viridis)\n\n```\nOf course, these aren’t just automatically on your computer, so you have to install the packages first. Then you can open them in R. To do that, you use the function install.packages(). For the packages used today, you can use this call just once like so:\ninstall.packages(\n  c(\"archdata\", \"ggplot2\", \"palmerpenguins\", \"skimr\", \"viridis\")\n)\nNote that you only need to run this once, so don’t put this as a line in your Quarto document, which you might render multiple times. Just run it in the console.\nExercises\n\nOpen a new Quarto document and add the R Preamble with an R code chunk with the library() calls that load the R packages required for this lab.\nNow actually run each library() call. You can do that by either highlighting them and hitting Ctrl + Enter (Cmd + Enter) or by clicking the green arrow that appears in the top right of the code chunk."
  },
  {
    "objectID": "labs/02-probability-lab.html#summary-statistics",
    "href": "labs/02-probability-lab.html#summary-statistics",
    "title": "Lab 02: Statistical Graphics",
    "section": "Summary Statistics",
    "text": "Summary Statistics\n\n\n\n\n\nFigure 1: Artwork by Allison Horst.\n\n\nLet’s use R to describe some properties of a sample of penguins from Palmer Station in Antarctica. These data became available in R when you loaded the palmerpenguins package. They aren’t currently visible in your environment (for complicated reasons), but trust me, they’re there. The name of the dataset is penguins, so you can call it that way.\n\nhead(penguins, n = 5)\n\n# A tibble: 5 × 8\n  species island    bill_length_mm bill_depth_mm flipper_l…¹ body_…² sex    year\n  <fct>   <fct>              <dbl>         <dbl>       <int>   <int> <fct> <int>\n1 Adelie  Torgersen           39.1          18.7         181    3750 male   2007\n2 Adelie  Torgersen           39.5          17.4         186    3800 fema…  2007\n3 Adelie  Torgersen           40.3          18           195    3250 fema…  2007\n4 Adelie  Torgersen           NA            NA            NA      NA <NA>   2007\n5 Adelie  Torgersen           36.7          19.3         193    3450 fema…  2007\n# … with abbreviated variable names ¹​flipper_length_mm, ²​body_mass_g\n\n\n\n\nCentral tendency\nThe central tendency is, as its name suggests, a value around which other values tend to cluster. There are two primary measures of central tendency: the mean and the median. As you may recall, the mean or average of a sample is simply the sum of a finite set of values, \\(x_i\\), divided by the number of values, \\(n\\).\n\\[\\bar{x} = \\frac{1}{n} \\sum_{i=1}^{n} x_i\\]\nRemember that this is only an estimate of the central tendency of a population, \\(\\mu\\)! In R, you can calculate the mean by hand if you like, but it’s probably easier to use the built-in R function, mean(). Let’s use this to calculate the mean bill length of penguins.\n\nmean(penguins$bill_length_mm)\n\n[1] NA\n\n\nWhoops! Need to set na.rm = TRUE to ignore missing or NA values.\n\nmean(penguins$bill_length_mm, na.rm = TRUE)\n\n[1] 43.9\n\n\nAn important limitation of the mean is its sensitivity to outliers - you know, like rich people. If you calculate the mean household income in the United States, for example, the incomes of obscenely wealthy individuals like Jeff Bezos and Elon Musk will pull that measure up, thus painting a much rosier picture of the US than the reality the rest of us live in. When there are extreme outliers, it is often advisable to use the median because it is less sensitive to outliers as it is the “middle” number or value that evenly divides the sample in half.\n\nmedian(penguins$bill_length_mm, na.rm = TRUE)\n\n[1] 44.5\n\n\n\n\n\n\n\n\nNote\n\n\n\n\n\nBy the way, did you notice the weird use of the dollar sign $? This is used to index data and pull out a specific element by its name. For example, when we run the code snippet penguins$bill_length_mm, we are asking R to pull the variable bill_length_mm from the penguins table and give us its values. One important implication of this sort of indexing is that we can assign the variable to its own object outside of the table, e.g.\nbill_length <- penguins$bill_length_mm\nYou’ll actually learn more about this, in particular how to work with tabular data, in the next lab.\n\n\n\nDispersion\nDispersion describes the spread of data around its central tendency. Are the values tightly clustered around the central tendency or highly dispersed? Is there, in other words, a lot of variability? This is what dispersion seeks to characterize. As with the central tendency, it has two primary measures: variance and standard deviation. The variance of a sample is the mean squared error.\n\\[s^{2} = \\frac{1}{n-1} \\sum_{i=1}^{n} (x_i - \\bar{x})^{2}\\]\nThis is an estimate of the population variance, \\(\\sigma^2\\). To calculate the variance of a sample with R, use var().\n\nvar(penguins$body_mass_g, na.rm = TRUE)\n\n[1] 643131\n\n\nWhoa! That’s a really big number. Do penguins get that big? The answer, of course, is No. The number is large because variance is squared error, so this is in units of squared-grams, \\(g^2\\), not grams, \\(g\\). Why do we square it? Well, if you think about it, some of the errors will be negative and some will be positive (some penguins will be larger than average and some smaller), but on balance - on average - the sum of the errors will tend towards zero, which is not terribly informative of the spread of the data. Or, put that another way, what we want with a measure of dispersion is not just difference but distance from the mean, and distance is always positive. Squaring the errors is one way of ensuring that the values are positive (similar to taking the absolute value).\nThere is one small catch to squaring though. It makes it a really weird measure to think about - like, what is a square gram? Hard to say. That’s why it is common to take the square root of the variance, to get the measure back into units of the data. This value is known as the standard deviation, \\(s\\). You can calculate it with the sd() function.\n\nsd(penguins$body_mass_g, na.rm = TRUE)\n\n[1] 802\n\n\nThat’s about 1.8 pounds (if you prefer Imperial units).\nTable summaries\nTo generate summary statistics for all the variables in your data, base R provides a really nice summary() function that you can apply to a table like so:\n\nsummary(penguins)\n\n      species          island    bill_length_mm bill_depth_mm \n Adelie   :152   Biscoe   :168   Min.   :32.1   Min.   :13.1  \n Chinstrap: 68   Dream    :124   1st Qu.:39.2   1st Qu.:15.6  \n Gentoo   :124   Torgersen: 52   Median :44.5   Median :17.3  \n                                 Mean   :43.9   Mean   :17.1  \n                                 3rd Qu.:48.5   3rd Qu.:18.7  \n                                 Max.   :59.6   Max.   :21.5  \n                                 NA's   :2      NA's   :2     \n flipper_length_mm  body_mass_g       sex           year     \n Min.   :172       Min.   :2700   female:165   Min.   :2007  \n 1st Qu.:190       1st Qu.:3550   male  :168   1st Qu.:2007  \n Median :197       Median :4050   NA's  : 11   Median :2008  \n Mean   :201       Mean   :4202                Mean   :2008  \n 3rd Qu.:213       3rd Qu.:4750                3rd Qu.:2009  \n Max.   :231       Max.   :6300                Max.   :2009  \n NA's   :2         NA's   :2                                 \n\n\nAs you can see, this prints out summary statistics for the variables in your data. However, the printout is not easy to read and it provides a somewhat limited set of summary statistics. As an alternative, you might try the skim() function from the skimr package.\n\nNote that I have applied css styling to this table output to make it more compact and fit on the screen. Yours will look slightly different.\n\nskim(penguins)\n\n\nData summary\n\n\nName\npenguins\n\n\nNumber of rows\n344\n\n\nNumber of columns\n8\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n3\n\n\nnumeric\n5\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\nspecies\n0\n1.00\nFALSE\n3\nAde: 152, Gen: 124, Chi: 68\n\n\nisland\n0\n1.00\nFALSE\n3\nBis: 168, Dre: 124, Tor: 52\n\n\nsex\n11\n0.97\nFALSE\n2\nmal: 168, fem: 165\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\nbill_length_mm\n2\n0.99\n43.9\n5.46\n32.1\n39.2\n44.5\n48.5\n59.6\n▃▇▇▆▁\n\n\nbill_depth_mm\n2\n0.99\n17.1\n1.97\n13.1\n15.6\n17.3\n18.7\n21.5\n▅▅▇▇▂\n\n\nflipper_length_mm\n2\n0.99\n200.9\n14.06\n172.0\n190.0\n197.0\n213.0\n231.0\n▂▇▃▅▂\n\n\nbody_mass_g\n2\n0.99\n4201.8\n801.95\n2700.0\n3550.0\n4050.0\n4750.0\n6300.0\n▃▇▆▃▂\n\n\nyear\n0\n1.00\n2008.0\n0.82\n2007.0\n2007.0\n2008.0\n2009.0\n2009.0\n▇▁▇▁▇\n\n\n\n\n\n\n\nAs you can see, there are three major sections of this printout: (i) Data Summary, (ii) Factor variables, and (iii) Numeric variables. The Data Summary gives you an overview of your table, with counts of the number of rows and columns, as well as counts of the different types of variables (factor, numeric, etc). The section on factor variables gives you counts for each level of the factor (for example, counts of the different species of penguins), as well as information about missing data. Finally, the section on numeric variables gives you information on missing data, as well as measures of dispersion and central tendency, including the mean, median (p50), and standard deviation (sd), the range or minimum and maximum values (p0 and p100), and the inner quartiles (p25 and p75).11 You’ll learn more about quartiles and how to visualize distributions in the next lab.\nExercises\n\nHave a look at the penguins data again. Use head() to preview the first 15 rows.\nWith the penguins data, calculate all of the following:\n\nmedian body mass\nmean bill depth\nvariance in bill depth\nstandard deviation in bill length"
  },
  {
    "objectID": "labs/02-probability-lab.html#the-grammar-of-graphics",
    "href": "labs/02-probability-lab.html#the-grammar-of-graphics",
    "title": "Lab 02: Statistical Graphics",
    "section": "The Grammar of Graphics",
    "text": "The Grammar of Graphics\nIt’s easy to imagine how you would go about with pen and paper drawing a bar chart of, say, the number of penguins in each species in the penguins dataset. But, what if you had to dictate the steps to make that graph to another person, one you can’t see or physically interact with? All you can do is use words to communicate the graphic you want. How would you do it? The challenge here is that you and your illustrator must share a coherent vocabulary for describing graphics. That way you can unambiguously communicate your intent. That’s essentially what the grammar of graphics is, a language with a set of rules (a grammar) for specifying each component of a graphic.\nNow, if you squint just right, you can see that R has a sort of grammar built-in with the base graphics package. To visualize data, it provides the default plot() function, which you learned about in the last lab. This is a workhorse function in R that will give you a decent visualization of your data fast, with minimal effort. It does have its limitations though. For starters, the default settings are, shall we say, less than appealing. I mean, they’re fine if late-nineties styles are your thing, but less than satisfying if a more modern look is what you’re after.2 Second, taking fine-grained control over graphics generated with plot() can be quite frustrating, especially when you want to have a faceted figure (a figure with multiple plot panels).2 But, you know, opinions, everyone has them, and there’s no accounting for taste.\nThat’s where the ggplot2 package comes in. It provides an elegant implementation of the grammar of graphics, one with more modern aesthetics and with a more standardized framework for fine-tuning figures, so that’s what we’ll be using here. From time to time, I’ll try to give you examples of how to do things with the plot() function, too, so you can speak sensibly to the die-hard holdouts, but we’re going to focus on learning ggplot.\n\n\n\n\n\n\nTip\n\n\n\n\n\nThere are several excellent sources of additional information on statistical graphics in R and statistical graphics in general that I would recommend.\n\nThe website for the ggplot2 package: https://ggplot2.tidyverse.org/. This has loads of articles and references that will answer just about any question you might have.\n\nThe R graph gallery website: https://r-graph-gallery.com/. This has straightforward examples of how to make all sorts of different plot visualizations, both with base R and ggplot.\n\nClaus Wilke’s free, online book Fundamentals of Data Visualization, which provides high-level rules or guidelines for generating statistical graphics in a way that clearly communicates its meaning or intent and is visually appealing.\n\nThe free, online book ggplot2: Elegant Graphics for Data Analysis (3ed) by Hadley Wickham, Danielle Navarro, and Thomas Lin Pedersen. This is a more a deep dive into the grammar of graphics than a cookbook, but it also has lots of examples of making figures with ggplot2.\n\n\n\n\nSo, to continue our analogy above, we’re going to treat R like our illustrator, and ggplot2 is the language we are going to speak to R to visualize our data. So, how do we do that? Well, let’s start with the basics. Suppose we want to know if there’s some kind of relationship (an allometric relationship) among the Palmer Station penguins between their body mass and bill length. Here’s how we would visualize that.\n\nggplot(data = penguins) + \n  geom_point(\n    mapping = aes(x = body_mass_g, y = bill_length_mm)\n  )\n\n\n\n\nHere, we have created what is known as a scatterplot, a representation of the raw data as points on a Cartesian grid. There are several things to note about the code used to generate this plot.\n\nFirst, it begins with a call to the ggplot() function. This takes a data argument. In this case, we say that we want to make a plot to visualize the penguins data.\nThe next function call is geom_point(). This is a way of specifying the geometry we want to plot. Here we chose points, but we could have used another choice (lines, for example, or polygons).\nNote that the geom_point() call takes a mapping argument. You use this to specify how variables in your data are mapped to properties of the graphic. Here, we chose to map the body_mass_g variable to the x-coordinates and the bill_length_mm variable to the y-coordinates. Importantly, we use the aes() function to supply an aesthetic to the mapping parameter. This is always the case.\nThe final thing to point out here is that we combined or connected these arguments using the plus-sign, +. You should read this literally as addition, as in “make this ggplot of the penguins data and add a point geometry to it.” Be aware that the use of the plus-sign in this way is unique to the ggplot2 package and won’t work with other graphical tools in R.\n\nWe can summarize these ideas with a simple template. All that is required to make a graph in R is to replace the elements in the bracketed sections with a dataset, a geometry function, and an aesthetic mapping.\nggplot(data = <DATA>) + \n  <GEOM_FUNCTION>(mapping = aes(<MAPPINGS>))\nOne of the great things about ggplot, something that makes it stand out compared to alternative graphics engines in R, is that you can assign plots to a variable and call it in different places, or modify it as needed.\npenguins_plot <- ggplot(data = penguins) + \n  geom_point(\n    mapping = aes(x = body_mass_g, y = bill_length_mm)\n  )\n\npenguins_plot\nExercises\n\nRecreate the scatterplot above, but switch the axes. Put bill length on the x-axis and body mass on the y-axis.\nNow create a scatterplot of bill length (on the y-axis) by bill depth (on the x-axis)."
  },
  {
    "objectID": "labs/02-probability-lab.html#aesthetics",
    "href": "labs/02-probability-lab.html#aesthetics",
    "title": "Lab 02: Statistical Graphics",
    "section": "Aesthetics",
    "text": "Aesthetics\nIn the plot above, we only specified the position of the points (the x- and y-coordinates) in the aesthetic mapping, but there are many aesthetics (see the figure below), and we can map the same or other variables to those.\n\n\nFigure 2: Commonly used aesthetics. Figure from Claus O. Wilke. Fundamentals of Data Visualization. O’Reilly, 2019.\n\n\nConsider, for example, the fact that there are three penguin species in our dataset: Adelie, Gentoo, and Chinstrap. Do we think the relationship between body mass and bill length holds for all of them? Let’s add penguin species to our aesthetic mapping (specifically to the color parameter) and see what happens.\n\nggplot(data = penguins) + \n  geom_point(\n    mapping = aes(x = body_mass_g, y = bill_length_mm, color = species)\n  )\n\n\n\n\nNotice that ggplot2 automatically assigns a unique color to each species and adds a legend to the right that explains each color. In this way, the color doesn’t just change the look of the figure. It conveys information about the data. Rather than mapping a variable in the data to a specific aesthetic, though, we can also define an aesthetic manually for the geometry as a whole. In this case, the aesthetics do not convey information about the data. They merely change the look of the figure. The key to doing this is to move the specification outside the aes(), but still inside the geom_point() function.\n\nggplot(data = penguins) + \n  geom_point(\n    mapping = aes(x = body_mass_g, y = bill_length_mm),\n    shape = 21,\n    size = 4,\n    color = \"darkred\",\n    fill = \"darkgoldenrod1\"\n  )\n\n\n\n\nNotice that we specified the shape with a number. R has 25 built-in shapes that you can specify with a number, as shown in the figure below. Some important differences in these shapes concern the border and fill colors. The hollow shapes (0-14) have a border that you specify with color, the solid shapes (15-20) have a border and fill, both specified with color, and the filled shapes (21-24) have separate border and fill colors, specified with color and fill respectively.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\n\n\nNote that you can use hexadecimal codes like #004F2D instead of “forestgreen” to specify a color. This also allows you to specify a much wider range of colors. See https://htmlcolorcodes.com/ for one way of exploring colors.\n\n\n\nExercises\n\nChange the code below to map the species variable to the x-axis (in addition to the color).\n\nggplot(data = penguins) + \n  geom_point(\n    mapping = aes(x = body_mass_g, y = bill_length_mm, color = species)\n  )\n\nWhat does this do to the position of the points?\nChange the code below to map the species variable to the shape aesthetic (in addition to the color).\n\n# hint: use shape = ...\nggplot(data = penguins) + \n  geom_point(\n    mapping = aes(x = body_mass_g, y = bill_length_mm, color = species)\n  )\n\nChange the code below to map the species variable to the size aesthetic (replacing color).\n\n# hint: use size = ...\nggplot(data = penguins) + \n  geom_point(\n    mapping = aes(x = body_mass_g, y = bill_length_mm, color = species)\n  )\n\nFor the following code, change the color, size, and shape aesthetics for the entire geometry (do not map them to the data).\n\nggplot(data = penguins) + \n  geom_point(\n    mapping = aes(x = body_mass_g, y = bill_length_mm),\n    color = , # <------- insert value here\n    size = ,  # <------- \n    shape =   # <------- \n  )"
  },
  {
    "objectID": "labs/02-probability-lab.html#geometries",
    "href": "labs/02-probability-lab.html#geometries",
    "title": "Lab 02: Statistical Graphics",
    "section": "Geometries",
    "text": "Geometries\nHave a look at these two plots.\n\n\n\n\n\n\n\n\n\n\nBoth represent the same data and the same x and y variables, but they do so in very different ways. That difference concerns their different geometries. As their name suggests, these are geometrical objects used to represent the data. To change the geometry, simply change the geom_*() function. For example, to create the plots above, use the geom_point() and geom_smooth() functions.\n# left\nggplot(data = penguins) + \n  geom_point(\n    mapping = aes(x = body_mass_g, y = bill_length_mm)\n  )\n\n# right\nggplot(data = penguins) + \n  geom_smooth(\n    mapping = aes(x = body_mass_g, y = bill_length_mm)\n  )\nWhile every geometry function takes a mapping argument, not every aesthetic works (or is needed) for every geometry. For example, there’s no shape aesthetic for lines, but there is a linetype. Conversely, points have a shape, but not a linetype.\n\nggplot(data = penguins) + \n  geom_smooth(\n    mapping = aes(x = body_mass_g, y = bill_length_mm, linetype = species),\n  )\n\n\n\n\nOne really important thing to note here is that you can add multiple geometries to the same plot to represent the same data. Simply add them together with +.\n\nggplot(data = penguins) + \n  geom_smooth(\n    mapping = aes(x = body_mass_g, y = bill_length_mm, linetype = species),\n  ) + \n  geom_point(\n    mapping = aes(x = body_mass_g, y = bill_length_mm, color = species)\n  )\n\n\n\n\nThat’s a hideous figure, though it should get the point across. While layering in this way is a really powerful tool for visualizing data, it does have one important drawback. Namely, it violates the DRY principle (Don’t Repeat Yourself), as it specifies the x and y variables twice. This makes it harder to make changes, forcing you to edit the same aesthetic parameters in multiple locations. To avoid this, ggplot2 allows you to specify a common set of aesthetic mappings in the ggplot() function itself. These will then apply globally to all the geometries in the figure.\nggplot(\n  data = penguins,\n  mapping = aes(x = body_mass_g, y = bill_length_mm)\n) +\n  geom_smooth(mapping = aes(linetype = species)) +\n  geom_point(mapping = aes(color = species))\nNotice that you can still specify specific aesthetic mappings in each geometry function. These will apply only locally to that specific geometry rather than globally to all geometries in the plot. In the same way, you can specify different data for each geometry.\n\nggplot(\n  data = penguins,\n  mapping = aes(x = body_mass_g, y = bill_length_mm)\n) +\n  geom_smooth(data = filter(penguins, species == \"Adelie\")) +\n  geom_point(mapping = aes(color = species))\n\n\n\n\nSome of the more important geometries you are likely to use include:\n\ngeom_point()\ngeom_line()\ngeom_segment()\ngeom_polygon()\ngeom_boxplot()\ngeom_histogram()\ngeom_density()\n\nWe’ll actually cover those last three in the section on plotting distributions. For a complete list of available geometries, see the layers section of the ggplot2 website reference page."
  },
  {
    "objectID": "labs/02-probability-lab.html#facets",
    "href": "labs/02-probability-lab.html#facets",
    "title": "Lab 02: Statistical Graphics",
    "section": "Facets",
    "text": "Facets\nSometimes mapping variables to aesthetics can generate a lot of noise and clutter, making it hard to read or interpret a figure. One way to handle this is to split your plot into multiple plots or facets based on levels of a categorical variable like species. To do this for one categorical variable, you use the facet_wrap() function.\n\nggplot(data = penguins) +\n  geom_point(\n    mapping = aes(x = body_mass_g, y = bill_length_mm)\n  ) +\n  facet_wrap(vars(species))\n\n\n\n\nPlacing species in the vars() function tells facet_wrap() to “split the plot by species.” If you want to split the plot by two categorical variables, like species and sex, use the facet_grid() function.\n\nggplot(data = penguins) + \n  geom_point(\n    mapping = aes(x = body_mass_g, y = bill_length_mm)\n  ) +\n  facet_grid(rows = vars(species), cols = vars(sex))\n\n\n\n\nEvidently, there are some penguins for whom the sex is unknown. To remove these penguins from the dataset, you can use the na.omit() function.\n\nggplot(data = na.omit(penguins)) + \n  geom_point(\n    mapping = aes(x = body_mass_g, y = bill_length_mm)\n  ) +\n  facet_grid(rows = vars(species), cols = vars(sex))\n\n\n\n\nExercises\n\nUse facet_wrap() to split the following scatterplot of the penguins data by sex.\n\nggplot(data = na.omit(penguins)) + \n  geom_point(\n    mapping = aes(x = body_mass_g, y = bill_length_mm)\n  ) + \n  facet_wrap() # <------- insert value here\n\nNow, map the species to the color aesthetic for the point geometry.\nUse facet_grid() to split the following scatterplot of the penguins data by species and island.\n\nggplot(data = na.omit(penguins)) + \n  geom_point(\n    mapping = aes(x = body_mass_g, y = bill_length_mm)\n  ) + \n  facet_grid() # <------- insert value here\n\nWhat does this tell you about how species are distributed across islands?"
  },
  {
    "objectID": "labs/02-probability-lab.html#scales",
    "href": "labs/02-probability-lab.html#scales",
    "title": "Lab 02: Statistical Graphics",
    "section": "Scales",
    "text": "Scales\nScales provide the basic structure that determines how data values get mapped to visual properties in a graph. The most obvious example is the axes because these determine where things will be located in the graph, but color scales are also important if you want your figure to provide additional information about your data. Here, we will briefly cover two aspects of scales that you will often want to change: axis labels and color palettes, in particular palettes that are colorblind safe.\nLabels\nBy default, ggplot2 uses the names of the variables in the data to label the axes. This, however, can lead to poor graphics as naming conventions in R are not the same as those you might want to use to visualize your data. Fortunately, ggplot2 provides tools for renaming the axis and plot titles. The one you are likely to use most often is probably the labs() function. Here is a standard usage:\n\nggplot(data = penguins) + \n  geom_point(\n    mapping = aes(x = body_mass_g, y = bill_length_mm)\n  ) +\n  labs(\n    x = \"Body mass (g)\",\n    y = \"Bill length (mm)\",\n    title = \"Palmer Penguins\"\n  )\n\n\n\n\nColor Palettes\nWhen you map a variable to an aesthetic property, ggplot2 will supply a default color palette. This is fine if you are just wanting to explore the data yourself, but when it comes to publication-ready graphics, you should be a little more thoughtful. The main reason for this is that you want to make sure your graphics are accessible. For instance, the default ggplot2 color palette is not actually colorblind safe. To address this shortcoming, you can specify colorblind safe color palettes using the scale_color_viridis() function from the viridis package.3 It works like this:3 When it comes to colors in R, the sky is the limit. As far as I am aware, the most comprehensive list of palettes for use in R is provided by the paletteer package, which attempts to collect all of the palettes scattered across the R ecosystem into one place. See also this beautiful website for creating custom color palettes: https://coolors.co/.\nggplot(data = penguins) + \n  geom_point(\n    mapping = aes(x = body_mass_g, y = bill_length_mm, color = species)\n  ) +\n  labs(\n    x = \"Body mass (g)\",\n    y = \"Bill length (mm)\",\n    title = \"Palmer Penguins\"\n  ) +\n  scale_color_viridis(\n    option = \"viridis\", \n    discrete = TRUE\n  )\n\n\n\n\n\n\n\n\n\n\nFor comparison, I’m showing the viridis scale next to the default ggplot2 scale, so you can see the difference. Two things to note about scale_color_viridis(). First, you choose a specific colorblind safe palette with the option parameter. In this case, I chose viridis, but there are others, including magma, cividis, and inferno, to name a few. Second, if the variable is continuous rather than discrete, you will have to set discrete = FALSE in the function, otherwise it will throw an error.\nExercises\n\nUsing the penguins dataset, plot body mass (y variable) by bill length (x variable) and change the axis labels to reflect this.\nUsing the penguins dataset, plot bill length (y variable) by bill depth (x variable) and change the axis labels to reflect this.\n\nUsing the code below, try out these different colorblind safe palettes from the viridis package:\n\nmagma\ncividis\ninferno\n\n\n\nggplot(data = penguins) + \n  geom_point(\n    mapping = aes(x = body_mass_g, y = bill_length_mm, color = species)\n  ) +\n  labs(\n    x = \"Body mass (g)\",\n    y = \"Bill length (mm)\",\n    title = \"Palmer Penguins\"\n  ) +\n  scale_color_viridis(\n    option = , # <------- insert value here\n    discrete = TRUE\n  )"
  },
  {
    "objectID": "labs/02-probability-lab.html#themes",
    "href": "labs/02-probability-lab.html#themes",
    "title": "Lab 02: Statistical Graphics",
    "section": "Themes",
    "text": "Themes\nTo control the display of non-data elements in a figure, you can specify a theme. This is done with the theme() function. Using this can get pretty complicated, pretty quick, as there are many many elements of a figure that can be modified, so rather than elaborate on it in detail, I want to draw your attention to pre-defined themes that you can use to modify your plots in a consistent way.\nHere is an example of the black and white theme, which removes filled background grid squares, leaving only the grid lines.\n\nggplot(data = penguins) + \n  geom_point(\n    mapping = aes(x = body_mass_g, y = bill_length_mm, color = species)\n  ) +\n  labs(\n    x = \"Body mass (g)\",\n    y = \"Bill length (mm)\",\n    title = \"Palmer Penguins\"\n  ) +\n  theme_bw()\n\n\n\n\nExercises\n\nComplete the code below, trying out each separate theme:\n\ntheme_minimal()\ntheme_classic()\ntheme_void()\n\n\n\nggplot(data = penguins) + \n  geom_point(\n    mapping = aes(x = body_mass_g, y = bill_length_mm, color = species)\n  ) +\n  labs(\n    x = \"Body mass (g)\",\n    y = \"Bill length (mm)\",\n    title = \"Palmer Penguins\"\n  ) +\n  theme_    # <------- complete function call to change theme"
  },
  {
    "objectID": "labs/02-probability-lab.html#homework",
    "href": "labs/02-probability-lab.html#homework",
    "title": "Lab 02: Statistical Graphics",
    "section": "Homework",
    "text": "Homework\n\n\nLoad data. For this homework exercise, we’ll work with the DartPoints dataset from the archdata package. To load that dataset, use data(DartPoints).\n\nSummary statistics. Let’s summarize these data now.\n\nUse head() to print out the first 10 rows of the table.\nUse mean() and median() on the Length, Width, Thickness, and Weight variables. (Hint: use DartPoints$<VARIABLE> as in DartPoints$Length.)\nUse var() and sd() on the same.\nNow use skim() to summarize the DartPoints data.\n\n\n\nGraphics. And now to visualize them.\n\nUse ggplot() to make a scatterplot showing dart point length as a function of weight. (Hint: use geom_point().)\nIs there a trend?\nMap the dart point Name (this is the dart point type) to the color aesthetic. (Hint: this should go inside the aes() mapping!)\nDo you see any meaningful differences between dart point types?\nChange the size of all points to 2.5. (Hint: this should go outside the aes() mapping but inside geom_point()!)\nUse scale_color_viridis() to make the color scale colorblind safe. Feel free to use whichever palette you prefer. (Hint: dart point type is a categorical variable, so you need to set discrete = TRUE!)\nTry out facet_wrap() on the dart point Name variable.\nDoes this make it easier or harder to see differences between types?"
  },
  {
    "objectID": "labs/03-inference-lab.html",
    "href": "labs/03-inference-lab.html",
    "title": "Lab 03: Statistical Inference",
    "section": "",
    "text": "This lab will guide you through the process of\n\nmaking a tidy data.frame\nvisualizing distributions with histograms\nrunning a t-test\nperforming an ANOVA\n\n\narchdata\nggplot2\npalmerpenguins\nskimr\n\nMake sure to load these into your R session with library(). This should always go at the start of your document!\n\nlibrary(archdata)\nlibrary(ggplot2)\nlibrary(palmerpenguins)\nlibrary(skimr)\n\n\n\n\npenguins\n\nIncludes measurements for penguin species, island in Palmer Archipelago, size (flipper length, body mass, bill dimensions), and sex.\npackage: palmerpenguins\n\nreference: https://allisonhorst.github.io/palmerpenguins/reference/penguins.html\n\n\n\n\nDartPoints\n\nIncludes measurements of 91 Archaic dart points recovered during surface surveys at Fort Hood, Texas.\npackage: archdata\n\nreference: https://cran.r-project.org/web/packages/archdata/archdata.pdf\n\n\n\n\ntitanic\n\nProvides information on the fate of passengers of the Titanic, including economic status, sex, age, and survival.\npackage: none\nreference: https://wilkelab.org/SDS375/\n\nnote: we’re going to download this one rather than get it from a package\n\n\n\nSnodgrass\n\nIncludes measurements of size, location, and contents of 91 pit houses at the Snodgrass site in Butler County, Missouri.\nreference: https://cran.r-project.org/web/packages/archdata/archdata.pdf"
  },
  {
    "objectID": "labs/03-inference-lab.html#data-frames",
    "href": "labs/03-inference-lab.html#data-frames",
    "title": "Lab 03: Statistical Inference",
    "section": "Data Frames",
    "text": "Data Frames\n\n\n\n\n\nFigure 1: Elements of a Data Frame.\n\n\nConsider this research scenario: you have a museum collection of projectile points that you want to use for an analysis, maybe you want to know whether East Gate and Rose Spring points are actually the same type of point, commonly referred to using the portmanteau Rosegate. Following tradition, you think maybe it’s the size and shape that will you help finally put this old question to rest, so for each individual point, you set about measuring its length, width, and height.\nIn this course, we’ll refer to each individual measure that you make as a value, each set of such measures for an individual point we will call (somewhat awkwardly) an observation, and each individual type of measurement will be a variable. It is almost certainly the case that you will be collecting this data and storing it in something like a spreadsheet or table, like that shown in Figure 1. You will sometimes hear data scientists refer to data stored in this way as rectangular data. All they mean by that is that the data come as collections of values organized into rows and columns of equal length. While data may admit of many different ways of being organized into rows and columns, here we will focus on a rectangular format known as tidy data. As defined by Hadley Wickham in R for Data Science (2e), tidy data must follow three simple rules:\n\nEach variable must have its own column,\nEach observation must have its own row, and\nEach value must have its own cell.\n\nIt is important to note, as Wickham cautions, that the opposite of tidy data is not necessarily messy data, for data can come in many formats (sound and video, for example). However, when we want to conduct some statistical analysis, and especially when we want to conduct such an analysis in R, we will almost certainly want our data to be tidy.\nCreating tables\nIn R, tabular datasets are known as data frames. To create a data frame, we use the eponymous data.frame() function. Here, for example, is how we would create the table in Figure 1 above:\n\nprojectiles <- data.frame(\n  type = c(\"Elko\", \"Rosegate\", \"DSN\", \"Elko\", \"Clovis\"),\n  length = c(2.03, 1.4, 1.9, 2.1, 3.3),\n  width = c(0.8, 0.4, 0.3, 0.7, 0.95),\n  height = c(3.23, 2.4, 1.29, 2.7, 4.15)\n)\n\nprojectiles\n\n      type length width height\n1     Elko   2.03  0.80   3.23\n2 Rosegate   1.40  0.40   2.40\n3      DSN   1.90  0.30   1.29\n4     Elko   2.10  0.70   2.70\n5   Clovis   3.30  0.95   4.15\n\n\nNote that the values (or measurements) contained in each variable are wrapped in the c() function (short for concatenate). This variable tells R that all these values belong to this one variable. Each variable in turn is an argument to data.frame() having the form <variable> = c(<value-1>, <value-2>, ..., <value-n>).\nGetting basic meta-data from tables\nWhen you want to know what variables a table includes, you can use the names() function.\n\nnames(projectiles)\n\n[1] \"type\"   \"length\" \"width\"  \"height\"\n\n\nIf you want to know how many variables or observations the table has, you can use nrow() and ncol() respectively.\n\n# number of observations\nnrow(projectiles)\n\n[1] 5\n\n# number of variables\nncol(projectiles)\n\n[1] 4\n\n\nExercises\n\nGet the names of the variables in the penguins table with names().\nHow many observations and variables are in this dataset? Hint: use nrow() and ncol().\nExtract the bill_length_mm variable from this table and assign it to an object called bill_length. Do the same for bill_depth_mm and call it bill_depth. You can use either <table>[[<variable>]] or <table>$<variable>."
  },
  {
    "objectID": "labs/03-inference-lab.html#histograms",
    "href": "labs/03-inference-lab.html#histograms",
    "title": "Lab 03: Statistical Inference",
    "section": "Histograms",
    "text": "Histograms\nA histogram is an excellent aid for visualizing the distribution of numerical data. Making one involves “binning” a continuous variable, counting the number of its values that fall into each bin, then drawing a rectangle for each bin whose height is proportional to the count for that bin. A good example of this is the distribution of a population over age ranges, like the age distribution of the passengers on the ill-fated voyage of the Titanic. Before we get to what the histogram looks like for this macabre example, let’s first have a look at the raw data:11 These data come from Claus Wilke’s Fundamentals of Data Visualization.\n\n\n\n\n\n\nWarning\n\n\n\n\n\nTo be able to work through the examples in this section, you need to run the code below. We’ll explain what this code does in another lab.\n\ntitanic <- read.csv(\"https://raw.githubusercontent.com/wilkelab/SDS375/master/datasets/titanic.csv\")\n\n\n\n\n\nhead(titanic)\n\n  class   age    sex survived\n1   1st 29.00 female survived\n2   1st  2.00 female     died\n3   1st 30.00   male     died\n4   1st 25.00 female     died\n5   1st  0.92   male survived\n6   1st 47.00   male survived\n\n\nYou see that our data has an age for each individual passenger. If we bin those ages into 5 year intervals (0-5, 5-10, 10-15, and so on) and count the number of passengers that fall into each bin or age range, we get a summary table that looks like this.\n\n\n\n\n  \n  \n\nAge Range\n      Count\n       \n      Age Range\n      Count\n    \n\n\n0–5\n36\n   \n41–45\n54\n\n\n6–10\n19\n   \n46–50\n50\n\n\n11–15\n18\n   \n51–55\n26\n\n\n16–20\n99\n   \n56–60\n22\n\n\n21–25\n139\n   \n61–65\n16\n\n\n26–30\n121\n   \n66–70\n3\n\n\n31–35\n76\n   \n71–75\n3\n\n\n36–40\n74\n   \n76–80\n0\n\n\n\n\n\n\nWe can actually visualize this distribution with a histogram using ggplot() and geom_histogram(). Importantly, we pass ggplot() the raw tidy data, not the summary table. Here is how that looks:\n\nggplot(titanic) + \n  geom_histogram(\n    aes(age)\n  )\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\nNotice that ggplot2 provides an informative message that the default number of bins used in this plot is 30, which makes the age interval for each bin about 2.5 years. We can (and SHOULD!) change this by specifying a different number of bins with bins, as the number of bins can dramatically change the interpretation of the distribution.\n\nggplot(titanic) +\n  geom_histogram(\n    aes(age),\n    bins = 10\n  )\n\n\n\n\n\n\n\nUnfortunately, the default histogram produced by ggplot2 doesn’t do a great job of conveying the relationship between the axis text and the ranges represented by the widths of the rectangles. For instance, zero falls at the center of the first histogram, which would seem to suggest that the range of values represented by that rectangle includes negative ages. What it is really saying is that zero falls in the range of that bin, but that is not obvious. One solution to this issue is to set the binwidth (or bin width, as in the range of values for that bin) and the bin boundary (or where the left side of the rectangle is positioned relative to the bin range). That’s a tad confusing, I know, but the basic idea is that if you set the width to 5 and the boundary to 0, then the bin for the range 0-5 will start at 0, and the bin for the range 5-10 will start at 5, and the bin for 10-15 will start at 10, and so on. Finding the best options for visualizing your data will, of course, involve some trial and error.\n\nggplot(titanic) +\n  geom_histogram(\n    aes(age),\n    binwidth = 5,\n    boundary = 0\n  )\n\n\n\n\n\n\n\nAs always, you can change the fill and outline color for these plots by supplying those parameters to the geometry. We can also clean up the labels with labs() and change the theme if we like (and we do). And since the heights of these bars are what we care about, we can turn off the light vertical grid lines that ggplot2 adds by default by specifying this as a theme() option.\n\nggplot(titanic) +\n  geom_histogram(\n    aes(age),\n    binwidth = 5,\n    boundary = 0,\n    fill = \"#56B4E9\",\n    color = \"#01587A\"\n  ) +\n  labs(\n    x = \"Age (years)\",\n    y = \"Count\",\n    title = \"Age Distribution\",\n    subtitle = \"Passengers of the Titanic\"\n  ) +\n  theme_minimal(14) +\n  theme(\n    panel.grid.major.x = element_blank(),\n    panel.grid.minor.x = element_blank()\n  )\n\n\n\n\n\n\n\nOne last point before moving on. You may have noticed that the data contains a binary variable survived. Do you think maybe the age distribution of those that survived differs from the distribution for those that did not survive? Do you think maybe they were older? Younger? Or is there no difference? Let’s explore this a little by visualizing the different distributions using facet_wrap().\n\nggplot(titanic) +\n  geom_histogram(\n    aes(age),\n    binwidth = 5,\n    boundary = 0,\n    fill = \"#56B4E9\",\n    color = \"#01587A\"\n  ) +\n  labs(\n    x = \"Age (years)\",\n    y = \"Count\",\n    title = \"Age Distribution\",\n    subtitle = \"Passengers of the Titanic\"\n  ) +\n  facet_wrap(vars(survived)) + # <----- adding facets here\n  theme_minimal(14) +\n  theme(\n    panel.grid.major.x = element_blank(),\n    panel.grid.minor.x = element_blank()\n  )\n\n\n\n\n\n\n\nHmmmm 🤔. What do you think this means? And how might it help to compare this to the total distribution?\nExercises\n\nCreate a histogram of penguin bill length using the penguins dataset. Then do all of the following:\n\nChange the number of bins (try two different options).\nTry specifying the bin width and boundary.\nChange the fill and outline color.\nReset the labels to be more informative.\nChange the theme and remove the vertical grid lines.\nFacet the distributions by penguin species.\n\n\nRepeat (1), but use the DartPoints dataset from the archdata package, creating a histogram of dart length (Length in the table) and facet by dart type (Name).\n\nDoes it look like the dart types might differ in length? Or maybe they’re all basically the same?"
  },
  {
    "objectID": "labs/03-inference-lab.html#t-test",
    "href": "labs/03-inference-lab.html#t-test",
    "title": "Lab 03: Statistical Inference",
    "section": "t-test",
    "text": "t-test\nWe use a t-test (technically, Welch’s two-sample t-test) to evaluate whether two samples come from the same population. This test starts by computing the \\(t\\) statistic, or the standardized difference in sample means:\n\\[t = \\frac{\\bar{x}_1 - \\bar{x}_2}{s_{\\bar{x}}}\\]\nwhere \\(s_{\\bar{x}}\\) is the standard error of the sample mean. This is then compared to a t-distribution to determine the probability of that difference - the more improbable, the less likely the samples come from the same distribution. Why is that? Well, if you think about it, we are starting from the assumption that the null hypothesis is true. Then we are asking, if the null hypothesis is true, how likely is that we would get this difference in sample means? And, if it’s extremely unlikely, that would presumably count against the null model being true.\nTo perform this test in R, we use the t.test() function, providing it with two samples. Suppose, for example, that we have two samples of projectile points, namely their lengths in millimeters, and we want to answer this question: Are these samples of the same point type (the same population)? Here are our two samples:\n\nsample1 <- c(59.10, 61.97, 56.23, 53.83, 60.24, 44.27, 61.41, 55.07, 55.01, 50.56)\nsample2 <- c(58.42, 68.09, 60.85, 61.60, 57.25, 63.08, 58.57, 57.34, 64.60, 60.49)\n\nmean(sample1) - mean(sample2)\n\n[1] -5.26\n\n\nBefore we run the test, let’s make sure we have our hypotheses clear.\n\n\n\\(H_0\\): There is NO difference in mean length (\\(\\bar{x}_1 = \\bar{x}_2\\))\n\n\\(H_1\\): There is a difference in mean length (\\(\\bar{x}_1 \\neq \\bar{x}_2\\))\n\nWe also need to specify our critical value. Here, we’ll stick with a common standard of 0.05:\n\n\\(\\alpha = 0.05\\)\n\nNow, we can run our test!\n\nt.test(sample1, sample2)\n\n\n    Welch Two Sample t-test\n\ndata:  sample1 and sample2\nt = -3, df = 15, p-value = 0.02\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -9.598 -0.922\nsample estimates:\nmean of x mean of y \n     55.8      61.0 \n\n\nNotice the output that R has provided here. You have the name of the test: Welch Two Sample t-test (this is a version of Student’s t-test for two independent samples). It gives you the t-statistic, the degrees of freedom (df), and the p-value. It also states the alternative hypothesis and gives you the mean of each sample. In this case, \\(p < \\alpha\\). Hence, we reject the null. A significant difference exists between the means of these two samples.\nIf your samples are in a data.frame, you can also call t.test() using R’s formula syntax. So, assume your data above are in a table like so:\n\nn <- length(sample1)\n\nsamples <- data.frame(\n  sample = c(rep(1, n), rep(2, n)),\n  length = c(sample1, sample2)\n)\n\nhead(samples)\n\n  sample length\n1      1   59.1\n2      1   62.0\n3      1   56.2\n4      1   53.8\n5      1   60.2\n6      1   44.3\n\n\nThen you would run the t.test() this way:\n\nt.test(length ~ sample, data = samples)\n\n\n    Welch Two Sample t-test\n\ndata:  length by sample\nt = -3, df = 15, p-value = 0.02\nalternative hypothesis: true difference in means between group 1 and group 2 is not equal to 0\n95 percent confidence interval:\n -9.598 -0.922\nsample estimates:\nmean in group 1 mean in group 2 \n           55.8            61.0 \n\n\nThe formula in this example is length ~ sample. The tilde expresses a relation of dependence. In this case, we want to know whether the length of a point differs in some meaningful way between samples. So, here we are, in effect, asking R to run a t-test comparing the mean length in each sample using the samples dataset. You will notice that the results are the same, we just called the function in a slightly different way because of how we had the data stored.\nExercises\n\nPerform a t-test on these two samples of dart length (drawn from the DartPoints dataset).\n\nCalculate the mean for each sample.\n\nBe sure to specify the null and alternate hypotheses.\n\nState the critical value.\n\nRun the test and print the results.\n\n\n\n\ndarl <- c(42.8,40.5,37.5,40.3,30.6,41.8,40.3,48.5,47.7,33.6,32.4,42.2,33.5,\n          41.8,38,35.5,31.2,34.5,33.1,32,38.1,47.6,42.3,38.3,50.6,54.2,44.2,40)\ntravis <- c(56.5,54.6,46.3,57.6,49.1,64.6,69,40.1,41.5,46.3,39.6)\n\n\nBased on the test, do you accept or reject the null hypothesis? Why?"
  },
  {
    "objectID": "labs/03-inference-lab.html#anova",
    "href": "labs/03-inference-lab.html#anova",
    "title": "Lab 03: Statistical Inference",
    "section": "ANOVA",
    "text": "ANOVA\nLike the t-test, the ANOVA is used to test for a difference between samples, to see whether they come from the same population. Unlike the t-test, however, the ANOVA can be used on more than two samples. It does that by decomposing the total variance into within and between group variance. The ratio of those standardized variances defines the F-statistic:\n\\[F = \\frac{\\text{between-group variance}}{\\text{within-group variance}}\\]\nBy comparing the value of \\(F\\) to the F-distribution, we can evaluate the probability of getting that value. A highly improbable F-statistic means that at least one group or sample comes from a different population. To perform this test in R, we use the aov() function.\nTo illustrate this method, let’s return to the DartPoints dataset, which contains various measures for five different dart point types (Darl, Ensor, Pedernales, Travis, and Wells). We’ll try to answer the question: Are these samples from the same dart point type (the same population)? Less formally, we want to know whether the types we have lumped these darts into are really all that different. As always, we first specify our null and alternate hypotheses and the critical value we will use to determine whether to reject the null.\n\n\n\\(H_0\\): There is no difference in length between groups.\n\n\\(H_1\\): At least one group differs in length.\n\nAnd our critical value, again, will be 0.05.\n\n\\(\\alpha = 0.05\\)\n\nNow, we can conduct our test.\n\ndata(DartPoints)\n\naov_test <- aov(Length ~ Name, data = DartPoints)\n\nThere are two things to note here. First, we are assigning the output of this ANOVA test to an object. Second, we call the test using a formula, in this case Length ~ Name. The full call to aov() you can read as saying, “Perform an analysis of variance comparing the lengths for each sample in this dataset.”\nANOVA Table\nIf we print out the result of the aov() call, it looks like this:\n\naov_test\n\nCall:\n   aov(formula = Length ~ Name, data = DartPoints)\n\nTerms:\n                Name Residuals\nSum of Squares  5532      9067\nDeg. of Freedom    4        86\n\nResidual standard error: 10.3\nEstimated effects may be unbalanced\n\n\nWhile this printout offers some important information, a much more useful ummary of the test is provided by an ANOVA table. We can generate one of these by running the summary() function on our test.\n\nsummary(aov_test)\n\n            Df Sum Sq Mean Sq F value      Pr(>F)    \nName         4   5532    1383    13.1 0.000000022 ***\nResiduals   86   9067     105                        \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThis table has two rows, one for the between-group variance and one for the within-group variance respectively. As you can see, R refers to these, somewhat cryptically, as the Name (the grouping variable in the data) and Residuals. The reason for this concerns the fact that the aov() function is actually fitting a linear model, but let’s leave that detail to the side. For now, just note that the columns are, in order,\n\n\nDf = the degrees of freedom,\n\n\nSum Sq = the sum of squares,\n\n\nMean Sq = the mean sum of squares (the sum of squares divided by the degrees of freedom),\n\n\nF value = the F-statistic (the ratio of the mean sum of squares), and\n\n\nPr(>F) = the p-value, formally the probability of getting a value of the statistic greater than observed (determined by comparing the F-statistic to the F-distribution).\n\nIn this case, \\(p < \\alpha\\), so we reject the null hypothesis, meaning that at least one of these groups is different.\nExercises\n\nPerform an ANOVA on the DartPoints dataset to see if at least one dart type differs signficantly in its Width.\n\nBe sure to specify the null and alternate hypotheses.\nState the critical value.\nRun the test and print the results.\nBe sure to assign the test to an object.\nProvide a summary() of the test.\n\n\nBased on the test, do you accept or reject the null hypothesis? Why?"
  },
  {
    "objectID": "labs/03-inference-lab.html#homework",
    "href": "labs/03-inference-lab.html#homework",
    "title": "Lab 03: Statistical Inference",
    "section": "Homework",
    "text": "Homework\n\nLoad the Snodgrass dataset from the archdata package using data(Snodgrass). This dataset includes measurements of pithouses found in a small village affiliated with maize farmers in Missouri about 650 years ago.\nGenerate a summary table using the skim() function from skimr.\nFor practice, let’s get some of those summary statistics manually. Calculate the mean and standard deviation of the inside floor area of each pithouse using mean() and sd(). You’ll need to pull the Area variable out of the table with Snodgrass$Area. Hint: if you were to calculate the variance, you would use var(Snodgrass$Area).\nUse ggplot() and geom_histogram() to plot a histogram showing the distribution of floor area. Make sure to do all of the following (only need to make one graph):\n\nChange the default number of bins (or optionally, you can specify the breaks).\nChange the fill and outline color.\nUpdate the labels and the plot title.\nChoose an appropriate theme and remove the vertical grid lines.\nFacet by the Inside variable. Then try faceting using the Segment variable.\nDo the distributions look different?\n\n\nSome of these pithouses occur inside a walled-in portion of the village. This information is provided by the Inside variable in the dataset. Run a t-test to determine whether the mean floor area of pithouses inside the walled-in area differs significantly from the mean floor area of pithouses outside the wall.\n\nThese data are in a table, so make sure to use the formula notation.\nState the null and alternative hypotheses.\nSpecify the critical value.\nRun the test and print the result.\nDoes the test confirm or refute the null hypothesis? Why?\nHow would you interpret this result?\n\n\nThe pithouses were initially divided into three groups or “segments” (Segment in the table) based on their location: segment one includes those inside the wall, segment two those to the north and west of the wall, and segment three those to the east and south of the wall. Run an ANOVA test to determine whether mean floor area for these groups is significantly different.\n\nState the null and alternative hypotheses.\nSpecify the critical value.\nRun the test and print the result.\nBe sure to assign the result to an object.\nProvide a summary() of the test.\nDoes the test confirm or refute the null hypothesis? Why?\nHow would you interpret this result?"
  },
  {
    "objectID": "labs/04-ols-lab.html",
    "href": "labs/04-ols-lab.html",
    "title": "Lab 04: Ordinary Least Squares",
    "section": "",
    "text": "This lab will guide you through the process of\n\nindexing data.frames with base R\nvisualizing distributions with density plots\ncalculating covariance\ncalculating correlation and evaluating with the t-test\nbuilding a simple linear model\n\nthe formula notation\nthe lm() function\nthe model summary()\n\n\n\n\nWe will be using the following packages:\n\narchdata\nggplot2\npalmerpenguins\nviridis\n\n\nlibrary(archdata)\nlibrary(ggplot2)\nlibrary(palmerpenguins)\nlibrary(viridis)\n\n\n\n\ncars\n\nIncludes measurements of car speed and stopping distance.\npackage: datasets\n\nreference: https://stat.ethz.ch/R-manual/R-devel/library/datasets/html/cars.html\n\n\n\n\nDartPoints\n\nIncludes measurements of 91 Archaic dart points recovered during surface surveys at Fort Hood, Texas.\npackage: archdata\n\nreference: https://cran.r-project.org/web/packages/archdata/archdata.pdf\n\n\n\n\nOxfordPots\n\nIncludes percentages of late Romano-British Oxford Pottery on 30 sites, along with their distance from Oxford.\nreference: https://cran.r-project.org/web/packages/archdata/archdata.pdf\n\n\n\n\npenguins\n\nIncludes measurements for penguin species, island in Palmer Archipelago, size (flipper length, body mass, bill dimensions), and sex.\npackage: palmerpenguins\n\nreference: https://allisonhorst.github.io/palmerpenguins/reference/penguins.html\n\n\n\n\ntitanic\n\nProvides information on the fate of passengers of the Titanic, including economic status, sex, age, and survival.\npackage: none\nreference: https://wilkelab.org/SDS375/\n\nnote: we’re going to download this one rather than get it from a package"
  },
  {
    "objectID": "labs/04-ols-lab.html#indexing-tables-with-base-r",
    "href": "labs/04-ols-lab.html#indexing-tables-with-base-r",
    "title": "Lab 04: Ordinary Least Squares",
    "section": "Indexing tables with base R",
    "text": "Indexing tables with base R\n\n\n\n\n\nFigure 1: Elements of a Data Frame.\n\n\nIt will often be the case that you do not need all the data in a table, but only a subset of rows and columns. To get the rows and columns you want, though, you need to be able to, as it were, point to them or point R to them. Another word for this is indexing.\nLet’s start with the use of square brackets, [,]. The basic idea here is that you can take a table and index it by row and column by appending the square brackets to it. The basic syntax is this:\ntable[row,column]\nAs an example, let’s say we are working with our simple projectile point table:\n\nprojectiles <- data.frame(\n  type = c(\"Elko\", \"Rosegate\", \"DSN\", \"Elko\", \"Clovis\"),\n  length = c(2.03, 1.4, 1.9, 2.1, 3.3),\n  width = c(0.8, 0.4, 0.3, 0.7, 0.95),\n  height = c(3.23, 2.4, 1.29, 2.7, 4.15)\n)\n\nprojectiles\n\n      type length width height\n1     Elko   2.03  0.80   3.23\n2 Rosegate   1.40  0.40   2.40\n3      DSN   1.90  0.30   1.29\n4     Elko   2.10  0.70   2.70\n5   Clovis   3.30  0.95   4.15\n\n\nAnd maybe we want the value at the 3rd row and 2nd column, so we’re wanting the length of that particular desert side-notched (or DSN). Here is one way to do that with just the numeric position (or coordinates) of that value:\n\nprojectiles[3,2]\n\n[1] 1.9\n\n\nWhile we did specify both a row and a column in this example, that is not required.\n\nprojectiles[3,]\n\n  type length width height\n3  DSN    1.9   0.3   1.29\n\nprojectiles[,2]\n\n[1] 2.03 1.40 1.90 2.10 3.30\n\n\n\n\n\n\n\n\nDanger\n\n\n\n\n\nDid you notice the different outputs? projectiles[3,] returns a data.frame, but projectiles[,2] returns a vector. This is a “gotcha” in R, a little bit of unexpected behavior. The most common situation in which indexing returns a vector from a data.frame is when a single variable is selected. Sometimes getting just the variable is intentional (see below), but other times it is not, so it’s worth being aware of.\n\n\n\nWe can also subset multiple rows and columns, though this requires that we use vectors of data, and not just single values. A useful tool in this regard is the colon, :, which allows you to create a sequence of integers, starting with the number on the left and proceeding by one to the number on the right.\n\n1:3\n\n[1] 1 2 3\n\n\nNow, we can use this syntax to index the last four rows of our table and the first three columns.\n\nprojectiles[2:5, 1:3]\n\n      type length width\n2 Rosegate    1.4  0.40\n3      DSN    1.9  0.30\n4     Elko    2.1  0.70\n5   Clovis    3.3  0.95\n\n\nIf we want to get rows or columns that are not next to each other in the table, we can use the c() function, as in concatenate.\n\nc(1,2,4)\n\n[1] 1 2 4\n\n\nWhen applied to the projectiles table, we get the following.\n\nprojectiles[c(2,4), c(1,2,4)]\n\n      type length height\n2 Rosegate    1.4    2.4\n4     Elko    2.1    2.7\n\n\nImportantly, you can also index columns by name.\n\nprojectiles[1:3, c(\"type\", \"length\")]\n\n      type length\n1     Elko   2.03\n2 Rosegate   1.40\n3      DSN   1.90\n\n\nOne advantage of using names rather than numbers is that it is much more readable as it is not immediately obvious with numbers what columns you are actually selecting. More importantly, though, using names is more robust. Were the length column for whatever reason to move to the right of the height column, its numeric position in the table would be 4, not 2. So, using projectiles[,2] will work to index the length variable only if length is at that position. Using projectiles[,\"length\"] to index it will work either way, though, regardless of the numeric position of that variable.\nSo, that’s pretty much the basics of indexing rectangular data with base R. Before moving on, though, let’s talk about one additional thing you might want to do with a data.frame, and that’s extract an entire variable or column. There are two primary ways to achieve this. You can use double brackets, <table>[[<variable>]], or you can use the dollar-sign operator, <table>$<variable>.\n\nprojectiles[[\"type\"]]\n\n[1] \"Elko\"     \"Rosegate\" \"DSN\"      \"Elko\"     \"Clovis\"  \n\nprojectiles$length\n\n[1] 2.03 1.40 1.90 2.10 3.30\n\n\nNote that you can and often will want to assign these to their own objects, so you can use them again later.\n\np_type <- projectiles[[\"type\"]]\n\np_length <- projectiles$length\n\nAnd, if you want, you can index specific values in the vector as you would rows in the table.\n\nprojectiles$length\n\n[1] 2.03 1.40 1.90 2.10 3.30\n\nprojectiles$length[c(1,2,4)]\n\n[1] 2.03 1.40 2.10\n\n\nExercises\n\nGet the names of the variables in the penguins table with names().\nHow many observations and variables are in this dataset? Hint: use nrow() and ncol().\nTry indexing multiple rows and columns of the penguins data using the square brackets with row numbers and column names, for example, penguins[1:25, c(\"species\", \"island\", \"body_mass_g\")]. Try doing this a couple of different ways.\nExtract the bill_length_mm variable from this table and assign it to an object called bill_length. Do the same for bill_depth_mm and call it bill_depth. You can use either <table>[[<variable>]] or <table>$<variable>."
  },
  {
    "objectID": "labs/04-ols-lab.html#density-plots",
    "href": "labs/04-ols-lab.html#density-plots",
    "title": "Lab 04: Ordinary Least Squares",
    "section": "Density Plots",
    "text": "Density Plots\nHere you will learn how to generate a “density” plot or a plot of the underlying probability density of a variable using ggplot() and geom_density(). This is similar to a histogram in that it seeks to show the distribution of a continuous random variable. It differs, however, in the way it goes about doing that. To see how they differ, let’s first have a look at these two plots of the titanic data. The one on the left is a histogram, the one on the right a density plot.\n\n\n\n\n\n\nWarning\n\n\n\n\n\nTo be able to work through the examples in this section, you need to run the code below. We’ll explain what this code does in another lab.\n\ntitanic <- read.csv(\"https://raw.githubusercontent.com/wilkelab/SDS375/master/datasets/titanic.csv\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe big difference here is that histograms discretize a sample using bins, counts the number of observations that fall into each bin, and then represents those counts using the heights of the rectangles. A density plot uses a method known as kernel density estimate (or KDE) to estimate the density of each observation and represents it using the height of a smooth and continuous curve. The KDE approach works like this. First, you set each data point as the mean of a distribution, typically the normal or Gaussian distribution (also called the kernel). Each distribution is assumed to have the same varianc eor standard deviation (called the bandwidth), which is set to some arbitrary value. The heights of the kernels are then summed to produce a curve like the one above.\nAs with the histogram, we specify a density geometry for ggplot using geom_density().\n\nggplot(titanic, aes(age)) + \n  geom_density()\n\n\n\n\nAgain, we can specify different aesthetics like fill and color and update the labels with labs().\n\nggplot(titanic, aes(age)) + \n  geom_density(\n    fill = \"#A8BFF0\", \n    color = \"#183C8C\"\n  ) + \n  labs(\n    x = \"Age\",\n    y = \"Density\",\n    title = \"Age Distribution\",\n    subtitle = \"Passengers of the Titanic\"\n  )\n\n\n\n\nWe can also map these aesthetics to other variables like the sex of the passenger.\n\nggplot(titanic, aes(age, fill = sex)) + \n  geom_density() + \n  labs(\n    x = \"Age\",\n    y = \"Density\",\n    title = \"Age Distribution\",\n    subtitle = \"Passengers of the Titanic\"\n  )\n\n\n\n\nAnd, we can change the default fill colors using scale_fill_manual(), too.\n\nggplot(titanic, aes(age, fill = sex)) + \n  geom_density() + \n  scale_fill_manual(\n    values = c(\"#A8BFF0\", \"#FFE66D\")\n  ) +\n  labs(\n    x = \"Age\",\n    y = \"Density\",\n    title = \"Age Distribution\",\n    subtitle = \"Passengers of the Titanic\"\n  )\n\n\n\n\nIn this case, however, it’s hard to see where the two distributions overlap, which makes it hard to compare them. One way to fix this is to change the opacity of the fill color using the alpha() function (note that the alpha or transparency of a color can range from 0 to 1.).\n\nggplot(titanic, aes(age, fill = sex)) + \n  geom_density() + \n  scale_fill_manual(\n    values = alpha(c(\"#A8BFF0\", \"#FFE66D\"), 0.5)\n  ) +\n  labs(\n    x = \"Age\",\n    y = \"Density\",\n    title = \"Age Distribution\",\n    subtitle = \"Passengers of the Titanic\"\n  )\n\n\n\n\nThis is still a little hard to read, so let’s try faceting instead of alpha. Let’s also drop the background vertical grid lines using the theme() function. At the same time, we’ll go ahead and drop the label “sex” from the legend as that should be obvious from the key. We do that by setting name = NULL in scale_fill_manual().\n\nggplot(titanic, aes(age, fill = sex)) + \n  geom_density() + \n  scale_fill_manual(\n    name = NULL,\n    values = c(\"#A8BFF0\", \"#FFE66D\")\n  ) +\n  labs(\n    x = \"Age\",\n    y = \"Density\",\n    title = \"Age Distribution\",\n    subtitle = \"Passengers of the Titanic\"\n  ) +\n  facet_wrap(~sex) +\n  theme_minimal() +\n  theme(\n    panel.grid.major.x = element_blank(),\n    panel.grid.minor.x = element_blank(),\n  )\n\n\n\n\nNotice that the facet labels are redundant with the legend key here, so let’s remove those, too. We do that by setting the theme arguments strip.background and strip.text to element_blank(). Finally, we can move the legend to the bottom of the plot and make it horizontal with legend.position and legend.direction respectively.\n\nggplot(titanic, aes(age, fill = sex)) + \n  geom_density() + \n  scale_fill_manual(\n    name = NULL,\n    values = c(\"#A8BFF0\", \"#FFE66D\")\n  ) +\n  labs(\n    x = \"Age\",\n    y = \"Density\",\n    title = \"Age Distribution\",\n    subtitle = \"Passengers of the Titanic\"\n  ) +\n  facet_wrap(~sex) +\n  theme_minimal() +\n  theme(\n    legend.position = \"bottom\",\n    legend.direction = \"horizontal\",\n    panel.grid.major.x = element_blank(),\n    panel.grid.minor.x = element_blank(),\n    strip.background = element_blank(),\n    strip.text = element_blank()\n  )\n\n\n\n\nExercises\n\nMake a kernel density plot of penguin bill length using ggplot() and geom_density(). Then make all of the following changes:\n\nMap penguin species to the fill aesthetic.\nUpdate the axis labels and plot title using labs().\nUse scale_fill_viridis to use colorblind safe colors for the fill. Note! Species is a discrete or categorical variable, so make sure to set discrete = TRUE!\nUse facet_wrap() to facet by species.\nChoose a suitable theme, like theme_minimal().\nRemove vertical grid lines.\nChange the legend position to bottom and make it horizontal.\nRemove strip text and background."
  },
  {
    "objectID": "labs/04-ols-lab.html#bivariate-statistics",
    "href": "labs/04-ols-lab.html#bivariate-statistics",
    "title": "Lab 04: Ordinary Least Squares",
    "section": "Bivariate Statistics",
    "text": "Bivariate Statistics\nBivariate statistics provide simple measures of the relationship between two variables. Here we will learn how to calculate two such statistics in R: covariance and correlation. These allow us to describe the direction of the relationship (is it positive or negative?) and the strength of the relationship (is it strong or weak?). In this case, we’ll investigate the relationship between penguin body mass and bill length. We’ll be asking this question: Is there a relationship between bill length and body mass? Is it positive or negative?\nBefore we do that, however, it is useful to visualize our data. Since we are concerned with a potential relationship, we will use a scatterplot, or a cloud of points arrayed along the dimensions of two variables, in this case body mass and bill length.\n\nggplot(penguins, aes(x = body_mass_g, y = bill_length_mm)) + \n  geom_point(\n    shape = 21,\n    fill = \"#A8BFF0\",\n    color = \"#15357A\",\n    size = 2\n  ) +\n  labs(\n    x = \"Body Mass (g)\",\n    y = \"Bill Length (mm)\",\n    title = \"Palmer Penguins\"\n  ) +\n  theme_bw()\n\n\n\n\nWhat does this tell you about the relationship between these variables?\nCovariance\nCovariance provides a measure of the extent to which two variables vary together. The sign of the covariance reflects a positive or negative trend, but not magnitude. To calculate this value in R, use the cov() function.\n\nbill_length <- penguins$bill_length_mm\nbody_mass_g <- penguins$body_mass_g\n\ncov(bill_length, body_mass_g, use = \"complete.obs\") # complete.obs means ignore NA values\n\n[1] 2606\n\n\nThis is a positive number, meaning the relationship between bill length and body mass is positive (the one tends to increase as the other increases). The size of the number by itself is unhelpful, however, and cannot be used to infer anything about the strength of the relationship. That is because covariance is sensitive to the unit of measure. If, for example, we convert body_mass from grams to kilograms, we will get a different covariance statistic.\n\n# convert to kilograms by dividing by 1000\nbody_mass_kg <- body_mass_g/1000\n\ncov(bill_length, body_mass_kg, use = \"complete.obs\")\n\n[1] 2.61\n\n\nCorrelation\nTo remove units of measure and prevent resulting changes in the magnitude of the covariance, we can scale the covariance by the standard deviations of the samples. The resulting value is known as Pearson’s Correlation Coefficient, which ranges from -1 to 1.\n\ncor(bill_length, body_mass_g, use = \"complete.obs\")\n\n[1] 0.595\n\n\nJust to demonstrate that this isn’t sensitive to units of measure, let’s see what happens when use body mass measures in kilograms.\n\ncor(bill_length, body_mass_kg, use = \"complete.obs\")\n\n[1] 0.595\n\n\nThere’s no change! In either case, the resulting coefficient is greater than zero, suggesting a positive trend, but is this value significantly different than zero? To answer that question, we can convert this coefficient to a t-statistic and compare it to a t-distribution. This is done with the cor.test() function. For this test, we have the following hypotheses:\n\n\n\\(H_0\\): the coefficient is equal to zero\n\n\\(H_1\\): the coefficient is not equal to zero\n\nAnd, of course, we must stipulate a critical value. In this case, we will stick with tradition:\n\\(\\alpha = 0.05\\)\nSo, now, here is our test:\n\ncor.test(bill_length, body_mass_g, use = \"complete.obs\")\n\n\n    Pearson's product-moment correlation\n\ndata:  bill_length and body_mass_g\nt = 14, df = 340, p-value <0.0000000000000002\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.522 0.660\nsample estimates:\n  cor \n0.595 \n\n\nIn this case, you see that \\(p < \\alpha\\), hence we reject the null hypothesis, meaning our coefficient estimate is significantly different than zero. There is, in other words, a significant positive relationship between body mass and bill length among the Palmer penguins.\nExercises\n\nUsing the penguins dataset, do all of the following:\n\ncalculate the covariance between bill length and bill depth,\ncalculate Pearson’s Correlation Coefficient for bill length and bill depth,\ndo a correlation test to determine whether the coefficient is significantly different than zero, and\nbe sure to state your null and alternative hypotheses, as well as the critical value!\n\n\nWhat does the correlation test tell you about the relationship between bill length and bill depth?"
  },
  {
    "objectID": "labs/04-ols-lab.html#linear-models",
    "href": "labs/04-ols-lab.html#linear-models",
    "title": "Lab 04: Ordinary Least Squares",
    "section": "Linear Models",
    "text": "Linear Models\nIn this section, we will learn how to fit a linear model to our data. We will look, specifically, at a scenario involving an experiment with cars recorded in the cars dataset. We want to know what kind of relationship there is between the distance (in feet) a car travels after the brakes are applied and the speed (in miles per hour) the car was going when the brakes were applied. We will be doing this by fitting a linear model with the lm() function. Here are our hypotheses:\n\n\n\\(H_0\\): there is no relationship between speed and distance.\n\n\\(H_1\\): there is a relationship between speed and distance.\n\nModel formula\nFirst, however, let’s discuss the formula syntax that the lm() function uses. You were already introduced to this with the t.test(), but let’s go into a little more detail now. To fit a model, we must first specify a formula. This involves three components: a predictor variable, the tilde ~, and a response variable. The syntax is this:\n<response> ~ <predictor> or <dependent> ~ <independent>\nIn the case of the cars data, that’s:\ndist ~ speed\nThis can be read as saying, in effect, “distance as a function of speed.” Note that you do not have to put the variables in quotes or anything like that. It’s just the names of the variables separated by a tilde.\nModel fitting\nIn addition to specifyfing the formula, we must also tell the lm() function what data set our observations are coming from. We do this by specifying the data argument. The whole function call looks like this:\n\ncars_model <- lm(dist ~ speed, data = cars)\n\ncars_model\n\n\nCall:\nlm(formula = dist ~ speed, data = cars)\n\nCoefficients:\n(Intercept)        speed  \n     -17.58         3.93  \n\n\nHere, the model estimates a coefficient for both the intercept and the relationship between speed and distance.\nModel summary\nA more informative report of the model is provided by the summary() function. In addition to reporting on the model coefficients, this will also conduct a t-test on each coefficient, evaluating whether they are significantly different than zero.\n\nsummary(cars_model)\n\n\nCall:\nlm(formula = dist ~ speed, data = cars)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-29.07  -9.53  -2.27   9.21  43.20 \n\nCoefficients:\n            Estimate Std. Error t value        Pr(>|t|)    \n(Intercept)  -17.579      6.758   -2.60           0.012 *  \nspeed          3.932      0.416    9.46 0.0000000000015 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 15.4 on 48 degrees of freedom\nMultiple R-squared:  0.651, Adjusted R-squared:  0.644 \nF-statistic: 89.6 on 1 and 48 DF,  p-value: 0.00000000000149\n\n\nWe’ll go over this summary() in more detail later. For now, note that it reports the coefficient “Estimate”, the t-statistic (or “t value”) for each coefficient estimate, and the p-value for the respective t-tests. In each case, the null hypothesis is that the coefficient is zero. A small p-value then gives us reason to reject the null and accept the coefficient estimate as significant. In this case, the p-value is very small, so we can accept both the intercept and speed coefficients. This tells us (as you might expect) that there is a significant positive relationship between the speed the car was going when it applied the brakes and the distance it traveled after applying the brakes.\nExercises\n\nUsing the penguins dataset, build a linear model of the relationship between bill length and bill depth.\nWhat are the coefficients reported by this model? Specifically, the intercept and the coefficient of relationship between bill length and bill depth.\nApply the summary() function to your model. Are the coefficients significant?"
  },
  {
    "objectID": "labs/04-ols-lab.html#homework",
    "href": "labs/04-ols-lab.html#homework",
    "title": "Lab 04: Ordinary Least Squares",
    "section": "Homework",
    "text": "Homework\n\nLoad the following datasets from the archdata package using data().\n\nDartPoints\nOxfordPots\n\n\nPractice extracting variables from these tables.\n\nFrom each, remove one variable and assign it to an object with an informative name.\nCalculate the mean and variance for each variable.\n\n\nUsing the DartPoints dataset, make a kernel density plot of dart Length to visualize its distribution. Make sure to do all of the following:\n\nMap the dart Name (or type) to the fill aesthetic.\nUpdate the axis labels and plot title using labs().\nUse scale_fill_viridis to use colorblind safe colors for the fill. Note! Species is a discrete or categorical variable, so make sure to set discrete = TRUE!\nUse facet_wrap() to facet by Name (or type).\nChoose a suitable theme, like theme_minimal().\nRemove vertical grid lines.\nChange the legend position to bottom and make it horizontal.\nRemove strip text and background.\n\n\nUsing the DartPoints dataset, calculate the covariance and correlation between dart length and width.\n\nThen conduct a correlation test to evaluate the significance of Pearson’s Correlation Coefficient.\nBe sure to state the null and alternative hypotheses, as well as the critical value.\nIs the coefficient significant?\nWhat does this mean about the relationship between dart length and width?\n\n\nUsing the DartPoints dataset, build a linear model showing the relationship (if any) between the length and width of dart points. Be sure to do all of the following:\n\nTo visualize the relationship, make a scatter plot of dart Length and Width using ggplot() and geom_point(). Hint: your aesthetic mapping should be aes(x = Width, y = Length).\nUse the correct formula syntax. In this case, the dependent variable is Length and the independent variable is Width.\nUse summary() to report the model.\nAre the coefficient estimates significant?\nWhat does this mean about the relationship between the length and width of dart points? Hint: it’s called allometry.\n\n\nUsing the OxfordPots dataset, build a linear model showing the relationship (if any) between the percentage of Oxford Pots found on an archaeological site and the distance of that site from the city of Oxford. Be sure to do all of the following:\n\nTo visualize the relationship, make a scatter plot of the proportion of Oxford pots and distance to Oxford using ggplot() and geom_point(). Hint: your aesthetic mapping should be aes(x = OxfordDst, y = OxfordPct).\nUse the correct formula syntax. In this case, the dependent variable is OxfordPct and the independent variable is OxfordDst.\nUse summary() to report the model.\nAre the coefficient estimates significant?\nWhat does this mean about the relationship between the proportion of Oxford pots on an archaeological site and distance from Oxford?"
  },
  {
    "objectID": "labs/05-distributions-lab.html",
    "href": "labs/05-distributions-lab.html",
    "title": "Lab 05: Visualizing Distributions",
    "section": "",
    "text": "This lab will guide you through the process of\n\nvisualizing amounts with bar charts\nvisualizing distributions with\n\nhistograms\nprobability density plots\ncumulative distribution plots\nboxplots\n\n\nbase R alternatives to ggplot\n\nWe will be using the following packages:\n\narchdata\npalmerpenguins\ntidyverse\nviridis\n\n\nlibrary(archdata)\nlibrary(palmerpenguins)\nlibrary(tidyverse)\nlibrary(viridis)\n\n\n\n\n\n\n\nNote\n\n\n\nYou do not have to explicitly attach the {graphics} package, as it comes pre-loaded every time you open a new R session. That’s partly what it means for it to be a “base” R package.\n\n\n\n\n\npenguins\n\nIncludes measurements for penguin species, island in Palmer Archipelago, size (flipper length, body mass, bill dimensions), and sex.\npackage: palmerpenguins\n\nreference: https://allisonhorst.github.io/palmerpenguins/reference/penguins.html\n\n\n\n\nDartPoints\n\nIncludes measurements of 91 Archaic dart points recovered during surface surveys at Fort Hood, Texas.\npackage: archdata\n\nreference: https://cran.r-project.org/web/packages/archdata/archdata.pdf\n\n\n\n\ntitanic\n\nProvides information on the fate of passengers of the Titanic, including economic status, sex, age, and survival.\npackage: none\nreference: https://wilkelab.org/SDS375/\n\nnote: we’re going to download this one rather than get it from a package"
  },
  {
    "objectID": "labs/05-distributions-lab.html#bar-charts",
    "href": "labs/05-distributions-lab.html#bar-charts",
    "title": "Lab 05: Visualizing Distributions",
    "section": "Bar charts",
    "text": "Bar charts\nBar charts or bar plots use the length or height of bars to represent the amount of some variable across categories or groups. With ggplot2, you can create a bar chart with geom_bar(). As an example, we’ll use the penguins data set.\n\npenguins\n\n# A tibble: 344 × 8\n   species island    bill_length_mm bill_depth_mm flipper_…¹ body_…² sex    year\n   <fct>   <fct>              <dbl>         <dbl>      <int>   <int> <fct> <int>\n 1 Adelie  Torgersen           39.1          18.7        181    3750 male   2007\n 2 Adelie  Torgersen           39.5          17.4        186    3800 fema…  2007\n 3 Adelie  Torgersen           40.3          18          195    3250 fema…  2007\n 4 Adelie  Torgersen           NA            NA           NA      NA <NA>   2007\n 5 Adelie  Torgersen           36.7          19.3        193    3450 fema…  2007\n 6 Adelie  Torgersen           39.3          20.6        190    3650 male   2007\n 7 Adelie  Torgersen           38.9          17.8        181    3625 fema…  2007\n 8 Adelie  Torgersen           39.2          19.6        195    4675 male   2007\n 9 Adelie  Torgersen           34.1          18.1        193    3475 <NA>   2007\n10 Adelie  Torgersen           42            20.2        190    4250 <NA>   2007\n# … with 334 more rows, and abbreviated variable names ¹​flipper_length_mm,\n#   ²​body_mass_g\n\n\n\nggplot(penguins) +\n  geom_bar(\n    aes(x = species)\n  )\n\n\n\n\nYou can reorder the species variable based on their frequencies (with the most frequent first, the least frequent last) using fct_infreq(), short for factor infrequent. The word ‘factor’ here refers to the way that R represents categorical or grouping variables.\n\nggplot(penguins) +\n  geom_bar(\n    aes(x = fct_infreq(species))\n  )\n\n\n\n\nIt will sometimes be preferable to orient bar charts horizontally. The simplest way to do that is to pass the factor or grouping variable to the “y” argument in the aes(). Notice that you have to change the labels, too, since the count will now be on the x-axis, species on the y-axis.\n\nggplot(penguins) +\n  geom_bar(\n    aes(y = fct_infreq(species)),\n  )\n\n\n\n\nYou will notice now that the Adelie penguins, the most frequent species in our data, appear at the bottom. This is because the ordering starts at the origin, as it did when these were arrayed on the x-axis. In this case and in many other cases, it will make sense to re-order these with the most frequent category or species at the top and the least frequent on the bottom. To do that, we use fct_rev(), short for factor reverse, as in “reverse the order of this factor.”\n\nggplot(penguins) +\n  geom_bar(\n    aes(y = fct_rev(fct_infreq(species)))\n  )\n\n\n\n\nNow, let’s update the default fill color and theme. We’ll also remove most of the grid lines, as they do not contribute to interpreting the data.\n\nggplot(penguins) +\n  geom_bar(\n    aes(y = fct_rev(fct_infreq(species))),\n    fill = \"#6B8F7E\"\n  ) +\n  labs(\n    x = \"Count\", \n    y = NULL, \n    title = \"Palmer Penguins\"\n  ) +\n  theme_bw() +\n  theme(\n    panel.grid.minor = element_blank(), # remove all minor grid lines\n    panel.grid.major.y = element_blank() # remove major grid lines only on the y-axis\n  )\n\n\n\n\nExercises\n\nCreate a bar chart of the counts of dart point types using the DartPoints dataset (the types are stored in the Name variable). Remember to load that data into R with data(\"DartPoints\"). Then do all of the following:\n\nRe-orient the figure horizontally.\nOrder the types by their frequency, with the most frequent on the top, the least frequent on the bottom.\nAdd appropriate labels.\nChange the theme and remove unnecessary grid lines."
  },
  {
    "objectID": "labs/05-distributions-lab.html#histograms",
    "href": "labs/05-distributions-lab.html#histograms",
    "title": "Lab 05: Visualizing Distributions",
    "section": "Histograms",
    "text": "Histograms\nA histogram is an excellent aid for visualizing the distribution of numerical data. Making one involves “binning” a continuous variable, counting the number of its values that fall into each bin, then drawing a rectangle for each bin whose height is proportional to the count for that bin. A good example of this is the distribution of a population over age ranges, like the age distribution of the passengers on the ill-fated voyage of the Titanic. Before we get to what the histogram looks like for this macabre example, let’s first have a look at the raw data:11 These data come from Claus Wilke’s Fundamentals of Data Visualization.\n\n\n\n\n\n\nWarning\n\n\n\n\n\nTo be able to work through the examples in this section, you need to run the code below. We’ll explain what this code does in another lab.\n\ntitanic <- read.csv(\"https://raw.githubusercontent.com/wilkelab/SDS375/master/datasets/titanic.csv\")\n\n\n\n\n\nhead(titanic)\n\n  class   age    sex survived\n1   1st 29.00 female survived\n2   1st  2.00 female     died\n3   1st 30.00   male     died\n4   1st 25.00 female     died\n5   1st  0.92   male survived\n6   1st 47.00   male survived\n\n\nYou see that our data has an age for each individual passenger. If we bin those ages into 5 year intervals (0-5, 5-10, 10-15, and so on) and count the number of passengers that fall into each bin or age range, we get a summary table that looks like this.\n\n\n\n\n  \n  \n\nAge Range\n      Count\n       \n      Age Range\n      Count\n    \n\n\n0–5\n36\n   \n41–45\n54\n\n\n6–10\n19\n   \n46–50\n50\n\n\n11–15\n18\n   \n51–55\n26\n\n\n16–20\n99\n   \n56–60\n22\n\n\n21–25\n139\n   \n61–65\n16\n\n\n26–30\n121\n   \n66–70\n3\n\n\n31–35\n76\n   \n71–75\n3\n\n\n36–40\n74\n   \n76–80\n0\n\n\n\n\n\n\nHere’s how those values look as bins:\n\n\n\n\n\nTo construct the actual histogram, we use ggplot() and geom_histogram(). Importantly, we pass ggplot() the raw tidy data, not the summary table, and in this case, we are going to use the default number of bins. Here is how that looks:\n\nggplot(titanic) + \n  geom_histogram(\n    aes(age)\n  )\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nNotice that ggplot2 provides an informative message that the default number of bins used in this plot is 30, which makes the age interval for each bin about 2.5 years. We can (and SHOULD!) change this by specifying a different number of bins with bins, as the number of bins can dramatically change the interpretation of the distribution.\n\nggplot(titanic) +\n  geom_histogram(\n    aes(age),\n    bins = 10\n  )\n\n\n\n\nUnfortunately, the default histogram produced by ggplot2 doesn’t do a great job of conveying the relationship between the axis text and the ranges represented by the widths of the rectangles. For instance, zero falls at the center of the first histogram, which would seem to suggest that the range of values represented by that rectangle includes negative ages. What it is really saying is that zero falls in the range of that bin, but that is not obvious. One solution to this issue is to set the binwidth (or bin width, as in the range of values for that bin) and the bin boundary (or where the left side of the rectangle is positioned relative to the bin range). That’s a tad confusing, I know, but the basic idea is that if you set the width to 5 and the boundary to 0, then the bin for the range 0-5 will start at 0, and the bin for the range 5-10 will start at 5, and the bin for 10-15 will start at 10, and so on. Finding the best options for visualizing your data will, of course, involve some trial and error.\n\nggplot(titanic) +\n  geom_histogram(\n    aes(age),\n    binwidth = 5,\n    boundary = 0\n  )\n\n\n\n\nAs always, you can change the fill and outline color for these plots by supplying those parameters to the geometry. We can also clean up the labels with labs() and change the theme if we like (and we do). And since the heights of these bars are what we care about, we can turn off the light vertical grid lines that ggplot2 adds by default by specifying this as a theme() option.\n\nggplot(titanic) +\n  geom_histogram(\n    aes(age),\n    binwidth = 5,\n    boundary = 0,\n    fill = \"#6B8F7E\",\n    color = \"#4C6257\",\n  ) +\n  labs(\n    x = \"Age (years)\",\n    y = \"Count\",\n    title = \"Passengers of the Titanic\"\n  ) +\n  theme_minimal() +\n  theme(\n    panel.grid.major.x = element_blank(),\n    panel.grid.minor = element_blank()\n  )\n\n\n\n\nOne last point before moving on. You may have noticed that the data contains a binary variable survived. Do you think maybe the age distribution of those that survived differs from the distribution for those that did not survive? Do you think maybe they were older? Younger? Or is there no difference? Let’s explore this a little by visualizing the different distributions using facet_wrap().\n\nggplot(titanic) +\n  geom_histogram(\n    aes(age),\n    binwidth = 5,\n    boundary = 0,\n    fill = \"#6B8F7E\",\n    color = \"#4C6257\",\n  ) +\n  labs(\n    x = \"Age (years)\",\n    y = \"Count\",\n    title = \"Passengers of the Titanic\"\n  ) +\n  facet_wrap(vars(survived)) + # <----- adding facets here\n  theme_minimal() +\n  theme(\n    panel.grid.major.x = element_blank(),\n    panel.grid.minor.x = element_blank()\n  )\n\n\n\n\nHmmmm 🤔. What do you think this means? And how might it help to compare this to the total distribution?\nExercises\n\nCreate a histogram of penguin bill length using the penguins dataset from the palmerpenguins package. Then do all of the following:\n\nChange the number of bins (try two different options).\nTry specifying the bin width and boundary.\nChange the fill and outline color.\nReset the labels to be more informative.\nChange the theme and remove the vertical grid lines.\n\n\nRepeat (1), but use the DartPoints dataset from the {archdata} package, creating a histogram of dart length (Length in the table).\n\nDoes it look like the dart types might differ in length? Or maybe they’re all basically the same?"
  },
  {
    "objectID": "labs/05-distributions-lab.html#density-plots",
    "href": "labs/05-distributions-lab.html#density-plots",
    "title": "Lab 05: Visualizing Distributions",
    "section": "Density Plots",
    "text": "Density Plots\nHere you will learn how to generate a “density” plot or a plot of the underlying probability density of a variable using ggplot() and geom_density(). This is similar to a histogram in that it seeks to show the distribution of a continuous random variable. It differs, however, in the way it goes about doing that. To see how they differ, let’s first have a look at these two plots of the titanic data. The one on the left is a histogram, the one on the right a density plot.\n\n\n\n\n\n\nWarning\n\n\n\n\n\nTo be able to work through the examples in this section, you need to run the code below. We’ll explain what this code does in another lab.\n\ntitanic <- read.csv(\"https://raw.githubusercontent.com/wilkelab/SDS375/master/datasets/titanic.csv\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe big difference here is that histograms discretize a sample using bins, counts the number of observations that fall into each bin, and then represents those counts using the heights of the rectangles. A density plot uses a method known as kernel density estimate (or KDE) to estimate the density of each observation and represents it using the height of a smooth and continuous curve. The KDE approach works like this. First, you define the kernel, often the Gaussian distribution with constant variance but a mean defined by each observation. Then you define a bandwidth, which is used to scale each kernel. The heights of the kernels are then summed to produce a curve like the one above.\n\n\n\n\n\nAs with the histogram, we specify a density geometry for ggplot using geom_density(). We can also update the fill and outline colors, remove unnecessary grid lines, specify the labels, and choose a simpler theme.\n\nggplot(titanic) + \n  geom_density(\n    aes(age),\n    fill = \"#6B8F7E\",\n    color = \"#4C6257\"\n  ) +\n  theme_minimal() +\n  theme(\n    panel.grid.major.x = element_blank(),\n    panel.grid.minor = element_blank()\n  )\n\n\n\n  labs(\n    x = \"Age (years)\",\n    y = \"Density\",\n    title = \"Passengers of the Titanic\"\n  )\n\n$x\n[1] \"Age (years)\"\n\n$y\n[1] \"Density\"\n\n$title\n[1] \"Passengers of the Titanic\"\n\nattr(,\"class\")\n[1] \"labels\"\n\n\nWe can also map these aesthetics to other variables like the sex of the passenger. And, we can change the default fill colors using scale_fill_manual(), too.\n\nggplot(titanic) + \n  geom_density(\n    aes(age, fill = sex)\n  ) + \n  scale_fill_manual(\n    values = c(\"#A8BFF0\", \"#FFE66D\")\n  ) +\n  labs(\n    x = \"Age (years)\",\n    y = \"Density\",\n    title = \"Passengers of the Titanic\"\n  )\n\n\n\n\nIn this case, however, it’s hard to see where the two distributions overlap, which makes it hard to compare them. One way to fix this is to change the opacity of the fill color using the alpha() function (note that the alpha or transparency of a color can range from 0 to 1.).\n\nggplot(titanic) + \n  geom_density(\n    aes(age, fill = sex)\n  ) + \n  scale_fill_manual(\n    values = alpha(c(\"#A8BFF0\", \"#FFE66D\"), 0.5)\n  ) +\n  labs(\n    x = \"Age\",\n    y = \"Density\",\n    title = \"Passengers of the Titanic\"\n  )\n\n\n\n\nThis is still a little hard to read, so let’s try faceting instead. Let’s also drop the background vertical grid lines using the theme() function. At the same time, we’ll go ahead and drop the label “sex” from the legend as that should be obvious from the key. We do that by setting name = NULL in scale_fill_manual().\n\nggplot(titanic, aes(age, fill = sex)) + \n  geom_density() + \n  scale_fill_manual(\n    name = NULL,\n    values = c(\"#A8BFF0\", \"#FFE66D\")\n  ) +\n  labs(\n    x = \"Age\",\n    y = \"Density\",\n    title = \"Age Distribution\",\n    subtitle = \"Passengers of the Titanic\"\n  ) +\n  facet_wrap(~sex) +\n  theme_minimal() +\n  theme(\n    panel.grid.major.x = element_blank(),\n    panel.grid.minor.x = element_blank(),\n  )\n\n\n\n\nNotice that the facet labels are redundant with the legend key here, so let’s remove those, too. We do that by setting the theme arguments strip.background and strip.text to element_blank(). Finally, we can move the legend to the bottom of the plot and make it horizontal with legend.position and legend.direction respectively.\n\nggplot(titanic, aes(age, fill = sex)) + \n  geom_density() + \n  scale_fill_manual(\n    name = NULL,\n    values = c(\"#A8BFF0\", \"#FFE66D\")\n  ) +\n  labs(\n    x = \"Age\",\n    y = \"Density\",\n    title = \"Age Distribution\",\n    subtitle = \"Passengers of the Titanic\"\n  ) +\n  facet_wrap(~sex) +\n  theme_minimal() +\n  theme(\n    legend.position = \"bottom\",\n    legend.direction = \"horizontal\",\n    panel.grid.major.x = element_blank(),\n    panel.grid.minor.x = element_blank(),\n    strip.background = element_blank(),\n    strip.text = element_blank()\n  )\n\n\n\n\nExercises\n\nMake a kernel density plot of penguin bill length using ggplot() and geom_density(). Then make all of the following changes:\n\nMap penguin species to the fill aesthetic.\nUpdate the axis labels and plot title using labs().\nUse scale_fill_viridis to use colorblind safe colors for the fill. Note! Species is a discrete or categorical variable, so make sure to set discrete = TRUE!\nUse facet_wrap() to facet by species.\nChoose a suitable theme, like theme_minimal().\nRemove vertical grid lines.\nChange the legend position to bottom and make it horizontal.\nRemove strip text and background.\n\n\n\nDo the same as (1), but for dart point length, and substitute dart point type for species."
  },
  {
    "objectID": "labs/05-distributions-lab.html#cumulative-distribution-plots",
    "href": "labs/05-distributions-lab.html#cumulative-distribution-plots",
    "title": "Lab 05: Visualizing Distributions",
    "section": "Cumulative Distribution Plots",
    "text": "Cumulative Distribution Plots\nWhen constructed from a sample, the cumulative distribution is technically referred to as the empirical cumulative distribution function (or eCDF). It has one critical advantage over histograms and probability density plots, namely, that you don’t have to specify a binwidth or bandwidth. That’s because you first order the data from smallest to largest value, then count the number of observations that are equal to or less than each unique value, and increment the cumulative proportion of observations by that amount.\nAs a simple example, consider this vector or sample: (0.3, 2.0, 3.4, 1.2, 2.2, 1.9).\nRearranging it smallest to largest value, we get: (0.3, 1.2, 1.9, 2.0, 2.2, 3.4).\nNow, for each unique value, we count the number of observations that are less than or equal to it, so\n0.0 -> none of them, so 0\n0.3 -> just 0.3, so 1\n1.2 -> 0.3 and 1.2, so 2\n1.9 -> 0.3, 1.2, and 1.9, so 3\n2.0 -> 0.3, 1.2, 1.9, and 2.0, so 4\n2.2 -> 0.3, 1.2, 1.9, 2.0, and 2.2, so 5\n3.4 -> all of them, so 6\nAs proportions of the total sample, which has six observations, that’s\n0.0 -> 0/6 = 0.00\n0.3 -> 1/6 = 0.17\n1.2 -> 2/6 = 0.33 \n1.9 -> 3/6 = 0.50 \n2.0 -> 4/6 = 0.67 \n2.2 -> 5/6 = 0.83\n3.4 -> 6/6 = 1.00\nNow, we can plot that.\n\n\n\n\n\nSo, fewer assumptions here, but it’s also a smidge harder to interpret since it gives you the probability of being less than or equal to x, for example, the probability of being less than or equal to 1.2 is 0.33.\nUnfortunately, plotting the eCDF of, for example, the ages of passengers on the Titanic, is not straightforward with ggplot2 because you have to use what is known as a stat_*() function, in this case, stat_ecdf(), rather than the more familiar geometry functions. This is also an example of when it would be useful to have major grid lines along both axes, so we will only remove the minor ones.\n\nggplot(titanic) +\n  stat_ecdf(\n    aes(age),\n    geom = \"step\",\n    color = \"#4C6257\",\n    linewidth = 1.2\n  ) +\n  theme_minimal() +\n  theme(\n    panel.grid.minor = element_blank()\n  ) +\n  labs(\n    x = \"Age (years)\",\n    y = \"Probability\",\n    title = \"Passengers of the Titanic\"\n  )\n\n\n\n\nYou can facet these, too, if you so desire.\n\nggplot(titanic) +\n  stat_ecdf(\n    aes(age, color = class),\n    geom = \"step\",\n    linewidth = 1.2\n  ) +\n  scale_color_manual(\n    name = \"Class\",\n    values = c(\"#942911\", \"#37423D\", \"#0094C6\")\n  ) +\n  theme_minimal() +\n  theme(\n    panel.grid.minor = element_blank(),\n    legend.justification = c(\"right\", \"bottom\"),\n    legend.position = c(0.98, 0.05)\n  ) +\n  labs(\n    x = \"Age (years)\",\n    y = \"Probability\",\n    title = \"Passengers of the Titanic\"\n  )\n\n\n\n\nTwo things to note here. First, there’s the interpretation. Consider the age 40. If you follow that vertical grid line up to where it intersect the line for each class, you will see that only about 50% of first class passengers were 40 years old or younger, but for second and third class, that number is closer to 80 or even 85%. The implication here is that first class passengers on the Titanic were generally older than second and third class passengers.\nThe second thing to note is that I moved the legend into the plot area by specifying the legend justification and the legend position. The position may consist of character strings like “top” or “bottom” or a vector of x,y coordinates (both ranging from 0 to 1, zero for left and bottom, one for top and right). The justification determines how the legend is oriented relative to the position coordinates. In this case, the bottom right corner of the legend will be at x = 0.98 and y = 0.05. This usually requires some trial and error before finding a position you like.\nExercises\n\nMake an eCDF plot of penguin bill length using ggplot() and stat_ecdf(). Then make all of the following changes:\n\nMap penguin species to the color aesthetic.\nUpdate the axis labels and plot title using labs().\nUse scale_color_viridis to use colorblind safe colors for the fill. Note! Species is a discrete or categorical variable, so make sure to set discrete = TRUE!\nChoose a suitable theme, like theme_minimal().\nRemove minor grid lines on each axis.\nMove the legend into the plot panel.\n\n\nDo the same as (1), but for dart point length, and substitute dart point type for species."
  },
  {
    "objectID": "labs/05-distributions-lab.html#boxplots",
    "href": "labs/05-distributions-lab.html#boxplots",
    "title": "Lab 05: Visualizing Distributions",
    "section": "Boxplots",
    "text": "Boxplots\nIn this section we’ll learn how to make boxplots, which provide a simple but effective way of representing the distribution of a variable. For one or two variables, it’s often better to use a density plot, but if you’re comparing the distributions of lots of variables or lots of samples of the same variable across multiple categories, a density plot can get crowded quick. That’s when it’s useful to turn to boxplots, so we’ll focus on that here.\n\nset.seed(42)\n\ny <- rnorm(250)\n\nlabels <- tibble(\n  y = c(boxplot.stats(y)$stats, max(y)),\n  x = 0.5,\n  label = c(\"1.5 x IQR\", \"first quartile\", \"median\", \"third quartile\", \"1.5 x IQR\", \"outlier\")\n)\n\nggplot(tibble(x = 0, y), aes(x, y)) + \n  stat_boxplot(geom ='errorbar', width = 0.33) +\n  geom_boxplot(\n    fill = \"#6B8F7E\", \n    width = 0.6,\n    outlier.size = 4,\n    notch = TRUE,\n    notchwidth = 0.75\n  ) +\n  geom_text(\n    data = labels,\n    aes(x, y, label = label), \n    hjust = 0,\n    size = 11/.pt\n  ) +\n  geom_point(\n    data = tibble(x = runif(length(y), -1.2, -0.5), y = y),\n    aes(x, y),\n    size = 3,\n    color = \"#4C6257\",\n    alpha = 0.85\n  ) +\n  coord_cartesian(xlim = c(-2.2, 3)) +\n  theme_void()\n\n\n\n\nAs you can see, the boxplot shows the distribution of a variable using a five-number summary, which includes all of the following:\n\nMinimum: the lowest value excluding outliers\nMaximum: the greatest value excluding outliers\nMedian: the middle value that separates the data in half (also called the second quartile)\nFirst quartile: the middle value of the lower half of the data, meaning 75% of the data fall above it and %25 below it (also called the lower quartile)\nThird quartile: the middle value of the upper half of the data, meaning 25% of the data fall above it and 75% below it (also called the upper quartile)\n\nBy extension, this includes the\n\nInterquartile Range: the distance between the first and third quartile, which includes 50% of the data.\n\nNotice that the minimum and maximum values are connected to the first and third quartiles by lines commonly referred to as “whiskers.” These are drawn to the largest and smallest values that fall within 1.5 * IQR of the first and third quartiles, respectively. Observations that fall above or below the whiskers are considered “outliers”.\nTo make a boxplot with ggplot, we need our table of data (as always), which we pass to the ggplot() function. We then specify the boxplot geometry with geom_boxplot().\n\nggplot(penguins) +\n  geom_boxplot(\n    aes(x = species, y = bill_length_mm),\n    fill = \"#6B8F7E\"\n  )\n\n\n\n\nNotice that we specified that the distribution of bill length should range over the y-axis. This makes the boxplots display vertically in the graph. We can change them to horizontal by having the variables distribution range over the x-axis.\n\nggplot(penguins) +\n  geom_boxplot(\n    aes(x = bill_length_mm, y = species),\n    fill = \"#6B8F7E\"\n  )\n\n\n\n\nNow, let’s update the axis labels and add a title. In this case, we’ll drop the y-axis label because it should be obvious from the values there that these are different species. While we’re at it, let’s also change the theme settings to minimal and remove the horizontal grid lines. And, we’ll add perpendicular lines at the end of the whiskers with stat_boxplot(geom = \"errorbar\"). Note that we move the aes() call up to ggplot(), so that these arguments can be shared between the stat and the geometry.\n\nggplot(penguins, aes(x = bill_length_mm, y = species)) +\n  stat_boxplot(geom ='errorbar', width = 0.33) +\n  geom_boxplot(fill = \"#6B8F7E\") +\n  labs(\n    x = \"Bill Length (mm)\",\n    y = NULL,\n    title = \"Palmer Penguins\"\n  ) +\n  theme_bw() +\n  theme(\n    panel.grid.major.y = element_blank(),\n    panel.grid.minor = element_blank(),\n  )\n\n\n\n\nExercises\n\nMake a boxplot showing the distribution of penguin body mass by island. Do all of the following:\n\nPosition the boxes horizontally.\nChange the fill color to a color of your choice.\nUpdate the labels and add a title.\nChange the theme to one of your choice.\nRemove the horizontal grid lines.\nAdd perpendicular lines to the whiskers.\n\n\nDo the same as (1), but for dart point length, substituting dart type for island."
  },
  {
    "objectID": "labs/05-distributions-lab.html#base-r-graphics",
    "href": "labs/05-distributions-lab.html#base-r-graphics",
    "title": "Lab 05: Visualizing Distributions",
    "section": "Base R {graphics}\n",
    "text": "Base R {graphics}\n\nWhile we have focused on visualizing distributions using ggplot2, it’s also useful to learn how to construct them with the {graphics} package from base R. The latter is not something you would want to rely on to create publication-quality figures, but it does provide simple and powerful tools for visually exploring your data. This means we will not spend much time learning the ends and outs of manipulating base R graphics to try and make them look similar to those generated with ggplot(). Believe me when I say that would be a painful experience. You will be best served only turning to {graphics} for rough and ready exploratory visualizations and not for the sort of fine-tuning you would do with ggplot2 to get a figure ready for publication. With that, then, here are the base R alternatives.\nFair warning, we’re going to make sure we have plenty of margin space for these plots, so labels and whatnot don’t get cutoff. We set the margins using the par() function (for graphical parameters) and pass a vector of length 4 to the mar (for margins) argument, one value for the size of each margin.\nFirst, we’ll get the default margins.\n\ndefault_margins <- par()$mar\n\nbig_margins <- c(6, 8, 4, 1)\n\nBar chart\nThe base R equivalent of ggplot() + geom_bar() is the barplot() function. There are many differences between these two methods, but the primary one is that barplot() requires you to compute the counts for each category beforehand. To do that, we’ll use the handy - though misleadingly named - table() function.\n\nspecies <- table(penguins$species)\n\nspecies\n\n\n   Adelie Chinstrap    Gentoo \n      152        68       124 \n\nbarplot(\n  height = sort(species, decreasing = TRUE),\n  col = \"#6B8F7E\",\n  xlab = NULL,\n  ylab = \"Count\"\n)\n\n\n\n\nNotice that we re-ordered the species variable using the sort() function with decreasing = TRUE rather than fct_infreq(). This will work only on the summarized data generated by the table() function, not the raw data passed to geom_bar().\nIf you want to make this horizontal, just specify horiz = TRUE. You’ll also want to change the sorting to increasing, to get the most frequent species on top. And use las = 1 to rotate the axis text so that they are horizontal, too.\n\npar(mar = big_margins)\n\nbarplot(\n  height = sort(species, decreasing = FALSE),\n  horiz = TRUE,\n  las = 1,\n  col = \"#6B8F7E\",\n  xlab = \"Count\",\n  ylab = NULL\n)\n\n\n\n\nHistogram\nThe base R equivalent of ggplot() + geom_histogram() is the extremely compact hist() function. It looks like this.\n\n# reset margins\npar(mar = default_margins)\n\nhist(\n  titanic$age,\n  breaks = seq(0, 75, by = 5),\n  xlab = \"Age (years)\",\n  ylab = \"Count\",\n  main = \"Passengers of the Titanic\",\n  col = \"#6B8F7E\",\n  border = \"#4C6257\"\n)\n\n\n\n\nTwo important things to note here. First, you have to pass the histogram function the variable age, so you have to pull out of the data.frame with titanic$age. Second, you use breaks instead of bins or binwidth to define the bins.\nProbability density\nThe base R equivalent of ggplot() + geom_density() uses the density() and plot() functions. It looks like this.\n\nkde <- density(titanic$age)\n\nplot(\n  kde,\n  xlab = \"Age (years)\",\n  ylab = \"Density\",\n  main = \"Passengers of the Titanic\"\n)\n\n\n\n\nNote that the density() function does not like missing values, so if there are any NA values in your data, you will need to include the argument na.rm = TRUE (this tells the function to ignore missing values).\nTo control the fill and outline color, it’s necessary to add the KDE as an additional layer to the plot with the polygon() function.\n\nkde <- density(titanic$age)\n\nplot(\n  kde,\n  xlab = \"Age (years)\",\n  ylab = \"Density\",\n  main = \"Passengers of the Titanic\"\n)\n\npolygon(\n  kde,\n  col = \"#6B8F7E\",\n  border = \"#4C6257\"\n)\n\n\n\n\nAnd that’s about as far as I want to take you with that.\nCumulative distribution\nThe base R equivalent of ggplot() + stat_ecdf() uses the ecdf() and plot() functions. It looks like this.\n\necdf_function <- ecdf(titanic$age)\n\nplot(\n  ecdf_function,\n  col = \"#4C6257\",\n  xlab = \"Age (years)\",\n  ylab = \"Density\",\n  main = \"Passengers of the Titanic\"\n)\n\n\n\n\nThat’s a little weird. To get this into a proper stewise line, we need to do a little work. Specifically, we need to sort the unique age values of passengers on the Titanic. Then we need to feed those to the ecdf_function() we created with ecdf(). We also need to specify that we want the stepwise plot type with type = \"s\". Ugh… 😕\n\nx <- sort(unique(titanic$age))\n\nplot(\n  x,\n  ecdf_function(x),\n  type = \"s\",\n  col = \"#4C6257\",\n  xlab = \"Age (years)\",\n  ylab = \"Density\",\n  main = \"Passengers of the Titanic\"\n)\n\n\n\n\nMother of dragons… 🐉\nBoxplot\nThe base R equivalent of ggplot() + geom_boxplot() is, thankfully, boxplot(). It looks like this.\n\nboxplot(\n  penguins$bill_length_mm ~ penguins$species,\n  col = \"#4C6257\",\n  xlab = NULL,\n  ylab = \"Bill Length (mm)\",\n  main = \"Palmer Penguins\"\n)\n\n\n\n\nNotice that I used the formula notation. This is to specify the grouping structure for the boxes, so a separate box for each penguin species.\nTo make that figure horizontal, use horizontal = TRUE.\n\npar(mar = big_margins)\n\nboxplot(\n  penguins$bill_length_mm ~ penguins$species,\n  horizontal = TRUE,\n  las = 1,\n  col = \"#4C6257\",\n  xlab = \"Bill Length (mm)\",\n  ylab = NULL,\n  main = \"Palmer Penguins\"\n)\n\n\n\n\nHey, in the base R bar chart above, didn’t we specify a horizontal orientation with horiz = TRUE?\nYeah, yeah you did… 🤷\nExercises\nFor each of the following, make sure to change the fill (col) and outline (border) colors and update the labels.\n\nCreate a bar chart of the count of dart point types using table() and the barplot() function. Give the plot a horizontal orientation.\nCreate a histogram of penguin bill length using the hist() function.\nCreate a histogram of dart point length using the hist() function.\nCreate a probability density plot of penguin bill length using the density() and plot() functions. (Note that the density() function does not like missing values, so if there are any NA values in your data, you will need to include the argument na.rm = TRUE (this tells the function to ignore missing values).)\nCreate a probability density plot of dart point length using the density() and plot() functions.\nCreate an eCDF plot of penguin bill length using the ecdf() and plot() functions. Make sure the plot is stepwise.\nCreate an eCDF plot of dart point length using the ecdf() and plot() functions. Make sure the plot is stepwise.\nCreate a boxplot of penguin bill length grouped by species using the boxplot() function. Give the plot a horizontal orientation.\nCreate a boxplot of dart point length grouped by dart point type using the boxplot() function. Give the plot a horizontal orientation."
  },
  {
    "objectID": "labs/06-dataframes-lab.html",
    "href": "labs/06-dataframes-lab.html",
    "title": "Lab 06: Working with Tables",
    "section": "",
    "text": "This lab will guide you through the process of\n\nconstructing data.frames and tibbles\nworking with the pipe\nworking with columns (variables) in a table\nworking with rows (observations) in a table\nworking with grouped data in a table\n\n\narchdata\npalmerpenguins\ndplyr\n\n\nlibrary(archdata)\nlibrary(palmerpenguins)\nlibrary(dplyr)\n\n\n\n\npenguins\n\nIncludes measurements for penguin species, island in Palmer Archipelago, size (flipper length, body mass, bill dimensions), and sex.\npackage: palmerpenguins\n\nreference: https://allisonhorst.github.io/palmerpenguins/reference/penguins.html\n\n\n\n\nDartPoints\n\nIncludes measurements of 91 Archaic dart points recovered during surface surveys at Fort Hood, Texas.\npackage: archdata\n\nreference: https://cran.r-project.org/web/packages/archdata/archdata.pdf"
  },
  {
    "objectID": "labs/06-dataframes-lab.html#tables",
    "href": "labs/06-dataframes-lab.html#tables",
    "title": "Lab 06: Working with Tables",
    "section": "Tables",
    "text": "Tables\n\n\n\n\n\nFigure 1: Elements of a Data Frame.\n\n\nConsider this research scenario: you have a museum collection of projectile points that you want to use for an analysis, maybe you want to know whether East Gate and Rose Spring points are actually the same type of point, commonly referred to using the portmanteau Rosegate. Following tradition, you think maybe it’s the size and shape that will you help finally put this old question to rest, so for each individual point, you set about measuring its length, width, and height.\nIn this course, we’ll refer to each individual measure that you make as a value, each set of such measures for an individual point we will call (somewhat awkwardly) an observation, and each individual type of measurement will be a variable. It is almost certainly the case that you will be collecting this data and storing it in something like a spreadsheet or table, like that shown in Figure 1. You will sometimes hear data scientists refer to data stored in this way as rectangular data. All they mean by that is that the data come as collections of values organized into rows and columns. Crucially, all the columns must have the same length, so you can describe the data as having dimensions, specifically length (the number of observations or rows) and width (the number of variables or columns).\nCreating tables\nIn R, tabular datasets are known as data frames. To create a data frame, we use the eponymous data.frame() function. Here, for example, is how we would create the table in Figure 1 above:\n\nprojectiles_df <- data.frame(\n  type = c(\"Elko\", \"Rosegate\", \"DSN\", \"Elko\", \"Clovis\"),\n  length = c(2.03, 1.4, 1.9, 2.1, 3.3),\n  width = c(0.8, 0.4, 0.3, 0.7, 0.95),\n  height = c(3.23, 2.4, 1.29, 2.7, 4.15)\n)\n\nprojectiles_df\n\n      type length width height\n1     Elko   2.03  0.80   3.23\n2 Rosegate   1.40  0.40   2.40\n3      DSN   1.90  0.30   1.29\n4     Elko   2.10  0.70   2.70\n5   Clovis   3.30  0.95   4.15\n\n\nNote that the values (or measurements) contained in each variable are wrapped in the c() function (short for concatenate). This variable tells R that all these values belong to this one variable. Each variable in turn is an argument to data.frame() having the form <variable> = c(<value-1>, <value-2>, ..., <value-n>).\nGetting basic meta-data from tables\nWhen you want to know what variables a table includes, you can use the names() function.\n\nnames(projectiles_df)\n\n[1] \"type\"   \"length\" \"width\"  \"height\"\n\n\nIf you want to know how many variables or observations the table has, you can use nrow() and ncol() respectively.\n\n# number of observations\nnrow(projectiles_df)\n\n[1] 5\n\n# number of variables\nncol(projectiles_df)\n\n[1] 4\n\n\nPretty tibbles\nAs an alternative to the default data.frame, you can also use a tibble, which is a standard format used by packages within the tidyverse, including the tibble package that defines it. In the grand scheme of things, the differences between tibbles and data.frames are quite small, mostly relating to their nicer (some might say prettier) print methods. It has some other nice features, but most of them are not things that should concern a beginning R user, so we won’t dwell on them here.\nYou create a tibble just like you would a data.frame, but with the tibble() function.\n\nprojectiles_tbl <- tibble(\n  type = c(\"Elko\", \"Rosegate\", \"DSN\", \"Elko\", \"Clovis\"),\n  length = c(2.03, 1.4, 1.9, 2.1, 3.3),\n  width = c(0.8, 0.4, 0.3, 0.7, 0.95),\n  height = c(3.23, 2.4, 1.29, 2.7, 4.15)\n)\n\nNow you can see how the print method for tibbles differs from that of data.frames.\n\nprojectiles_df\n\n      type length width height\n1     Elko   2.03  0.80   3.23\n2 Rosegate   1.40  0.40   2.40\n3      DSN   1.90  0.30   1.29\n4     Elko   2.10  0.70   2.70\n5   Clovis   3.30  0.95   4.15\n\nprojectiles_tbl\n\n# A tibble: 5 × 4\n  type     length width height\n  <chr>     <dbl> <dbl>  <dbl>\n1 Elko       2.03  0.8    3.23\n2 Rosegate   1.4   0.4    2.4 \n3 DSN        1.9   0.3    1.29\n4 Elko       2.1   0.7    2.7 \n5 Clovis     3.3   0.95   4.15\n\n\nThere are a few big differences here:\n\nBy default tibble() will only print the first ten rows of data. This saves you from having to use the head() function. It will also tell you how many rows were not printed at the bottom, assuming there are more than ten. With data.frame(), the default print will print the maximum number of rows defined in your default options.\n\ntibble() will only print columns that fit on your screen. It will then tell you how many columns were not printed.\n\ntibble() provides information about the dimensions of your table: the number of rows and columns.\n\ntibble() provides information about the type of data each variable is: factor (fct), character, numeric (dbl for “double” or int for “integer”).\n\nWhile the choice between tibbles and data.frames is largely a stylistic one, I would encourage you to use tibbles as much as possible. It’ll make things easier for you in the long run. You can always convert between them if you want with as_tibble() and as.data.frame().\n\n# turn a tibble into a data.frame\nprojectiles_df <- as.data.frame(projectiles_tbl)\n\n# turn a data.frame into a tibble\nprojectiles_tbl <- as_tibble(projectiles_df)\n\nIn what follows, we are going to learn how to perform some tasks that almost always arise when working with tables using a powerful set of tools provided by the dplyr package, which is part of the tidyverse.\nExercises\n\nGet the names of the variables in the penguins table with names().\nHow many observations and variables are in this dataset? Hint: use nrow() and ncol().\nLoad the DartPoints dataset and convert the data.frame to a tibble with as_tibble(). Assign it to a new object called “darts” using the arrow <-. Then remove the original DartPoints dataframe from your environment with remove(DartPoints)."
  },
  {
    "objectID": "labs/06-dataframes-lab.html#the-pipe",
    "href": "labs/06-dataframes-lab.html#the-pipe",
    "title": "Lab 06: Working with Tables",
    "section": "The Pipe",
    "text": "The Pipe\n\n\nFigure 2: The Treachery of Images (1929) by Rene Magritte.\n\n\nBefore we get to the ways in which you can shape and otherwise manipulate tables in R, though, let’s introduce a style of coding referred to as piping first, which allows you to express a series of operations in a clear and concise way. To motivate the use of pipes, consider this extremely common scenario: you want to select some specific variables in your table, filter the observations to include a subset based on some condition, then add a new variable by transforming or mutating another one. Your code might look like this (we will explain how to use these functions in just a moment):\n\nbob <- select(penguins, species, island, bill_length_mm, body_mass_g)\ntom <- filter(bob, sex == \"male\")\nsue <- mutate(tom, bill_length_cm = bill_length_mm/10)\n\nremove(bob, tom)\n\nYou will sometimes see people write the same series of function calls this way:\n\nsue <- mutate(filter(select(penguins, species, island, bill_length_mm, body_mass_g), island == \"Torgersen\"), bill_length_cm = bill_length_mm/10)\n\nThis is called nesting. It’s primary advantage is that it does not require saving intermediate objects (like bob and tom) and thus polluting your R environment with unnecessary copies of your data. Nesting function calls in this way is fine as long as it remains relatively shallow, but the deeper the nest (like 2 or 3 functions deep), the harder it is to read, as you must follow results from the inside out and bottom up, as opposed to left to right and top down, which is more natural.\nTo avoid deep nests and temporary assignments, you can use pipes. This involves calling the special operator, |>, consisting of a vertical bar and the greater-than sign that was introduced to base R in 2021 with version 4.1.0. If you squint (and depending on the font you are using), it looks a bit like an arrow pointing to the right. Using the pipe-operator looks like this:\n\nsue <- penguins |> \n  select(species, sex, body_mass_g) |> \n  filter(sex == \"male\") |> \n  mutate(bill_length_cm = bill_length_mm/10)\n\nIf you were to transpose this code into a natural language like English, it would sound something like this,\n\nTake the penguins table, then select the variables species, sex, and body mass, then filter the males, then create a new variable by converting bill length from millimeters to centimeters.\n\nThe ‘then’ in each case is the pipe operator. Crucially, the pipe assumes that the data being transformed is the first argument to the function, which is true for all the functions in the tidyverse, as they were all designed very intentionally to work with the pipe. Unfortunately, many parts of R were written long before the pipe was ever introduced, so they may not work as easily with it.\n\n\n\n\n\n\nTip: keyboard shortcut\n\n\n\nThis is easily one of the most powerful tools for working with data in R, which is probably why it is so popular these days. RStudio gives a keyboard shortcut to make inserting this operator into your code easier. It’s Ctrl + Shift + M on Windows and Cmd + Shift + M on Mac. To ensure that this keyboard shortcut inserts the base R pipe operator, go to Tools > Global Options… > Code > Editing and check that the option “Use native pipe operator, |> (requires R 4.1+)” is selected."
  },
  {
    "objectID": "labs/06-dataframes-lab.html#columns",
    "href": "labs/06-dataframes-lab.html#columns",
    "title": "Lab 06: Working with Tables",
    "section": "Columns",
    "text": "Columns\nNow that we have the pipe, let’s look at different ways of working with variables or columns in a table. Some of the most common tasks involve selecting a subset of variables, relocating them, renaming them, and creating new variables. As mentioned before, the focus here will be on dplyr functions, but we will also take some time to introduce base R methods where appropriate.\npull()\nTo pull a variable out of a table and return it as a vector, you can pass the table along with the name of the target variable to the pull() function from dplyr.\n\nprojectiles_tbl |> pull(type)\n\n[1] \"Elko\"     \"Rosegate\" \"DSN\"      \"Elko\"     \"Clovis\"  \n\nprojectiles_tbl |> pull(length)\n\n[1] 2.03 1.40 1.90 2.10 3.30\n\n\nThe base R way of doing this relies on double brackets or the dollar sign, as in <table>[[\"<variable>\"]] and <table>$<variable>.\n\nprojectiles_tbl$type\n\n[1] \"Elko\"     \"Rosegate\" \"DSN\"      \"Elko\"     \"Clovis\"  \n\nprojectiles_tbl[[\"length\"]]\n\n[1] 2.03 1.40 1.90 2.10 3.30\n\n\nAs you can see, using the dplyr function is quite a bit easier to read and intuit what you are doing. It also plays nice with the pipe.\nselect()\nIf you want to select one or more variables but return the result as a table, pass the table along with the names of the desired variables to select(). As an example, suppose (for reasons) that we want a smaller table that only has the species and bill_length_mm variables. To get that from the larger table, I simply do this\n\npenguins |> select(species, bill_length_mm)\n\n# A tibble: 344 × 2\n   species bill_length_mm\n   <fct>            <dbl>\n 1 Adelie            39.1\n 2 Adelie            39.5\n 3 Adelie            40.3\n 4 Adelie            NA  \n 5 Adelie            36.7\n 6 Adelie            39.3\n 7 Adelie            38.9\n 8 Adelie            39.2\n 9 Adelie            34.1\n10 Adelie            42  \n# … with 334 more rows\n\n\nrename()\nTo rename variables in a table, use the rename() function. This is similar to pull() and select() in that you pass it column names, but you are also going to signal new names for those columns using the equal sign. The idea here is to do rename(<new name> = <old name>). Here, for example, is how you might remove the units of measure from the variable names in the penguins table.\n\npenguins |> names()\n\n[1] \"species\"           \"island\"            \"bill_length_mm\"   \n[4] \"bill_depth_mm\"     \"flipper_length_mm\" \"body_mass_g\"      \n[7] \"sex\"               \"year\"             \n\npenguins |> \n  rename(\n    bill_length = bill_length_mm,\n    bill_depth = bill_depth_mm,\n    flipper_length = flipper_length_mm,\n    body_mass = body_mass_g\n  ) |> \n  names()\n\n[1] \"species\"        \"island\"         \"bill_length\"    \"bill_depth\"    \n[5] \"flipper_length\" \"body_mass\"      \"sex\"            \"year\"          \n\n\n\n\n\n\n\n\nA good rule of 👍\n\n\n\nHere are a few good rules of thumb for variable names in tables (and for all names in R!).\n\nUse all lower-case characters. This makes them easier to type.\nAvoid special characters. They require special treatment when included.\nUse underscores _ instead of spaces for similar reasons.\nTry to keep the names as simple as possible but…\nAir on the side of longer names that clearly communicate the meaning of each variable.\n\n\n\nHere is one trick you might find useful. Sometimes you will find yourself wanting to rename all the variables in a consistent way, for instance, by making them all lower case. This is something I find myself doing quite often actually. Base R has this nice function called tolower() that converts capitalized letters to lowercase. For instance, tolower(\"ABC\") returns abc. There’s a corresponding toupper() that does the opposite. You can apply these functions to every column in one go with rename_with().\n\npenguins |> \n  rename_with(toupper) |> \n  names()\n\n[1] \"SPECIES\"           \"ISLAND\"            \"BILL_LENGTH_MM\"   \n[4] \"BILL_DEPTH_MM\"     \"FLIPPER_LENGTH_MM\" \"BODY_MASS_G\"      \n[7] \"SEX\"               \"YEAR\"             \n\n\nrelocate()\nYou will sometimes find it helpful to reorder your variables. To do that, you can use relocate(), again passing it the column names. By default, it moves variables to the front, but you can also move them before or after specific variables with the .before and .after arguments.\n\npenguins |> names()\n\n[1] \"species\"           \"island\"            \"bill_length_mm\"   \n[4] \"bill_depth_mm\"     \"flipper_length_mm\" \"body_mass_g\"      \n[7] \"sex\"               \"year\"             \n\npenguins |> relocate(sex, year) |> names()\n\n[1] \"sex\"               \"year\"              \"species\"          \n[4] \"island\"            \"bill_length_mm\"    \"bill_depth_mm\"    \n[7] \"flipper_length_mm\" \"body_mass_g\"      \n\npenguins |> relocate(sex, year, .after = island) |> names()\n\n[1] \"species\"           \"island\"            \"sex\"              \n[4] \"year\"              \"bill_length_mm\"    \"bill_depth_mm\"    \n[7] \"flipper_length_mm\" \"body_mass_g\"      \n\n\nmutate()\nOne of the most important and powerful functions provided by dplyr is mutate(). This allows you to add or change variables, often based on the values of other variables in your table. For instance, you can create a new variable that codes bill length in centimeters rather than millimeters. You just divide millimeters by 10.\n\npenguins |> \n  mutate(bill_length_cm = bill_length_mm/10) |> \n  select(species, bill_length_mm, bill_length_cm)\n\n# A tibble: 344 × 3\n   species bill_length_mm bill_length_cm\n   <fct>            <dbl>          <dbl>\n 1 Adelie            39.1           3.91\n 2 Adelie            39.5           3.95\n 3 Adelie            40.3           4.03\n 4 Adelie            NA            NA   \n 5 Adelie            36.7           3.67\n 6 Adelie            39.3           3.93\n 7 Adelie            38.9           3.89\n 8 Adelie            39.2           3.92\n 9 Adelie            34.1           3.41\n10 Adelie            42             4.2 \n# … with 334 more rows\n\n\nYou can refer to as many variables in the table as you like, too. For instance, you can calculate the ratio of bill length to flipper length.\n\npenguins |> \n  mutate(bill_to_flipper = bill_length_mm/flipper_length_mm) |> \n  select(species, bill_length_mm, flipper_length_mm, bill_to_flipper)\n\n# A tibble: 344 × 4\n   species bill_length_mm flipper_length_mm bill_to_flipper\n   <fct>            <dbl>             <int>           <dbl>\n 1 Adelie            39.1               181           0.216\n 2 Adelie            39.5               186           0.212\n 3 Adelie            40.3               195           0.207\n 4 Adelie            NA                  NA          NA    \n 5 Adelie            36.7               193           0.190\n 6 Adelie            39.3               190           0.207\n 7 Adelie            38.9               181           0.215\n 8 Adelie            39.2               195           0.201\n 9 Adelie            34.1               193           0.177\n10 Adelie            42                 190           0.221\n# … with 334 more rows\n\n\nWhen you work with mutate(), keep in mind that data.frames and tibbles are rectangular. That means each variable in the table must have the same number of values (equal to the number of rows or observations in the table). In the penguins table, there are 344 observations, so any new variable that you add with mutate() must have 344 values (it must have that length). If the variable you want to add does not have the same length, you will receive an error like the following:\n\npenguins |> mutate(id = 1:5)\n\nError in `mutate()`:\n! Problem while computing `id = 1:5`.\n✖ `id` must be size 344 or 1, not 5.\n\n\nNote, too, that you can refer to new variables you have created in the same mutate() call.\n\npenguins |> \n  select(species, island) |> \n  mutate(\n    id = 1:n(),\n    id2 = id * 2\n  )\n\n# A tibble: 344 × 4\n   species island       id   id2\n   <fct>   <fct>     <int> <dbl>\n 1 Adelie  Torgersen     1     2\n 2 Adelie  Torgersen     2     4\n 3 Adelie  Torgersen     3     6\n 4 Adelie  Torgersen     4     8\n 5 Adelie  Torgersen     5    10\n 6 Adelie  Torgersen     6    12\n 7 Adelie  Torgersen     7    14\n 8 Adelie  Torgersen     8    16\n 9 Adelie  Torgersen     9    18\n10 Adelie  Torgersen    10    20\n# … with 334 more rows\n\n\nExercises\n\nRemind yourself of what variables the penguins table has with names().\nNow, extract a variable of your choice with pull().\nSubset the data by choosing only four variables of your choice with select().\nUse relocate() to move the species and island variables after bill_length_mm.\nUse mutate() on the penguins data to create variables that\n\nrepresent body mass in kilograms,\nrepresent bill depth in centimeters,\ncompare bill length to bill depth to see whether the former is greater than (>) the latter.\n\n\nUsing the darts tibble (formerly the DartPoints data.frame), rename the variables so that they are all lower case, rename the “name” variable to “type”, and subset to the type, length, width, and thickness variables. Reassign this to darts, so you can save the result. Hint:\n\n\ndarts <- darts |> \n  rename_with() |> \n  rename() |> \n  selet()\n\n\nUse mutate() to add a variable to darts that is the ratio of point length to width."
  },
  {
    "objectID": "labs/06-dataframes-lab.html#rows",
    "href": "labs/06-dataframes-lab.html#rows",
    "title": "Lab 06: Working with Tables",
    "section": "Rows",
    "text": "Rows\narrange()\nSometimes it can be useful to sort a table, so you can more easily navigate the information it contains. To do this, dplyr provides the arrange() function. This sorts observations based on a supplied variable or variables. By default, it sorts observations in ascending order. If you provide multiple variables, it will sort the first variable first, then sort the second variable within that variable, and so on. In effect, it breaks ties in the sorting process.\n\npenguins |> arrange(species, island)\n\n# A tibble: 344 × 8\n   species island bill_length_mm bill_depth_mm flipper_len…¹ body_…² sex    year\n   <fct>   <fct>           <dbl>         <dbl>         <int>   <int> <fct> <int>\n 1 Adelie  Biscoe           37.8          18.3           174    3400 fema…  2007\n 2 Adelie  Biscoe           37.7          18.7           180    3600 male   2007\n 3 Adelie  Biscoe           35.9          19.2           189    3800 fema…  2007\n 4 Adelie  Biscoe           38.2          18.1           185    3950 male   2007\n 5 Adelie  Biscoe           38.8          17.2           180    3800 male   2007\n 6 Adelie  Biscoe           35.3          18.9           187    3800 fema…  2007\n 7 Adelie  Biscoe           40.6          18.6           183    3550 male   2007\n 8 Adelie  Biscoe           40.5          17.9           187    3200 fema…  2007\n 9 Adelie  Biscoe           37.9          18.6           172    3150 fema…  2007\n10 Adelie  Biscoe           40.5          18.9           180    3950 male   2007\n# … with 334 more rows, and abbreviated variable names ¹​flipper_length_mm,\n#   ²​body_mass_g\n\n\nYou can use the desc() function to sort descending.\n\npenguins |> arrange(desc(species))\n\n# A tibble: 344 × 8\n   species island bill_length_mm bill_depth_mm flipper_len…¹ body_…² sex    year\n   <fct>   <fct>           <dbl>         <dbl>         <int>   <int> <fct> <int>\n 1 Gentoo  Biscoe           46.1          13.2           211    4500 fema…  2007\n 2 Gentoo  Biscoe           50            16.3           230    5700 male   2007\n 3 Gentoo  Biscoe           48.7          14.1           210    4450 fema…  2007\n 4 Gentoo  Biscoe           50            15.2           218    5700 male   2007\n 5 Gentoo  Biscoe           47.6          14.5           215    5400 male   2007\n 6 Gentoo  Biscoe           46.5          13.5           210    4550 fema…  2007\n 7 Gentoo  Biscoe           45.4          14.6           211    4800 fema…  2007\n 8 Gentoo  Biscoe           46.7          15.3           219    5200 male   2007\n 9 Gentoo  Biscoe           43.3          13.4           209    4400 fema…  2007\n10 Gentoo  Biscoe           46.8          15.4           215    5150 male   2007\n# … with 334 more rows, and abbreviated variable names ¹​flipper_length_mm,\n#   ²​body_mass_g\n\n\ndistinct()\nObservations will sometimes be duplicated in your data. The simplest way to remove them is with the distinct() function. For demonstration purposes, suppose you have this simple dataset.\n\ntbl <- tibble(\n  x = c(1, 1, 1, 3, 2), \n  y = c(\"a\", \"a\", \"a\", \"b\", \"c\"),\n  z = c(TRUE, TRUE, FALSE, TRUE, TRUE)\n)\n\ntbl\n\n# A tibble: 5 × 3\n      x y     z    \n  <dbl> <chr> <lgl>\n1     1 a     TRUE \n2     1 a     TRUE \n3     1 a     FALSE\n4     3 b     TRUE \n5     2 c     TRUE \n\n\nFor whatever reason, your first observation or row has been duplicated (there are two rows with the values x=1, y=a, and z=TRUE). To remove that row, do this:\n\ntbl |> distinct()\n\n# A tibble: 4 × 3\n      x y     z    \n  <dbl> <chr> <lgl>\n1     1 a     TRUE \n2     1 a     FALSE\n3     3 b     TRUE \n4     2 c     TRUE \n\n\nYou may also want distinct combinations of a specific variable or variables.\n\ntbl |> distinct(x, y)\n\n# A tibble: 3 × 2\n      x y    \n  <dbl> <chr>\n1     1 a    \n2     3 b    \n3     2 c    \n\n\nfilter()\nSubsetting data by filtering observations is a little bit more involved than simply selecting variables, but intuitively, you are simply asking for those observations that satisfy a certain condition. Getting filter() to return those observations requires that you pass it an expression containing a comparison operator. The expression is then evaluated by R for its truth or falsity, with observations that evaluate to TRUE being returned, observations that evaluate to FALSE being ignored. Let’s walk through an example, then try to break down what is happening in a little more detail. Suppose we want only those observations of penguins residing on Biscoe Island. Here is how we would go about collecting those observations from our penguins data.frame.\n\npenguins |> filter(island == \"Biscoe\")\n\n# A tibble: 168 × 8\n   species island bill_length_mm bill_depth_mm flipper_len…¹ body_…² sex    year\n   <fct>   <fct>           <dbl>         <dbl>         <int>   <int> <fct> <int>\n 1 Adelie  Biscoe           37.8          18.3           174    3400 fema…  2007\n 2 Adelie  Biscoe           37.7          18.7           180    3600 male   2007\n 3 Adelie  Biscoe           35.9          19.2           189    3800 fema…  2007\n 4 Adelie  Biscoe           38.2          18.1           185    3950 male   2007\n 5 Adelie  Biscoe           38.8          17.2           180    3800 male   2007\n 6 Adelie  Biscoe           35.3          18.9           187    3800 fema…  2007\n 7 Adelie  Biscoe           40.6          18.6           183    3550 male   2007\n 8 Adelie  Biscoe           40.5          17.9           187    3200 fema…  2007\n 9 Adelie  Biscoe           37.9          18.6           172    3150 fema…  2007\n10 Adelie  Biscoe           40.5          18.9           180    3950 male   2007\n# … with 158 more rows, and abbreviated variable names ¹​flipper_length_mm,\n#   ²​body_mass_g\n\n\nHere we supplied this key information to filter():\nisland == \"Biscoe\"\nWhat does this expression mean exactly? In effect, it is directing filter() to scan through our data, specifically the island column, and select only those rows where the value is Biscoe. The so-called comparison operator here is the double equal sign, ==. This is importantly different than the single equal sign, =, which is used inside a function as part of a key=value or argument=value pair. R provides several helpful comparison operators:\n\n\n== for equals in the sense of a perfect match,\n\n\n!= for not equals,\n\n\n> for greater than,\n\n\n>= for greater than or equal to,\n\n\n< for less than, and\n\n\n<= for less than or equal to.\n\nYou can use the first two, == and !=, for comparisons with either character or numeric variables, but the rest apply only to the latter. Let’s run through a few more examples:\nFilter penguins with body mass greater than 3500 grams.\n\npenguins |> filter(body_mass_g > 3500)\n\n# A tibble: 264 × 8\n   species island    bill_length_mm bill_depth_mm flipper_…¹ body_…² sex    year\n   <fct>   <fct>              <dbl>         <dbl>      <int>   <int> <fct> <int>\n 1 Adelie  Torgersen           39.1          18.7        181    3750 male   2007\n 2 Adelie  Torgersen           39.5          17.4        186    3800 fema…  2007\n 3 Adelie  Torgersen           39.3          20.6        190    3650 male   2007\n 4 Adelie  Torgersen           38.9          17.8        181    3625 fema…  2007\n 5 Adelie  Torgersen           39.2          19.6        195    4675 male   2007\n 6 Adelie  Torgersen           42            20.2        190    4250 <NA>   2007\n 7 Adelie  Torgersen           37.8          17.3        180    3700 <NA>   2007\n 8 Adelie  Torgersen           38.6          21.2        191    3800 male   2007\n 9 Adelie  Torgersen           34.6          21.1        198    4400 male   2007\n10 Adelie  Torgersen           36.6          17.8        185    3700 fema…  2007\n# … with 254 more rows, and abbreviated variable names ¹​flipper_length_mm,\n#   ²​body_mass_g\n\n\nFilter penguins with beaks longer than 39 millimeters.\n\npenguins |> filter(bill_length_mm > 39)\n\n# A tibble: 260 × 8\n   species island    bill_length_mm bill_depth_mm flipper_…¹ body_…² sex    year\n   <fct>   <fct>              <dbl>         <dbl>      <int>   <int> <fct> <int>\n 1 Adelie  Torgersen           39.1          18.7        181    3750 male   2007\n 2 Adelie  Torgersen           39.5          17.4        186    3800 fema…  2007\n 3 Adelie  Torgersen           40.3          18          195    3250 fema…  2007\n 4 Adelie  Torgersen           39.3          20.6        190    3650 male   2007\n 5 Adelie  Torgersen           39.2          19.6        195    4675 male   2007\n 6 Adelie  Torgersen           42            20.2        190    4250 <NA>   2007\n 7 Adelie  Torgersen           41.1          17.6        182    3200 fema…  2007\n 8 Adelie  Torgersen           42.5          20.7        197    4500 male   2007\n 9 Adelie  Torgersen           46            21.5        194    4200 male   2007\n10 Adelie  Biscoe              40.6          18.6        183    3550 male   2007\n# … with 250 more rows, and abbreviated variable names ¹​flipper_length_mm,\n#   ²​body_mass_g\n\n\nMultiple conditions\nOften enough, we will want to combine a number of these simple conditions into one complex expression. In R, this is done with Boolean operators:\n\n\n& for and,\n\n\n| for or, and\n\n\n! for not.\n\nTo demonstrate the underlying logic of these Boolean operators, consider these shapes and colors. You can think of each of A, B, and C as its own observation or row in a data.frame that includes two variables color and shape.\n\n\n\n\n\n\n\n\n  \n  \n\nBoolean\n      Filter\n      Result\n    \n\n\nx\n\ncolor == \"yellow\"\n\nA, B\n\n\ny\n\nshape == \"circle\"\n\nB, C\n\n\nx & y\n\ncolor == \"yellow\" & shape == \"circle\"\n\nB\n\n\nx | y\n\ncolor == \"yellow\" | shape == \"circle\"\n\nA, B, C\n\n\nx & !y\n\ncolor == \"yellow\" & shape != \"circle\"\n\nA\n\n\n!x & y\n\ncolor != \"yellow\" & shape == \"circle\"\n\nC\n\n\n!(x & y)\n\n!(color == \"yellow\" & shape == \"circle\")\n\nA, C\n\n\n!(x | y)\n\n!(color == \"yellow\" | shape == \"circle\")\n\nNULL\n\n\n\n\n\n\nAnd here is an example with our penguins data, where we ask R to return those observations in which (a) penguins reside on Biscoe Island and (b) their bills are longer than 39 millimeters.\n\npenguins |> filter(island == \"Biscoe\" & bill_length_mm > 39)\n\n# A tibble: 143 × 8\n   species island bill_length_mm bill_depth_mm flipper_len…¹ body_…² sex    year\n   <fct>   <fct>           <dbl>         <dbl>         <int>   <int> <fct> <int>\n 1 Adelie  Biscoe           40.6          18.6           183    3550 male   2007\n 2 Adelie  Biscoe           40.5          17.9           187    3200 fema…  2007\n 3 Adelie  Biscoe           40.5          18.9           180    3950 male   2007\n 4 Adelie  Biscoe           39.6          17.7           186    3500 fema…  2008\n 5 Adelie  Biscoe           40.1          18.9           188    4300 male   2008\n 6 Adelie  Biscoe           42            19.5           200    4050 male   2008\n 7 Adelie  Biscoe           41.4          18.6           191    3700 male   2008\n 8 Adelie  Biscoe           40.6          18.8           193    3800 male   2008\n 9 Adelie  Biscoe           41.3          21.1           195    4400 male   2008\n10 Adelie  Biscoe           41.1          18.2           192    4050 male   2008\n# … with 133 more rows, and abbreviated variable names ¹​flipper_length_mm,\n#   ²​body_mass_g\n\n\nNote that filter() let’s you separate conditions with a comma, which it interprets as conjunction, represented by the &.\n\npenguins |> \n  filter(\n    island == \"Biscoe\", \n    species == \"Adelie\", \n    body_mass_g < 3500\n  )\n\n# A tibble: 13 × 8\n   species island bill_length_mm bill_depth_mm flipper_len…¹ body_…² sex    year\n   <fct>   <fct>           <dbl>         <dbl>         <int>   <int> <fct> <int>\n 1 Adelie  Biscoe           37.8          18.3           174    3400 fema…  2007\n 2 Adelie  Biscoe           40.5          17.9           187    3200 fema…  2007\n 3 Adelie  Biscoe           37.9          18.6           172    3150 fema…  2007\n 4 Adelie  Biscoe           35            17.9           190    3450 fema…  2008\n 5 Adelie  Biscoe           34.5          18.1           187    2900 fema…  2008\n 6 Adelie  Biscoe           36.5          16.6           181    2850 fema…  2008\n 7 Adelie  Biscoe           35.7          16.9           185    3150 fema…  2008\n 8 Adelie  Biscoe           36.4          17.1           184    2850 fema…  2008\n 9 Adelie  Biscoe           35.5          16.2           195    3350 fema…  2008\n10 Adelie  Biscoe           37.7          16             183    3075 fema…  2009\n11 Adelie  Biscoe           37.9          18.6           193    2925 fema…  2009\n12 Adelie  Biscoe           38.1          17             181    3175 fema…  2009\n13 Adelie  Biscoe           39.7          17.7           193    3200 fema…  2009\n# … with abbreviated variable names ¹​flipper_length_mm, ²​body_mass_g\n\n\nExercises\n\nTry all of the following with filter():\n\nFilter penguins that reside on Torgersen island.\nFilter penguins that have a flipper length greater than 185 mm.\nFilter penguins that reside on Torgersen island and have a body mass less than 3500 g."
  },
  {
    "objectID": "labs/06-dataframes-lab.html#groups",
    "href": "labs/06-dataframes-lab.html#groups",
    "title": "Lab 06: Working with Tables",
    "section": "Groups",
    "text": "Groups\nYour data will often include natural groupings (like species in the penguins table or dart types in the darts table), and you will want to summarize information about each of those groups. The way that you do this with dplyr involves the use of group_by() and summarize().\n\npenguins |> group_by(species)\n\n# A tibble: 344 × 8\n# Groups:   species [3]\n   species island    bill_length_mm bill_depth_mm flipper_…¹ body_…² sex    year\n   <fct>   <fct>              <dbl>         <dbl>      <int>   <int> <fct> <int>\n 1 Adelie  Torgersen           39.1          18.7        181    3750 male   2007\n 2 Adelie  Torgersen           39.5          17.4        186    3800 fema…  2007\n 3 Adelie  Torgersen           40.3          18          195    3250 fema…  2007\n 4 Adelie  Torgersen           NA            NA           NA      NA <NA>   2007\n 5 Adelie  Torgersen           36.7          19.3        193    3450 fema…  2007\n 6 Adelie  Torgersen           39.3          20.6        190    3650 male   2007\n 7 Adelie  Torgersen           38.9          17.8        181    3625 fema…  2007\n 8 Adelie  Torgersen           39.2          19.6        195    4675 male   2007\n 9 Adelie  Torgersen           34.1          18.1        193    3475 <NA>   2007\n10 Adelie  Torgersen           42            20.2        190    4250 <NA>   2007\n# … with 334 more rows, and abbreviated variable names ¹​flipper_length_mm,\n#   ²​body_mass_g\n\n\nNotice that the data are not changed, but the tibble now includes information about the groupings (Groups: species [3]). Now, if you apply summarize() to it, it will know that you want summaries for each group.\n\npenguins |> \n  group_by(species) |> \n  summarize(\n    mean_bill_length = mean(bill_length_mm, na.rm = TRUE),\n    var_bill_length = var(bill_length_mm, na.rm = TRUE)\n  )\n\n# A tibble: 3 × 3\n  species   mean_bill_length var_bill_length\n  <fct>                <dbl>           <dbl>\n1 Adelie                38.8            7.09\n2 Chinstrap             48.8           11.2 \n3 Gentoo                47.5            9.50\n\n\nNote that you can summarize by multiple groups.\n\npenguins |> \n  group_by(species, island) |> \n  summarize(\n    mean_bill_length = mean(bill_length_mm, na.rm = TRUE),\n    var_bill_length = var(bill_length_mm, na.rm = TRUE)\n  )\n\n# A tibble: 5 × 4\n# Groups:   species [3]\n  species   island    mean_bill_length var_bill_length\n  <fct>     <fct>                <dbl>           <dbl>\n1 Adelie    Biscoe                39.0            6.15\n2 Adelie    Dream                 38.5            6.08\n3 Adelie    Torgersen             39.0            9.15\n4 Chinstrap Dream                 48.8           11.2 \n5 Gentoo    Biscoe                47.5            9.50\n\n\nA handy little function you will often want to use in summaries is n(). This counts the number of observations in each group.\n\npenguins |> \n  group_by(species, island) |> \n  summarize(\n    count = n(),\n    mean_bill_length = mean(bill_length_mm, na.rm = TRUE),\n    var_bill_length = var(bill_length_mm, na.rm = TRUE)\n  )\n\n# A tibble: 5 × 5\n# Groups:   species [3]\n  species   island    count mean_bill_length var_bill_length\n  <fct>     <fct>     <int>            <dbl>           <dbl>\n1 Adelie    Biscoe       44             39.0            6.15\n2 Adelie    Dream        56             38.5            6.08\n3 Adelie    Torgersen    52             39.0            9.15\n4 Chinstrap Dream        68             48.8           11.2 \n5 Gentoo    Biscoe      124             47.5            9.50\n\n\nNow, suppose you wanted to get the three penguins in each species with the longest bills. To do that, you can apply the slice_max() function to the grouped penguins table.\n\npenguins |> \n  group_by(species) |> \n  slice_max(bill_length_mm, n = 3)\n\n# A tibble: 9 × 8\n# Groups:   species [3]\n  species   island    bill_length_mm bill_depth_mm flipper…¹ body_…² sex    year\n  <fct>     <fct>              <dbl>         <dbl>     <int>   <int> <fct> <int>\n1 Adelie    Torgersen           46            21.5       194    4200 male   2007\n2 Adelie    Torgersen           45.8          18.9       197    4150 male   2008\n3 Adelie    Biscoe              45.6          20.3       191    4600 male   2009\n4 Chinstrap Dream               58            17.8       181    3700 fema…  2007\n5 Chinstrap Dream               55.8          19.8       207    4000 male   2009\n6 Chinstrap Dream               54.2          20.8       201    4300 male   2008\n7 Gentoo    Biscoe              59.6          17         230    6050 male   2007\n8 Gentoo    Biscoe              55.9          17         228    5600 male   2009\n9 Gentoo    Biscoe              55.1          16         230    5850 male   2009\n# … with abbreviated variable names ¹​flipper_length_mm, ²​body_mass_g\n\n\nYou can also get a random sample from each group (known as a stratified random sample) with slice_sample().\n\npenguins |> \n  group_by(species) |> \n  slice_sample(n = 3)\n\n# A tibble: 9 × 8\n# Groups:   species [3]\n  species   island bill_length_mm bill_depth_mm flipper_le…¹ body_…² sex    year\n  <fct>     <fct>           <dbl>         <dbl>        <int>   <int> <fct> <int>\n1 Adelie    Dream            42.3          21.2          191    4150 male   2007\n2 Adelie    Biscoe           38.1          16.5          198    3825 fema…  2009\n3 Adelie    Dream            36            17.9          190    3450 fema…  2007\n4 Chinstrap Dream            48.5          17.5          191    3400 male   2007\n5 Chinstrap Dream            52            20.7          210    4800 male   2008\n6 Chinstrap Dream            51.3          19.9          198    3700 male   2007\n7 Gentoo    Biscoe           48.4          14.6          213    5850 male   2007\n8 Gentoo    Biscoe           46.4          15.6          221    5000 male   2008\n9 Gentoo    Biscoe           45.2          16.4          223    5950 male   2008\n# … with abbreviated variable names ¹​flipper_length_mm, ²​body_mass_g\n\n\nSome other useful slice_*() functions include the following\n\n\nslice_head(n) takes the first n rows in each group.\n\nslice_tail(n) takes the last n rows in each group.\n\nslice_min(x, n) takes the n rows with the smallest values of x.\n\nslice_max(x, n) takes the n rows with the largest value of x.\n\nslice_sample(n) takes n random rows in each group.\n\nExercises\n\nGroup the penguins data by species and summarize the mean flipper length.\nGroup the penguins data by island and count the number of penguins on each. Hint: you can use n() to do this.\nGroup the penguins data by sex and summarize the mean and variance in body mass.\nGroup the penguins data by sex and species and summarize the mean and variance in body mass.\nGroup the darts data by type and summarize the mean length, width, and height.\nGroup the darts data by type and take a random sample of length five from each group.\nRemove any potential duplicates from the darts data with distinct()."
  },
  {
    "objectID": "labs/07-evaluation-lab.html",
    "href": "labs/07-evaluation-lab.html",
    "title": "Lab 07: Evaluating Linear Models",
    "section": "",
    "text": "This lab will guide you through the process of\n\nimporting and exporting data tables\nVisualizing a linear model\n\nUsing AB lines\nUsing predictions\nAdding prediction and confidence intervals\n\n\nEvaluating a linear model\n\ninterpreting model summaries in R\nt-tests\nANOVA\n\n\nDiagnostic plots with plot() and check_model()\n\nResidual Histogram\nRaw Residuals v Fitted\nStandardized Residuals v Fitted\nResiduals v Leverage (and Cook’s Distance)\nQ-Q Plot\n\n\n\nWe will be using the following packages:\n\narchdata\npalmerpenguins\nperformance\nskimr\ntidyverse\n\nYou’ll want to install performance with install.packages(\"performance\").\n\nlibrary(archdata)\nlibrary(palmerpenguins)\nlibrary(performance)\nlibrary(skimr)\nlibrary(tidyverse)\n\n\n\n\ncars\n\nIncludes measurements of car speed and stopping distance.\npackage: datasets\n\nreference: https://stat.ethz.ch/R-manual/R-devel/library/datasets/html/cars.html\n\n\n\n\nDartPoints\n\nIncludes measurements of 91 Archaic dart points recovered during surface surveys at Fort Hood, Texas.\npackage: archdata\n\nreference: https://cran.r-project.org/web/packages/archdata/archdata.pdf\n\n\n\n\nOxfordPots\n\nIncludes percentages of late Romano-British Oxford Pottery on 30 sites, along with their distance from Oxford.\nreference: https://cran.r-project.org/web/packages/archdata/archdata.pdf\n\n\n\n\npenguins\n\nIncludes measurements for penguin species, island in Palmer Archipelago, size (flipper length, body mass, bill dimensions), and sex.\npackage: palmerpenguins\n\nreference: https://allisonhorst.github.io/palmerpenguins/reference/penguins.html"
  },
  {
    "objectID": "labs/07-evaluation-lab.html#readwrite-data",
    "href": "labs/07-evaluation-lab.html#readwrite-data",
    "title": "Lab 07: Evaluating Linear Models",
    "section": "Read/Write Data",
    "text": "Read/Write Data\n\n\n\n\n\nFigure 1: Elements of a Data Frame.\n\n\nThis sections covers how to import data into and export data out of R, with a focus on rectangular data or tables. While many formats exist for representing rectangular data in plain-text file, the most common is perhaps common-separated values. You can spot this format by looking for files on your computer with the .csv extension. If we had saved the data represented in Figure 1 to a csv file, the plain text of that file would look like this:\ntype, length, width, height\nElko, 2.03, 0.8, 3.23 \nRosegate, 1.4, 0.4, 2.4\nDSN, 1.9, 0.3, 1.29\nElko, 2.1, 0.7, 2.7\nClovis, 3.3, 0.95, 4.15 \nwith the first row typically assumed to contain the column names, also known as the header. To prove this to yourself, you can open a .csv file using a text editor like Notepad.\nBase R provides two functions for reading and writing these sorts of files: read.csv() and write.csv(). These have some unfortunate default behavior, however, so we are going to focus on their tidyverse equivalents. Those are read_csv() and write_csv() from the readr package. There are three main differences between these functions:\n\nthe tidy functions cannot be used without loading readr,\n\nread_csv() reads the data in as a tibble rather than a data.frame ,and\n\nwrite_csv() does not by default add a column with row names or row ids to the exported csv file.\n\n\nis a cost of doing business, but (2) and (3) more than make up for that.\n\nFile Paths\nTo read and write data into R, you have to point R to where that data lives on your computer, meaning you have to specify a path to the file that holds the data. Suppose, for example, the penguins.csv file lives on Leslie Knope’s computer in this location:\nC:/Users/Leslie_Knope/qaad-course/data/penguins.csv\nThis is a file path. The “/” denotes containment, meaning the thing to the right is within the thing to the left, so the penguins.csv file is in the data folder, which is in the qaad-course folder, which is in the Leslie_Knope folder, which is in the Users folder, which is in the C: folder. I have called them “folders” here, of course, but you will also often hear them referred to as “directories.”\nTo read the penguins data into R from the location specified by this file path, you simply feed the file path as a character string to read_csv() like so:\n\npenguins <- read_csv(\"C:/Users/Leslie_Knope/qaad-course/data/penguins.csv\")\n\nNote that you will usually want to assign the table to an object, so that you can reference it later on during your R session.\nIf you have the data in R and you want to write to this location, it works much the same way. The only difference is that you have to tell write_csv() what table you are wanting to write to disc, hence:\n\nwrite_csv(penguins, \"C:/Users/Leslie_Knope/qaad-course/data/penguins.csv\")\n\nAbsolute v Relative Paths\nThe path listed above is known as an absolute path because it starts at the top most directory in the file path, namely “C:”, and works out from there to the exact location of the file. This top level directory is also known as the root directory. That’s “C:” for Windows systems. For MacOS the root is, well, “root”.\nWhenever you open R, your session happens somewhere on your computer, meaning it has some sense of its location within your file system. This is known as the working directory. For instance, when Leslie opens an R session, it might open in her “qaad-course” folder. The location of her working directory is, thus, “C:/Users/Leslie_Knope/qaad-course.” Because of this, she does not have to specify the entire file path to the penguins file. All she needs to specify is “data/penguins.csv,” as in read_csv(\"data/penguins.csv\"). This is known as a relative file path, as it is relative to the working directory.\nOf course, R doesn’t just open anywhere on your computer. By default, it will always open in the same place (usually the Documents folder on Windows). But you can then point it to a specific directory on your computer where you would like to work. The traditional way of doing this is with the function setwd(), as in\n\nsetwd(\"C:/Users/Leslie_Knope/qaad-course/data/penguins.csv\")\n\nThis is a brittle method, however, as it requires specifying the working directory using an absolute file path. So, if Leslie changes the location of qaad-course on her computer, if she moves it to, say, the parks-and-rec folder, it will no longer work. And, if Leslie wants to share her qaad-course folder - all the work and data and R code - with someone else, so they save that folder onto their own computer, again the call to setwd() with the absolute path will not work. The actual path could be “C:/Users/Gerry/…” or “C:/Users/Jerry/…” or something to that effect, and yet the call to setwd() uses “C:/Users/Leslie_Knope/…”.\nThe alternative to setting the working directory with setwd() is to use R projects. Fortunately, you have already done this for this class! Whenever you open an R project, the R session will automatically be set to the directory where that R project lives, whether that is on Leslie’s computer or Gerry’s or Andy’s or April’s. Doesn’t matter. Then you can use relative paths as you please, as long as you keep all the necessary data and code together in that project folder, which is what you should do!\nThere are some extra advantages to using projects, but we’ll leave it at that for now.\nExercises\n\nMake sure your working directory is set to the R project folder for the course!\nUse write_csv() to write the cars data to a file called cars.csv in your project’s data folder.\nRemove the cars data from your environment with remove(cars). If you look in the environment pane of RStudio (in the top right corner), you should see that the cars table has been removed.\nNow read cars.csv back into R with read_csv() and assign it to cars. Check to make sure the table is back in your environment."
  },
  {
    "objectID": "labs/07-evaluation-lab.html#visualize-model",
    "href": "labs/07-evaluation-lab.html#visualize-model",
    "title": "Lab 07: Evaluating Linear Models",
    "section": "Visualize Model",
    "text": "Visualize Model\nHere we are going to use ordinary least squares to build a simple linear model of penguin bill length as a function of flipper length. To aid in the interpretation of this model, it is useful to visualize the relationship or trend it suggests (if it does suggest one!). Before building that model, however, you should, as always, make sure to visualize your data! In this case, we make a simple scatter plot.\n\n# clean up data, remove rows with missing data\npenguins <- na.omit(penguins)\n\nggplot(penguins, aes(flipper_length_mm, bill_length_mm)) +\n  geom_point(size = 2) +\n  labs(\n    x = \"Flipper Length (mm)\",\n    y = \"Bill Length (mm)\"\n  ) +\n  theme_minimal()\n\n\n\n\nWhat is the relationship here? Let’s see if a linear model can help us out.\n\npenguins_model <- lm(bill_length_mm ~ flipper_length_mm, data = penguins)\n\nsummary(penguins_model)\n#> \n#> Call:\n#> lm(formula = bill_length_mm ~ flipper_length_mm, data = penguins)\n#> \n#> Residuals:\n#>    Min     1Q Median     3Q    Max \n#> -8.637 -2.698 -0.579  2.066 19.095 \n#> \n#> Coefficients:\n#>                   Estimate Std. Error t value            Pr(>|t|)    \n#> (Intercept)        -7.2186     3.2717   -2.21               0.028 *  \n#> flipper_length_mm   0.2548     0.0162   15.69 <0.0000000000000002 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 4.15 on 331 degrees of freedom\n#> Multiple R-squared:  0.427,  Adjusted R-squared:  0.425 \n#> F-statistic:  246 on 1 and 331 DF,  p-value: <0.0000000000000002\n\nThere are two ways to visualize this model:\n\nWith abline. Use the estimated coefficients (the slope and intercept) to construct a formula that will calculate values of y across the range of x. The formula has the form: \\(y \\sim a + bx\\), where is \\(a\\) is the intercept and \\(b\\) is the slope, hence abline.\nWith predict. Use the model to estimate values of \\(y\\) for specified values of \\(x\\) and construct a line from those values.\n\nOK. There’s actually three ways to do it. The most direct way that ggplot() offers uses geom_smooth(), but we’ll save that one for another time. The point here isn’t just to learn how to visualize a linear model, but to learn what it is that we are visualizing and what it means. So, let’s try an example of each, so you can get a feel for how to do this.\nABline\nggplot() has a geometry for plotting AB lines. As you might have guessed, it’s geom_abline(). All we need to do is extract the values of the coefficients from the model and feed these to the slope and intercept parameters, respectively. To do that, we will use the coefficients() function. This provides a named vector that we can use to get our estimates. Notice that we use <vector>[[<variable]] like we do with tables, only this time we are extracting a single value.\n\nbetas <- coefficients(penguins_model)\n\nbetas\n#>       (Intercept) flipper_length_mm \n#>            -7.219             0.255\n\nintercept <- betas[[\"(Intercept)\"]]\nslope <- betas[[\"flipper_length_mm\"]]\n\nNow, we can plot our model over the data. This is always useful, as you can see how the model compares to the actual observations.\n\nggplot(penguins, aes(flipper_length_mm, bill_length_mm)) +\n  geom_point(size = 2) +\n  labs(\n    x = \"Flipper Length (mm)\",\n    y = \"Bill Length (mm)\"\n  ) +\n  theme_minimal() +\n  geom_abline(\n    slope = slope,\n    intercept = intercept,\n    color = \"darkred\",\n    linewidth = 1\n  )\n\n\n\n\nWith this method, we simply supply the coefficients. ggplot() then uses those to estimate values of y for each value of x shown within the range of x represented by the plot. Notice that the line continues across the full range of the graph. This shows that the model assumes the relationship is linear, meaning in this case that it will always increase to the right (to infinity) and always decrease to the left (to negative infinity)\nPredict\nWe can also generate values of y manually with the predict() function. The key here is to supply it with our model, which it will then use to make predictions.\n\npenguins <- penguins |> mutate(estimates = predict(penguins_model))\n\nggplot(penguins) +\n  geom_point(\n    aes(flipper_length_mm, bill_length_mm),\n    size = 2\n  ) +\n  labs(\n    x = \"Flipper Length (mm)\",\n    y = \"Bill Length (mm)\"\n  ) +\n  theme_minimal() +\n  geom_line(\n    aes(flipper_length_mm, estimates),\n    color = \"darkred\",\n    size = 1\n  )\n\n\n\n\nThis is very similar to our abline graph above, but with the important difference that the trend line or modeled relationship does not extend across the entire graph.\nNote that you can use predict() to estimate the value of the response at specific values of the independent variable. To do that, you simply feed the predict() function, specifically its newdata parameter, a table with the values of the independent variable that interest you. For example, suppose you wanted to know what bill length this model would expect for a penguin having a body mass of, say, 4,500 grams. We can figure that out this way:\n\nnew_data <- tibble(flipper_length_mm = 200)\n\npredict(penguins_model, newdata = new_data)\n#>    1 \n#> 43.7\n\nIf you like, you can also do that for multiple values like so:\n\nnew_data <- tibble(flipper_length_mm = c(190, 195, 200, 205))\n\npredict(penguins_model, newdata = new_data)\n#>    1    2    3    4 \n#> 41.2 42.5 43.7 45.0\n\nIntervals\nYou can use predict() to calculate the prediction interval for these estimates by specifying interval = \"prediction\". Note, too, that we ask it to provide that interval at level = 0.95 to ensure the prediction interval is estimated at the 95% level. Recall that this is the interval within which we expect the value of Y to fall with 95% probability for each value of X.\n\npredict(\n  penguins_model, \n  newdata = new_data,\n  interval = \"prediction\",\n  level = 0.95\n)\n#>    fit  lwr  upr\n#> 1 41.2 33.0 49.4\n#> 2 42.5 34.3 50.6\n#> 3 43.7 35.6 51.9\n#> 4 45.0 36.8 53.2\n\nIf we set interval = \"confidence\", we can get the confidence interval, or the interval within which we expect the average value of Y to fall with 95% probability for each value of X.\n\npredict(\n  penguins_model, \n  newdata = new_data,\n  interval = \"confidence\",\n  level = 0.95\n)\n#>    fit  lwr  upr\n#> 1 41.2 40.6 41.8\n#> 2 42.5 42.0 43.0\n#> 3 43.7 43.3 44.2\n#> 4 45.0 44.6 45.5\n\nWe can actually add these to our model using the function geom_ribbon() like so.\n\n\nconfidence <- predict(\n  penguins_model, \n  interval = \"confidence\",\n  level = 0.95\n)\n\n# coerce to a table\nconfidence <- as_tibble(confidence)\n\n# add the X values\nconfidence <- confidence |> mutate(flipper_length_mm = penguins$flipper_length_mm)\n\nggplot() +\n  geom_ribbon(\n    data = confidence,\n    aes(x = flipper_length_mm, ymin = lwr, ymax = upr),\n    fill = \"gray85\"\n  ) +\n  geom_line(\n    data = confidence, \n    aes(flipper_length_mm, fit),\n    color = \"darkred\",\n    linewidth = 1\n  ) +\n  geom_point(\n    data = penguins,\n    aes(flipper_length_mm, bill_length_mm),\n    size = 2\n  ) +\n  theme_minimal() +\n  labs(\n    x = \"Flipper Length (mm)\",\n    y = \"Bill Length (mm)\"\n  )\n\n\n\n\nThe ribbon geometry is, in effect, a polygon defined by an upper and lower line (ymax and ymin, respectively). Note that we added the ribbon before adding the trend line. If we did the reverse, the ribbon would be plotted over the trend line, thus obscuring it.\nExercises\n\nBuild a model of penguin flipper length by body mass.\n\nMake sure to visualize your data first! Make a scatter plot!\n\n\nNow plot the modeled relationship between flipper length and body mass.\n\nUse coefficients() and geom_abline().\nUse predict() and geom_line().\nAdd the confidence interval to the second plot using geom_ribbon()."
  },
  {
    "objectID": "labs/07-evaluation-lab.html#model-summary",
    "href": "labs/07-evaluation-lab.html#model-summary",
    "title": "Lab 07: Evaluating Linear Models",
    "section": "Model Summary",
    "text": "Model Summary\nLet’s return to the summary() of our model and discuss it in a little more detail.\n\nsummary(penguins_model)\n#> \n#> Call:\n#> lm(formula = bill_length_mm ~ flipper_length_mm, data = penguins)\n#> \n#> Residuals:\n#>    Min     1Q Median     3Q    Max \n#> -8.637 -2.698 -0.579  2.066 19.095 \n#> \n#> Coefficients:\n#>                   Estimate Std. Error t value            Pr(>|t|)    \n#> (Intercept)        -7.2186     3.2717   -2.21               0.028 *  \n#> flipper_length_mm   0.2548     0.0162   15.69 <0.0000000000000002 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 4.15 on 331 degrees of freedom\n#> Multiple R-squared:  0.427,  Adjusted R-squared:  0.425 \n#> F-statistic:  246 on 1 and 331 DF,  p-value: <0.0000000000000002\n\nThere are four major components of the summary: (i) the function call, (ii) the residual distribution, (iii) the coefficients table, and (iv) the summary statistics. Let’s work through each one of these separately.\nThe Call. This refers to your “call” to the lm() function, which includes the model specification (meaning, the formula used to specify the model) and the data used to fit that model specification.\nThe Residual Distribution. This refers to the distribution of the residuals, here represented using quartiles and the minimum and maximum values. You can visualize the quartiles for the residuals really quickly with a boxplot.\n\nboxplot(residuals(penguins_model), ylab = \"Raw Residuals\")\n\n\n\n\nThe thing to keep in mind here is that the linear model fit with OLS assumes that the residuals are normally distributed, \\(\\epsilon \\approx N(0, \\sigma)\\) with a mean of zero and standard deviation, \\(\\sigma\\). Crucially, in a normal distribution, the median value is equal to the mean. In the boxplot, that means the dark line representing the median should be really close to zero. The first and third quartiles should also be equidistant from the median. In the boxplot, that means the median bar should be about halfway between top (3Q) and bottom (1Q) of the box. If it’s not, that means your residuals are not normally distributed. The minimum and maximum values kinda have this flavor, but not to the same degree.\nThe Coefficients Table. The number of rows in the coefficients table is equal to the number of variables in the model specification plus one. In this case, that’s two rows, one for the intercept and one for the flipper length variable. There are also five columns, one for the variables and the other four listing their coefficient estimates, the standard errors of those estimates, the t-statistics associated with the estimates, and the p-values for those t-statistics. If you just want the coefficients, as noted above, you can use the coefficients() function to get them. You can also get the variable names with variable.names().\n\nvariable.names(penguins_model)\n#> [1] \"(Intercept)\"       \"flipper_length_mm\"\n\ncoefficients(penguins_model)\n#>       (Intercept) flipper_length_mm \n#>            -7.219             0.255\n\nNote that the t-statistic is just the ratio of the coefficient estimate to its standard error:\n\\[t = \\frac{\\beta_i}{se(\\beta_i)}\\]\nIf you multiply the standard error of the coefficients by $$1.96, you get the 95% confidence interval for the coefficient estimate. You can extract these confidence intervals from the linear model with the confint() function.\n\nconfint(penguins_model)\n#>                     2.5 % 97.5 %\n#> (Intercept)       -13.655 -0.783\n#> flipper_length_mm   0.223  0.287\n\nYou can also visualize those intervals using a dot and whisker plot using geom_errorbar() like so:\n\n# getting the standard errors is a smidge awkward\nbetas <- tibble(\n  x = coefficients(penguins_model),\n  y = variable.names(penguins_model),\n  se = coefficients(summary(penguins_model))[, \"Std. Error\"]\n)\n\nggplot(betas, aes(x, y)) +\n  geom_vline(\n    xintercept = 0, \n    linewidth = 1, \n    color = \"darkred\"\n  ) +\n  geom_point(size = 3) +\n  geom_errorbar(\n    aes(xmin = x - (1.96*se), xmax = x + (1.96*se)),\n    width = 0.33 # height of whiskers\n  ) +\n  labs(\n    x = \"Estimate\",\n    y = NULL,\n    title = \"Model Coefficients\"\n  ) +\n  theme_minimal()\n\n\n\n\nTwo things to note here. First, the width of the confidence interval tells you something about how certain we are about the estimate, wider intervals mean less certainty, narrow intervals more certainty. Second, you can visually inspect whether the 95% confidence intervals overlap with zero. In this case, they do not. This tells you that it is extremely improbable that the coefficients could equal zero, since there is only a 2.5% chance or less of the coefficient being zero for each estimate. So, the dot and whisker plot with the coefficient estimates and their standard errors is effectively a visual representation of the t-test.\nSummary statistics. The last element of the summary() is a set of statistics. This includes the residual standard error or the standard deviation in the error. The formula for this statistic is\n\\[\n\\begin{aligned}\n\\sigma^2_\\epsilon &= \\frac{1}{n-k}\\sum \\epsilon^2 \\\\\nse_\\epsilon &= \\sqrt{\\sigma^2}\n\\end{aligned}\n\\] where \\(n-k\\) is the degrees of freedom, in this case the number of observation \\(n\\) minus the number of model parameters \\(k\\) (meaning \\(\\beta_0\\) and \\(\\beta_1\\)). In R, you can calculate this standard error like so:\n\ne <- residuals(penguins_model)\ndf <- summary(penguins_model)$df[2]\n\nsqrt(sum(e^2)/df)\n#> [1] 4.15\n\nThe set of summary statistics for a linear model also includes the R-squared value, which - if you recall - is the ratio of the Model Sum of Squares \\(SS_M\\) to the Total Sum of Squares \\(SS_T\\) (defined by the null or intercept-only model). This provides a measure of the proportion of the total variance explained by the model. You will also see here the so-called Adjusted R-squared. This is simply the R-squared statistic weighted by the complexity of the model, which is important because R-squared tends to increase with increases in model complexity, meaning each time you add an independent variable to a model it’s going to capture some amount of the total variance in the dependent variable.\nFinally, there is the F-statistic, which is similar to the R-squared value, but is estimated by the ratio of the model variance to the residual variance. The p-value is obtained by comparing the F-statistic to the F-distribution with \\(k-1=1\\) and \\(n-k=331\\) degrees of freedom. These are the summary statistics provided by the ANOVA, so let’s dive into that in the next section.\nExercises\n\nBuild a model of penguin flipper length by body mass.\n\nMake sure to visualize your data first! Make a scatter plot!\n\n\nRun summary() on your model.\n\nDo the residuals appear to be normally distributed?\nWhat is the R-Squared value?\nDoes body mass explain much of the variance in flipper length?\nHow would you interpret this result? What is the relationship between flipper length and body mass?"
  },
  {
    "objectID": "labs/07-evaluation-lab.html#anova",
    "href": "labs/07-evaluation-lab.html#anova",
    "title": "Lab 07: Evaluating Linear Models",
    "section": "ANOVA",
    "text": "ANOVA\nA really powerful way to evaluate a linear model is to submit it to an ANOVA test. If you recall, an ANOVA for comparing the distribution of a variable across groups compares the ratio of the between-group variance to the within-group variance. This is known as an F-statistic. It gives us a sense of how much variance is coming from the groups themselves versus how much is coming from the individual observations within each group. If more of the variance is coming from the groups and not the individual observations, that indicates that at least one of the groups is not like the others. Crucially, the F-statistic can be compared to an F-distribution to determine whether the statistic is likely under the null hypothesis that the groups are not different.\nAn ANOVA applied to a statistical model works in much the same way. The only catch is that we are not comparing the variance between groups but between models. More precisely, we are comparing the variance captured by the model to the remaining or residual variance. That is,\n\\[F = \\frac{\\text{model variance}}{\\text{residual variance}}\\]\nwhere model variance is equal to \\(SS_M/k\\) and residual variance is equal to \\(SS_R/(n-k-1)\\) (or the square of the residual standard error). The sum of these two squared errors (\\(SS_M\\) and \\(SS_R\\)) is actually the total sum of squares \\(SS_T\\), which is defined by the mean or intercept-only model (sometimes referred to as the null model). Because the model includes at least one more parameter than the null model, an ANOVA applied to such a model can be thought of as answering the following\nQuestion: does the model capture a sufficient amount of the variance to make its increased complexity worthwhile?\nAgain, taking the ratio of the variance explained by the model to the residual variance provides an estimate of the F-statistic, which can be compared to the F-distribution to determine how likely it is under the null hypothesis that the more complex model does not explain more variance than the null model.\nTo conduct this test in R, it’s quite simple. Simply supply a fitted linear model to the aov() function. First, however, make sure to visualize your data! Note that summarizing the ANOVA with summary() prints an ANOVA table.\n\npenguins_model_anova <- aov(penguins_model)\n\nsummary(penguins_model_anova)\n#>                    Df Sum Sq Mean Sq F value              Pr(>F)    \n#> flipper_length_mm   1   4235    4235     246 <0.0000000000000002 ***\n#> Residuals         331   5694      17                                \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nThe Mean Squared Error (Mean Sq) is calculated by dividing the Sum of Squares (Sum Sq) by the degrees of freedom (Df), the latter being a function of the number of independent variables or predictors in the model and the number of observations. MSE is, thus, another way of referring to the model and residual variance. The F-statistic is then calculated by dividing the Model variance or MSE (in this case flipper_length_mm) by the Residual variance or MSE (Residuals), in this case \\(F = 4235/17 = 246.2\\). By comparing this statistic to an F-distribution with those degrees of freedom, we get a very small p-value, much less than the standard critical value of 0.05. We can thus reject the null hypothesis and conclude that the variance explained by this model is indeed worth the price of increased complexity.\nExercises\n\nUse aov() to conduct an ANOVA on the penguins model you created in the previous section.\n\nBe sure to state your null and alternative hypotheses.\nSpecify your critical value, too.\n\n\nSummarize the penguins model again.\nNow summarize the ANOVA with summary().\n\nDoes the model provide a significant improvement over the null model?\nDid you get the same F-statistic and p-value as in the model summary?"
  },
  {
    "objectID": "labs/07-evaluation-lab.html#diagnostic-plots",
    "href": "labs/07-evaluation-lab.html#diagnostic-plots",
    "title": "Lab 07: Evaluating Linear Models",
    "section": "Diagnostic Plots",
    "text": "Diagnostic Plots\nWhenever you build a model, it is critically important to visualize the model and its assumptions as this will give you some indication about whether those assumptions have been met. Here, we’ll visualize our penguins model, starting with the residuals.\nResiduals\nOne important assumption of OLS regression is that the errors are normally distributed. A simple histogram of the residuals will give us some indication of that. To get the residuals in our model, we can use the residuals() function.\n\npenguin_fit <- tibble(residuals = residuals(penguins_model))\n\nggplot(penguin_fit, aes(residuals)) +\n  geom_histogram() +\n  labs(\n    x = \"Residual Bill Length (mm)\",\n    y = \"Count\"\n  )\n\n\n\n\nDo these look normally distributed to you? Do they have the approximate shape of a bell curve? If a visual check does not suffice, you can always try a Shapiro-Wilk test for normality. To do that in R, you can use the shapiro.test() function. Note that the null hypothesis for this test is that the variable is not normally distributed.\n\nshapiro.test(residuals(penguins_model))\n#> \n#>  Shapiro-Wilk normality test\n#> \n#> data:  residuals(penguins_model)\n#> W = 1, p-value = 0.00000005\n\nAdditional Diagnostics\nBase R provides a really useful plot() method for linear models. You just feed this function your linear model and tell it which kind of plot you want to make. Here is a list of options, which you can supply to the which parameter:\n\nResiduals vs Fitted plot\nNormal Q-Q\nScale-Location\nCook’s Distance\nResiduals vs Leverage\nCook’s Distance vs Leverage\n\nHere is an example of the Q-Q Plot for our penguins model.\n\nplot(penguins_model, which = 2)\n\n\n\n\nThe plot() function in this case is extremely utilitarian. If you just want a quick visual diagnostic for your own edification, I recommend using this. However, if you want to present all these plots together in a clean way that easily communicates the assumptions being tested by each, I recommend using the check_model() function from the performance package.\n\ncheck_model(\n  penguins_model, \n  check = c(\"linearity\", \"homogeneity\", \"outliers\", \"qq\")\n)\n\n\n\n\nWith this, we can hazard all the following conclusions:\n\nThe relationship is roughly linear, though you can see some wiggle.\nThe model departs from homoscedasticity.\nNo observations have an undue influence on the model.\nLooks like the residuals might be right skewed. (compare to histogram above)\n\nNote that none of the assumptions are met perfectly. This will never be the case, not with real world data. It should also clue you in to the fact that model evaluation is never certain. It always involves some risks that you might be wrong about your model.\nExercises\n\nExtract the residuals from your model of flipper length by body mass and visualize their distribution with a histogram.\n\nDo the residuals look normally distributed?\nUse the Shapiro Wilk test to verify this.\n\n\nExplore the model with base plot().\nNow, use check_model().\n\nWhat do these plots tell you about the model?"
  },
  {
    "objectID": "labs/07-evaluation-lab.html#homework",
    "href": "labs/07-evaluation-lab.html#homework",
    "title": "Lab 07: Evaluating Linear Models",
    "section": "Homework",
    "text": "Homework\n\nLoad the following datasets from the archdata package using data().\n\nDartPoints\nOxfordPots\n\n\nPractice writing these to disc and reading them back into R.\n\nFor each dataset, use write_csv() to write the table to a csv file. Put it in the data folder in your QAAD course project folder!\nRemove the table from your environment with remove(), e.g., remove(DartPoints). (This is just so you can see it returning to your environment in the next step, but also to practice using remove().)\nRead the table back into R with read_csv(). Make sure to assign it to an object! A good idea is to avoid using upper-case letters as much as possible!\n\n\nUsing the DartPoints dataset, build a linear model showing the relationship (if any) between the length and width of dart points. Be sure to do all of the following:\n\nUse skim() to quickly summarize your data.\nVisualize the data with a scatter plot!\nState your null and alternative hypotheses.\nBuild a model with lm().\nUse summary() to report the model.\nDoes the model refute the null hypothesis?\nConduct an ANOVA of the model with aov().\nUse summary() to print the ANOVA table.\nUse coefficients() and geom_abline() to visualize the modeled relationship. Be sure to plot this over the data!\nAdd a confidence interval with geom_ribbon().\nUse check_model() to visually inspect the model.\nDoes the model satisfy the assumptions of linear regression?\n\n\nUsing the OxfordPots dataset, build a linear model showing the relationship (if any) between the percentage of Oxford Pots found on an archaeological site and the distance of that site from the city of Oxford. Be sure to do all of the following:\n\nUse skim() to quickly summarize your data.\nVisualize the data with a scatter plot!\nState your null and alternative hypotheses.\nBuild a model with lm().\nUse summary() to report the model.\nDoes the model refute the null hypothesis?\nConduct an ANOVA of this with aov().\nUse summary() to print the ANOVA table.\nUse predict() and geom_line() to visualize the modeled relationship. Be sure to plot this over the data!\nAdd a confidence interval with geom_ribbon().\nUse check_model() to visually inspect the model.\nDoes the model satisfy the assumptions of linear regression?"
  },
  {
    "objectID": "labs/08-multiple-lm-lab.html",
    "href": "labs/08-multiple-lm-lab.html",
    "title": "Lab 08: Multiple Linear Models",
    "section": "",
    "text": "This lab will guide you through the process of\n\nCreating multiple linear models\nEvaluating model assumptions with diagnostic plots\nChecking for correlation in the predictors\n\nScatterplot matrix with plot()\n\nPearson’s Correlation with cor()\n\nVariance Inflation Factor with vif()\n\n\n\n\nWe will be using the following packages:\n\narchdata\ncar\nISLR2\nperformance\nskimr\ntidyverse\n\nYou’ll want to install ISLR2 and car with install.packages(c(\"ISLR2\", \"car\")).\n\nlibrary(archdata)\nlibrary(car)\nlibrary(ISLR2)\nlibrary(performance)\nlibrary(skimr)\nlibrary(tidyverse)\n\n\n\n\nadverts\n\nObservations of product sales and investments in advertising across various media.\npackage: none\nreference: https://www.statlearning.com/resources-second-edition\n\n\n\n\nBoston\n\nA dataset containing housing values in 506 suburbs of Boston.\npackage: ISLR2\n\nreference: https://cran.r-project.org/web/packages/ISLR2/ISLR2.pdf\n\n\n\n\nHandAxes\n\nIncludes measurements of handaxes from the Furze Platt site stored at the Royal Ontario Museum.\npackage: archdata\n\nreference: https://cran.r-project.org/web/packages/archdata/archdata.pdf"
  },
  {
    "objectID": "labs/08-multiple-lm-lab.html#multiple-regression",
    "href": "labs/08-multiple-lm-lab.html#multiple-regression",
    "title": "Lab 08: Multiple Linear Models",
    "section": "Multiple Regression",
    "text": "Multiple Regression\nIn this section, we will walk through how to build a multiple linear model in R. This is not fundamentally different than building a simple linear model. The only difference is that we need to update the model formula. For the simple linear model, the formula is just y ~ x, for the multiple linear model, it’s y ~ x1 + x2 + … + xn. We simply add the covariates together using the plus-sign.\nLet’s work through an example with the adverts data set used in the textbook An Introduction to Statistical Learning With Applications in R. We want to know whether investment in different advertising media increases sales of some product. The website for the ISLR book hosts several data sets. The authors have also written companion R packages (ISLR and ISLR2, for the first and second editions, respectively). Now, as it happens, the path to a csv file does not have to be a local path pointing to a location on your computer. It can be also be a url pointing to where a file is stored remotely on a website, so reading in the advertising data from the ISLR website is as easy as this.\n\nadverts <- read_csv(\"https://www.statlearning.com/s/Advertising.csv\") |> \n  rename_with(tolower) |> \n  select(sales, tv, radio, newspaper)\n\nBefore fitting the model, let’s have a look at some summaries of the data with skim().\n\nskim(adverts)\n\n\nData summary\n\n\nName\nadverts\n\n\nNumber of rows\n200\n\n\nNumber of columns\n4\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n4\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\nsales\n0\n1\n14.0\n5.22\n1.6\n10.38\n12.9\n17.4\n27.0\n▁▇▇▅▂\n\n\ntv\n0\n1\n147.0\n85.85\n0.7\n74.38\n149.8\n218.8\n296.4\n▇▆▆▇▆\n\n\nradio\n0\n1\n23.3\n14.85\n0.0\n9.97\n22.9\n36.5\n49.6\n▇▆▆▆▆\n\n\nnewspaper\n0\n1\n30.6\n21.78\n0.3\n12.75\n25.8\n45.1\n114.0\n▇▆▃▁▁\n\n\n\n\n\nAnd now the model.\n\nadverts_lm <- lm(sales ~ tv + radio + newspaper, data = adverts)\n\nsummary(adverts_lm)\n#> \n#> Call:\n#> lm(formula = sales ~ tv + radio + newspaper, data = adverts)\n#> \n#> Residuals:\n#>    Min     1Q Median     3Q    Max \n#> -8.828 -0.891  0.242  1.189  2.829 \n#> \n#> Coefficients:\n#>             Estimate Std. Error t value            Pr(>|t|)    \n#> (Intercept)  2.93889    0.31191    9.42 <0.0000000000000002 ***\n#> tv           0.04576    0.00139   32.81 <0.0000000000000002 ***\n#> radio        0.18853    0.00861   21.89 <0.0000000000000002 ***\n#> newspaper   -0.00104    0.00587   -0.18                0.86    \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 1.69 on 196 degrees of freedom\n#> Multiple R-squared:  0.897,  Adjusted R-squared:  0.896 \n#> F-statistic:  570 on 3 and 196 DF,  p-value: <0.0000000000000002\n\nNote the change in how we interpret the coefficients! Each coefficient must be interpreted relative to some value of the other covariates. For example, the coefficient estimate for television is 0.046. We interpret this as saying, “For some given investment in radio and newspaper advertising, increasing tv advertising by $1000 will increase the number of units sold by approximately 46 units (because the units are measured in thousands, so 1000 x 0.046).”\nExercises\n\nLoad the Boston dataset from the ISLR2 package with data().\nSubset the variables in the dataset using the select() function from dplyr. Choose all the following variables:\n\n\nmedv = median household value\n\nrm = number of rooms\n\ncrim = per capita crime rate\n\nlstat = percent of households with low socioeconomic status\n\n\nSummarize the table with skim().\nMake a simple linear model of median household value (medv) as a function of average number of rooms (rm).\n\nCall the model simple_lm.\n\n\nNow make a multiple linear model of median household value (medv) as a function of average number of rooms, per capita crime rate, and percent of household with low socioeconomic status (rm, crim, and lstat, respectively).\n\nCall the model boston_lm.\n\n\nSummarize the model with summary().\n\nAre all the coefficients significantly different than zero?\nHow much of the variance in house value is explained by these variables (ie what is the R-squared value)?\n\n\nHow do you interpret the coefficient for number of rooms? What effect does increasing the number of rooms have?"
  },
  {
    "objectID": "labs/08-multiple-lm-lab.html#model-evaluation",
    "href": "labs/08-multiple-lm-lab.html#model-evaluation",
    "title": "Lab 08: Multiple Linear Models",
    "section": "Model Evaluation",
    "text": "Model Evaluation\nAs always, we can use the base plot() function to check model assumptions, using the which argument to specify the type of plot we want to make. For example, we can make a Q-Q plot like so:\n\nplot(adverts_lm, which = 2)\n\n\n\n\nOr, we can use the check_model() function from the performance package.\n\ncheck_model(\n  adverts_lm,\n  check = c(\"linearity\", \"homogeneity\", \"outliers\", \"qq\")\n)\n\n\n\n\nHow does it look? Have we met the assumptions of linear regression? Consider, for exmaple, that Q-Q plot. It would seem to suggest that the residuals are skewed to the left. That’s not good because the linear model assumes that the residuals are normally distributed and centered on zero. Another way to check for this is to plot a histogram of the residuals and check whether it has the shape of a bell curve.\n\nadverts_residuals <- tibble(residuals = residuals(adverts_lm))\n\nggplot(adverts_residuals, aes(residuals)) +\n  geom_histogram() +\n  labs(\n    x = \"Residual Sales\",\n    y = \"Count\"\n  )\n\n\n\n\n😬 You can see the left skew as indicated by the Q-Q plot.\nExercises\n\nPlot a histogram of the residuals in boston_lm.\n\nUse residuals() to get the residuals out of the model object.\nUse geom_histogram().\n\n\nRun check_model() on boston_lm and check for linearity, homogeneity, outliers, and qq.\nDoes the model meet the assumptions of linear regression?"
  },
  {
    "objectID": "labs/08-multiple-lm-lab.html#anova",
    "href": "labs/08-multiple-lm-lab.html#anova",
    "title": "Lab 08: Multiple Linear Models",
    "section": "ANOVA",
    "text": "ANOVA\nWhen evaluating the model, we should also check whether adding variables actually makes a significant improvement. To do that, we need to compare the complex model to a simpler model. In R, we do that with the anova() function. Here, we’re going to compare the full model to set of simpler models that are nested within it. Note that in each case the simpler model is a subset of the more complex model (ie, these are nested models)! The ANOVA wouldn’t work otherwise!\n\nm1 <- lm(sales ~ tv, data = adverts)\nm2 <- lm(sales ~ tv + radio, data = adverts)\nm3 <- lm(sales ~ tv + radio + newspaper, data = adverts)\n\nanova(m1, m2, m3)\n#> Analysis of Variance Table\n#> \n#> Model 1: sales ~ tv\n#> Model 2: sales ~ tv + radio\n#> Model 3: sales ~ tv + radio + newspaper\n#>   Res.Df  RSS Df Sum of Sq      F              Pr(>F)    \n#> 1    198 2103                                            \n#> 2    197  557  1      1546 544.05 <0.0000000000000002 ***\n#> 3    196  557  1         0   0.03                0.86    \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nThis is a sequential ANOVA that does comparisons in order, first comparing the tv model to the intercept model, then the tv + radio model to the tv model, and so on, to the full model. For each step, it reports the residual sum of squares (RSS or the error in the model) and evaluates whether the difference in that value between the models is significant. The degrees of freedom (DF) represents the difference in residual degrees of freedom (Res.Df) between the simple and complex models (for row 2, that’s 199-198=1). The sum of squares (Sum of Sq) is the difference in the Residual Sum of Squares (RSS) for each model (for row 2, that’s 5417-2103=3315). The F statistic is then the ratio of (Sum of Sq/Df) to what is effectively the mean squared error of the full model (for row 2 that’s (3315/5)/(557/196) = 1166). The intuitive idea here is that an F-statistic close to zero means that adding the covariate does not reduce the model’s RSS or error to any meaningful degree. As always, the F-statistic is then compared to an F distribution with the degrees of freedom to determine how likely that particular value is, assuming the null hypothesis that there is no significant improvement. Notice that each of these covariates makes a significant contribution to the model of sales except newspaper spending. Why might that be?\nOne last point, before moving on. You do not have to explicitly generate a sequence of nested models. You can simply call anova() on the full model and it will generate those models for you. However, do be aware that the printed ANOVA table is slightly different (additional columns and different column names) in the two cases. Here it is the for the full advertising model:\n\nanova(adverts_lm)\n#> Analysis of Variance Table\n#> \n#> Response: sales\n#>            Df Sum Sq Mean Sq F value              Pr(>F)    \n#> tv          1   3315    3315 1166.73 <0.0000000000000002 ***\n#> radio       1   1546    1546  544.05 <0.0000000000000002 ***\n#> newspaper   1      0       0    0.03                0.86    \n#> Residuals 196    557       3                                \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nExercises\n\nUse the anova() function to compare simple_lm to boston_lm.\n\nState your null and alternate hypotheses.\n\n\nIs the complex model worth it?"
  },
  {
    "objectID": "labs/08-multiple-lm-lab.html#collinearity",
    "href": "labs/08-multiple-lm-lab.html#collinearity",
    "title": "Lab 08: Multiple Linear Models",
    "section": "Collinearity",
    "text": "Collinearity\nLinear regression assumes that the predictor or independent variables are uncorrelated. However, this is often not the case. To evaluate this assumption, you can do one of two things. I actually recommend that you always do both. First, you can check for pairwise correlations visually with the pairs() function from base R or analytically with the cor() argument. Both work with tables. Second, you check for multicollinearity using the vif() function to estimate the Variance Inflation Factor for each covariate.\nCheck for collinearity\n\npairs(adverts)\n\n\n\ncor(adverts)\n#>           sales     tv  radio newspaper\n#> sales     1.000 0.7822 0.5762    0.2283\n#> tv        0.782 1.0000 0.0548    0.0566\n#> radio     0.576 0.0548 1.0000    0.3541\n#> newspaper 0.228 0.0566 0.3541    1.0000\n\nNotice that pairs() generates a scatterplot matrix with all pairwise combination of variables. If there’s no correlation, the scatterplot should look like a cloud of random points. If there is a correlation, the points will cluster along some line. The cor() function generates a correlation matrix (by default a Pearson’s correlation matrix). Of course, every variable correlates perfectly with itself, hence a value of 1 along the diagonal. Values above and below the diagonal are simply mirrors of each other, as is the case with the scatterplot matrix. The fact that each variable correlates with sales is not problematic in this case, as that is the relationship we are trying to model. What we want to focus on here is the degree of correlation between the predictors tv, radio, and newspaper. It is preferable that the correlations between these be as close to zero as possible, but again, it will rarely be the case that they are exactly zero. Any value greater than 0.7 is a cause for concern and should lead you to update your model to address it.\nCheck for multicollinearity\nGenerally, it’s a good idea to test for correlations between predictors before building a model. However, the vif() function only works on model objects, which makes sense as the variance being inflated is variance around coefficient estimates made by the model. That means, of course, that you have to build a model first before evaluating its covariates for multicollinearity. Using the linear model we made with the advertising data, that looks like this:\n\nvif(adverts_lm)\n#>        tv     radio newspaper \n#>      1.00      1.14      1.15\n\nThis generates a named vector, with one value for each covariate. A general rule of thumb is that VIF for a coefficient should be less than 5. After that, you should consider changes to your model.\nExercises\n\nUse pairs() on the Boston data to visualize pairwise relationships between variables.\n\nDo you see any potential trends?\n\n\nUse cor() on the Boston data to estimate the Pearson correlation coefficient for each pair of variables.\n\nAre there any strong correlations?\n\n\nUse vif() on the bostom_lm model to estimate the Variance Inflation Factor for each covariate.\n\nAre any greater than 5 for any of them?"
  },
  {
    "objectID": "labs/08-multiple-lm-lab.html#homework",
    "href": "labs/08-multiple-lm-lab.html#homework",
    "title": "Lab 08: Multiple Linear Models",
    "section": "Homework",
    "text": "Homework\n\nLoad the Handaxes dataset from the archdata package using data().\nNow let’s explore this dataset.\n\nCheck the variables that it includes with names(). You should see maximum length (L), breadth (B), and thickness (T), among others.\nTry renaming those to make them more informative and readable with rename().\nUse select() to subset this table, taking only those three variables (length, breadth, and thickness).\nSummarize this table with skim().\nVisualize the pairwise relationships between variables with pairs().\nEstimate Pearson’s correlation coefficient for each pair of variables with cor().\n\n\nNow making a simple linear model showing the relationship (if any) between the length and breadth of handaxes. Be sure to do all of the following:\n\nUse summary() to report the model.\nUse predict() and geom_line() to visualize the modeled relationship. Be sure to plot this over the data!\nAdd a confidence interval with geom_ribbon().\nUse check_model() to visually inspect the model.\nDoes the model satisfy the assumptions of linear regression?\n\n\nBuild a multiple linear model of handaxe length as a function of breadth and thickness. Be sure to do all of the following:\n\nUse summary() to report the model.\nUse check_model() to visually inspect the model.\nDoes the model satisfy the assumptions of linear regression?\n\n\nUse the vif() function on the multiple linear model.\n\nIs the VIF for either variable greater than 5?\n\n\nConduct an ANOVA using the anova() function to compare the simple and complex models.\n\nDoes the complex model significantly improve on the simpler model?"
  },
  {
    "objectID": "labs/10-transforms-lab.html",
    "href": "labs/10-transforms-lab.html",
    "title": "Lab 10: Transforming Variables",
    "section": "",
    "text": "This lab will guide you through the process of\n\nadding qualitative variables to a model\n\nintercept offset\ninteraction terms\n\n\ncentering and scaling variables\nother transforms\n\npolynomials\nlogarithm\n\n\n\nWe will be using the following packages:\n\npalmerpenguins\ntidyverse\n\n\nlibrary(palmerpenguins)\nlibrary(tidyverse)\n\n\n\n\npenguins\n\nIncludes measurements for penguin species, island in Palmer Archipelago, size (flipper length, body mass, bill dimensions), and sex.\npackage: palmerpenguins\n\nreference: https://allisonhorst.github.io/palmerpenguins/reference/penguins.html"
  },
  {
    "objectID": "labs/10-transforms-lab.html#qualitative-variables",
    "href": "labs/10-transforms-lab.html#qualitative-variables",
    "title": "Lab 10: Transforming Variables",
    "section": "Qualitative Variables",
    "text": "Qualitative Variables\nShade-loving plants thrive in areas that get less sunlight. Sun-loving plants thrive in areas that get more. This is a fairly intuitive and uncontroversial idea, but how would we incorporate this kind of difference between groups into a linear model? In R, we use dummy variables that re-code a qualitative variable into a numeric variable with two values, zero for the reference group (every member of this group has value zero) and one for the comparison group (every member of this group has value one). For a qualitative variable with m categories, it takes \\(m-1\\) dummy variables to code it. You can do this manually if you like, but R will do it for you, too. All you need to do is include the qualitative variable in the model formula. This can be done in one of two ways: either by adding the variable with + or multiplying the variable with *. The first allows the intercept to change for each group, the second allows the intercept and slope to change.\nLet’s look at the penguins data to see how this works.\n\nlm_base <- lm(bill_length_mm ~ body_mass_g, data = penguins) \n\nsummary(lm_base)\n#> \n#> Call:\n#> lm(formula = bill_length_mm ~ body_mass_g, data = penguins)\n#> \n#> Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -10.125  -3.043  -0.809   2.071  16.111 \n#> \n#> Coefficients:\n#>              Estimate Std. Error t value            Pr(>|t|)    \n#> (Intercept) 26.898872   1.269148    21.2 <0.0000000000000002 ***\n#> body_mass_g  0.004051   0.000297    13.6 <0.0000000000000002 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 4.39 on 340 degrees of freedom\n#>   (2 observations deleted due to missingness)\n#> Multiple R-squared:  0.354,  Adjusted R-squared:  0.352 \n#> F-statistic:  186 on 1 and 340 DF,  p-value: <0.0000000000000002\n\nHere we have a perfectly respectable model of a familiar allometric relationship between length and weight. Both the intercept and the coefficient for body mass are significant, and it explains 35% of the variation in bill length, but can we do better?\nIntercept Offset\nMaybe some species have longer bills on average than other species. To incorporate this idea, we can add a constant offset to the intercept representing the difference in mean bill length for each species. This is done by adding the species variable in the formula.\n\nlm_species_offset <- lm(\n  bill_length_mm ~ body_mass_g + species, \n  data = penguins\n) \n\nsummary(lm_species_offset)\n#> \n#> Call:\n#> lm(formula = bill_length_mm ~ body_mass_g + species, data = penguins)\n#> \n#> Residuals:\n#>    Min     1Q Median     3Q    Max \n#> -6.813 -1.672  0.134  1.472  9.290 \n#> \n#> Coefficients:\n#>                   Estimate Std. Error t value             Pr(>|t|)    \n#> (Intercept)      24.919471   1.063003   23.44 < 0.0000000000000002 ***\n#> body_mass_g       0.003748   0.000282   13.28 < 0.0000000000000002 ***\n#> speciesChinstrap  9.920884   0.351079   28.26 < 0.0000000000000002 ***\n#> speciesGentoo     3.557978   0.485790    7.32      0.0000000000018 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 2.4 on 338 degrees of freedom\n#>   (2 observations deleted due to missingness)\n#> Multiple R-squared:  0.808,  Adjusted R-squared:  0.806 \n#> F-statistic:  474 on 3 and 338 DF,  p-value: <0.0000000000000002\n\nHello! This model explains 80% of the variance and it’s coefficients are all significant. But, is the difference between this more complex, species-sensitive model and the simpler, species-insensitive model real or just a happy accident? As always, we can perform an ANOVA to find out.\n\nanova(lm_base, lm_species_offset)\n#> Analysis of Variance Table\n#> \n#> Model 1: bill_length_mm ~ body_mass_g\n#> Model 2: bill_length_mm ~ body_mass_g + species\n#>   Res.Df  RSS Df Sum of Sq   F              Pr(>F)    \n#> 1    340 6564                                         \n#> 2    338 1952  2      4613 399 <0.0000000000000002 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nWith a p-value really, really closer to zero, the answer is Yes, the species-sensitive model with a constant offset does indeed explain significantly more of the variance in bill length than we would expect by chance.\nHow do we interpret this model? If you look at the coefficients table, you’ll see there are four terms there, not two. Of course, there’s the intercept estimate and the coefficient for body mass, but there are also two species terms, speciesChinstrap and speciesGentoo.1 If you recall, however, there are actually three penguin species in the data.1 The first part of the term is the actual variable name, in this case species, and the second part of the term represents the unique values of the variable, here Chinstrap and Gentoo.\n\nunique(penguins$species)\n#> [1] Adelie    Gentoo    Chinstrap\n#> Levels: Adelie Chinstrap Gentoo\n\nWhere did Adelie go? Adelie is actually the reference class in the model! That means it’s always coded as a zero when the model builds dummy variables for the other species, so the term (Intercept) in the summary table is actually the intercept estimate for Adelie penguins. In this case, it tells you what the average bill length is for Adelie penguins when (impossibly) body mass equals zero. The other two species terms then tell you how different the intercepts are for the other species. For instance, speciesChinstrap has an estimate of 9.92, that means when body mass equals zero, Chinstrap penguins on average have a bill length 9.92 mm longer than Adelie penguins. Similarly, Gentoo penguins have a bill length 3.56 mm longer than Adelie penguins on average.\nIf we were to write out the model formula for this with the dummy variables and coefficients, it would look like this:\n\\[y_i = (\\beta_{0} + \\gamma_{1} D_{1i} + \\gamma_{2} D_{2i}) + \\beta_{1}x_{i} + \\epsilon_{i}\\]\nHere, \\(D_{1i}\\) would take a value 1 if penguin \\(i\\) is a Chinstrap penguin and 0 otherwise, and \\(D_{2i}\\) would take a value 1 if penguin \\(i\\) is a Gentoo penguin and 0 otherwise. That means, if \\(i\\) is an Adelie penguin, both \\(D_{1}\\) and \\(D_{2}\\) equal zero, and the model simplifies to\n\\[y_i = \\beta_{0} + \\beta_{1}x_{i} + \\epsilon_{i}\\]\nHence, \\(B_{0}\\) gives the intercept for Adelie penguins (the reference class). And if penguin \\(i\\) is a Chinstrap, \\(D_{1}\\) equals 1 and \\(D_{2}\\) equals 0, so the model simplifies to\n\\[y_i = (\\beta_{0} + \\gamma_{1}) + \\beta_{1}x_{i} + \\epsilon_{i}\\]\nHence, \\(\\gamma_{1}\\) tells you how different the intercept is for Chinstrap penguins relative to Adelie penguins. Notice, that \\(gamma_{1}\\) is not the intercept estimate for Chinstrap, but the difference in intercepts between Chinstrap and Adelie. To get the actual intercept estimate for Chinstrap, you have to add \\(\\gamma_{1}\\) to \\(\\beta_{0}\\), or \\(2.49 + 9.92 = 12.41\\). Similar reasoning applies to the Gentoo penguins, where \\(\\gamma_{2}\\) tells you how different the intercept is for them relative to Adelie.\nHere are the same ideas represented in a graph.\n\nCodefit <- predict(\n  lm_species_offset, \n  interval = \"confidence\",\n  level = 0.95\n)\n\npenguin_model <- penguins |> \n  select(species, body_mass_g, bill_length_mm) |> \n  filter(!is.na(body_mass_g), !is.na(bill_length_mm)) |> \n  bind_cols(fit)\n\nggplot(\n  penguin_model, \n  aes(x = body_mass_g, color = species)\n  ) +\n  geom_ribbon(\n    aes(ymin = lwr, ymax = upr, fill = species), \n    alpha = 0.3\n    ) +\n  geom_line(\n    aes(y = fit), \n    linewidth = 1\n    ) +\n  geom_point(\n    aes(y = bill_length_mm), \n    size = 2, \n    alpha = 0.7\n    ) + \n  scale_color_brewer(name = NULL, type = \"qual\", palette = \"Dark2\") +\n  scale_fill_brewer(name = NULL, type = \"qual\", palette = \"Dark2\") +\n  labs(\n    x = \"Body Mass (g)\", \n    y = \"Bill Length (mm)\"\n  ) +\n  theme_minimal(14) +\n  theme(\n    legend.justification = c(\"left\", \"top\"),\n    legend.position = c(0.03, 0.97)\n  )\n\n\n\n\nNotice that they all share the same slope, but the intercepts are allowed to vary. The consequence of this is that each species has its own “model,” so to speak, or a line of expected values.\nInteractions\nWhat about interactions? Is it possible that penguin species not only have differences in mean bill length, but they also have different relationships between bill length and body mass? To answer this question, we can multiply body_mass_g by the species variable in the model formula.\n\nlm_species_interaction <- lm(\n  bill_length_mm ~ body_mass_g * species, \n  data = penguins\n) \n\nsummary(lm_species_interaction)\n#> \n#> Call:\n#> lm(formula = bill_length_mm ~ body_mass_g * species, data = penguins)\n#> \n#> Residuals:\n#>    Min     1Q Median     3Q    Max \n#> -6.421 -1.646  0.092  1.472  9.314 \n#> \n#> Coefficients:\n#>                               Estimate Std. Error t value             Pr(>|t|)\n#> (Intercept)                  26.994139   1.592601   16.95 < 0.0000000000000002\n#> body_mass_g                   0.003188   0.000427    7.46     0.00000000000073\n#> speciesChinstrap              5.180054   3.274672    1.58                 0.11\n#> speciesGentoo                -0.254591   2.713866   -0.09                 0.93\n#> body_mass_g:speciesChinstrap  0.001275   0.000874    1.46                 0.15\n#> body_mass_g:speciesGentoo     0.000903   0.000607    1.49                 0.14\n#>                                 \n#> (Intercept)                  ***\n#> body_mass_g                  ***\n#> speciesChinstrap                \n#> speciesGentoo                   \n#> body_mass_g:speciesChinstrap    \n#> body_mass_g:speciesGentoo       \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 2.4 on 336 degrees of freedom\n#>   (2 observations deleted due to missingness)\n#> Multiple R-squared:  0.81,   Adjusted R-squared:  0.807 \n#> F-statistic:  286 on 5 and 336 DF,  p-value: <0.0000000000000002\n\nThe first thing to note here is that just as before, the model provides separate intercept estimates for each species: (Intercpet) for Adelie, the reference class, and speciesChinstrap and speciesGentoo. The second thing to note is that the model also includes separate slope estimates for each species. Again, the reference class is Adelie, so the coefficient estimate for body_mass_g represents the relationship between bill length and body for Adelie penguins, and body_mass_g:speciesChinstrap and body_mass_g:speciesGentoo represent differences in that relationship for those species relative to Adelie penguins.\nAs you can see, most of these coefficients are non-significant. It appears that the relationship between body mass and bill length is fairly robust across species. Still, we might wonder whether the model as a whole does better than the base model and the offset only model. To find out, let’s turn again to the ANOVA.\n\nanova(lm_base, lm_species_offset, lm_species_interaction)\n#> Analysis of Variance Table\n#> \n#> Model 1: bill_length_mm ~ body_mass_g\n#> Model 2: bill_length_mm ~ body_mass_g + species\n#> Model 3: bill_length_mm ~ body_mass_g * species\n#>   Res.Df  RSS Df Sum of Sq      F              Pr(>F)    \n#> 1    340 6564                                            \n#> 2    338 1952  2      4613 400.80 <0.0000000000000002 ***\n#> 3    336 1933  2        19   1.62                 0.2    \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nPerhaps unsurprisingly, the intercept offset model is significantly better than the species-insensitive model, but the interaction model is not significantly better than the intercept only model.\nIf we were to write out the model formula for this with the dummy variables and coefficients, it would look like this:\n\\[\n\\begin{align}\ny_i &= (\\beta_{0} + \\gamma_{1} D_{1i} + \\gamma_{2} D_{2i})\\, + \\\\\n    & \\,\\,\\,\\,\\,\\,\\,\\, (\\beta_{1}x_{i} + \\omega_{1} D_{1i}x_{i} + \\omega_{2} D_{2i}x_{i})\\, + \\\\\n    & \\,\\,\\,\\,\\,\\,\\,\\, \\epsilon_{i}\n\\end{align}\n\\]\nThere’s quite a bit going on here, but the logic is the same as above, so for now, let’s just focus on the changes in slope. Again, \\(D_{1i}\\) would take a value 1 if penguin \\(i\\) is a Chinstrap penguin and 0 otherwise, and \\(D_{2i}\\) would take a value 1 if penguin \\(i\\) is a Gentoo penguin and 0 otherwise. That means, if \\(i\\) is an Adelie penguin, both \\(D_{1}\\) and \\(D_{2}\\) equal zero, and the model simplifies to\n\\[y_i = \\beta_{0} + \\beta_{1}x_{i} + \\epsilon_{i}\\]\nHence, \\(B_{1}\\) gives the slope (the relationship between bill length and body mass) for Adelie penguins (the reference class). And if penguin \\(i\\) is a Chinstrap, \\(D_{1}\\) equals 1 and \\(D_{2}\\) equals 0, so the model simplifies to\n\\[y_i = (\\beta_{0} + \\gamma_{1}) + (\\beta_{1}x_{i} + \\omega_{1} D_{1i}x_{i}) + \\epsilon_{i}\\]\nHence, \\(\\omega_{1}\\) tells you how different the slope is for Chinstrap penguins relative to Adelie penguins. As above, it is important to note that \\(\\omega_{1}\\) is not the slope estimate for Chinstrap, but the difference in slopes between Chinstrap and Adelie. To get the actual slope estimate for Chinstrap, you have to add \\(\\omega_{1}\\) to \\(\\beta_{1}\\), or \\(0.003 + 0.001 = 0.004\\). Similar reasoning applies to the Gentoo penguins, where \\(\\omega_{2}\\) tells you how different the slope is for them relative to Adelie.\nOf course, in this case, the differences in slope are not significantly different than zero, meaning the relationship between bill length and body mass does not really change between species. You will notice, too, that the differences in intercept are no longer significant, either. That is because allowing the slopes to vary, even a little, means that the intercepts may get closer to each other. In effect, all the slopes point to the same basically place on the y-axis.\nHere are the same ideas represented in a graph.\n\nCodefit <- predict(\n  lm_species_interaction, \n  interval = \"confidence\",\n  level = 0.95\n)\n\npenguin_model <- penguins |> \n  select(species, body_mass_g, bill_length_mm) |> \n  filter(!is.na(body_mass_g), !is.na(bill_length_mm)) |> \n  bind_cols(fit)\n\nggplot(\n  penguin_model, \n  aes(x = body_mass_g, color = species)\n  ) +\n  geom_ribbon(\n    aes(ymin = lwr, ymax = upr, fill = species), \n    alpha = 0.3\n    ) +\n  geom_line(\n    aes(y = fit), \n    linewidth = 1\n    ) +\n  geom_point(\n    aes(y = bill_length_mm), \n    size = 2, \n    alpha = 0.7\n    ) + \n  scale_color_brewer(name = NULL, type = \"qual\", palette = \"Dark2\") +\n  scale_fill_brewer(name = NULL, type = \"qual\", palette = \"Dark2\") +\n  labs(\n    x = \"Body Mass (g)\", \n    y = \"Bill Length (mm)\"\n  ) +\n  theme_minimal(14) +\n  theme(\n    legend.justification = c(\"left\", \"top\"),\n    legend.position = c(0.03, 0.97)\n  )\n\n\n\n\nExercises\n\n\n\n\n\n\nWarning\n\n\n\nBefore starting these exercises, you should remove all the obserations (rows) in your table with missing values. The function lm() implicitly drops missing values, so you can sometimes get models fit to different subsets of your data when you start adding variables. That’s not really a problem in itself, but if you want to compare nested models with an ANOVA, for instance, that won’t do because of the way that the test calculates the degrees of freedom necessary to evaluate the F-statistic. So, what I suggest you do is remove all the missing values in one fell swoop with na.omit(). So, add this code chunk before you start the exercises.\n\npenguins <- na.omit(penguins)\n\n\n\n\nTry running the same models, but substitute sex for species. Create all the following:\n\na sex-insensitive model\na sex-sensitive offset model\na sex-sensitive interaction model\nprovide a summary of each\nidentify the reference class\ncompare them with an ANOVA\n\n\nNow answer these questions:\n\nhow do you interpret the coefficients?\nare there differences in average bill length between males and females?\ndifferences in the relationship between bill length and body mass between males and females?\nhow do you interpret the results of the ANOVA test?"
  },
  {
    "objectID": "labs/10-transforms-lab.html#centering-and-scaling",
    "href": "labs/10-transforms-lab.html#centering-and-scaling",
    "title": "Lab 10: Transforming Variables",
    "section": "Centering And Scaling",
    "text": "Centering And Scaling\nAbove, we made a model of penguin bill length as a function of body mass. This proved to be significant (as expected). However, it also has an intercept that is nonsensical. A penguin with zero mass has an expected bill length of 26.899 mm. What?!!! This is a classic example where we would want to center our predictor variable (in this case, body mass) around its mean. ### Centering\nThe simplest way to center a variable in R involves the subtraction operator - and the mean() or median() function, depending on which measure of central tendency seems more appropriate for your variable. We can also wrap this in mutate().\n\npenguins <- penguins |> \n  mutate(\n    body_mass_cntr = body_mass_g - mean(body_mass_g, na.rm = TRUE)\n  )\n\nThis code says, in effect, “Update the penguins table by adding a variable representing the mean-centered body mass of each penguin.” Now let’s have a look at an updated model with the centered variable. Notice that we used na.rm = TRUE to ignore missing values! If we hadn’t, the variable would be filled with NA or missing values.\n\nlm_base_centered <- lm(bill_length_mm ~ body_mass_cntr, data = penguins)\n\nsummary(lm_base_centered)\n#> \n#> Call:\n#> lm(formula = bill_length_mm ~ body_mass_cntr, data = penguins)\n#> \n#> Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -10.125  -3.043  -0.809   2.071  16.111 \n#> \n#> Coefficients:\n#>                 Estimate Std. Error t value            Pr(>|t|)    \n#> (Intercept)    43.921930   0.237601   184.9 <0.0000000000000002 ***\n#> body_mass_cntr  0.004051   0.000297    13.6 <0.0000000000000002 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 4.39 on 340 degrees of freedom\n#>   (2 observations deleted due to missingness)\n#> Multiple R-squared:  0.354,  Adjusted R-squared:  0.352 \n#> F-statistic:  186 on 1 and 340 DF,  p-value: <0.0000000000000002\n\n\nCodefit <- predict(\n  lm_base_centered, \n  interval = \"confidence\",\n  level = 0.95\n)\n\npenguin_model <- penguins |> \n  select(species, body_mass_cntr, bill_length_mm) |> \n  filter(!is.na(body_mass_cntr), !is.na(bill_length_mm)) |> \n  bind_cols(fit)\n\nggplot(\n  penguin_model, \n  aes(x = body_mass_cntr)\n  ) +\n  geom_ribbon(\n    aes(ymin = lwr, ymax = upr), \n    alpha = 0.3\n  ) +\n  geom_line(\n    aes(y = fit), \n    linewidth = 1\n  ) +\n  geom_point(\n    aes(y = bill_length_mm), \n    size = 2, \n    alpha = 0.7\n  ) + \n  labs(\n    x = \"Body Mass (g, x̄=0)\", \n    y = \"Bill Length (mm)\"\n  ) +\n  theme_minimal(14)\n\n\n\n\nScaling\nDoes body mass or flipper length have a stronger effect on bill length? This is a somewhat confused question because the relationships aren’t actually causal in nature, so ‘effect’ is not the right word. Putting that aside, however, there is also the problem that these variables have different units of measure. To make them more comparable in a linear model, we can standardize them, converting them both to z-scores. Note that this is a form of scaling, but it’s a scaling that ensures all variables are now available in the same units. That’s not quite right. In fact, the units are, in a sense, inherent to variables like mass and length. Standardizing them, however, gives us a precise way of answering the question: for these penguins data, what is the equivalent in mass for a unit change in length? The z-score is one specific way of answering that.\nSpecifically, a z-score is mean centered and divided by the standard deviation. In a formula, that’s\n\\[z_i = \\frac{x_i - \\bar{x}}{s_x}\\]\nThe base scale() function does this for you, saving you having to calculate the mean and standard deviation by hand. One draw back, however, is that it was designed to work on matrices, a special sort of table, so it also returns a matrix. To get around this you can collapse the matrix to a simple vector with c(). In a call to mutate(), that looks like the following.\n\npenguins <- penguins |> \n  mutate(\n    flipper_length_z = c(scale(flipper_length_mm)),\n    body_mass_z = c(scale(body_mass_g))\n  )\n\nAnd now we can use it in a model.\n\nlm_standardized <- lm(\n  bill_length_mm ~ flipper_length_z + body_mass_z, \n  data = penguins\n)\n\nsummary(lm_standardized)\n#> \n#> Call:\n#> lm(formula = bill_length_mm ~ flipper_length_z + body_mass_z, \n#>     data = penguins)\n#> \n#> Residuals:\n#>    Min     1Q Median     3Q    Max \n#> -8.806 -2.590 -0.705  1.991 18.829 \n#> \n#> Coefficients:\n#>                  Estimate Std. Error t value             Pr(>|t|)    \n#> (Intercept)        43.922      0.223  196.97 < 0.0000000000000002 ***\n#> flipper_length_z    3.120      0.455    6.86       0.000000000033 ***\n#> body_mass_z         0.531      0.455    1.17                 0.24    \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 4.12 on 339 degrees of freedom\n#>   (2 observations deleted due to missingness)\n#> Multiple R-squared:  0.433,  Adjusted R-squared:  0.43 \n#> F-statistic:  129 on 2 and 339 DF,  p-value: <0.0000000000000002\n\nStrange. When flipper length is added to the model, body mass is no longer significant. Why do you think that is?\nExercises\n\nWhy do you think body mass is no longer significant in the final model of bill length?\nWhat test would you perform to investigate this result? Try it out and see what result it gives you.\nTry standardizing bill depth using mutate() and make a model of bill length by bill depth. Make sure to summarize the model."
  },
  {
    "objectID": "labs/10-transforms-lab.html#other-transforms",
    "href": "labs/10-transforms-lab.html#other-transforms",
    "title": "Lab 10: Transforming Variables",
    "section": "Other Transforms",
    "text": "Other Transforms\nIn the section, we’re going to use some made-up data sets, the same ones from the lecture. This is just for illustration purposes. Plus, I ran out of time putting this together.\nPolynomials\nIt’s rare that you’ll want to use polynomials as they are hard to interpret and somewhat inflexible. This example will just serve to provide further motivation for the idea that you sometimes need to transform your data before fitting a model. It’ll also show you how to do that in R by specifying the transformation in the model formula.\n\nset.seed(25)\n\nn <- 100\n\nobservations <- tibble(\n  x = seq(0, 1, length = n),\n  e = rnorm(n, sd = 0.2),\n  y = 1 - 3.7 * (x - 0.5)^2 + (0.75 * x) + e\n)\n\nNow to fit the model with a second-order polynomial (a square term) we will use the poly() function. This expands a single variable to include the highest order specified in the function call and every order below it. Here, for example, is a third-order polynomial.\n\npoly(1:5, 3)\n#>           1      2                    3\n#> [1,] -0.632  0.535 -0.31622776601683811\n#> [2,] -0.316 -0.267  0.63245553203367622\n#> [3,]  0.000 -0.535 -0.00000000000000041\n#> [4,]  0.316 -0.267 -0.63245553203367544\n#> [5,]  0.632  0.535  0.31622776601683783\n#> attr(,\"coefs\")\n#> attr(,\"coefs\")$alpha\n#> [1] 3 3 3\n#> \n#> attr(,\"coefs\")$norm2\n#> [1]  1.0  5.0 10.0 14.0 14.4\n#> \n#> attr(,\"degree\")\n#> [1] 1 2 3\n#> attr(,\"class\")\n#> [1] \"poly\"   \"matrix\"\n\nThis takes your base \\(x\\) variable and expands it into three separate variables, \\(x^1\\), \\(x^2\\), and \\(x^3\\). Rather than try to update your table of observations, a simple way of incorporating this into the model is to simply add it to the model formula like so.\n\nlm_poly <- lm(y ~ poly(x, 2), data = observations)\n\nsummary(lm_poly)\n#> \n#> Call:\n#> lm(formula = y ~ poly(x, 2), data = observations)\n#> \n#> Residuals:\n#>    Min     1Q Median     3Q    Max \n#> -0.427 -0.169  0.005  0.111  0.520 \n#> \n#> Coefficients:\n#>             Estimate Std. Error t value            Pr(>|t|)    \n#> (Intercept)   1.0240     0.0203    50.4 <0.0000000000000002 ***\n#> poly(x, 2)1   2.2855     0.2030    11.3 <0.0000000000000002 ***\n#> poly(x, 2)2  -2.7734     0.2030   -13.7 <0.0000000000000002 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 0.203 on 97 degrees of freedom\n#> Multiple R-squared:  0.764,  Adjusted R-squared:  0.759 \n#> F-statistic:  157 on 2 and 97 DF,  p-value: <0.0000000000000002\n\nNotice that it estimates a coefficient for each polynomial (just 1 and 2 in this case). For comparison purposes, let’s see how this fairs against a simple model without any polynomial terms.\n\nlm_base <- lm(y ~ x, data = observations)\n\nanova(lm_base, lm_poly)\n#> Analysis of Variance Table\n#> \n#> Model 1: y ~ x\n#> Model 2: y ~ poly(x, 2)\n#>   Res.Df  RSS Df Sum of Sq   F              Pr(>F)    \n#> 1     98 11.7                                         \n#> 2     97  4.0  1      7.69 187 <0.0000000000000002 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nHere is what these models look like graphically.\n\nCodeestimates <- observations |> \n  mutate(\n    base = fitted(lm_base),\n    poly = fitted(lm_poly)\n  ) |> \n  pivot_longer(c(base, poly)) |> \n  select(x, name, value)\n\nggplot() +\n  geom_line(\n    data = estimates,\n    aes(x, value, color = name),\n    linewidth = 1\n  ) + \n  geom_point(\n    data = observations, \n    aes(x, y),\n    size = 3\n  ) +\n  labs(\n    x = NULL,\n    y = NULL\n  ) +\n  scale_color_brewer(name = NULL, type = \"qual\", palette = \"Dark2\") +\n  theme_minimal(14) +\n  theme(\n    legend.justification = c(\"left\", \"top\"),\n    legend.position = c(0.03, 0.97),\n    axis.text = element_blank(),\n    axis.ticks = element_blank()\n  )\n\n\n\n\nLogarithm\nWe’ll go over the logarithm and other log transformations of the data when we get to generalized linear models, but for now, just try out this example.\n\nn <- 100\n\nset.seed(1)\n\nobservations <- tibble(\n  x = seq(0.1,5,length.out = n),\n  e = rnorm(n, mean = 0, sd = 0.2),\n  y = exp(0.3 + 0.5 * x + e)\n)\n\nIn this case, we’re going to make a log-linear model, so we are going to log the y-variable with log(), and we’re going to do that inside the model formula.\n\nlm_log <- lm(log(y) ~ x, data = observations)\n\nsummary(lm_log)\n#> \n#> Call:\n#> lm(formula = log(y) ~ x, data = observations)\n#> \n#> Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -0.4680 -0.1212  0.0031  0.1170  0.4595 \n#> \n#> Coefficients:\n#>             Estimate Std. Error t value             Pr(>|t|)    \n#> (Intercept)   0.3264     0.0369    8.84     0.00000000000004 ***\n#> x             0.4982     0.0126   39.42 < 0.0000000000000002 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 0.181 on 98 degrees of freedom\n#> Multiple R-squared:  0.941,  Adjusted R-squared:  0.94 \n#> F-statistic: 1.55e+03 on 1 and 98 DF,  p-value: <0.0000000000000002\n\nNote that we can’t compare this to a simpler model without the log transformation using ANOVA because those are not nested models! They have different responses!"
  },
  {
    "objectID": "labs/10-transforms-lab.html#homework",
    "href": "labs/10-transforms-lab.html#homework",
    "title": "Lab 10: Transforming Variables",
    "section": "Homework",
    "text": "Homework\nNo homework!"
  },
  {
    "objectID": "labs/11-another-R-lab.html",
    "href": "labs/11-another-R-lab.html",
    "title": "Lab 11: Another R Lab",
    "section": "",
    "text": "This lab will guide you through the process of\n\nsaving ggplot figures with ggsave()\n\nbuilding robust relative paths in project folders with here()\n\n\nWe will be using the following packages:\n\narchdata\nhere\npalmerpenguins\ntidyverse\nviridis\n\n\nlibrary(archdata)\nlibrary(here)\nlibrary(palmerpenguins)\nlibrary(tidyverse)\nlibrary(viridis)\n\n\n\n\npenguins\n\nIncludes measurements for penguin species, island in Palmer Archipelago, size (flipper length, body mass, bill dimensions), and sex.\npackage: palmerpenguins\n\nreference: https://allisonhorst.github.io/palmerpenguins/reference/penguins.html\n\n\n\n\nDartPoints\n\nIncludes measurements of 91 Archaic dart points recovered during surface surveys at Fort Hood, Texas.\npackage: archdata\n\nreference: https://cran.r-project.org/web/packages/archdata/archdata.pdf\n\n\n\n\ntitanic\n\nProvides information on the fate of passengers of the Titanic, including economic status, sex, age, and survival.\npackage: none\nreference: https://wilkelab.org/SDS375/\n\nnote: we’re going to download this one rather than get it from a package"
  },
  {
    "objectID": "labs/11-another-R-lab.html#saving-figures",
    "href": "labs/11-another-R-lab.html#saving-figures",
    "title": "Lab 11: Another R Lab",
    "section": "Saving figures",
    "text": "Saving figures\nFor examples here, we are going to use the titanic data set again.\n\nsauce <- paste0(\n  \"https://raw.githubusercontent.com/\",\n  \"wilkelab/SDS375/master/datasets/titanic.csv\"\n)\n\ntitanic <- read_csv(sauce)\n\nYou will often want to save figures you generate in R to files that you can then insert into other documents or manuscripts for publication or in presentations. There are many ways to do this in R and RStudio (including a point and click method in the Plots pane - just click “Export”). All of them in one way or another require the specification of a graphics device, meaning a tool for encoding the visual information contained in your figure. By default, R sends figures to the screen device so it can appear on, well, your computer screen. For Windows, the screen device is just windows(). For Mac, it’s quartz(). There are also file devices for familiar image file formats, the most common being pdf(), png(), jpeg(), and tiff(). The latter three are raster-based devices, meaning they represent images as a grid with cells or pixels having specific color values. The pdf() device is vector-based, so it represents shapes as mathematical formulas that can be used to reconstruct the image in different media. Another example of a vector-based device is svg(), which is more commonly in simple web graphics.\nWhen working with {ggplot2}, it’s rare that you will want to work with these devices directly. Instead, you will call the function ggsave() and it will pick the appropriate device for you based on the file extension you specify. For instance, ggsave(filename = \"my-plot.png\") will save the figure as a PNG file using the png() device and ggsave(filename = \"my-plot.jpeg\") will save the figure as a JPEG file using the jpeg() device.\nOK, so, what device should you use? There’s no simple answer to that question, but as a general rule, I would recommend the PNG format. It uses lossless compression and thus offers a sharper image in most cases. If your sole purpose is to construct an image for print publication and it’s a fairly uncomplicated figure, you might consider the PDF format, as it is much more robust at different scales.\nAt any rate, let’s walk through some more details of ggsave(). The first thing to note is that ggsave() defaults to saving the last image you printed to your screen device, so you don’t have to tell it explicitly which figure you want to save. You can do that, if you like, you just have to assign the plot to an object and then refer to that object with the plot argument. That means the only argument that is strictly required is filename, which takes a character string specifying the file path where you want to save the image.\nHere are examples using the default last image displayed method and the explicit reference method.\n\n# method 1 - save last displayed image\nggplot(titanic) + geom_histogram(aes(age))\n\nggsave(filename = \"titanic.png\")\n\n# method 2 - save image by explicit reference\nbob <- ggplot(titanic) + geom_histogram(aes(age))\n\nggsave(filename = \"titanic.png\", plot = bob)\n\nA common scenario when saving figures to file is to explicitly set its dimensions and resolution. These are controlled by the width, height, and dpi arguments respectively. Note that dpi does not apply to vector-based devices like PDF, as it is a way of specifying - in effect - the size of each pixel in a raster image. Here is a complete example you might see in the wild.\n\nggplot(titanic) + geom_histogram(aes(age))\n\nggsave(\n  filename = \"titanic.png\",\n  width = 6,\n  height = 6 * 0.8,\n  dpi = 300\n)\n\nThere are a couple of things to draw your attention to here. First, the default unit of measure for width and height is inches, but you can use different units if you like by passing an abbreviation to the units argument. For instance, ggsave(filename = \"titanic.png\", width = 5, units = \"cm\") will produce an image that is 5 centimeters wide. Second, notice that to specify the height, I multiplied 6 (the same value as the width) by 0.8. In this case 0.8 is playing the role of the figure’s aspect ratio, meaning the ratio of the height of the figure to its width. So, for instance, a figure with a height of 8 inches and a width of 10 inches will have an aspect ratio of 8/10 or 0.8. That means any two of these measures entails the third. If you know the width and the aspect ratio, for example, you can derive the height by multiplying them (as was done in the above example). Conversely, if you know the height and aspect ratio, you can derive the width by dividing them. An aspect ratio of 1, of course, means the figure is square shaped. In terms of best practices, I think it’s a good idea to get in the habit of thinking about figure dimensions in terms of width and aspect ratio, as this will help you keep multiple figures more consistently formatted within papers or presentations.\nOK, so what about dimensions? What’s a good width and height for most figures? Well, again, here it just depends on what you’re trying to do, but if you’re thinking about good dimensions for a scientific publication in the US, it’s probably a good idea to think about figure size in terms of the dimensions of a standard piece of paper, meaning 8.5 x 11 inches, with standard margins set at 1 inch. That gives you about 6.5 inches of width to play with (unless the paper has a two column format, in which case you have about half that). So, to be safe, I would recommend saving figures at around 5 inches in width, typically with an aspect ratio around 0.7 or 0.8 (though the aspect should vary depending on what kind of figure it is)."
  },
  {
    "objectID": "labs/11-another-R-lab.html#here-here",
    "href": "labs/11-another-R-lab.html#here-here",
    "title": "Lab 11: Another R Lab",
    "section": "Here here()\n",
    "text": "Here here()\n\nAbove, when we specified the file path with filename, ggsave() just assumed we wanted to save the figure to the working directory. When you’re working within an R project, the working directory will be set to the top level directory of the project (typically where the .Rproj file is located). This is useful because it means you can use relative paths to save your figures and other files, like tables you write to csv files. For instance,\nggsave(filename = \"figures/titanic.png\", plot = ggtitanic)\nwill save the figure to the following location:\nC:/Users/blake/rstuff/our-r-project/figures/titanic.png\nThis is useful, but there’s a catch. This assumes that all your files live in the top level directory of your project, that it has no depth in other words. But often you will want to organize your project using various sub-folders. Your R project might, for example, look like this:\n\n\n\n\n📁 my-r-project\n\n|- 📁 _misc ← the kitchen sink\n\n|- 📁 data\n\n|-  titanic.csv\n\n\n\n|- 📁 figures\n\n|-  titanic.png\n\n\n|- 📁 R\n\n|- 📄 analysis.qmd\n\n|- 📄 data-wrangling.R\n\n\n\n|- 📄 my-r-project.Rproj ← this makes it an R project!\n\n\n\nI happen to think this is a good setup for an R project.\n\nYou have the _misc/ folder, short for “miscalleneous.” You can think of this folder like that drawer in your kitchen where you just cram stuff you can’t find a place for. You let that drawer be chaos inside, so the rest of your kitchen can have order and harmony. Just so with project folders. The _misc/ folder is for all the stuff that isn’t essential to your project, but you just can’t bring yourself to delete it entirely.\nThe data/ folder is, as its name suggests, where all the data for your project goes.\nThe figures/ folder is, again, where you save all your figures.\nAnd the R/ folder is where all your R code goes, including .qmd files.\n\nNow, here’s where the catch comes in. Many files you work with, including Quarto (.qmd) documents will assume by default that the working directory is the directory of the file itself, not necessarily the top level project directory. So, if you to compile a .qmd file in your R folder, all the R code will get run in a session where the working directory is set to\n\"C:/Users/blake/rstuff/our-r-project/R\"\nmeaning the R/ folder, not the top level project folder! That means if you try to use a relative path like\ntitanic <- read_csv(\"data/titanic.csv\")\nit’ll look for that data in\n\"C:/Users/blake/rstuff/our-r-project/R/data/titanic.csv\"\nbut there is no our-r-project/R/data/ folder! Of course, there’s a our-r-project/data/ folder, but that’s not where you’re telling R to look, so you’ll get an error saying it can’t find the data you are asking for. Issues like this are compounded like a billion times over when you start thinking about sharing your code with other people. To solve this, I recommend using the {here} package and, in particular, the eponymous here() function. This allows you to build file paths that always start at the top level of a project folder. So, for the toy project above, you’ll get this result if you run the code in the R/analysis.qmd file:\nlibrary(here)\n\nhere() \n#> [1] \"C:/Users/blake/rstuff/our-r-project\"\n\nhere(\"data\", \"titanic.csv\")\n#> [1] \"C:/Users/blake/rstuff/our-r-project/data/titanic.csv\"\n\nhere(\"figures\", \"titanic.png\")\n#> [1] \"C:/Users/blake/rstuff/our-r-project/figures/titanic.png\"\nSo, you can also use this to read or write data.frames and to save ggplot figures.\ntitanic <- read_csv(here(\"data\", \"titanic.csv\"))\n\nggplot(titanic) + geom_histogram(aes(age))\n\nggsave(filename = here(\"figures\", \"titanic.png\"))\nAnd, every time, it should just work. But, and this is critically important,\n\n⚠️ THIS ONLY WORKS IN A PROJECT FOLDER."
  },
  {
    "objectID": "labs/11-another-R-lab.html#homework",
    "href": "labs/11-another-R-lab.html#homework",
    "title": "Lab 11: Another R Lab",
    "section": "Homework",
    "text": "Homework\n\nCreate a histogram of penguin bill length using the penguins dataset from the palmerpenguins package. Then do all of the following:\n\nChange the number of bins (try two different options).\nTry specifying the bin width and boundary.\nChange the fill and outline color.\nReset the labels to be more informative.\nChange the theme and remove the vertical grid lines.\nSave the plot to the figures/ folder in your project directory using ggsave() and here().\n\n\nRepeat (1), but use the DartPoints dataset from the {archdata} package, creating a histogram of dart length (Length in the table).\n\nDoes it look like the dart types might differ in length? Or maybe they’re all basically the same?\n\n\nMake a kernel density plot of penguin bill length using ggplot() and geom_density(). Then make all of the following changes:\n\nMap penguin species to the fill aesthetic.\nUpdate the axis labels and plot title using labs().\nUse scale_fill_viridis to use colorblind safe colors for the fill. Note! Species is a discrete or categorical variable, so make sure to set discrete = TRUE!\nUse facet_wrap() to facet by species.\nChoose a suitable theme, like theme_minimal().\nRemove vertical grid lines.\nChange the legend position to bottom and make it horizontal.\nRemove strip text and background.\n\nSave the plot to the figures/ folder in your project directory using ggsave() and here().\n\n\nDo the same as (3), but for dart point length, and substitute dart point type for species.\nMake a boxplot showing the distribution of penguin body mass by island. Do all of the following:\n\nPosition the boxes horizontally.\nChange the fill color to a color of your choice.\nUpdate the labels and add a title.\nChange the theme to one of your choice.\nRemove the horizontal grid lines.\nAdd perpendicular lines to the whiskers.\nSave the plot to the figures/ folder in your project directory using ggsave() and here().\n\n\nDo the same as (5), but for dart point length, substituting dart type for island."
  },
  {
    "objectID": "labs/12-logistic-regression-lab.html",
    "href": "labs/12-logistic-regression-lab.html",
    "title": "Lab 12: Logistic Regression",
    "section": "",
    "text": "This lab will guide you through the process of\n\nFitting a GLM with a\n\nGaussian distribution\nBinomial distribution\n\n\nEvaluating models with a Likelihood Ratio Test\n\nWe will be using the following packages:\n\narchdata\nhere\ntidyverse\nviridis\n\n\nlibrary(archdata)\nlibrary(here)\nlibrary(tidyverse)\nlibrary(viridis)\n\n\n\n\nDartPoints\n\nIncludes measurements of 91 Archaic dart points recovered during surface surveys at Fort Hood, Texas.\npackage: archdata\n\nreference: https://cran.r-project.org/web/packages/archdata/archdata.pdf\n\n\n\n\nHandaxes\n\nIncludes measurements of length, width, and thickness for handaxes from the Furze Platt site in England.\npackage: archdata\n\nreference: https://cran.r-project.org/web/packages/archdata/archdata.pdf\n\n\n\n\nOxfordPots\n\nIncludes proportions of Romano-British Oxford pottery on 30 sites and their distances from Oxford.\nreference: https://cran.r-project.org/web/packages/archdata/archdata.pdf\n\n\n\n\nSnodgrass\n\nIncludes measurements of size, location, and contents of 91 pit houses at the Snodgrass site in Butler County, Missouri.\nreference: https://cran.r-project.org/web/packages/archdata/archdata.pdf"
  },
  {
    "objectID": "labs/12-logistic-regression-lab.html#gaussian",
    "href": "labs/12-logistic-regression-lab.html#gaussian",
    "title": "Lab 12: Logistic Regression",
    "section": "Gaussian",
    "text": "Gaussian\nIn this section and the following, I will walk you through how to make the Generalized Linear Models (or GLMs) introduced in the lecture with R. Then, in the exercises for each section, we’ll work through an additional example for each. We’ll start by fitting a model with a normal or Gaussian family distribution. The question we hope to answer is this:\nQuestion Does the length of an archaic dart point vary as a function of its weight.\nTo answer that question, we will use the DartPoints data collected from a site in Fort Hood, Texas.\nFirst, we will load the data into R. For practice, we’ll also write it to disk and read it back in, in each case using the here() function to specify relative file paths in our project folder. Note that I am going to change the name of the object to darts. While it’s current name is fine, I find that keeping names short - but descriptive! - and in lowercase format - as far as that is possible anyway - saves you some typing effort and reduces the potential for errors in your R code. While we’re at it, we’ll set all the variable names in the table to lower case using the base R function tolower() and the dplyr function rename_with(). Then, to make it clearer that the name variable is actually a dart type, we’ll rename it using the dplyr verb rename(). Note that rename() uses the \"new_name\" = old_name syntax. I’ll also convert it to a tibble with as_tibble() to make it print pretty.\n\n\n\n\ndata(\"DartPoints\")\n\nfilename <- here(\"data\", \"darts.csv\")\n\nwrite_csv(DartPoints, file = filename)\n\ndarts <- read_csv(filename) |> \n  as_tibble() |> \n  rename_with(tolower) |> \n  rename(\"type\" = name)\n\nremove(DartPoints, filename) # remove these objects from your environment\n\ndarts\n\n\n#> # A tibble: 91 × 17\n#>    type  catalog tarl  quad  length width thick…¹ b.width j.width h.len…² weight\n#>    <fct> <chr>   <chr> <chr>  <dbl> <dbl>   <dbl>   <dbl>   <dbl>   <dbl>  <dbl>\n#>  1 Darl  41-0322 41CV… 26/59   42.8  15.8     5.8    11.3    10.6    11.6    3.6\n#>  2 Darl  35-2946 41CV… 21/63   40.5  17.4     5.8    NA      13.7    12.9    4.5\n#>  3 Darl  35-2921 41CV… 20/63   37.5  16.3     6.1    12.1    11.3     8.2    3.6\n#>  4 Darl  36-3487 41CV… 10/54   40.3  16.1     6.3    13.5    11.7     8.3    4  \n#>  5 Darl  36-3321 41CV… 12/58   30.6  17.1     4      12.6    11.2     8.9    2.3\n#>  6 Darl  35-2959 41CV… 21/63   41.8  16.8     4.1    12.7    11.5    11      3  \n#>  7 Darl  35-2866 41CV… 25/65   40.3  20.7     5.9    11.7    11.4     7.6    3.9\n#>  8 Darl  41-0323 41CV… 26/59   48.5  18.7     6.9    14.7    13.4     9.2    6.2\n#>  9 Darl  35-2325 41CV… 20/48   47.7  17.5     7.2    14.3    11.8     8.9    5.1\n#> 10 Darl  40-0847 41CV… 05/48   33.6  15.8     5.1    NA      12.5    11.5    2.8\n#> # … with 81 more rows, 6 more variables: blade.sh <fct>, base.sh <fct>,\n#> #   should.sh <fct>, should.or <fct>, haft.sh <fct>, haft.or <fct>, and\n#> #   abbreviated variable names ¹​thickness, ²​h.length\n\nNow, of course, we’ll want to visualize our data, in particular the relationship between dart length and weight. These are quantitative measures, so let’s make a scatter plot.\n\nggplot(\n  darts, \n  aes(weight, length)\n) +\n  geom_point(\n    size = 3,\n    alpha = 0.6 # increase transparency to address the over-plotting of points\n  ) +\n  labs(\n    x = \"Weight (g)\",\n    y = \"Length (mm)\"\n  )\n\n\n\n\nNow, let’s fit a GLM! To do that, we’ll use the glm() function. The syntax for specifying a model with this function should be familiar to you. It’s just like fitting a linear model with lm(), albeit with one important exception. You have to specify an exponential distribution and a link function. To do that, we use the family argument to glm(), providing it with a family function that itself takes a link argument. It looks like this:\n\nglm_darts <- glm(\n  length ~ weight,\n  family = gaussian(link = \"identity\"),\n  data = darts\n)\n\nNote that the identity link function is the default for the gaussian() function, so you don’t actually have to specify it. The parentheses are not strictly necessary either (for reasons beyond the scope of this class), so we could instead call the glm() function this way:\n\nglm_darts <- glm(\n  length ~ weight,\n  family = gaussian,\n  data = darts\n)\n\nThat said, in all the examples that follow, I am going to use the first formulation to make it as explicit as possible that you are always, always, always including a distribution and link function when fitting a GLM.\nSo, now we have our model. Let’s look at a summary.\n\nsummary(glm_darts)\n#> \n#> Call:\n#> glm(formula = length ~ weight, family = gaussian(link = \"identity\"), \n#>     data = darts)\n#> \n#> Deviance Residuals: \n#>     Min       1Q   Median       3Q      Max  \n#> -10.540   -3.865   -0.598    2.249   30.255  \n#> \n#> Coefficients:\n#>             Estimate Std. Error t value            Pr(>|t|)    \n#> (Intercept)   28.971      1.328    21.8 <0.0000000000000002 ***\n#> weight         2.664      0.152    17.5 <0.0000000000000002 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> (Dispersion parameter for gaussian family taken to be 37)\n#> \n#>     Null deviance: 14599.0  on 90  degrees of freedom\n#> Residual deviance:  3294.7  on 89  degrees of freedom\n#> AIC: 590.9\n#> \n#> Number of Fisher Scoring iterations: 2\n\n\n\n\nThere are four things to note with this summary.\n\nThe dispersion parameter is 37. For a GLM fit with a Gaussian distribution, this is equivalent to the square of the residual standard error in a linear model, ie, the mean squared error.\n\nThe null deviance is 14599. This is the product of -2 and the difference in the log-Likelihood of an intercept-only model and a saturated model (a model with a parameter for each observation, ie, a perfect fit).\n\nThe residual deviance is 3294.7. This is the product of -2 and the difference in log-Likelihood of the fitted model and a saturated model.\n\nThe AIC (or Akaike Information Criterion) is 590.866. This is calculated using the formula:\n\n\\[AIC = -2\\,log\\,\\mathcal{L} + 2p\\]\nwhere \\(log\\,\\mathcal{L}\\) is the log-Likelihood and \\(p\\) is the number of parameters in the model (+1 for the estimate of the error variance). In this case, the model has an intercept, one covariate, weight, and the error variance, so \\(p = 3\\).\nThese are all ways of evaluating the model’s goodness-of-fit, but as always, we would like to know if the increased complexity is worth it. There are two ways we can try to answer this question. The first is simply to compare the AIC score of the model we have just proposed to an intercept-only model, as the AIC incorporates a penalty for complexity. As this compares 2 times the negative log-Likelihood (ie the deviance), a smaller score is always better, so we want the AIC of our proposed model to be less than the AIC of the null model. To extract the AIC estimate from a model, we use AIC().\n\nglm_null <- glm(\n  length ~ 1,\n  family = gaussian(link = \"identity\"),\n  data = darts\n)\n\n# is the AIC of the proposed model less than the AIC of the null model?\nAIC(glm_darts) < AIC(glm_null)\n#> [1] TRUE\n\nThat’s a bingo! But, let’s take this idea a bit further and try some inference with it. In particular, let’s use an ANOVA (specifically, a Likelihood Ratio Test or LRT) to compare the ratio of the likelihoods of these models to a \\(\\chi^2\\) distribution. This will tell us if they are significantly different. To do that in R, we use the anova() function, setting its test argument to \"LRT\".\n\nanova(glm_null, glm_darts, test = \"LRT\")\n#> Analysis of Deviance Table\n#> \n#> Model 1: length ~ 1\n#> Model 2: length ~ weight\n#>   Resid. Df Resid. Dev Df Deviance            Pr(>Chi)    \n#> 1        90      14599                                    \n#> 2        89       3295  1    11304 <0.0000000000000002 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nBecause the p-value here is significantly less than the critical value \\(\\alpha = 0.05\\), we can reject the null hypothesis that there is no significant difference in the log-Likelihood of these models. So, our model of dart length as a function of dart weight does indeed fit the data (re: explain the data) better than an intercept-only null model.\nExercises\nFor these exercises, we’ll use the Handaxes dataset from the archdata package. We are going to see if we can predict handaxe length as a function of thickness.\n\nFirst, load the Handaxes table with data(\"Handaxes\").\nNow, let’s do some data wrangling with this table. Bonus points if you can put this all through one pipe. And make sure to assign it back to your table, so that it saves the changes!\n\nChange all the names to lowercase with rename_with(tolower).\nUse select() to grab the catalog number (catalog), length (l), breadth (b), and thickness (t) variables.\nUse rename() to rename l, b, and t to length, width, and thickness, respectively. This will make it clearer what these variables are. Hint: use the \"new_name\" = old_name syntax, for example, \"length\" = l.\n\n\nPractice writing the data to disk and reading it back in using write_csv() and read_csv(). Use here() to set the file path. While you do it, replace Handaxes with handaxes, and remove(Handaxes).\nMake a scatter plot of the data.\n\nBuild a GLM of handaxe length as a function of thickness using a Gaussian distribution and the identity link.\nBuild an intercept-only GLM of handaxe length using the same distribution and link.\nCompare the AIC of these two models.\n\nIs the AIC of the proposed model less than or greater than the AIC of the intercept-only model?\n\n\nNow compare these models using a Likelihood Ratio Test with anova() and test = \"LRT\".\n\nWhat is the result? Is there a significant improvement?"
  },
  {
    "objectID": "labs/12-logistic-regression-lab.html#binomial",
    "href": "labs/12-logistic-regression-lab.html#binomial",
    "title": "Lab 12: Logistic Regression",
    "section": "Binomial",
    "text": "Binomial\nWe’re going to do the exact same thing we just did with a Gaussian GLM, but we’re going to do it with a binary response variable. That means logistic regression, which requires that we specify a binomial distribution with a logit link. Here, we’ll be using the Snodgrass data to answer the following\nQuestion Does the size of a house structure (measured in square feet) make it more or less likely that the structure is found inside the inner walls of the site?\nSo, first, we’ll load in the data. Again, we’ll write it to disk and read it back in, changing the name to snodgrass and remove Snodgrass from our environment. When we read it back, we’ll also convert it to a tibble and convert all the variable names to lowercase. Note that Snodgrass has a number of additional variables we don’t actually need for this exercise, so we’ll subset the table using select() to grab only the variables of interest, namely the response variable inside and our predictor area. In its current form, the response variable inside is a character variable consisting of two values \"Inside\" and \"Outside\". We’ll want to convert this to a binary numeric variable with a value 1 if \"Inside\" and a value 0 if \"Outside\". We’ll use mutate() and a really nice programming function called ifelse() to do that.\n\n\n\n\ndata(\"Snodgrass\")\n\nfilename <- here(\"data\", \"snodgrass.csv\")\n\nwrite_csv(Snodgrass, file = filename)\n\nsnodgrass <- read_csv(filename) |> \n  as_tibble() |> \n  rename_with(tolower) |> \n  select(inside, area) |> \n  mutate(inside = ifelse(inside == \"Inside\", 1, 0)) # read as \"if Inside, set to 1, else 0\"\n\nremove(Snodgrass, filename)  \n\nsnodgrass\n\n\n#> # A tibble: 91 × 2\n#>    inside  area\n#>     <dbl> <dbl>\n#>  1      0  144 \n#>  2      0  256 \n#>  3      1  306 \n#>  4      1  452.\n#>  5      1  410 \n#>  6      1  264 \n#>  7      1  342 \n#>  8      1  399 \n#>  9      0   60 \n#> 10      0  217 \n#> # … with 81 more rows\n\nAs before, we’ll plot these data using a scatterplot.\n\nggplot(snodgrass, aes(area, inside)) + \n  geom_point(\n    size = 3,\n    alpha = 0.6\n  ) +\n  labs(\n    x = \"Area (sq ft)\",\n    y = \"Inside inner wall\"\n  )\n\n\n\n\nNotice anything suspicious? Well, let’s confirm (or deny) that suspicion with a GLM! Again, we’ll specify an exponential distribution and a link function using the family argument to glm(), providing it with a family function that itself takes a link argument. It looks like this:\n\nglm_snodgrass <- glm(\n  inside ~ area,\n  family = binomial(link = \"logit\"),\n  data = snodgrass\n)\n\nsummary(glm_snodgrass)\n#> \n#> Call:\n#> glm(formula = inside ~ area, family = binomial(link = \"logit\"), \n#>     data = snodgrass)\n#> \n#> Deviance Residuals: \n#>    Min      1Q  Median      3Q     Max  \n#> -2.110  -0.481  -0.184   0.288   2.571  \n#> \n#> Coefficients:\n#>             Estimate Std. Error z value  Pr(>|z|)    \n#> (Intercept) -8.66307    1.81844   -4.76 0.0000019 ***\n#> area         0.03476    0.00751    4.63 0.0000037 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> (Dispersion parameter for binomial family taken to be 1)\n#> \n#>     Null deviance: 123.669  on 90  degrees of freedom\n#> Residual deviance:  57.728  on 89  degrees of freedom\n#> AIC: 61.73\n#> \n#> Number of Fisher Scoring iterations: 6\n\nLooks like our intercept and slope estimates are significant. How about the deviance? Let’s compare the AIC of this model to the AIC of an intercept-only model and use LRT to see if this model is significantly better than an intercept-only model.\n\nglm_null <- glm(\n  inside ~ 1,\n  family = binomial(link = \"logit\"),\n  data = snodgrass\n)\n\nAIC(glm_snodgrass) < AIC(glm_null)\n#> [1] TRUE\n\nanova(glm_null, glm_snodgrass, test = \"LRT\")\n#> Analysis of Deviance Table\n#> \n#> Model 1: inside ~ 1\n#> Model 2: inside ~ area\n#>   Resid. Df Resid. Dev Df Deviance            Pr(>Chi)    \n#> 1        90      123.7                                    \n#> 2        89       57.7  1     65.9 0.00000000000000046 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nA bingo two-times!\nExercises\nFor these exercises, we’ll use the DartPoints dataset from the archdata package. We are going to use length to see if we can discriminate Pedernales dart points from the other dart points.\n\nFirst, load the DartPoints table with data(\"DartPoints\").\nNow, let’s do some data wrangling with this table. Make sure to assign it back to your table, so that it saves the changes!And bonus points if you can put this all through one pipe!\n\nChange all the names to lowercase with rename_with(tolower).\nUse select() to grab the name (name) and length (length) variables.\nUse rename() to rename name to type. Hint: use the \"new_name\" = old_name syntax, for example, \"length\" = l.\n\nTHIS IS IMPORTANT!!! Use mutate() and ifelse() to add a column pedernales that is 1 if the type is Pedernales and 0 otherwise. Hint: fill in the ellipsis in mutate(pedernales = ifelse(type == ...)).\n\n\nPractice writing the data to disk and reading it back in using write_csv() and read_csv(). Use here() to set the file path. While you do it, replace DartPoints with darts, and remove(DartPoints).\nMake a scatter plot of the data.\n\nBuild a GLM of the Pedernales type as a function of dart length using a Binomial distribution and the logit link.\nBuild an intercept-only GLM of the Pedernales type using the same distribution and link.\nCompare the AIC of these two models.\n\nIs the AIC of the proposed model less than or greater than the AIC of the intercept-only model?\n\n\nNow compare these models using a Likelihood Ratio Test with anova() and test = \"LRT\".\n\nWhat is the result? Is there a significant improvement?"
  },
  {
    "objectID": "labs/12-logistic-regression-lab.html#homework",
    "href": "labs/12-logistic-regression-lab.html#homework",
    "title": "Lab 12: Logistic Regression",
    "section": "Homework",
    "text": "Homework\nFor the homework, we’ll use the OxfordPots dataset from the archdata package. We are going to see if we can estimate the fraction of Romano-British pottery as a function of distance from Oxford.\n\nFirst, load the OxfordPots table with data(\"OxfordPots\").\nNow, let’s do some data wrangling with this table. Make sure to assign it back to your table, so that it saves the changes!And bonus points if you can put this all through one pipe!\n\nChange all the names to lowercase with rename_with(tolower).\nUse select() to grab the fraction of Romano-British pots (OxfordPct) and distance (OxfordDst) from Oxford variables.\nUse rename() to rename OxfordPct to proportion and OxfordDst to distance. Hint: use the \"new_name\" = old_name syntax, for example, \"proportion\" = OxfordPct.\n\nTHIS IS IMPORTANT!!! The OxfordPct or proportion variable is currently a percentage, but a GLM with a binomial response wants the range of the response to be between 0 and 1, not 0 and 100. So, we need to divide the percentage values by 100. To do this, use mutate(). Hint: fill in the ellipsis in mutate(proportion = ...).\n\n\nPractice writing the data to disk and reading it back in using write_csv() and read_csv(). Use here() to set the file path. While you do it, replace DartPoints with darts, and remove(DartPoints).\nMake a scatter plot of the data.\n\nBuild a GLM of the proportion of Romano-British pots as a function of distance from Oxford using a Binomial distribution and the logit link.\nBuild an intercept-only GLM using the same distribution and link.\nCompare the AIC of these two models.\n\nIs the AIC of the proposed model less than or greater than the AIC of the intercept-only model?\n\n\nNow compare these models using a Likelihood Ratio Test with anova() and test = \"LRT\".\n\nWhat is the result? Is there a significant improvement?"
  },
  {
    "objectID": "labs/13-poisson-regression-lab.html",
    "href": "labs/13-poisson-regression-lab.html",
    "title": "Lab 13: Poisson Regression",
    "section": "",
    "text": "This lab will guide you through the process of\n\nFitting a GLM with a\n\nGaussian distribution\nPoisson distribution\n\n\nOffsets\nDispersion\n\nWe will be using the following packages:\n\narchdata\nhere\nMASS\ntidyverse\nviridis\n\n\nlibrary(archdata)\nlibrary(here)\nlibrary(MASS)\nlibrary(tidyverse)\nlibrary(viridis)\n\n\n\n\ngrave_goods\n\nA hypothetical dataset including counts of grave goods and measures of distance (in meters) from a great house in the American Southwest.\npackage: NA\nreference: https://github.com/kbvernon/qaad/tree/master/datasets\n\n\n\n\nsite_counts\n\nA hypothetical dataset including site counts per kilometer and estimates of elevation (in meters) on an east-west transect through the state of Utah.\npackage: NA\nreference: https://github.com/kbvernon/qaad/tree/master/datasets"
  },
  {
    "objectID": "labs/13-poisson-regression-lab.html#poisson-regression",
    "href": "labs/13-poisson-regression-lab.html#poisson-regression",
    "title": "Lab 13: Poisson Regression",
    "section": "Poisson regression",
    "text": "Poisson regression\nHere, we’re going to work with a response variable consisting of counts. That means poisson regression, which requires that we specify a poisson distribution with a log link. Here, we’ll be using the grave_goods data to answer the following\nQuestion Does distance from a great house (measured in kilometers) drive variation in the number of grave goods at archaeological sites?\nSo, first, we’ll load in the data. In this case, we’ll have to pull the data in from a remote source. This is a made-up data set, so you should not draw any conclusions from it. It does have some intuition behind it, but it’s mainly for illustration purposes.\n\nfile_url <- \"https://raw.githubusercontent.com/kbvernon/qaad/master/labs/grave_goods.csv\"\n\ngrave_goods <- read_csv(file_url)\n\ngrave_goods\n#> # A tibble: 100 × 2\n#>    goods distance\n#>    <dbl>    <dbl>\n#>  1     3    1.16 \n#>  2     0    3.16 \n#>  3     2    1.64 \n#>  4     2    3.53 \n#>  5     0    3.76 \n#>  6     9    0.192\n#>  7     3    2.12 \n#>  8     0    3.57 \n#>  9     1    2.21 \n#> 10     0    1.83 \n#> # … with 90 more rows\n\nAs always, we’ll plot these data using a scatterplot. Note that I use scale_y_continuous() to change the breaks and labels on the y-axis to remove any decimal values. That’s just to emphasize that this is count data.\n\nggplot(grave_goods, aes(distance, goods)) + \n  geom_point(\n    size = 3,\n    alpha = 0.6\n  ) +\n  scale_y_continuous(\n    breaks = seq(0, 10, by = 2),\n    labels = seq(0, 10, by = 2)\n  ) +\n  labs(\n    x = \"Distance from great house (km)\",\n    y = \"Number of grave goods\"\n  )\n\n\n\n\nLooks like there could be a trend, maybe… Let’s use a GLM to find out! Again, we’ll specify an exponential distribution and a link function using the family argument to glm(), providing it with a family function that itself takes a link argument. It looks like this:\n\nglm_goods <- glm(\n  goods ~ distance,\n  family = poisson(link = \"log\"),\n  data = grave_goods\n)\n\nRecall that you do not have to explicitly state the link argument as the log link is the default for poisson(), so you could also specify the model this way:\n\nglm_goods <- glm(\n  goods ~ distance,\n  family = poisson,\n  data = grave_goods\n)\n\nsummary(glm_goods)\n#> \n#> Call:\n#> glm(formula = goods ~ distance, family = poisson, data = grave_goods)\n#> \n#> Deviance Residuals: \n#>    Min      1Q  Median      3Q     Max  \n#> -1.837  -0.858  -0.432   0.500   2.299  \n#> \n#> Coefficients:\n#>             Estimate Std. Error z value            Pr(>|z|)    \n#> (Intercept)   2.1100     0.1092    19.3 <0.0000000000000002 ***\n#> distance     -0.9551     0.0835   -11.4 <0.0000000000000002 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> (Dispersion parameter for poisson family taken to be 1)\n#> \n#>     Null deviance: 263.933  on 99  degrees of freedom\n#> Residual deviance:  88.369  on 98  degrees of freedom\n#> AIC: 280.4\n#> \n#> Number of Fisher Scoring iterations: 5\n\nLooks like our intercept and slope estimates are significant. How about the deviance? To put that in slightly less cryptic terms. We have tests for the model parameters (the coefficients), but how does the model as a whole do at describing the data? To answer that question, let’s first compare the AIC of our target model to the AIC of an intercept-only model. Then we’ll use the Likelihood Ratio Test to see if this model is significantly better than an intercept-only model.\n\nglm_null <- glm(\n  goods ~ 1,\n  family = poisson(link = \"log\"),\n  data = grave_goods\n)\n\nAIC(glm_goods) < AIC(glm_null)\n#> [1] TRUE\n\nanova(glm_goods, test = \"LRT\")\n#> Analysis of Deviance Table\n#> \n#> Model: poisson, link: log\n#> \n#> Response: goods\n#> \n#> Terms added sequentially (first to last)\n#> \n#> \n#>          Df Deviance Resid. Df Resid. Dev            Pr(>Chi)    \n#> NULL                        99      263.9                        \n#> distance  1      176        98       88.4 <0.0000000000000002 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nLooks like we got the result we’re looking for! How fortunate!"
  },
  {
    "objectID": "labs/13-poisson-regression-lab.html#offset",
    "href": "labs/13-poisson-regression-lab.html#offset",
    "title": "Lab 13: Poisson Regression",
    "section": "Offset",
    "text": "Offset\nIn Poisson regression, we assume that the counts are the result of an underlying process (a “Poisson process” if you prefer statistical jargon). The mean or central tendency of that process is assumed to be a log-linear function of some covariates. In the example above, we used distance from great house as our covariate. It’s not that distance per se is the underlying cause or explanation, but it may be a good approximation to the cause. For instance, proximity to the great house can indicate status and status can lead to increased wealth and increased wealth can lead to more grave goods. But, that might not be the only reason we find more grave goods nearer to great houses. It might be that goods nearer to the great house have better preservation. Or, it could be that as archaeologists we are more interested in things happening around the great house and are, thus, more inclined to sample archaeological materials nearer to the great house. That is to say, the counts in our sample may not just be a function of the underlying process (ie status and wealth) but also a function of sampling intensity. To account for differences in sampling intensity or to “control” for sampling intensity, we can include a constant offset.\nTo illustrate the use of offsets, consider a model of site counts. With this model, we are trying to assess whether some relationship exists between the number of sites and elevation, perhaps because elevation is a good proxy for environmental productivity in our study region. However, we also know that we surveyed more intensely in some places than others. As a simple measure of differences in survey intensity, we have the size or area of each of our survey blocks. And, so we want to weight the site counts in each survey block by the area of the block. In effect, we want a measure of density, of counts per unit area. In a log linear model, that has this form:\n\\[\n\\begin{align}\nlog\\; (\\lambda_i/A_i) &= \\beta X_i \\\\\nlog\\; (\\lambda_i) - log\\; (A_i) &= \\beta X_i \\\\\nlog\\; (\\lambda_i) &= \\beta X_i + log\\; (A_i)\n\\end{align}\n\\]\nwhere \\(A_i\\) is the area of survey block \\(i\\) and \\(\\lambda_i\\) is the count at \\(i\\).\nNotice that this is still a log-linear model of the counts \\(\\lambda_i\\). However, we now include as a constant offset \\(log\\; (A_i)\\). We say constant offset because the coefficient for this term is just 1. Importantly, the constant offset is added to the effect of the covariates, so you can read the equation as saying that greater sampling intensity leads to larger counts. It’s also worth noting that we are using a spatial measure to describe our sampling intensity, but we could have used time as well.\nSo, now the question: how do we add these in R? There are a few ways to do this, but I’m going to show you my preferred way, which is to incorporate the offset as a term in the model formula using the offset() function. I prefer doing it this way because it’s more expressive of the underlying logic.\nOK, to see how this works, let’s first download our survey data.\n\nfile_url <- \"https://raw.githubusercontent.com/kbvernon/qaad/master/slides/surveys.csv\"\n\nsurveys <- read_csv(file_url)\n\nsurveys\n#> # A tibble: 100 × 4\n#>    block sites  area elevation\n#>    <dbl> <dbl> <dbl>     <dbl>\n#>  1     1    53  2.86      1.33\n#>  2     2    92  4.41      1.43\n#>  3     3   108  3.61      1.97\n#>  4     4    52  3.44      1.52\n#>  5     5    37  2.48      1.54\n#>  6     6   132  3.93      2.01\n#>  7     7    46  2.74      1.64\n#>  8     8    10  1.33      1.12\n#>  9     9    48  3.39      1.29\n#> 10    10   103  5.47      1.37\n#> # … with 90 more rows\n\nLet’s visualize the counts first.\n\nggplot(surveys, aes(elevation, sites)) + \n  geom_point(\n    size = 3,\n    alpha = 0.6\n  ) +\n  labs(\n    x = \"Elevation (km)\",\n    y = \"Number of sites\"\n  )\n\n\n\n\nIt kinda looks like there might be a trend there. What if we control for the area of each survey location?\n\nggplot(surveys, aes(elevation, sites/area)) + \n  geom_point(\n    size = 3,\n    alpha = 0.6\n  ) +\n  labs(\n    x = \"Elevation (km)\",\n    y = \"Density of sites (n/km2)\"\n  )\n\n\n\n\nNow, we fit a model with the offset included. It looks like this:\n\nglm_surveys <- glm(\n  sites ~ elevation + offset(log(area)),\n  family = poisson,\n  data = surveys\n)\n\nsummary(glm_surveys)\n#> \n#> Call:\n#> glm(formula = sites ~ elevation + offset(log(area)), family = poisson, \n#>     data = surveys)\n#> \n#> Deviance Residuals: \n#>    Min      1Q  Median      3Q     Max  \n#> -3.307  -1.527  -0.399   0.884   4.225  \n#> \n#> Coefficients:\n#>             Estimate Std. Error z value            Pr(>|z|)    \n#> (Intercept)   1.3899     0.0678    20.5 <0.0000000000000002 ***\n#> elevation     1.0537     0.0419    25.1 <0.0000000000000002 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> (Dispersion parameter for poisson family taken to be 1)\n#> \n#>     Null deviance: 915.97  on 99  degrees of freedom\n#> Residual deviance: 288.14  on 98  degrees of freedom\n#> AIC: 892.1\n#> \n#> Number of Fisher Scoring iterations: 4\n\nA couple of things to note here. First, we specify the log of the area as this is a log-linear model. Wrapping that in the offset() function simply tells R to keep it’s coefficient held fixed at 1. That is, to make it a constant offset. Second, the area does not occur as a term in the coefficients table. That’s because it is incorporated into the model as a constant offset.\nNow, let’s see how this model that controls for sampling intensity compares to a model that doesn’t. In this case, we do not do a Likelihood Ratio Test, as the degrees of freedom are the same in both cases (the offset does not add to model complexity), so a simple comparison of the AIC is sufficient.\n\nglm_null <- glm(\n  sites ~ elevation,\n  family = poisson,\n  data = surveys\n)\n\nAIC(glm_surveys) < AIC(glm_null)\n#> [1] TRUE\n\nNotice, too, that adding the offset changes coefficient estimates, quite substantially in the case of the intercept.\n\nsummary(glm_null)\n#> \n#> Call:\n#> glm(formula = sites ~ elevation, family = poisson, data = surveys)\n#> \n#> Deviance Residuals: \n#>     Min       1Q   Median       3Q      Max  \n#> -10.728   -4.043   -0.981    2.556   12.815  \n#> \n#> Coefficients:\n#>             Estimate Std. Error z value            Pr(>|z|)    \n#> (Intercept)   2.9257     0.0664    44.0 <0.0000000000000002 ***\n#> elevation     0.9279     0.0410    22.6 <0.0000000000000002 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> (Dispersion parameter for poisson family taken to be 1)\n#> \n#>     Null deviance: 2953.4  on 99  degrees of freedom\n#> Residual deviance: 2442.2  on 98  degrees of freedom\n#> AIC: 3046\n#> \n#> Number of Fisher Scoring iterations: 5\nsummary(glm_surveys)\n#> \n#> Call:\n#> glm(formula = sites ~ elevation + offset(log(area)), family = poisson, \n#>     data = surveys)\n#> \n#> Deviance Residuals: \n#>    Min      1Q  Median      3Q     Max  \n#> -3.307  -1.527  -0.399   0.884   4.225  \n#> \n#> Coefficients:\n#>             Estimate Std. Error z value            Pr(>|z|)    \n#> (Intercept)   1.3899     0.0678    20.5 <0.0000000000000002 ***\n#> elevation     1.0537     0.0419    25.1 <0.0000000000000002 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> (Dispersion parameter for poisson family taken to be 1)\n#> \n#>     Null deviance: 915.97  on 99  degrees of freedom\n#> Residual deviance: 288.14  on 98  degrees of freedom\n#> AIC: 892.1\n#> \n#> Number of Fisher Scoring iterations: 4"
  },
  {
    "objectID": "labs/13-poisson-regression-lab.html#dispersion",
    "href": "labs/13-poisson-regression-lab.html#dispersion",
    "title": "Lab 13: Poisson Regression",
    "section": "Dispersion",
    "text": "Dispersion\nA key assumption of Poisson regression and exponential models generally, is that the variance of the errors is equal to the mean response weighted by a scaling parameter \\(\\phi\\), which is assumed to be 1.\n\\[Var(\\epsilon) = \\phi\\mu\\]\nWhen \\(\\phi > 1\\), you get over-dispersion. This is important because it artificially shrinks the standard errors of the coefficients and, thus, reduces the p-values, biasing our inferences about the coefficients.\nThere are a couple of ways to check for over-dispersion. Unfortunately, base R doesn’t provide any functions that do that for you, so you have to do it manually. That said, this does give you an opportunity to interact more with a model object and to understand how to access its components.\nHere, we’re going to compare the dispersion estimate (calculated by squaring the deviance residuals) and comparing that to the residual degrees of freedom. Values greater than one are indicative of over-dispersion.\n\n# get deviance residuals\ndev.res <- residuals(glm_surveys, type = \"deviance\")\n\n# calculate dispersion\ndispersion <- sum(dev.res^2)\n\n# get residual degrees of freedom\ndf <- df.residual(glm_surveys)\n\ndispersion/df\n#> [1] 2.94\n\nYou can also use a \\(\\chi^2\\) test to evaluate whether the estimate is significantly different than 1.\n\n1 - pchisq(dispersion, df)\n#> [1] 0\n\nSo, looks like there’s dispersion. How do we account for this? My suggestion is to use a negative binomial model as it still uses Maximum Likelihood Estimation, meaning you get all the measures you need for inference on the model. The one downside to this is that the base R stats package does not provide for a GLM with a negative binomial error distribution. For that, you have to go to another package, in this case the venerable MASS package. The syntax is a smidge different, too, showing some of the age of R. Or, I should say, some of the historical constraints of R owing to its lengthy evolution.\n\nglmnb_surveys <- glm.nb(\n  sites ~ elevation + offset(log(area)),\n  data = surveys\n)\n\nsummary(glmnb_surveys)\n#> \n#> Call:\n#> glm.nb(formula = sites ~ elevation + offset(log(area)), data = surveys, \n#>     init.theta = 38.29760796, link = log)\n#> \n#> Deviance Residuals: \n#>     Min       1Q   Median       3Q      Max  \n#> -2.3082  -0.8547  -0.0715   0.6876   2.2208  \n#> \n#> Coefficients:\n#>             Estimate Std. Error z value            Pr(>|z|)    \n#> (Intercept)   1.3138     0.1196    11.0 <0.0000000000000002 ***\n#> elevation     1.0769     0.0759    14.2 <0.0000000000000002 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> (Dispersion parameter for Negative Binomial(38.3) family taken to be 1)\n#> \n#>     Null deviance: 315.05  on 99  degrees of freedom\n#> Residual deviance: 106.94  on 98  degrees of freedom\n#> AIC: 817.1\n#> \n#> Number of Fisher Scoring iterations: 1\n#> \n#> \n#>               Theta:  38.30 \n#>           Std. Err.:  8.93 \n#> \n#>  2 x log-likelihood:  -811.06\nsummary(glm_surveys)\n#> \n#> Call:\n#> glm(formula = sites ~ elevation + offset(log(area)), family = poisson, \n#>     data = surveys)\n#> \n#> Deviance Residuals: \n#>    Min      1Q  Median      3Q     Max  \n#> -3.307  -1.527  -0.399   0.884   4.225  \n#> \n#> Coefficients:\n#>             Estimate Std. Error z value            Pr(>|z|)    \n#> (Intercept)   1.3899     0.0678    20.5 <0.0000000000000002 ***\n#> elevation     1.0537     0.0419    25.1 <0.0000000000000002 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> (Dispersion parameter for poisson family taken to be 1)\n#> \n#>     Null deviance: 915.97  on 99  degrees of freedom\n#> Residual deviance: 288.14  on 98  degrees of freedom\n#> AIC: 892.1\n#> \n#> Number of Fisher Scoring iterations: 4\n\nNot much change between the two models in terms of coefficient estimates, but notice that the standard errors for the coefficients are considerably larger in the case of the negative binomial model. This is to be expected since over-dispersion shrinks the standard errors. In the case of the negative binomial, we can be much more confident that the coefficient estimates are significantly different than zero."
  },
  {
    "objectID": "labs/13-poisson-regression-lab.html#homework",
    "href": "labs/13-poisson-regression-lab.html#homework",
    "title": "Lab 13: Poisson Regression",
    "section": "Homework",
    "text": "Homework\nNo homework this week!"
  },
  {
    "objectID": "labs/14-partial-dependence-lab.html",
    "href": "labs/14-partial-dependence-lab.html",
    "title": "Lab 14: Partial Dependence",
    "section": "",
    "text": "This lab will guide you through the process of\n\nPredicting with GLMs\nBack-transforming estimates and standard errors\nEstimating partial dependence\nVisualizing partial dependence\n\nWe will be using the following packages:\n\narchdata\nggeffects\npalmerpenguins\ntidyverse\nviridis\n\n⚠️ Don’t forget to install ggeffects with install.packages(\"ggeffects\"). Best to run this in the console!\n\nlibrary(archdata)\nlibrary(ggeffects)\nlibrary(palmerpenguins)\nlibrary(tidyverse)\nlibrary(viridis)\n\n\n\n\nDartPoints\n\nIncludes measurements of 91 Archaic dart points recovered during surface surveys at Fort Hood, Texas.\npackage: archdata\n\nreference: https://cran.r-project.org/web/packages/archdata/archdata.pdf\n\n\n\n\npenguins\n\nIncludes measurements for penguin species, island in Palmer Archipelago, size (flipper length, body mass, bill dimensions), and sex.\npackage: palmerpenguins\n\nreference: https://allisonhorst.github.io/palmerpenguins/reference/penguins.html\n\n\n\n\nSnodgrass\n\nIncludes measurements of size, location, and contents of 91 pit houses at the Snodgrass site in Butler County, Missouri.\nreference: https://cran.r-project.org/web/packages/archdata/archdata.pdf"
  },
  {
    "objectID": "labs/14-partial-dependence-lab.html#motivation",
    "href": "labs/14-partial-dependence-lab.html#motivation",
    "title": "Lab 14: Partial Dependence",
    "section": "Motivation",
    "text": "Motivation\nWith the exception of pure mathematics and maybe fundamental physics, most of our reasoning about the world and our place in it is defeasible, meaning additional information (or additional premises) can alter the goodness of the conclusions we want to draw. If I strike a dry, well-made match, for example, I feel pretty confident that it will light. If I strike a dry, well-made match in the vacuum of space, however, it probably won’t light. Whether the match strikes or not is a Bernoulli outcome, of course, but the general point holds for any discrete or continuous outcome. Its values will only partially depend on the values of a specific covariate or predictor variable. When we fit a simple linear model, we’re sort of just hoping that this problem takes care of itself or, at the very least, that we have good theoretical reasons for thinking it’s not a problem to begin with. When we fit a general linear model, though, or, for that matter, any model with multiple covariates, we have to be a little more deliberate in how we approach this issue. This is especially true when our model includes lots of interactions between and non-linear transformations of variables. Basically, the more complicated the model, the harder it is to summarize the contribution of a covariate using a single number, like coefficient estimate.\nIn this lab, we are going to focus on how we visualize partial dependence, meaning how we visualize the relationship between an outcome and a single covariate (call it the target variable) while in some sense holding the other covariates fixed. These are sometimes referred to as partial dependence plots (or PDPs)."
  },
  {
    "objectID": "labs/14-partial-dependence-lab.html#warm-up",
    "href": "labs/14-partial-dependence-lab.html#warm-up",
    "title": "Lab 14: Partial Dependence",
    "section": "Warm-up",
    "text": "Warm-up\nAs a gentle warm-up to generating PDPs, let’s remind ourselves how to visualize an estimated trend in a simple linear model. The model we are going to visualize is a model of bill length by body mass fit to the Palmer Penguins data set.\nAs always, we’ll start by visualizing the data.\n\n# clean up data, remove rows with missing data\npenguins <- na.omit(penguins)\n\nggplot(penguins, aes(flipper_length_mm, bill_length_mm)) +\n  geom_point(size = 2) +\n  labs(\n    x = \"Flipper Length (mm)\",\n    y = \"Bill Length (mm)\"\n  ) +\n  theme_minimal()\n\n\n\n\nWhat is the relationship here? Let’s see if a linear model can help us out.\n\npenguins_model <- glm(\n  bill_length_mm ~ flipper_length_mm, \n  family = gaussian,\n  data = penguins\n)\n\nsummary(penguins_model)\n#> \n#> Call:\n#> glm(formula = bill_length_mm ~ flipper_length_mm, family = gaussian, \n#>     data = penguins)\n#> \n#> Deviance Residuals: \n#>    Min      1Q  Median      3Q     Max  \n#> -8.637  -2.698  -0.579   2.066  19.095  \n#> \n#> Coefficients:\n#>                   Estimate Std. Error t value            Pr(>|t|)    \n#> (Intercept)        -7.2186     3.2717   -2.21               0.028 *  \n#> flipper_length_mm   0.2548     0.0162   15.69 <0.0000000000000002 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> (Dispersion parameter for gaussian family taken to be 17.2)\n#> \n#>     Null deviance: 9928.9  on 332  degrees of freedom\n#> Residual deviance: 5693.9  on 331  degrees of freedom\n#> AIC: 1896\n#> \n#> Number of Fisher Scoring iterations: 2\n\nNow we use the predict() function to generate estimated values for bill length at observed values of flipper length. We then use geom_line() to visualize the trend line (the estimated or modeled values).\n\npenguins <- penguins |> mutate(fit = predict(penguins_model))\n\nggplot(penguins) +\n  geom_point(\n    aes(flipper_length_mm, bill_length_mm),\n    size = 2\n  ) +\n  labs(\n    x = \"Flipper Length (mm)\",\n    y = \"Bill Length (mm)\"\n  ) +\n  theme_minimal() +\n  geom_line(\n    aes(flipper_length_mm, fit),\n    color = \"darkred\",\n    linewidth = 1\n  )\n\n\n\n\nA simple linear trend as expected. To get the standard errors, we can run predict() with the argument se.fit = TRUE. We’ll then add the intervals to the plot with the function geom_ribbon() like so.\n\nestimates <- predict(penguins_model, se.fit = TRUE)\n\nstr(estimates)\n#> List of 3\n#>  $ fit           : Named num [1:333] 38.9 40.2 42.5 42 ...\n#>   ..- attr(*, \"names\")= chr [1:333] \"1\" \"2\" \"3\" ...\n#>  $ se.fit        : Named num [1:333] 0.396 0.333 0.247 0.262 ...\n#>   ..- attr(*, \"names\")= chr [1:333] \"1\" \"2\" \"3\" ...\n#>  $ residual.scale: num 4.15\n\nThis is a list with three items: fit, se.fit, and residual.scale. The fit is just the estimated response, se.fit the standard error of predicted means, and residual.scale the residual standard deviations. Multiplying the standard error (se.fit) by two and adding/subtracting that value from the fit gives the 95% confidence intervals.\n\npenguins <- penguins |> \n  mutate(\n    fit = estimates$fit,\n    se = estimates$se.fit,\n    conf_lo = fit - 2*se, # lower confidence interval\n    conf_hi = fit + 2*se  # higher confidence interval\n  )\n\nggplot(penguins) +\n  geom_ribbon(\n    aes(x = flipper_length_mm, ymin = conf_lo, ymax = conf_hi),\n    fill = \"gray85\"\n  ) +\n  geom_line(\n    aes(flipper_length_mm, fit),\n    color = \"darkred\",\n    linewidth = 1\n  ) +\n  geom_point(\n    aes(flipper_length_mm, bill_length_mm),\n    size = 2\n  ) +\n  theme_minimal() +\n  labs(\n    x = \"Flipper Length (mm)\",\n    y = \"Bill Length (mm)\"\n  )\n\n\n\n\nExercises\n\nBuild a model of penguin flipper length by body mass.\n\nMake sure to visualize your data first! Make a scatter plot!\nGenerate model fit and estimates for confidence intervals using predict() and se.fit = TRUE.\n\n\nNow visualize the response.\n\nAdd the confidence interval using geom_ribbon().\nNow plot the modeled relationship between flipper length and body mass using geom_line()."
  },
  {
    "objectID": "labs/14-partial-dependence-lab.html#inverse-link",
    "href": "labs/14-partial-dependence-lab.html#inverse-link",
    "title": "Lab 14: Partial Dependence",
    "section": "Inverse Link",
    "text": "Inverse Link\nA Gaussian GLM uses an identity link function, which is just a fancy way of saying it doesn’t transform the response variable. That means predictions made with a Gaussian GLM are already on the scale of the response. In the model above, bill length was measured in millimeters and predictions were returned in millimeters. Unfortunately, logistic and Poisson regression are a little bit more complicated than that because they apply logit and log transformations to the response variable. That means predictions made with logistic and Poisson models return predictions on the logit and log scales respectively. These are not always the easiest scales to interpret, so it’s customary to “back transform” the predictions onto the response scale. The transformations applied to the response variable are called link functions, so the functions applied to the predictions to back transform are often referred to as inverse link functions.\nIn this section, we’ll learn how to access and apply an inverse link to predictions made by a logistic regression model. We’ll be using the Snodgrass data to answer the following\nQuestion Does the size of a house structure (measured in square feet) make it more or less likely that the structure is found inside the inner walls of a site?\n\ndata(\"Snodgrass\")\n\nsnodgrass <- Snodgrass |> \n  as_tibble() |> \n  rename_with(tolower) |> \n  select(inside, area) |> \n  mutate(inside = ifelse(inside == \"Inside\", 1, 0))\n\nremove(Snodgrass)\n\nAs always, we’ll plot these data using a scatterplot.\n\nggplot(snodgrass, aes(area, inside)) + \n  geom_point(\n    size = 3,\n    alpha = 0.6\n  ) +\n  labs(\n    x = \"Area (sq ft)\",\n    y = \"Inside inner wall\"\n  )\n\n\n\n\nAnd, now we’ll fit a GLM with a binomial error distribution and a logit link function. Recall that the logit link is\n\\[logit(p) = log\\left(\\frac{p}{1-p}\\right)\\]\nand that we’re modeling this value as a linear function of covariates:\n\\[log\\left(\\frac{p}{1-p}\\right) = \\beta X\\]\nSo, here’s how we fit that model in R.\n\nglm_snodgrass <- glm(\n  inside ~ area,\n  family = binomial(link = \"logit\"),\n  data = snodgrass\n)\n\nsummary(glm_snodgrass)\n#> \n#> Call:\n#> glm(formula = inside ~ area, family = binomial(link = \"logit\"), \n#>     data = snodgrass)\n#> \n#> Deviance Residuals: \n#>    Min      1Q  Median      3Q     Max  \n#> -2.110  -0.481  -0.184   0.288   2.571  \n#> \n#> Coefficients:\n#>             Estimate Std. Error z value  Pr(>|z|)    \n#> (Intercept) -8.66307    1.81844   -4.76 0.0000019 ***\n#> area         0.03476    0.00751    4.63 0.0000037 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> (Dispersion parameter for binomial family taken to be 1)\n#> \n#>     Null deviance: 123.669  on 90  degrees of freedom\n#> Residual deviance:  57.728  on 89  degrees of freedom\n#> AIC: 61.73\n#> \n#> Number of Fisher Scoring iterations: 6\n\nTo predict with this, we use the predict() function, just as we did above.\n\nfit <- predict(glm_snodgrass)\n\n# showing the first five fit\nfit[1:5]\n#>      1      2      3      4      5 \n#> -3.658  0.235  1.973  7.031  5.588\n\nNote that these are on the logit scale (and that they have values less than zero and greater than one). To get these back onto the scale of the response (i.e., probability), we need to apply the inverse link function to these data. For logistic regression, the inverse link is just the logistic function!\n\\[p = \\frac{1}{1+exp(-\\beta X)}\\]\nTo do that, we extract the function from the error distribution of the model with the family() function and the $ operator.\n\ninverse_link <- family(glm_snodgrass)$linkinv\n\nThis is a function, so we can now apply it to our fit.\n\ntibble(\n  logit = fit[1:5],\n  probability = inverse_link(fit[1:5])\n)\n#> # A tibble: 5 × 2\n#>    logit probability\n#>    <dbl>       <dbl>\n#> 1 -3.66       0.0251\n#> 2  0.235      0.559 \n#> 3  1.97       0.878 \n#> 4  7.03       0.999 \n#> 5  5.59       0.996\n\nWe can use this now to plot the estimated response over our observations.\n\nsnodgrass <- snodgrass |> \n  mutate(\n    logit = predict(glm_snodgrass),\n    probability = inverse_link(logit)\n  )\n\n# plot\nggplot(snodgrass) + \n  geom_point(\n    aes(area, inside),\n    size = 3,\n    alpha = 0.6\n  ) +\n  geom_line(\n    aes(area, probability),\n    linewidth = 1,\n    color = \"#A20000\"\n  ) +\n  labs(\n    x = \"Area (sq ft)\",\n    y = \"Inside inner wall\"\n  )\n\n\n\n\nTo get the standard errors, we can run predict() with the argument se.fit = TRUE again. Note that the fit and standard error are both on the logit scale this time, so we need to apply the inverse link function.\n\nestimates <- predict(glm_snodgrass, se.fit = TRUE)\n\nsnodgrass <- snodgrass |> \n  mutate(\n    logit = estimates$fit,\n    se = estimates$se.fit,\n    probability = inverse_link(logit),\n    conf_hi = inverse_link(logit + 2*se),\n    conf_lo = inverse_link(logit - 2*se)\n  )\n\nHere, we add the logit response and se or standard errors to the snodgrass table by extracting the fit and se.fit items from the estimates list. We then convert the logit response to a probability by applying the inverse_link() function. Next, we estimate the upper confidence line conf_hi by applying the inverse_link() to the sum of the logit response and 2 times the standard error (se). To get the lower confidence line conf_lo, we do the same, but taking the difference.\nNow we have everything we need to plot the confidence ribbon with geom_ribbon(). Notice that I add the ribbon to the plot before adding the observed points and the estimated trend line! This ensures that points and line are not obscured by the confidence ribbon.\n\nggplot(snodgrass) +\n  geom_ribbon(\n    aes(area, ymin = conf_lo, ymax = conf_hi),\n    alpha = 0.5,\n    fill = \"gray75\"\n  ) + \n  geom_point(\n    aes(area, inside),\n    size = 3,\n    alpha = 0.6\n  ) +\n  geom_line(\n    aes(area, probability),\n    linewidth = 1,\n    color = \"#A20000\"\n  ) +\n  labs(\n    x = \"Area (sq ft)\",\n    y = \"Inside inner wall\"\n  )\n\n\n\n\nExercises\nFor these exercises, we’ll use the DartPoints dataset from the archdata package. As we did in the previous lab, we are going to use length to see if we can discriminate Pedernales dart points from the other dart points.\n\nFirst, load the DartPoints table with data(\"DartPoints\").\nNow, let’s do some data wrangling with this table. Make sure to assign it back to your table, so that it saves the changes!And bonus points if you can put this all through one pipe!\n\nChange all the names to lowercase with rename_with(tolower).\nUse select() to grab the name (name) and length (length) variables.\nUse rename() to rename name to type. Hint: use the \"new_name\" = old_name syntax, for example, \"length\" = l.\n\nTHIS IS IMPORTANT!!! Use mutate() and ifelse() to add a column pedernales that is 1 if the type is Pedernales and 0 otherwise. Hint: fill in the ellipsis in mutate(pedernales = ifelse(type == ...)).\n\n\nMake a scatter plot of the data.\n\nBuild a GLM of the Pedernales type as a function of dart length using a Binomial distribution and the logit link.\nUse predict() with se.fit = TRUE and assign this to an object called estimates.\nExtract the inverse link function from the model with family()$linkinv and give it the name inverse_link.\nNow, add the logit, probability, standard errors, and inverse-transformed standard errors to the darts table, applying the inverse link function where necessary.\nPlot the confidence ribbon. Make sure to add the ribbon to the plot before adding observations or the trend line!"
  },
  {
    "objectID": "labs/14-partial-dependence-lab.html#ggeffects",
    "href": "labs/14-partial-dependence-lab.html#ggeffects",
    "title": "Lab 14: Partial Dependence",
    "section": "ggeffects",
    "text": "ggeffects\nNow, let’s get to partial dependence. Here, we’re going to build a slightly more complicated model of bill length that incorporates information about the species of the penguin. This involves the use of a qualitative or categorical variable which gets coded in the model using dummy variables. In this case, we are estimating different intercepts for each species under the assumption that some species have longer bills than others on average.\nBefore we do that, though, let’s visualize the data!\n\nggplot(penguins, aes(flipper_length_mm, bill_length_mm) ) +\n  geom_point(\n    aes(color = species, fill = species),\n    shape = 21,\n    size = 3\n  ) +\n  scale_fill_viridis(\n    name = \"Species\",\n    alpha = 0.7,\n    discrete = TRUE\n  ) +\n  scale_color_viridis(\n    name = \"Species\",\n    discrete = TRUE\n  ) +\n  labs(\n    x = \"Flipper Length (mm)\",\n    y = \"Bill Length (mm)\"\n  ) +\n  theme_minimal()\n\n\n\n\nNow the model.\n\npenguins_model <- glm(\n  bill_length_mm ~ flipper_length_mm + species, \n  family = gaussian,\n  data = penguins\n)\n\nsummary(penguins_model)\n#> \n#> Call:\n#> glm(formula = bill_length_mm ~ flipper_length_mm + species, family = gaussian, \n#>     data = penguins)\n#> \n#> Deviance Residuals: \n#>    Min      1Q  Median      3Q     Max  \n#> -6.676  -1.748   0.024   1.783  12.387  \n#> \n#> Coefficients:\n#>                   Estimate Std. Error t value             Pr(>|t|)    \n#> (Intercept)        -2.4802     4.0795   -0.61                 0.54    \n#> flipper_length_mm   0.2173     0.0214   10.14 < 0.0000000000000002 ***\n#> speciesChinstrap    8.7669     0.4006   21.88 < 0.0000000000000002 ***\n#> speciesGentoo       2.8489     0.6641    4.29             0.000024 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> (Dispersion parameter for gaussian family taken to be 6.75)\n#> \n#>     Null deviance: 9928.9  on 332  degrees of freedom\n#> Residual deviance: 2219.9  on 329  degrees of freedom\n#> AIC: 1587\n#> \n#> Number of Fisher Scoring iterations: 2\n\nThis is a wee-bit more complicated than the simple GLM we fit above, so we are going to use a new package called ggeffects to model the relationship. The process is still the same. First, we generate a table of data by using the model to estimate the response across the range of the covariate. Then, we use that data to plot the response. In this case, we will use the ggpredict() function from ggeffects to generate a table of data for plotting. Then, we will use ggplot() to generate the figure.\nThe ggpredict() function is extremely useful. In fact, you might even consider using it to generate data for visualizing GLMs and even simple linear models, as it will do all the steps we outlined above in one fell swoop. Like with the base predict() function, it’s first argument is a fitted model. You can also specify what variables to visualize (in this case, we only have one, flipper_length_mm), and you can tell it what levels of a factor variable (in this case, species) to visualize with. You provide this to the terms argument.\nThe full function call is this.\n\nestimates <- ggpredict(\n  penguins_model,\n  terms = c(\"flipper_length_mm\", \"species\")\n)\n\nestimates\n#> # Predicted values of bill_length_mm\n#> \n#> # species = Adelie\n#> \n#> flipper_length_mm | Predicted |         95% CI\n#> ----------------------------------------------\n#>               170 |     34.46 | [33.51, 35.40]\n#>               180 |     36.63 | [36.03, 37.23]\n#>               190 |     38.80 | [38.38, 39.22]\n#>               200 |     40.97 | [40.38, 41.57]\n#>               220 |     45.32 | [43.99, 46.65]\n#>               240 |     49.67 | [47.52, 51.81]\n#> \n#> # species = Chinstrap\n#> \n#> flipper_length_mm | Predicted |         95% CI\n#> ----------------------------------------------\n#>               170 |     43.22 | [41.97, 44.48]\n#>               180 |     45.40 | [44.49, 46.31]\n#>               190 |     47.57 | [46.90, 48.24]\n#>               200 |     49.74 | [49.10, 50.39]\n#>               220 |     54.09 | [52.89, 55.28]\n#>               240 |     58.43 | [56.47, 60.39]\n#> \n#> # species = Gentoo\n#> \n#> flipper_length_mm | Predicted |         95% CI\n#> ----------------------------------------------\n#>               170 |     37.31 | [35.26, 39.35]\n#>               180 |     39.48 | [37.84, 41.12]\n#>               190 |     41.65 | [40.41, 42.89]\n#>               200 |     43.82 | [42.96, 44.69]\n#>               220 |     48.17 | [47.69, 48.65]\n#>               240 |     52.51 | [51.45, 53.58]\n\nWhile this printout is somewhat involved, under the hood, it’s really just a tibble with six important columns:\n\n\nx - the predictor variable,\n\npredicted - the estimated response,\n\nstd.error - the standard error\n\nconf.low - the lower confidence level at 95%, so roughly \\(predicted - 2\\cdot std.error\\).\n\nconf.high - the upper confidence level at 95%, so roughly \\(predicted + 2\\cdot std.error\\).\n\nspecies - the levels or species in the factor variable used, in this case, to estimate random effects.\n\nSo, we can use these variables in our estimates table to plot the responses. Here, we are going to do all of the following.\n\nWe are going to add the original observations as points.\nWe’ll also use the viridis color package to apply the viridis color palette to the species groups. Note that we use the same name = \"Species\" for both scale_fill_viridis() and scale_color_viridis(). This does two things.\n\nFirst, and most simply, it sets the title of the legend.\nSecond, it ensures that the legend for color and fill are combined into one legend.\n\n\nTo make the points more visible, we\n\nSet color = species and fill = species in the aes().\nReduce the opacity of the fill to 0.5 (in the scale_fill_viridis() function!), so we can see overlap in points.\nWe also set shape = 21, so we can add a darker border around each point to delineate them.\n\n\nThe facet labels are redundant with the color scheme and legend, so we will remove those with theme().\n\n\nggplot(estimates) + \n  geom_ribbon(\n    aes(x, ymin = conf.low, ymax = conf.high, color = group, fill = group),\n    alpha = 0.15\n  ) +\n  geom_point(\n    data = penguins,\n    aes(flipper_length_mm, bill_length_mm, color = species, fill = species),\n    shape = 21,\n    size = 3\n  ) +\n  scale_fill_viridis(\n    name = \"Species\",\n    alpha = 0.7,\n    discrete = TRUE\n  ) +\n  geom_line(\n    aes(x, predicted, color = group),\n    size = 1.3\n  ) +\n  scale_color_viridis(\n    name = \"Species\",\n    discrete = TRUE\n  ) +\n  # facet_wrap(~ species, ncol = 3) +\n  theme_minimal() +\n  theme(\n    legend.position = \"top\",\n    strip.background = element_blank(), # remove gray box above each facet\n    strip.text = element_blank() # remove text labels above each facet\n  ) +\n  labs(\n    x = \"Flipper length (mm)\",\n    y = \"Bill length (mm)\"\n  )\n\n\n\n\nThis figure has one very serious flaw. It plots the response along the full range of flipper length irrespective of the range of each species. This is a challenge of working with the ggeffects package, unfortunately. If someone can find a good work around for this, I’d like to see it!\nIf you prefer, the ggeffects package also has an automatic plot() method for generating a ggplot() of the response. In this case, it’s nice to use this because of the difficulty of plotting within the range of the actual data, but it has its own frustrating trade-offs that I won’t get into now.\n\nplot(\n  estimates,\n  rawdata = TRUE, \n  limit.range = TRUE,\n  colors = \"viridis\"\n) +\n  labs(\n    x = \"Flipper length (mm)\",\n    y = \"Bill length (mm)\",\n    title = NULL\n  ) +\n  theme_minimal()\n\n\n\n\nExercises\n\nBuild a model of penguin flipper length by body mass and species (let the intercepts vary by species).\n\nMake sure to visualize your data first! Make a scatter plot! And color points by species!\nGenerate model fit and estimates for confidence intervals using ggpredict() with terms = c(\"body_mass_g\", \"species\").\n\n\nNow visualize the response.\n\nYou can use the ggplot2 or plot method.\n\nAdd the confidence interval.\nAdd the modeled relationship.\nAdd the data points to the model."
  },
  {
    "objectID": "labs/14-partial-dependence-lab.html#partial-dependence",
    "href": "labs/14-partial-dependence-lab.html#partial-dependence",
    "title": "Lab 14: Partial Dependence",
    "section": "Partial dependence",
    "text": "Partial dependence\nOK, so here’s the basic idea behind partial dependence. For demonstration purposes, let’s say our data set is this subset of the penguins table:\n\nset.seed(42)\n\nmy_data <- penguins |> \n  select(flipper_length_mm, bill_length_mm, body_mass_g) |> \n  slice_sample(n = 12)\n\nmy_data\n#> # A tibble: 12 × 3\n#>    flipper_length_mm bill_length_mm body_mass_g\n#>                <int>          <dbl>       <int>\n#>  1               187           34.5        2900\n#>  2               197           52.2        3450\n#>  3               211           45.4        4800\n#>  4               195           42.1        4000\n#>  5               224           50          5350\n#>  6               201           41.5        4000\n#>  7               195           41.5        4300\n#>  8               198           50.2        3775\n#>  9               199           37.5        4475\n#> 10               210           52          4800\n#> 11               172           37.9        3150\n#> 12               187           36.2        3300\n\nAnd we fit our penguins model to this data.\n\npenguins_model <- glm(\n  bill_length_mm ~ flipper_length_mm + body_mass_g,\n  data = my_data\n)\n\nNow, let’s say we want to know how bill length varies as a function of body mass while holding flipper length constant. Here’s how we do that. First, we take the range of flipper length and choose and build a grid with arbitrary number of values within that range, so\n\nn <- 3\n\nx_grid <- with(\n  my_data, \n  seq(min(flipper_length_mm), max(flipper_length_mm), length = n)\n)\n\nx_grid\n#> [1] 172 198 224\n\nIn this case, our grid has 3 values. Now, for each of those values, we take our original data set, my_data, and set all the values of flipper length to that value and then estimate the response with our model. We then take the mean of those estimates. This gives us the average response of bill length at each of those three values of flipper length.\n\nnew_data1 <- my_data |> mutate(flipper_length_mm = x_grid[1])\n\nnew_data1\n#> # A tibble: 12 × 3\n#>    flipper_length_mm bill_length_mm body_mass_g\n#>                <dbl>          <dbl>       <int>\n#>  1               172           34.5        2900\n#>  2               172           52.2        3450\n#>  3               172           45.4        4800\n#>  4               172           42.1        4000\n#>  5               172           50          5350\n#>  6               172           41.5        4000\n#>  7               172           41.5        4300\n#>  8               172           50.2        3775\n#>  9               172           37.5        4475\n#> 10               172           52          4800\n#> 11               172           37.9        3150\n#> 12               172           36.2        3300\n\ne1 <- predict(penguins_model, newdata = new_data1)\n\ne1\n#>    1    2    3    4    5    6    7    8    9   10   11   12 \n#> 34.8 33.3 29.7 31.9 28.3 31.9 31.1 32.4 30.6 29.7 34.1 33.7\n\nmean(e1)\n#> [1] 31.8\n\nnew_data2 <- my_data |> mutate(flipper_length_mm = x_grid[2])\n\ne2 <- predict(penguins_model, newdata = new_data2)\n\nnew_data3 <- my_data |> mutate(flipper_length_mm = x_grid[3])\n\ne3 <- predict(penguins_model, newdata = new_data3)\n\npartial_dependence <- tibble(\n  flipper_length_mm = x_grid,\n  bill_length_mm = c(mean(e1), mean(e2), mean(e3))\n)\n\npartial_dependence\n#> # A tibble: 3 × 2\n#>   flipper_length_mm bill_length_mm\n#>               <dbl>          <dbl>\n#> 1               172           31.8\n#> 2               198           43.4\n#> 3               224           55.0\n\nAnd we end up with an estimate of partial dependence that looks like this.\n\nggplot(partial_dependence, aes(flipper_length_mm, bill_length_mm)) +\n  geom_line(\n    color = \"darkred\",\n    linewidth = 1\n  ) +\n  geom_point(size = 3) +\n  labs(\n    x = \"Flipper length (mm)\",\n    y = \"Bill length (mm)\"\n  ) +\n  theme_minimal()\n\n\n\n\nNow, obviously, this would take many many lines of code to estimate the partial dependence for an arbitrary model and data set, especially if we increase the number of points on the grid. Fortunately, the ggeffects package will do these calculations for you. Let’s just run through a full example to see how that works.\nFirst we build the model.\n\npenguins_model <- glm(\n  bill_length_mm ~ flipper_length_mm + body_mass_g + species,\n  data = penguins\n)\n\nsummary(penguins_model)\n#> \n#> Call:\n#> glm(formula = bill_length_mm ~ flipper_length_mm + body_mass_g + \n#>     species, data = penguins)\n#> \n#> Deviance Residuals: \n#>    Min      1Q  Median      3Q     Max  \n#> -7.026  -1.526   0.123   1.427  10.731  \n#> \n#> Coefficients:\n#>                   Estimate Std. Error t value             Pr(>|t|)    \n#> (Intercept)       9.202600   3.966540    2.32               0.0210 *  \n#> flipper_length_mm 0.099064   0.024101    4.11   0.0000499598995988 ***\n#> body_mass_g       0.002911   0.000349    8.34   0.0000000000000021 ***\n#> speciesChinstrap  9.364750   0.371388   25.22 < 0.0000000000000002 ***\n#> speciesGentoo     2.020637   0.612198    3.30               0.0011 ** \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> (Dispersion parameter for gaussian family taken to be 5.58)\n#> \n#>     Null deviance: 9928.9  on 332  degrees of freedom\n#> Residual deviance: 1831.4  on 328  degrees of freedom\n#> AIC: 1525\n#> \n#> Number of Fisher Scoring iterations: 2\n\nNow we estimate partial dependence of bill length on flipper length for each species.\n\npartial <- ggpredict(penguins_model, terms = c(\"flipper_length_mm\", \"species\"))\n\nAnd plot the result.\n\nplot(\n  partial,\n  rawdata = TRUE, \n  limit.range = TRUE,\n  colors = \"viridis\"\n) +\n  labs(\n    x = \"Flipper length (mm)\",\n    y = \"Bill length (mm)\",\n    title = NULL\n  ) +\n  theme_minimal()\n\n\n\n\nNotice that when we control for body mass, the effect of flipper length on fill length is not as large!\nExercises\n\nUse the model in the last example to make a plot of the partial dependence of bill length on body mass.\n\nHow does it compare to the relationship estimated by the model with just bill length and body mass, which you made above?"
  },
  {
    "objectID": "labs/14-partial-dependence-lab.html#homework",
    "href": "labs/14-partial-dependence-lab.html#homework",
    "title": "Lab 14: Partial Dependence",
    "section": "Homework",
    "text": "Homework\nNo homework this week."
  },
  {
    "objectID": "labs/15-regression-tables-lab.html",
    "href": "labs/15-regression-tables-lab.html",
    "title": "Lab 15: Regression Tables",
    "section": "",
    "text": "This lab will guide you through the process of\n\nCreating simple display tables\nCreating interactive displays for large HTML tables\nBuilding data summary tables\nBuilding regression tables\nExporting tables\n\nWe will be using the following packages:\n\narchdata\ngt\ngtsummary\nhere\npalmerpenguins\ntidyverse\n\n⚠️ Don’t forget to install gt and gtsummary with install.packages(c(\"gt\", \"gtsummary\")). Best to run this in the console!\n\nlibrary(archdata)\nlibrary(gt)\nlibrary(gtsummary)\nlibrary(here)\nlibrary(palmerpenguins)\nlibrary(tidyverse)\n\n\n\n\nDartPoints\n\nIncludes measurements of 91 Archaic dart points recovered during surface surveys at Fort Hood, Texas.\npackage: archdata\n\nreference: https://cran.r-project.org/web/packages/archdata/archdata.pdf\n\n\n\n\npenguins\n\nIncludes measurements for penguin species, island in Palmer Archipelago, size (flipper length, body mass, bill dimensions), and sex.\npackage: palmerpenguins\n\nreference: https://allisonhorst.github.io/palmerpenguins/reference/penguins.html\n\n\n\n\nSnodgrass\n\nIncludes measurements of size, location, and contents of 91 pit houses at the Snodgrass site in Butler County, Missouri.\nreference: https://cran.r-project.org/web/packages/archdata/archdata.pdf"
  },
  {
    "objectID": "labs/15-regression-tables-lab.html#grammar-of-tables",
    "href": "labs/15-regression-tables-lab.html#grammar-of-tables",
    "title": "Lab 15: Regression Tables",
    "section": "Grammar of Tables",
    "text": "Grammar of Tables\nTables have a grammar? 🤔 Well, sort of… the grammar here refers to a cohesive language for describing the parts of a table, not a data table, per se, but a display table, a table meant to represent your data rather than simply store it.\n\nA simple table of data includes column or variable labels and a body (all the rows and cells containing values of the variables). A display table, though, can also include (i) a header containing a title for the whole table (and not just a column!), (ii) a footer with, well, footnotes, and (iii) a “stub”, which includes row or observation labels and groupings of those observations. To create a display table, you can use the eponymous gt() function. It will create a gt object having all the components shown in the figure above, though some may be excluded if you do not explicitly specify them. Importantly, you can use gt() to generate tables in the most common formats, namely HTML, LaTeX, and RTF, but you can also export the tables in even larger number of formats, including HTML, PDF, Word, and PNG (if you want an image of the table). That said, the real power of gt is its support of HTML tables, and in particular interactive tables.\nAs a simple motivating example, consider the scenario where a reviewer asks you to include a table with all your raw data in the supplement. That’s pretty easy with gt(). In the following example, we’ll also specify the table header with tab_header() and add a footnote with tab_footnote().\n\nhead(penguins) |> \n  gt() |> \n  tab_caption(\"Table 1. Palmer Penguins Data\") |> \n  tab_header(title = \"this is the header\") |>  \n  tab_footnote(\"*I added a footnote to this table.\")\n\n\n\n\n\nTable 1. Palmer Penguins Data\n  \n\nthis is the header\n    \n\nspecies\n      island\n      bill_length_mm\n      bill_depth_mm\n      flipper_length_mm\n      body_mass_g\n      sex\n      year\n    \n\n\n\nAdelie\nTorgersen\n39.1\n18.7\n181\n3750\nmale\n2007\n\n\nAdelie\nTorgersen\n39.5\n17.4\n186\n3800\nfemale\n2007\n\n\nAdelie\nTorgersen\n40.3\n18.0\n195\n3250\nfemale\n2007\n\n\nAdelie\nTorgersen\nNA\nNA\nNA\nNA\nNA\n2007\n\n\nAdelie\nTorgersen\n36.7\n19.3\n193\n3450\nfemale\n2007\n\n\nAdelie\nTorgersen\n39.3\n20.6\n190\n3650\nmale\n2007\n\n\n\n *I added a footnote to this table.\n    \n\n\n\n\nBut what if your table has a lot of data, like hundreds of rows? Obviously, you can just dump all that data into a giant table and let the user suffer through navigating it. Alternatively, you know, if you have a 💟, you can use gt() to create an interactive HTML table with search and scroll features. You simply pass the gt table to opt_interactive().\n⚠️ A word of caution, this is actually a brand new feature that is under active development, so it might be a smidge buggy. It will also only work in HTML documents, not Word or PDF.\n\npenguins |> \n  gt() |> \n  tab_caption(\"Table 1. Palmer Penguins Data\") |> \n  tab_header(title = \"this is the header\") |>  \n  tab_footnote(\"I added a footnote to this table.\") |> \n  opt_interactive(\n    use_compact_mode = TRUE, # squish table\n    use_highlight = TRUE, # highlight rows on mouse hover\n    use_page_size_select = TRUE, # specify number of rows displayed\n    use_resizers = TRUE, # allow resizing columns\n    use_search = TRUE # add a search text box to table\n  ) |> \n  tab_options(container.height = px(500))\n\n\n\n\n\nthis is the header\n\n\n\n\n\n\n\nNotice that I used px(500) to specify the height of the container. The px() function is a helper for specifying height or width in pixels, rather than, say, inches or centimeters.\nWith the gt package, the sky is the limit on formatting beautiful tables in R. For instance, you can add color codes to columns with data_color() like so.\n\nhead(penguins) |> \n  gt() |> \n  tab_caption(\"Table 1. Palmer Penguins Data\") |> \n  tab_header(title = \"this is the header\") |>  \n  tab_footnote(\"*I added a footnote to this table.\") |> \n  data_color(\n    columns = bill_length_mm:body_mass_g,\n    palette = \"BrBG\"\n  )\n\n\n\n\n\nTable 1. Palmer Penguins Data\n  \n\nthis is the header\n    \n\nspecies\n      island\n      bill_length_mm\n      bill_depth_mm\n      flipper_length_mm\n      body_mass_g\n      sex\n      year\n    \n\n\n\nAdelie\nTorgersen\n39.1\n18.7\n181\n3750\nmale\n2007\n\n\nAdelie\nTorgersen\n39.5\n17.4\n186\n3800\nfemale\n2007\n\n\nAdelie\nTorgersen\n40.3\n18.0\n195\n3250\nfemale\n2007\n\n\nAdelie\nTorgersen\nNA\nNA\nNA\nNA\nNA\n2007\n\n\nAdelie\nTorgersen\n36.7\n19.3\n193\n3450\nfemale\n2007\n\n\nAdelie\nTorgersen\n39.3\n20.6\n190\n3650\nmale\n2007\n\n\n\n *I added a footnote to this table.\n    \n\n\n\n\nAnd that’s just the tip of the iceberg! To learn more, I recommend perusing the package website and playing around with different functions and settings.\nExercises\n\nLoad in the DartPoints data with data(\"DartPoints\").\n\nMake all the variable names lower case with rename_with(tolower).\n\nSubset the data to include only name, tarl (the Smithsonian Trinomial), length, width, and thickness.\n\nCreate an interactive table with the DartPoints data.\n\nAdd a header and footnote with tab_header() and tab_footnote().\n\nSpecify the height of the table’s container.\n\nTry experimenting with the different arguments you can pass to opt_interactive() to see what they do."
  },
  {
    "objectID": "labs/15-regression-tables-lab.html#data-summary",
    "href": "labs/15-regression-tables-lab.html#data-summary",
    "title": "Lab 15: Regression Tables",
    "section": "Data summary",
    "text": "Data summary\nThe number and variety of options available in the gt package can be overwhelming. It’s also not capable of summarizing data or models by itself. That’s where the gtsummary package comes in. It’s a wrapper around gt that automatically generates summary tables of data and regression models. To generate a summary table of data, we use the tbl_summary() function. This is similar to the skim function from the skimr package, but with a slightly different aesthetic. It also gives you more fine grained control over the output and style, which means it’s a smidge more complicated to work with, too.\n\npenguins |> tbl_summary()\n\n\n\n\n\n\nCharacteristic\n      \nN = 3441\n\n    \n\n\nspecies\n\n\n\n    Adelie\n152 (44%)\n\n\n    Chinstrap\n68 (20%)\n\n\n    Gentoo\n124 (36%)\n\n\nisland\n\n\n\n    Biscoe\n168 (49%)\n\n\n    Dream\n124 (36%)\n\n\n    Torgersen\n52 (15%)\n\n\nbill_length_mm\n44.5 (39.2, 48.5)\n\n\n    Unknown\n2\n\n\nbill_depth_mm\n17.30 (15.60, 18.70)\n\n\n    Unknown\n2\n\n\nflipper_length_mm\n197 (190, 213)\n\n\n    Unknown\n2\n\n\nbody_mass_g\n4,050 (3,550, 4,750)\n\n\n    Unknown\n2\n\n\nsex\n\n\n\n    female\n165 (50%)\n\n\n    male\n168 (50%)\n\n\n    Unknown\n11\n\n\nyear\n\n\n\n    2007\n110 (32%)\n\n\n    2008\n114 (33%)\n\n\n    2009\n120 (35%)\n\n\n\n\n1 n (%); Median (IQR)\n    \n\n\n\n\nBy default, the summary gives you the proportions for categorical data, and the median values of continuous variables (along with their first and third quartiles or their interquartile range, IQR, in parentheses). ‘Unknown’ here refers to missing values. These are fine as far as they go, but the styling leaves a lot to be desired, especially if vertical space is prime real estate in whatever document you are creating.\nIn this case, something we might actually care about in our analysis is differences between species, which requires that we summarize the data by species. To do that, we simply specify the species variable in the by= argument.\n\npenguins |> tbl_summary(by = species)\n\n\n\n\n\n\nCharacteristic\n      \nAdelie, N = 1521\n\n      \nChinstrap, N = 681\n\n      \nGentoo, N = 1241\n\n    \n\n\nisland\n\n\n\n\n\n    Biscoe\n44 (29%)\n0 (0%)\n124 (100%)\n\n\n    Dream\n56 (37%)\n68 (100%)\n0 (0%)\n\n\n    Torgersen\n52 (34%)\n0 (0%)\n0 (0%)\n\n\nbill_length_mm\n38.8 (36.8, 40.8)\n49.5 (46.3, 51.1)\n47.3 (45.3, 49.5)\n\n\n    Unknown\n1\n0\n1\n\n\nbill_depth_mm\n18.40 (17.50, 19.00)\n18.45 (17.50, 19.40)\n15.00 (14.20, 15.70)\n\n\n    Unknown\n1\n0\n1\n\n\nflipper_length_mm\n190 (186, 195)\n196 (191, 201)\n216 (212, 221)\n\n\n    Unknown\n1\n0\n1\n\n\nbody_mass_g\n3,700 (3,350, 4,000)\n3,700 (3,488, 3,950)\n5,000 (4,700, 5,500)\n\n\n    Unknown\n1\n0\n1\n\n\nsex\n\n\n\n\n\n    female\n73 (50%)\n34 (50%)\n58 (49%)\n\n\n    male\n73 (50%)\n34 (50%)\n61 (51%)\n\n\n    Unknown\n6\n0\n5\n\n\nyear\n\n\n\n\n\n    2007\n50 (33%)\n26 (38%)\n34 (27%)\n\n\n    2008\n50 (33%)\n18 (26%)\n46 (37%)\n\n\n    2009\n52 (34%)\n24 (35%)\n44 (35%)\n\n\n\n\n1 n (%); Median (IQR)\n    \n\n\n\n\nBy default, the table uses the column labels from the table. R doesn’t like spaces in names, so it’s common to see underscores in the labels. We can amend these in the table by specifying all the changes in a named list.\n\npenguins |> \n  tbl_summary(\n    by = species,\n    label = list(\n      island = \"Island\",\n      bill_length_mm = \"Bill length (mm)\",\n      bill_depth_mm = \"Bill depth (mm)\",\n      flipper_length_mm = \"Flipper length (mm)\",\n      body_mass_g = \"Body mass (g)\",\n      sex = \"Sex\",\n      year = \"Year\"\n    )\n  ) \n\n\n\n\n\n\nCharacteristic\n      \nAdelie, N = 1521\n\n      \nChinstrap, N = 681\n\n      \nGentoo, N = 1241\n\n    \n\n\nIsland\n\n\n\n\n\n    Biscoe\n44 (29%)\n0 (0%)\n124 (100%)\n\n\n    Dream\n56 (37%)\n68 (100%)\n0 (0%)\n\n\n    Torgersen\n52 (34%)\n0 (0%)\n0 (0%)\n\n\nBill length (mm)\n38.8 (36.8, 40.8)\n49.5 (46.3, 51.1)\n47.3 (45.3, 49.5)\n\n\n    Unknown\n1\n0\n1\n\n\nBill depth (mm)\n18.40 (17.50, 19.00)\n18.45 (17.50, 19.40)\n15.00 (14.20, 15.70)\n\n\n    Unknown\n1\n0\n1\n\n\nFlipper length (mm)\n190 (186, 195)\n196 (191, 201)\n216 (212, 221)\n\n\n    Unknown\n1\n0\n1\n\n\nBody mass (g)\n3,700 (3,350, 4,000)\n3,700 (3,488, 3,950)\n5,000 (4,700, 5,500)\n\n\n    Unknown\n1\n0\n1\n\n\nSex\n\n\n\n\n\n    female\n73 (50%)\n34 (50%)\n58 (49%)\n\n\n    male\n73 (50%)\n34 (50%)\n61 (51%)\n\n\n    Unknown\n6\n0\n5\n\n\nYear\n\n\n\n\n\n    2007\n50 (33%)\n26 (38%)\n34 (27%)\n\n\n    2008\n50 (33%)\n18 (26%)\n46 (37%)\n\n\n    2009\n52 (34%)\n24 (35%)\n44 (35%)\n\n\n\n\n1 n (%); Median (IQR)\n    \n\n\n\n\nThe gtsummary package also offers some helper functions for adding various columns to a summary table. For example, you can add a column with row totals using the add_overall() function.\n\npenguins |> \n  tbl_summary(\n    by = species,\n    label = list(\n      island = \"Island\",\n      bill_length_mm = \"Bill length (mm)\",\n      bill_depth_mm = \"Bill depth (mm)\",\n      flipper_length_mm = \"Flipper length (mm)\",\n      body_mass_g = \"Body mass (g)\",\n      sex = \"Sex\",\n      year = \"Year\"\n    )\n  ) |> \n  add_overall(last = TRUE)\n\n\n\n\n\n\nCharacteristic\n      \nAdelie, N = 1521\n\n      \nChinstrap, N = 681\n\n      \nGentoo, N = 1241\n\n      \nOverall, N = 3441\n\n    \n\n\nIsland\n\n\n\n\n\n\n    Biscoe\n44 (29%)\n0 (0%)\n124 (100%)\n168 (49%)\n\n\n    Dream\n56 (37%)\n68 (100%)\n0 (0%)\n124 (36%)\n\n\n    Torgersen\n52 (34%)\n0 (0%)\n0 (0%)\n52 (15%)\n\n\nBill length (mm)\n38.8 (36.8, 40.8)\n49.5 (46.3, 51.1)\n47.3 (45.3, 49.5)\n44.5 (39.2, 48.5)\n\n\n    Unknown\n1\n0\n1\n2\n\n\nBill depth (mm)\n18.40 (17.50, 19.00)\n18.45 (17.50, 19.40)\n15.00 (14.20, 15.70)\n17.30 (15.60, 18.70)\n\n\n    Unknown\n1\n0\n1\n2\n\n\nFlipper length (mm)\n190 (186, 195)\n196 (191, 201)\n216 (212, 221)\n197 (190, 213)\n\n\n    Unknown\n1\n0\n1\n2\n\n\nBody mass (g)\n3,700 (3,350, 4,000)\n3,700 (3,488, 3,950)\n5,000 (4,700, 5,500)\n4,050 (3,550, 4,750)\n\n\n    Unknown\n1\n0\n1\n2\n\n\nSex\n\n\n\n\n\n\n    female\n73 (50%)\n34 (50%)\n58 (49%)\n165 (50%)\n\n\n    male\n73 (50%)\n34 (50%)\n61 (51%)\n168 (50%)\n\n\n    Unknown\n6\n0\n5\n11\n\n\nYear\n\n\n\n\n\n\n    2007\n50 (33%)\n26 (38%)\n34 (27%)\n110 (32%)\n\n\n    2008\n50 (33%)\n18 (26%)\n46 (37%)\n114 (33%)\n\n\n    2009\n52 (34%)\n24 (35%)\n44 (35%)\n120 (35%)\n\n\n\n\n1 n (%); Median (IQR)\n    \n\n\n\n\nWe can also add some custom formatting, like adding a caption, removing the “Characteristic” label (that column should be obvious), and making the variable names or labels bold.\n\npenguins |> \n  tbl_summary(\n    by = species,\n    label = list(\n      island = \"Island\",\n      bill_length_mm = \"Bill length (mm)\",\n      bill_depth_mm = \"Bill depth (mm)\",\n      flipper_length_mm = \"Flipper length (mm)\",\n      body_mass_g = \"Body mass (g)\",\n      sex = \"Sex\",\n      year = \"Year\"\n    )\n  ) |> \n  add_overall(last = TRUE) |> \n  modify_header(label ~ \"\") |>\n  modify_caption(\"**Table 1. Penguin Characteristics**\") |> \n  bold_labels()\n\n\n\n\n\nTable 1. Penguin Characteristics\n  \n\n      \nAdelie, N = 1521\n\n      \nChinstrap, N = 681\n\n      \nGentoo, N = 1241\n\n      \nOverall, N = 3441\n\n    \n\n\nIsland\n\n\n\n\n\n\n    Biscoe\n44 (29%)\n0 (0%)\n124 (100%)\n168 (49%)\n\n\n    Dream\n56 (37%)\n68 (100%)\n0 (0%)\n124 (36%)\n\n\n    Torgersen\n52 (34%)\n0 (0%)\n0 (0%)\n52 (15%)\n\n\nBill length (mm)\n38.8 (36.8, 40.8)\n49.5 (46.3, 51.1)\n47.3 (45.3, 49.5)\n44.5 (39.2, 48.5)\n\n\n    Unknown\n1\n0\n1\n2\n\n\nBill depth (mm)\n18.40 (17.50, 19.00)\n18.45 (17.50, 19.40)\n15.00 (14.20, 15.70)\n17.30 (15.60, 18.70)\n\n\n    Unknown\n1\n0\n1\n2\n\n\nFlipper length (mm)\n190 (186, 195)\n196 (191, 201)\n216 (212, 221)\n197 (190, 213)\n\n\n    Unknown\n1\n0\n1\n2\n\n\nBody mass (g)\n3,700 (3,350, 4,000)\n3,700 (3,488, 3,950)\n5,000 (4,700, 5,500)\n4,050 (3,550, 4,750)\n\n\n    Unknown\n1\n0\n1\n2\n\n\nSex\n\n\n\n\n\n\n    female\n73 (50%)\n34 (50%)\n58 (49%)\n165 (50%)\n\n\n    male\n73 (50%)\n34 (50%)\n61 (51%)\n168 (50%)\n\n\n    Unknown\n6\n0\n5\n11\n\n\nYear\n\n\n\n\n\n\n    2007\n50 (33%)\n26 (38%)\n34 (27%)\n110 (32%)\n\n\n    2008\n50 (33%)\n18 (26%)\n46 (37%)\n114 (33%)\n\n\n    2009\n52 (34%)\n24 (35%)\n44 (35%)\n120 (35%)\n\n\n\n\n1 n (%); Median (IQR)\n    \n\n\n\n\nNotice that you can use markdown syntax, like including asterisks around text you want to bold or italicize, and it will be rendered appropriately.\nExercises\n\nCreate a summary table of the DartPoints data with tbl_summary().\n\nUse the by= argument to summarize by dart type (the variable name is name).\n\nUpdate the labels as needed.\n\nAdd the overall counts for each group.\n\nRemove the “Characteristic” label with modify_header().\n\nAdd a caption.\n\nMake the variable labels bold."
  },
  {
    "objectID": "labs/15-regression-tables-lab.html#regression-summary",
    "href": "labs/15-regression-tables-lab.html#regression-summary",
    "title": "Lab 15: Regression Tables",
    "section": "Regression summary",
    "text": "Regression summary\nSummarizing models works in a similar way to summarizing data tables. Consider this simple linear model of bill length by flipper length and species.\n\nlm_penguins <- lm(\n  bill_length_mm ~ flipper_length_mm + species, \n  data = penguins\n)\n\nHere is the base R summary of the model:\n\nlm_penguins |> summary()\n#> \n#> Call:\n#> lm(formula = bill_length_mm ~ flipper_length_mm + species, data = penguins)\n#> \n#> Residuals:\n#>    Min     1Q Median     3Q    Max \n#> -6.662 -1.746  0.028  1.825 12.354 \n#> \n#> Coefficients:\n#>                   Estimate Std. Error t value             Pr(>|t|)    \n#> (Intercept)        -2.0586     4.0386   -0.51                 0.61    \n#> flipper_length_mm   0.2151     0.0212   10.13 < 0.0000000000000002 ***\n#> speciesChinstrap    8.7801     0.3991   22.00 < 0.0000000000000002 ***\n#> speciesGentoo       2.8569     0.6586    4.34             0.000019 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 2.6 on 338 degrees of freedom\n#>   (2 observations deleted due to missingness)\n#> Multiple R-squared:  0.776,  Adjusted R-squared:  0.774 \n#> F-statistic:  390 on 3 and 338 DF,  p-value: <0.0000000000000002\n\nAnd here is the gtsummary:\n\nlm_penguins |> tbl_regression()\n\n\n\n\n\n\nCharacteristic\n      Beta\n      \n95% CI1\n\n      p-value\n    \n\n\nflipper_length_mm\n0.22\n0.17, 0.26\n<0.001\n\n\nspecies\n\n\n\n\n\n    Adelie\n—\n—\n\n\n\n    Chinstrap\n8.8\n8.0, 9.6\n<0.001\n\n\n    Gentoo\n2.9\n1.6, 4.2\n<0.001\n\n\n\n\n1 CI = Confidence Interval\n    \n\n\n\n\nThe first thing to note here is that this is just the coefficients table from the summary() output. Instead of “Estimate”, though, it uses the label “Beta” - just another way of referring to the coefficient estimates or estimates of the betas for the different variables. The second and way more important thing to note is that the intercept is not reported by default. I do not know why this is the case, but I recommend you always do so (at least if it’s a model that estimates an intercept). To do that, we add intercept = TRUE. While we’re at, let’s also rename the variables.\n\nlm_penguins |> \n  tbl_regression(\n    intercept = TRUE,\n    label = list(\n      bill_length_mm = \"Bill length (mm)\",\n      flipper_length_mm = \"Flipper length (mm)\",\n      species = \"Species\"\n    )\n  )\n\n\n\n\n\n\nCharacteristic\n      Beta\n      \n95% CI1\n\n      p-value\n    \n\n\n(Intercept)\n-2.1\n-10, 5.9\n0.6\n\n\nFlipper length (mm)\n0.22\n0.17, 0.26\n<0.001\n\n\nSpecies\n\n\n\n\n\n    Adelie\n—\n—\n\n\n\n    Chinstrap\n8.8\n8.0, 9.6\n<0.001\n\n\n    Gentoo\n2.9\n1.6, 4.2\n<0.001\n\n\n\n\n1 CI = Confidence Interval\n    \n\n\n\n\nAlso, the test statistic used to generate the p-value is currently suppressed, and instead of reporting the standard error of the estimates, the 95% confidence interval is shown instead. These are some odd design choices by the package authors, maybe related to the fact that they designed the package to work with every model you can think of putting together in R. Whatever the reason, I recommend that you unhide those columns like so.\n\nlm_penguins |> \n  tbl_regression(\n    intercept = TRUE,\n    label = list(\n      bill_length_mm = \"Bill length (mm)\",\n      flipper_length_mm = \"Flipper length (mm)\",\n      species = \"Species\"\n    )\n  ) |> \n  modify_column_unhide(columns = c(statistic, std.error))\n\n\n\n\n\n\nCharacteristic\n      Beta\n      \nSE1\n\n      Statistic\n      \n95% CI1\n\n      p-value\n    \n\n\n(Intercept)\n-2.1\n4.04\n-0.510\n-10, 5.9\n0.6\n\n\nFlipper length (mm)\n0.22\n0.021\n10.1\n0.17, 0.26\n<0.001\n\n\nSpecies\n\n\n\n\n\n\n\n    Adelie\n—\n—\n—\n—\n\n\n\n    Chinstrap\n8.8\n0.399\n22.0\n8.0, 9.6\n<0.001\n\n\n    Gentoo\n2.9\n0.659\n4.34\n1.6, 4.2\n<0.001\n\n\n\n\n1 SE = Standard Error, CI = Confidence Interval\n    \n\n\n\n\nFinally, the coefficients table doesn’t have information about the model as a whole, like the \\(R^2\\) value or the F-statistic used in the ANOVA. We can add that information to our table either as additional rows with add_glance_table() or as a footnote with add_glance_source_note(). My own preference is for the latter, but journals will have their own formatting guidelines.\n\nlm_penguins |> \n  tbl_regression(\n    intercept = TRUE,\n    label = list(\n      bill_length_mm = \"Bill length (mm)\",\n      flipper_length_mm = \"Flipper length (mm)\",\n      species = \"Species\"\n    )\n  ) |> \n  modify_column_unhide(columns = c(statistic, std.error)) |> \n  add_glance_source_note(\n    include = c(adj.r.squared, sigma, statistic, df.residual, df, p.value),\n    label = list(\n      sigma = \"σ\",\n      statistic = \"F-statistic\"\n    )\n  )\n\n\n\n\n\n\nCharacteristic\n      Beta\n      \nSE1\n\n      Statistic\n      \n95% CI1\n\n      p-value\n    \n\n\n(Intercept)\n-2.1\n4.04\n-0.510\n-10, 5.9\n0.6\n\n\nFlipper length (mm)\n0.22\n0.021\n10.1\n0.17, 0.26\n<0.001\n\n\nSpecies\n\n\n\n\n\n\n\n    Adelie\n—\n—\n—\n—\n\n\n\n    Chinstrap\n8.8\n0.399\n22.0\n8.0, 9.6\n<0.001\n\n\n    Gentoo\n2.9\n0.659\n4.34\n1.6, 4.2\n<0.001\n\n\n\nAdjusted R² = 0.774; σ = 2.60; F-statistic = 390; Residual df = 338; df = 3; p-value = <0.001\n    \n\n\n1 SE = Standard Error, CI = Confidence Interval\n    \n\n\n\n\nBy default, a number of statistics will be added to the table, but you should limit these to the ones that are most relevant to evaluating the type of model you have fit. In this case, we fit a linear model, so we included the adjusted \\(R^2\\), the residual standard error (sigma), and information about the ANOVA.\nLet’s do one more example with a GLM. Here we’ll see if we can differentiate Gentoo penguins from the rest. Hint: they’re the really big ones. Then we’ll walk through the steps of displaying the model.\n\ngentoo <- penguins |> \n  mutate(gentoo = ifelse(species == \"Gentoo\", 1, 0))\n\nglm_penguins <- glm(\n  gentoo ~ body_mass_g + bill_length_mm, \n  family = binomial,\n  data = gentoo\n)\n\nglm_penguins |> summary()\n#> \n#> Call:\n#> glm(formula = gentoo ~ body_mass_g + bill_length_mm, family = binomial, \n#>     data = gentoo)\n#> \n#> Deviance Residuals: \n#>     Min       1Q   Median       3Q      Max  \n#> -2.3865  -0.1753  -0.0390   0.0622   2.6125  \n#> \n#> Coefficients:\n#>                  Estimate Std. Error z value          Pr(>|z|)    \n#> (Intercept)    -31.990945   4.528334   -7.06 0.000000000001611 ***\n#> body_mass_g      0.006257   0.000821    7.62 0.000000000000026 ***\n#> bill_length_mm   0.091252   0.057726    1.58              0.11    \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> (Dispersion parameter for binomial family taken to be 1)\n#> \n#>     Null deviance: 446.80  on 341  degrees of freedom\n#> Residual deviance: 115.33  on 339  degrees of freedom\n#>   (2 observations deleted due to missingness)\n#> AIC: 121.3\n#> \n#> Number of Fisher Scoring iterations: 7\n\n\nglm_penguins |> \n  tbl_regression(\n    intercept = TRUE,\n    label = list(\n      gentoo = \"Gentoo\",\n      body_mass_g = \"Body mass (g)\",\n      bill_length_mm = \"Bill length (mm)\"\n    )\n  ) |> \n  modify_column_unhide(columns = c(statistic, std.error)) |> \n  add_glance_source_note(\n    include = c(AIC, null.deviance, df.null, deviance, df.residual)\n  )\n\n\n\n\n\n\nCharacteristic\n      \nlog(OR)1\n\n      \nSE1\n\n      Statistic\n      \n95% CI1\n\n      p-value\n    \n\n\n(Intercept)\n-32\n4.53\n-7.06\n-42, -24\n<0.001\n\n\nBody mass (g)\n0.01\n0.001\n7.62\n0.00, 0.01\n<0.001\n\n\nBill length (mm)\n0.09\n0.058\n1.58\n-0.02, 0.21\n0.11\n\n\n\nAIC = 121; Null deviance = 447; Null df = 341; Deviance = 115; Residual df = 339\n    \n\n\n1 OR = Odds Ratio, SE = Standard Error, CI = Confidence Interval\n    \n\n\n\n\nNote that we are given estimates of the coefficients on the logit or log odds ratio scale. This is a value that can be really hard to interpret, so it’s recommended that you exponentiate these terms to get them onto the odds ratio scale (in this case the probability of being a Gentoo penguin relative to the probability of not being a Gentoo penguin). To do that, we simply tell tbl_regression() to exponentiate the terms for us.\n\nglm_penguins |> \n  tbl_regression(\n    exponentiate = TRUE,\n    intercept = TRUE,\n    label = list(\n      gentoo = \"Gentoo\",\n      body_mass_g = \"Body mass (g)\",\n      bill_length_mm = \"Bill length (mm)\"\n    )\n  ) |> \n  modify_column_unhide(columns = c(statistic, std.error)) |> \n  add_glance_source_note(\n    include = c(AIC, null.deviance, df.null, deviance, df.residual)\n  )\n\n\n\n\n\n\nCharacteristic\n      \nOR1\n\n      \nSE1\n\n      Statistic\n      \n95% CI1\n\n      p-value\n    \n\n\n(Intercept)\n0.00\n4.53\n-7.06\n0.00, 0.00\n<0.001\n\n\nBody mass (g)\n1.01\n0.001\n7.62\n1.00, 1.01\n<0.001\n\n\nBill length (mm)\n1.10\n0.058\n1.58\n0.98, 1.23\n0.11\n\n\n\nAIC = 121; Null deviance = 447; Null df = 341; Deviance = 115; Residual df = 339\n    \n\n\n1 OR = Odds Ratio, SE = Standard Error, CI = Confidence Interval\n    \n\n\n\n\nA final note about formatting. All the things you can change in a data summary table, you can modify in a regression summary table, too.\n\nglm_penguins |> \n  tbl_regression(\n    exponentiate = TRUE,\n    intercept = TRUE,\n    label = list(\n      gentoo = \"Gentoo\",\n      body_mass_g = \"Body mass (g)\",\n      bill_length_mm = \"Bill length (mm)\"\n    )\n  ) |> \n  modify_column_unhide(columns = c(statistic, std.error)) |> \n  add_glance_source_note(\n    include = c(AIC, null.deviance, df.null, deviance, df.residual)\n  ) |>\n  modify_header(label ~ \"\") |>\n  modify_caption(\"**Table 2. Gentoo Model**\") |> \n  bold_labels()\n\n\n\n\n\nTable 2. Gentoo Model\n  \n\n      \nOR1\n\n      \nSE1\n\n      Statistic\n      \n95% CI1\n\n      p-value\n    \n\n\n(Intercept)\n0.00\n4.53\n-7.06\n0.00, 0.00\n<0.001\n\n\nBody mass (g)\n1.01\n0.001\n7.62\n1.00, 1.01\n<0.001\n\n\nBill length (mm)\n1.10\n0.058\n1.58\n0.98, 1.23\n0.11\n\n\n\nAIC = 121; Null deviance = 447; Null df = 341; Deviance = 115; Residual df = 339\n    \n\n\n1 OR = Odds Ratio, SE = Standard Error, CI = Confidence Interval\n    \n\n\n\n\nYou can also re-order the columns, but it’s a little tricky because of the structure of the summary table. We modify the table body, relocating columns or variables with the dplyr function relocate(), telling it to move the statistic column before the p.value column.\n\nglm_penguins |> \n  tbl_regression(\n    exponentiate = TRUE,\n    intercept = TRUE,\n    label = list(\n      gentoo = \"Gentoo\",\n      body_mass_g = \"Body mass (g)\",\n      bill_length_mm = \"Bill length (mm)\"\n    )\n  ) |> \n  modify_column_unhide(columns = c(statistic, std.error)) |> \n  modify_table_body(~.x |> relocate(statistic, .before = p.value)) |> \n  add_glance_source_note(\n    include = c(AIC, null.deviance, df.null, deviance, df.residual)\n  ) |>\n  modify_header(label ~ \"\") |>\n  modify_caption(\"**Table 2. Gentoo Model**\") |> \n  bold_labels()\n\n\n\n\n\nTable 2. Gentoo Model\n  \n\n      \nOR1\n\n      \nSE1\n\n      \n95% CI1\n\n      Statistic\n      p-value\n    \n\n\n(Intercept)\n0.00\n4.53\n0.00, 0.00\n-7.06\n<0.001\n\n\nBody mass (g)\n1.01\n0.001\n1.00, 1.01\n7.62\n<0.001\n\n\nBill length (mm)\n1.10\n0.058\n0.98, 1.23\n1.58\n0.11\n\n\n\nAIC = 121; Null deviance = 447; Null df = 341; Deviance = 115; Residual df = 339\n    \n\n\n1 OR = Odds Ratio, SE = Standard Error, CI = Confidence Interval\n    \n\n\n\n\nThis ordering makes a smidge more sense because SE and CI are naturally related to each other, as are the test statistic and p-value.\nExercises\n\nCreate a regression table for a logistic model fit to the Snodgrass data.\nLoad in the Snodgrass data with data(\"Snodgrass\").\n\nMake all the variable names lower case with rename_with(tolower).\n\nUse select() to subset the data to include only the area of each structure and whether it is inside the inner walls.\n\nUse a GLM to model whether the structure is found inside the inner walls as a function of its total area.\n\nFormula should be inside ~ area.\nSet family = binomial and data = Snodgrass.\n\n\nSummarize the model with summary().\nNow create a regression table with tbl_regression().\n\nBe sure to exponentiate the coefficient estimates.\n\nInclude the intercept.\n\nUnhide the test statistic and standard error for the coefficient estimates with modify_column_unhide().\n\nBe sure to re-arrange the columns with modify_table_body(~.x |> relocate(statistic, .before = p.value)).\n\nAdd model statistics with add_glance_source_note().\n\nAdd a caption with modify_caption().\n\nRemove the “Characteristic” label with modify_header(label ~ \"\")."
  },
  {
    "objectID": "labs/15-regression-tables-lab.html#exporting-tables",
    "href": "labs/15-regression-tables-lab.html#exporting-tables",
    "title": "Lab 15: Regression Tables",
    "section": "Exporting tables",
    "text": "Exporting tables\nThe gtsummary package doesn’t provide tools for exporting tables. Fortunately, it does let you convert its summary tables to gt tables, and the gt package does provide tools for exporting tables.\nHere’s the last table we made in the previous section:\n\ngtsummary_penguins <- glm_penguins |> \n  tbl_regression(\n    exponentiate = TRUE,\n    intercept = TRUE,\n    label = list(\n      gentoo = \"Gentoo\",\n      body_mass_g = \"Body mass (g)\",\n      bill_length_mm = \"Bill length (mm)\"\n    )\n  ) |> \n  modify_column_unhide(columns = c(statistic, std.error)) |> \n  modify_table_body(~.x |> relocate(statistic, .before = p.value)) |> \n  add_glance_source_note(\n    include = c(AIC, null.deviance, df.null, deviance, df.residual)\n  ) |>\n  # modify_header(label ~ \" \") |>\n  modify_caption(\"**Table 2. Gentoo Model**\") |> \n  bold_labels()\n\nWe convert this to a gt object using as_gt() and save it to disk with the gtsave() function from the gt package. This works similar to write_csv() and ggsave(). We simply tell it what table we want to save and where we want to save it.\n\ngtsave(\n  as_gt(gtsummary_penguins),\n  file = here(\"manuscript\", \"model-summary.png\")\n)\n\nSaving as a png actually involves taking a screen shot of the HTML table and requires the webshot2 package be installed. You’ll get a note to this effect if you trying saving to this format and don’t have webshot2 installed already. You can also save to other formats like PDF (.pdf) and Word (.docx), though the latter probably won’t work right now (there appears to be a bug in the code for gtsave() that is currently being worked on).\nExercises\n\nMake sure you are in the QAAD project directory.\n\nAdd a folder called “manuscript.”\n\nNow try saving the data summary and regression summary tables you made in the last two sections into that folder. Use here(\"manuscript\", <name of file.png>).\n\nNavigate to that folder and check that it successfully saved."
  },
  {
    "objectID": "labs/15-regression-tables-lab.html#homework",
    "href": "labs/15-regression-tables-lab.html#homework",
    "title": "Lab 15: Regression Tables",
    "section": "Homework",
    "text": "Homework\nNo homework this week."
  },
  {
    "objectID": "slides/01-intro-slides.html#lecture-outline",
    "href": "slides/01-intro-slides.html#lecture-outline",
    "title": "Lecture 01: Introduction",
    "section": "📋 Lecture Outline",
    "text": "📋 Lecture Outline\n\nCourse Mechanics\n\n🧱 Structure\n\n🎯 Objectives\n\n🏋 Expectations\n\n🤝 Ethics\n\n💾 Software\n\n\nCourse Content\n\nWhy statistics?\n\nWhat is an archaeological population?\n\nA note about terminology and notation\n\nStatistical programming with \n\nLiterate programming with Quarto"
  },
  {
    "objectID": "slides/01-intro-slides.html#course-structure",
    "href": "slides/01-intro-slides.html#course-structure",
    "title": "Lecture 01: Introduction",
    "section": "🧱 Course Structure",
    "text": "🧱 Course Structure\n\nMeetings are online every Tuesday from 2:00 to 5:00 PM MST.\n\nMeeting structure:\n\nhomework review and lecture (80 minutes),\n\nbreak (10 minutes), and\n\nlab (90 minutes).\n\n\nCourse work:\n\nlab and homework exercises due every Monday before class by 9:00 PM MST, and a\n\nterm project.\n\n\nAll course materials will be made available on the course website.\n\nAll graded materials will be submitted through Canvas."
  },
  {
    "objectID": "slides/01-intro-slides.html#course-objectives",
    "href": "slides/01-intro-slides.html#course-objectives",
    "title": "Lecture 01: Introduction",
    "section": "🎯 Course Objectives",
    "text": "🎯 Course Objectives\n\nStudents will develop programming skills by learning how to:\n\nimport and export data,\nwrangle (or prepare) data for analysis,\nexplore and visualize data, and\nbuild models of data and evaluate them.\n\n\n\nAnd students will gain statistical understanding by learning how to:\n\nformulate questions and alternative hypotheses,\nidentify and explain appropriate statistical tools,\nreport the results of analysis using scientific standards, and\ncommunicate the analysis to a general audience."
  },
  {
    "objectID": "slides/01-intro-slides.html#course-expectations",
    "href": "slides/01-intro-slides.html#course-expectations",
    "title": "Lecture 01: Introduction",
    "section": "🏋 Course Expectations",
    "text": "🏋 Course Expectations\nLearning is a lot like moving to a new city. You get lost, you get frustrated, you even get embarrassed! But gradually, over time, you come to know your way around. Unfortunately, you’ll only have four months in this new city, so we need to be realistic about what we can actually achieve here.\n\n\nYou won’t become fluent in R, markdown, or statistics, but…\n\n\nyou will gain some sense of the way things tend to go with those languages."
  },
  {
    "objectID": "slides/01-intro-slides.html#course-ethics",
    "href": "slides/01-intro-slides.html#course-ethics",
    "title": "Lecture 01: Introduction",
    "section": "🤝 Course Ethics",
    "text": "🤝 Course Ethics\nAll course policies and other University requirements can be found in the course syllabus. They are very, very thorough, so rather than enumerate them all, let’s just summarize them this way:\n\n\nThere are many ways to be a bully. Don’t be any of them.\n\nAnd if you see someone getting bullied, do something about it."
  },
  {
    "objectID": "slides/01-intro-slides.html#software",
    "href": "slides/01-intro-slides.html#software",
    "title": "Lecture 01: Introduction",
    "section": "💾 Software",
    "text": "💾 Software\nThe primary statistical tools for this class are\n\n Programming Language\nRStudio\nQuarto\n\nWe will go over how to install each of these during our first lab."
  },
  {
    "objectID": "slides/01-intro-slides.html#why-statistics",
    "href": "slides/01-intro-slides.html#why-statistics",
    "title": "Lecture 01: Introduction",
    "section": "Why statistics?",
    "text": "Why statistics?\n\n\n\n\n\n\n\n\n\n We want to understand something about a population.\n\nWe can never observe the entire population, so we draw a sample.\n\n\nWe then use a model to describe the sample.\n\n\nBy comparing that model to a null model, we can infer something about the population."
  },
  {
    "objectID": "slides/01-intro-slides.html#what-population-does-archaeology-study",
    "href": "slides/01-intro-slides.html#what-population-does-archaeology-study",
    "title": "Lecture 01: Introduction",
    "section": "What population does archaeology study?",
    "text": "What population does archaeology study?"
  },
  {
    "objectID": "slides/01-intro-slides.html#a-note-on-terminology-and-notation",
    "href": "slides/01-intro-slides.html#a-note-on-terminology-and-notation",
    "title": "Lecture 01: Introduction",
    "section": "A note on terminology and notation",
    "text": "A note on terminology and notation\n\n\n\nA statistic is a property of a sample.\n“We measured the heights of 42 actors who auditioned for the role of Aragorn and took the average.”\nA parameter is a property of a population.\n“Human males have an average height of 1.74 meters (5.7 feet).”\nNote: Parameters are usually capitalized.\n\n\n\n\n\n  \n  \n  \n  \n    \n      \n      population\n      sample\n    \n  \n  \n    Size\nN\nn\n    Mean\nμ\nx̄\n    Standard Deviation\nσ\ns\n    Proportion\nP\np\n    Correlation\nρ\nr"
  },
  {
    "objectID": "slides/01-intro-slides.html#why-fa-brands-r-project-colorsteelblue",
    "href": "slides/01-intro-slides.html#why-fa-brands-r-project-colorsteelblue",
    "title": "Lecture 01: Introduction",
    "section": "Why ?",
    "text": "Why ?\nWhy R?\n\n\n\n\nIt’s free!It’s transferrable!It’s efficient!It’s extensible!It’s pretty figures!It’s reproducible!It’s a community!\n\n\nR is free software under the terms of the Free Software Foundation’s GNU General Public License.\n\n\nR will run on any system: Mac OS, Windows, or Linux.\n\n\nR lets you exploit the awesome computing powers of the modern world. It also provides an elegant and concise syntax for writing complex statistical operations.\n\n\n\n\n\nR users can write add-on packages that provide additional functionality. Here are a few of my favorites.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR offers a lot of tools to produce really, really impressive graphics. For example, here is a simple plot of a normal distribution:\n\n\n\n\n\n\n\n\n\n\n\nR facilitates reproducible research in two ways. First, it forces you to declare explicitly each step in your analysis.\n\n# take the mean\nmean(my_data)\n\n# take the standard deviation\nsd(my_data)\n\nSecond, it makes R code shareable. In the simplest case, we use R scripts, but we can also use Quarto, a much more flexible tool for writing, running, and explaining R code.\n\n\nR is also an incredibly active and growing community."
  },
  {
    "objectID": "slides/01-intro-slides.html#literate-programming-with-markdown-fa-brands-markdown",
    "href": "slides/01-intro-slides.html#literate-programming-with-markdown-fa-brands-markdown",
    "title": "Lecture 01: Introduction",
    "section": "Literate programming with markdown ",
    "text": "Literate programming with markdown \n\nMarkdown is a lightweight markup language for creating formatted text using a plain-text editor. From the Wikipedia page.\n\n\n\nINPUT\n\n\nThis is a sentence in Markdown, containing `code`, **bold text**, and *italics*.\n\n\nOUTPUT\n\nThis is a sentence in Markdown, containing code, bold text, and italics."
  },
  {
    "objectID": "slides/01-intro-slides.html#quarto-markdown-r",
    "href": "slides/01-intro-slides.html#quarto-markdown-r",
    "title": "Lecture 01: Introduction",
    "section": "Quarto = Markdown + R",
    "text": "Quarto = Markdown + R\nQuarto allows you to run code and format text in one document.\n\n\nINPUT\n\nThis is an example of Quarto with markdown __syntax__ \nand __R code__.\n\n```{r}\n#| fig-width: 4\n#| fig-asp: 1\n#| fig-align: center\n\nfit <- lm(dist ~ speed, data = cars)\n\npar(pty = \"s\")\n\nplot(cars, pch = 19, col = 'darkgray')\nabline(fit, lwd = 2)\n```\n\nOUTPUT\n\nThis is an example of Quarto with markdown syntax and R code."
  },
  {
    "objectID": "slides/02-probability-slides.html#lecture-outline",
    "href": "slides/02-probability-slides.html#lecture-outline",
    "title": "Lecture 02: Probability as a Model",
    "section": "📋 Lecture Outline",
    "text": "📋 Lecture Outline\n\nWhy statistics?\n🧪 A simple experiment\nSome terminology\n🎰 Random variables\n🎲 Probability\n📊 Probability Distribution\nProbability Distribution as a Model\nProbability Distribution as a Function\nProbability Mass Functions\nProbability Density Functions\n🚗 Cars Model\nBrief Review\nA Simple Formula\nA Note About Complexity"
  },
  {
    "objectID": "slides/02-probability-slides.html#why-statistics",
    "href": "slides/02-probability-slides.html#why-statistics",
    "title": "Lecture 02: Probability as a Model",
    "section": "Why statistics?",
    "text": "Why statistics?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe want to understand something about a population.\nWe can never observe the entire population, so we draw a sample.\nWe then use a model to describe the sample.\nBy comparing that model to a null model, we can infer something about the population.\n\n\nHere, we’re going to focus on statistical description, aka models."
  },
  {
    "objectID": "slides/02-probability-slides.html#a-simple-experiment",
    "href": "slides/02-probability-slides.html#a-simple-experiment",
    "title": "Lecture 02: Probability as a Model",
    "section": "🧪 A simple experiment",
    "text": "🧪 A simple experiment\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe take ten cars, send each down a track, have them brake at the same point, and measure the distance it takes them to stop.\n\n\nQuestion: how far do you think it will take the next car to stop?\n\n\nQuestion: what distance is the most probable?\nBut, how do we determine this?"
  },
  {
    "objectID": "slides/02-probability-slides.html#some-terminology",
    "href": "slides/02-probability-slides.html#some-terminology",
    "title": "Lecture 02: Probability as a Model",
    "section": "Some terminology",
    "text": "Some terminology"
  },
  {
    "objectID": "slides/02-probability-slides.html#random-variables",
    "href": "slides/02-probability-slides.html#random-variables",
    "title": "Lecture 02: Probability as a Model",
    "section": "🎰 Random Variables",
    "text": "🎰 Random Variables\nTwo types of random variable:\n\n\nDiscrete random variables often take only integer (non-decimal) values.\n\nExamples: number of heads in 10 tosses of a fair coin, number of victims of the Thanos snap, number of projectile points in a stratigraphic level, number of archaeological sites in a watershed.\n\nContinuous random variables take real (decimal) values.\n\nExamples: cost in property damage of a superhero fight, kilocalories per kilogram, kilocalories per hour, ratio of isotopes\n\nNote: for continuous random variables, the sample space is infinite!"
  },
  {
    "objectID": "slides/02-probability-slides.html#probability",
    "href": "slides/02-probability-slides.html#probability",
    "title": "Lecture 02: Probability as a Model",
    "section": "🎲 Probability",
    "text": "🎲 Probability\nLet \\(X\\) be the number of heads in two tosses of a fair coin. What is the probability that \\(X=1\\)?"
  },
  {
    "objectID": "slides/02-probability-slides.html#probability-distribution-as-a-model",
    "href": "slides/02-probability-slides.html#probability-distribution-as-a-model",
    "title": "Lecture 02: Probability as a Model",
    "section": "Probability Distribution as a Model",
    "text": "Probability Distribution as a Model\n\n\n\nHas two components:\n\nCentral-tendency or “first moment”\n\nPopulation mean (\\(\\mu\\)). Gives the expected value of an experiment, \\(E[X] = \\mu\\).\n\nSample mean (\\(\\bar{x}\\)). Estimate of \\(\\mu\\) based on a sample from \\(X\\) of size \\(n\\).\n\n\n\n\nDispersion or “second moment”\n\nPopulation variance (\\(\\sigma^2\\)). The expected value of the squared difference from the mean.\n\nSample variance (\\(s^2\\)). Estimate of \\(\\sigma^2\\) based on a sample from \\(X\\) of size \\(n\\).\n\nStandard deviation (\\(\\sigma\\)) or \\(s\\) is the square root of the variance."
  },
  {
    "objectID": "slides/02-probability-slides.html#probability-distribution-as-a-function",
    "href": "slides/02-probability-slides.html#probability-distribution-as-a-function",
    "title": "Lecture 02: Probability as a Model",
    "section": "Probability Distribution as a Function",
    "text": "Probability Distribution as a Function\nThese can be defined using precise mathematical functions:\n\nA probability mass function (PMF) for discrete random variables.\n\nExamples: Bernoulli, Binomial, Negative Binomial, Poisson\n\nStraightforward probability interpretation.\n\n\n\n\nA probability density function (PDF) for continuous random variables.\n\nExamples: Normal, Chi-squared, Student’s t, and F\n\nHarder to interpret probability:\n\nWhat is the probability that a car takes 10.317 m to stop? What about 10.31742 m?\n\nBetter to consider probability across an interval.\n\n\nRequires that the function integrate to one (probability is the area under the curve)."
  },
  {
    "objectID": "slides/02-probability-slides.html#bernoulli",
    "href": "slides/02-probability-slides.html#bernoulli",
    "title": "Lecture 02: Probability as a Model",
    "section": "Bernoulli",
    "text": "Bernoulli\n\n\n\nDf. distribution of a binary random variable (“Bernoulli trial”) with two possible values, 1 (success) and 0 (failure), with \\(p\\) being the probability of success. E.g., a single coin flip.\n\\[f(x,p) = p^{x}(1-p)^{1-x}\\]\nMean: \\(p\\)\nVariance: \\(p(1-p)\\)"
  },
  {
    "objectID": "slides/02-probability-slides.html#binomial",
    "href": "slides/02-probability-slides.html#binomial",
    "title": "Lecture 02: Probability as a Model",
    "section": "Binomial",
    "text": "Binomial\n\n\n\nDf. distribution of a random variable whose value is equal to the number of successes in \\(n\\) independent Bernoulli trials. E.g., number of heads in ten coin flips.\n\\[f(x,p,n) = \\binom{n}{x}p^{x}(1-p)^{1-x}\\]\nMean: \\(np\\)\nVariance: \\(np(1-p)\\)"
  },
  {
    "objectID": "slides/02-probability-slides.html#poisson",
    "href": "slides/02-probability-slides.html#poisson",
    "title": "Lecture 02: Probability as a Model",
    "section": "Poisson",
    "text": "Poisson\n\n\n\nDf. distribution of a random variable whose value is equal to the number of events occurring in a fixed interval of time or space. E.g., number of orcs passing through the Black Gates in an hour.\n\\[f(x,\\lambda) = \\frac{\\lambda^{x}e^{-\\lambda}}{x!}\\]\nMean: \\(\\lambda\\)\nVariance: \\(\\lambda\\)"
  },
  {
    "objectID": "slides/02-probability-slides.html#normal-gaussian",
    "href": "slides/02-probability-slides.html#normal-gaussian",
    "title": "Lecture 02: Probability as a Model",
    "section": "Normal (Gaussian)",
    "text": "Normal (Gaussian)\n\n\n\nDf. distribution of a continuous random variable that is symmetric from positive to negative infinity. E.g., the height of actors who auditioned for the role of Aragorn.\n\\[f(x,\\mu,\\sigma) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\;exp\\left[-\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^2\\right]\\]\nMean: \\(\\mu\\)\nVariance: \\(\\sigma^2\\)"
  },
  {
    "objectID": "slides/02-probability-slides.html#cars-model",
    "href": "slides/02-probability-slides.html#cars-model",
    "title": "Lecture 02: Probability as a Model",
    "section": "🚗 Cars Model",
    "text": "🚗 Cars Model\n\n\n\n\n\nLet’s use the Normal distribution to describe the cars data.\n\n\\(Y\\) is stopping distance for population\n\\(Y\\) is normally distributed, \\(Y \\sim N(\\mu, \\sigma)\\)\nExperiment is a random sample of size \\(n\\) from \\(Y\\) with \\(y_1, y_2, ..., y_n\\) observations.\nSample statistics (\\(\\bar{y}, s\\)) approximate population parameters (\\(\\mu, \\sigma\\)).\n\n\n\nSample statistics:\n\nMean (\\(\\bar{y}\\)) = 10.54 m\n\n\n\nSample statistics:\n\nMean (\\(\\bar{y}\\)) = 10.54 m\n\nThis is our approximate expectation\n\n\\(E[Y] = \\mu \\approx \\bar{y}\\)\n\n\n\nSample statistics:\n\nMean (\\(\\bar{y}\\)) = 10.54 m\n\nBut, there’s error, \\(\\epsilon\\), in this estimate.\n\n\\(\\epsilon_i = y_i - \\bar{y}\\)\n\n\n\nSample statistics:\n\nMean (\\(\\bar{y}\\)) = 10.54 m\n\nThe average squared error is the variance:\n\n\\(s^2 = \\frac{1}{n-1}\\sum \\epsilon_{i}^{2}\\)\n\n\n\nSample statistics:\n\nMean (\\(\\bar{y}\\)) = 10.54 m\nS.D. (\\(s\\)) = 5.353 m\n\nThis is our uncertainty, how big we think any given error will be.\n\n\nSample statistics:\n\nMean (\\(\\bar{y}\\)) = 10.54 m\nS.D. (\\(s\\)) = 5.353 m\n\nSo, here is our probability model.\n\\[Y \\sim N(\\bar{y}, s)\\] This is only an estimate of \\(N(\\mu, \\sigma)\\)!\n\n\nSample statistics:\n\nMean (\\(\\bar{y}\\)) = 10.54 m\nS.D. (\\(s\\)) = 5.353 m\n\nWith it, we can say, for example, that the probability that a random draw from this distribution falls within one standard deviation (dashed lines) of the mean (solid line) is 68.3%."
  },
  {
    "objectID": "slides/02-probability-slides.html#a-simple-formula",
    "href": "slides/02-probability-slides.html#a-simple-formula",
    "title": "Lecture 02: Probability as a Model",
    "section": "A Simple Formula",
    "text": "A Simple Formula\n\n\n\n\n\nThis gives us a simple formula\n\\[y_i = \\bar{y} + \\epsilon_i\\] where\n\n\\(y_i\\): stopping distance for car \\(i\\), data\n\\(\\bar{y} \\approx E[Y]\\): expectation, predictable\n\\(\\epsilon_i\\): error, unpredictable\n\n\n\nThis gives us a simple formula\n\\[y_i = \\bar{y} + \\epsilon_i\\]\nIf we subtract the mean, we have a model of the errors centered on zero:\n\\[\\epsilon_i = 0 + (y_i - \\bar{y})\\]\n\n\nThis gives us a simple formula\n\\[y_i = \\bar{y} + \\epsilon_i\\]\nIf we subtract the mean, we have a model of the errors centered on zero:\n\\[\\epsilon_i = 0 + (y_i - \\bar{y})\\]\nThis means we can construct a probability model of the errors centered on zero."
  },
  {
    "objectID": "slides/02-probability-slides.html#probability-model-of-errors",
    "href": "slides/02-probability-slides.html#probability-model-of-errors",
    "title": "Lecture 02: Probability as a Model",
    "section": "Probability Model of Errors",
    "text": "Probability Model of Errors\n\n\n\n\n\n\nNote that the mean changes, but the variance stays the same."
  },
  {
    "objectID": "slides/02-probability-slides.html#summary",
    "href": "slides/02-probability-slides.html#summary",
    "title": "Lecture 02: Probability as a Model",
    "section": "Summary",
    "text": "Summary\nNow our simple formula is this:\n\\[y_i = \\bar{y} + \\epsilon_i\\] \\[\\epsilon \\sim N(0, s) \\]\n\nAgain, \\(\\bar{y} \\approx E[Y] = \\mu\\).\nFor any future outcome:\n\nThe expected value is deterministic\nThe error is stochastic\n\n\nMust assume that the errors are iid!\n\nindependent = they do not affect each other\nidentically distributed = they are from the same probability distribution\n\nThe distribution is now a model of the errors!"
  },
  {
    "objectID": "slides/03-inference-slides.html#lecture-outline",
    "href": "slides/03-inference-slides.html#lecture-outline",
    "title": "Lecture 03: Statistical Inference",
    "section": "📋 Lecture Outline",
    "text": "📋 Lecture Outline\n\nWhy statistics?\nStatistical Inference\nSimple Example\nHypotheses\nTests\nRejecting the null hypothesis\nStudent’s t-test\nANOVA"
  },
  {
    "objectID": "slides/03-inference-slides.html#why-statistics",
    "href": "slides/03-inference-slides.html#why-statistics",
    "title": "Lecture 03: Statistical Inference",
    "section": "Why statistics?",
    "text": "Why statistics?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe want to understand something about a population.\nWe can never observe the entire population, so we draw a sample.\nWe then use a model to describe the sample.\nBy comparing that model to a null model, we can infer something about the population.\n\n\nHere, we’re going to focus on statistical inference."
  },
  {
    "objectID": "slides/03-inference-slides.html#simple-example",
    "href": "slides/03-inference-slides.html#simple-example",
    "title": "Lecture 03: Statistical Inference",
    "section": "Simple Example",
    "text": "Simple Example\n\n\n\n\n\n\n\n\n\nTwo samples of length (mm, n=300).\n\nQuestion: Are these samples of the same type? The same population?"
  },
  {
    "objectID": "slides/03-inference-slides.html#sample-distribution",
    "href": "slides/03-inference-slides.html#sample-distribution",
    "title": "Lecture 03: Statistical Inference",
    "section": "Sample Distribution",
    "text": "Sample Distribution\nDo these really represent different types?\n\n\n\n\n\n\n\n\nTwo models:\n\nsame type (same population)\n\ndifferent types (different populations)\n\nNote that:\n\nthese models are mutually exclusive and\n\nthe second model is more complex."
  },
  {
    "objectID": "slides/03-inference-slides.html#hypotheses",
    "href": "slides/03-inference-slides.html#hypotheses",
    "title": "Lecture 03: Statistical Inference",
    "section": "Hypotheses",
    "text": "Hypotheses\nThe two models represent our hypotheses."
  },
  {
    "objectID": "slides/03-inference-slides.html#testing-method",
    "href": "slides/03-inference-slides.html#testing-method",
    "title": "Lecture 03: Statistical Inference",
    "section": "Testing Method",
    "text": "Testing Method\n\n\n\n\n\n\nProcedure:\n\nTake sample(s).\nCalculate test statistic.\n\nCompare to test probability distribution.\nGet p-value.\nCompare to critical value.\nAccept (or reject) null hypothesis."
  },
  {
    "objectID": "slides/03-inference-slides.html#average-of-differences",
    "href": "slides/03-inference-slides.html#average-of-differences",
    "title": "Lecture 03: Statistical Inference",
    "section": "Average of differences",
    "text": "Average of differences\n\n\n\n\n\n\nSuppose we take two samples from the same population, calculate the difference in their means, and repeat this 1,000 times.\nQuestion: What will the average difference be between the sample means?"
  },
  {
    "objectID": "slides/03-inference-slides.html#probability-of-differences",
    "href": "slides/03-inference-slides.html#probability-of-differences",
    "title": "Lecture 03: Statistical Inference",
    "section": "Probability of differences",
    "text": "Probability of differences\n\n\n\nIf we convert these differences into a probability distribution, we can estimate the probability of any given difference.\nThe p-value represents how likely it is that the difference we see arises by chance."
  },
  {
    "objectID": "slides/03-inference-slides.html#model-of-differences",
    "href": "slides/03-inference-slides.html#model-of-differences",
    "title": "Lecture 03: Statistical Inference",
    "section": "Model of Differences",
    "text": "Model of Differences\n\n\n\n\n\n\nIn classical statistics, we use a model of that distribution to estimate the probability of a given difference. Here we are using \\(N(0,\\) 0.69\\()\\), where the probability of getting a difference \\(\\pm\\) 1 mm (\\(\\pm2s\\)) or greater is 0.05."
  },
  {
    "objectID": "slides/03-inference-slides.html#rejecting-the-null-hypothesis",
    "href": "slides/03-inference-slides.html#rejecting-the-null-hypothesis",
    "title": "Lecture 03: Statistical Inference",
    "section": "Rejecting the Null Hypothesis",
    "text": "Rejecting the Null Hypothesis\n\n\n\nQuestion: How do we decide?\nDefine a critical limit (\\(\\alpha\\))\n\nMust be determined prior to the test!\nIf \\(p < \\alpha\\), reject. ← THIS IS THE RULE!\nGenerally, \\(\\alpha = 0.05\\)"
  },
  {
    "objectID": "slides/03-inference-slides.html#why-the-critical-limit",
    "href": "slides/03-inference-slides.html#why-the-critical-limit",
    "title": "Lecture 03: Statistical Inference",
    "section": "Why the critical limit?",
    "text": "Why the critical limit?\nBecause we might be wrong! But what kind of wrong?\n\n\n\nWith \\(\\alpha=0.05\\), we are saying, “If we run this test 100 times, only 5 of those tests should result in a Type 1 Error.”"
  },
  {
    "objectID": "slides/03-inference-slides.html#students-t-test",
    "href": "slides/03-inference-slides.html#students-t-test",
    "title": "Lecture 03: Statistical Inference",
    "section": "Student’s t-test",
    "text": "Student’s t-test\n\nProblemHypothesesDifferencet-statisticComplexityt-distribution\n\n\n\n\n\n\n\nWe have two samples of projectile points, each consisting of 300 measurements of length (mm).\nQuestion: Are these samples of the same point type? The same population?\n\n\n\n\nThe null hypothesis:\n\\(H_0: \\mu_1 = \\mu_2\\)\nThe alternate hypothesis:\n\\(H_1: \\mu_1 \\neq \\mu_2\\)\nThis is a two-sided t-test as the difference can be positive or negative.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion: Is this difference (-1.82) big enough to reject the null?\n\n\n\n\nA t-statistic standardizes the difference in means using the standard error of the sample mean.\n\\[t = \\frac{\\bar{x}_1 - \\bar{x}_2}{s_{\\bar{x}}}\\]\n\n\n\nFor our samples, \\(t =\\) -3.61. This is a model of our data!\nQuestion: How probable is this estimate?\nWe can answer this by comparing the t-statistic to the t-distribution.\n\n\nBut first, we need to evaluate how complex the t-statistic is. We do this by estimating the degrees of freedom, or the number of values that are free to vary after calculating a statistic. In this case, we have two samples with 300 observations each, hence:\n\ndf = 530\n\nCrucially, this affects the shape of the t-distribution and, thus, determines the location of the critical value we use to evaluate the null hypothesis.\n\n\n\n\n\n\n\n\n\n\n\nSummary:\n\n\\(\\alpha = 0.05\\)\n\n\\(H_{0}: \\mu_1 = \\mu_2\\)\n\n\\(p =\\) 0.0003\n\nTranslation: the null hypothesis is really, really unlikely. So, there must be some difference in the mean!"
  },
  {
    "objectID": "slides/03-inference-slides.html#anova",
    "href": "slides/03-inference-slides.html#anova",
    "title": "Lecture 03: Statistical Inference",
    "section": "ANOVA",
    "text": "ANOVA\n\n\n\n\nProblemHypothesesStrategySum of SquaresF-statisticF-distribution\n\n\n\n\n\n\n\nWe have five samples of points, each consisting of 100 measurements of length (mm).\nQuestion: Are these samples of the same point type? The same population?\nAnalysis of Variance (ANOVA) is like a t-test but for more than two samples.\n\n\n\n\nThe null hypothesis:\n\\(H_0:\\) no difference between groups\nThe alternate hypothesis:\n\\(H_1:\\) at least one group is different\n\n\n\n\nVariance Decomposition. When group membership is known, the contribution of any value \\(x_{ij}\\) to the variance can be split into two parts:\n\\[(x_{ij} - \\bar{x}) = (\\bar{x}_{j} - \\bar{x}) + (x_{ij} - \\bar{x}_{j})\\]\nwhere\n\n\\(i\\) is the \\(i\\)th observation,\n\n\\(j\\) is the \\(j\\)th group,\n\n\\(\\bar{x}\\) is the between-group mean, and\n\n\\(\\bar{x_{j}}\\) is the within-group mean (of group \\(j\\)).\n\n\n\n\n\n\n\n\n\n\n\n\n\nSum and square the differences for all \\(n\\) observation and \\(m\\) groups gives us:\n\\[SS_{T} = SS_{B} + SS_{W}\\]\nwhere\n\n\\(SS_{T}\\): Total Sum of Squares\n\\(SS_{B}\\): Between-group Sum of Squares\n\\(SS_{W}\\): Within-group Sum of Squares\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRatio of variances:\n\\[F = \\frac{\\text{between-group variance}}{\\text{within-group variance}}\\]\nwhere\n\nBetween-group variance = \\(SS_{B}/df_{B}\\) and \\(df_{B}=m-1\\).\nWithin-group variance = \\(SS_{W}/df_{W}\\) and \\(df_{W}=m(n-1)\\).\n\nQuestion: Here, \\(F=\\) 5.63. How probable is this estimate?\nWe can answer this question by comparing the F-statistic to the F-distribution.\n\n\n\n\n\n\n\n\n\n\n\nSummary:\n\n\\(\\alpha = 0.05\\)\n\\(H_{0}:\\) no difference\n\\(p=\\) 0.003\n\nTranslation: the null hypothesis is really, really unlikely, so there must be some difference between groups!"
  },
  {
    "objectID": "slides/04-ols-slides.html#lecture-outline",
    "href": "slides/04-ols-slides.html#lecture-outline",
    "title": "Lecture 04: Ordinary Least Squares",
    "section": "📋 Lecture Outline",
    "text": "📋 Lecture Outline\n\n🧪 A simple experiment\nCompeting models\nModel complexity\nBivariate statistics (covariance and correlation)\nA general formula\nSimple Linear Regression\nOrdinary Least Squares (OLS)\nMultiple Linear Regression\n🚗 Cars Model, again\nRegression assumptions"
  },
  {
    "objectID": "slides/04-ols-slides.html#a-simple-experiment",
    "href": "slides/04-ols-slides.html#a-simple-experiment",
    "title": "Lecture 04: Ordinary Least Squares",
    "section": "🧪 A simple experiment",
    "text": "🧪 A simple experiment\n\n\n\n\n\n\n\n\n We take ten cars, send each down a track, have them brake at the same point, and measure the distance it takes them to stop.\nQuestion: how far do you think it will take the next car to stop?"
  },
  {
    "objectID": "slides/04-ols-slides.html#competing-models",
    "href": "slides/04-ols-slides.html#competing-models",
    "title": "Lecture 04: Ordinary Least Squares",
    "section": "Competing Models",
    "text": "Competing Models\n\n\n\n\n\n\n\nE[Y]: mean distance\n\n\n\n\n\n\nE[Y]: some function of speed"
  },
  {
    "objectID": "slides/04-ols-slides.html#model-complexity",
    "href": "slides/04-ols-slides.html#model-complexity",
    "title": "Lecture 04: Ordinary Least Squares",
    "section": "Model Complexity",
    "text": "Model Complexity\n\n\n\n\n\n\n\n\n\n⚖️ The error is smaller for the more complex model. This is a good thing, but what did it cost us? Need to weigh this against the increased complexity!"
  },
  {
    "objectID": "slides/04-ols-slides.html#bivariate-statistics",
    "href": "slides/04-ols-slides.html#bivariate-statistics",
    "title": "Lecture 04: Ordinary Least Squares",
    "section": "Bivariate statistics",
    "text": "Bivariate statistics\n\n\n\n\nExplore the relationship between two variables:"
  },
  {
    "objectID": "slides/04-ols-slides.html#covariance",
    "href": "slides/04-ols-slides.html#covariance",
    "title": "Lecture 04: Ordinary Least Squares",
    "section": "Covariance",
    "text": "Covariance\n\n\nThe extent to which two variables vary together.\n\\[cov(x, y) = \\frac{1}{n-1} \\sum_{i=1}^{n}  (x_i - \\bar{x})(y_i - \\bar{y})\\]\n\nSign reflects positive or negative trend, but magnitude depends on units (e.g., \\(cm\\) vs \\(km\\)).\n\nVariance is the covariance of a variable with itself."
  },
  {
    "objectID": "slides/04-ols-slides.html#correlation",
    "href": "slides/04-ols-slides.html#correlation",
    "title": "Lecture 04: Ordinary Least Squares",
    "section": "Correlation",
    "text": "Correlation\n\n\nPearson’s Correlation Coefficient\n\\[r = \\frac{cov(x,y)}{s_{x}s_y}\\]\n\nScales covariance (from -1 to 1) using standard deviations, \\(s\\), thus making magnitude independent of units.\nSignificance can be tested by converting to a t-statistic and comparing to a t-distribution with \\(df=n-2\\)."
  },
  {
    "objectID": "slides/04-ols-slides.html#non-linear-correlation",
    "href": "slides/04-ols-slides.html#non-linear-correlation",
    "title": "Lecture 04: Ordinary Least Squares",
    "section": "Non-linear correlation",
    "text": "Non-linear correlation\n\n\nSpearman’s Rank Correlation Coefficient\n\\[\\rho = \\frac{cov\\left(R(x),\\; R(y) \\right)}{s_{R(x)}s_{R(y)}}\\]\n\nPearson’s correlation but with ranks (R).\n\nThis makes it a robust estimate, less sensitive to outliers."
  },
  {
    "objectID": "slides/04-ols-slides.html#correlation-between-categories",
    "href": "slides/04-ols-slides.html#correlation-between-categories",
    "title": "Lecture 04: Ordinary Least Squares",
    "section": "Correlation between categories",
    "text": "Correlation between categories\n\n\nFor counts or frequencies\n\\[\\chi^2 = \\sum \\frac{(O_{ij}-E_{ij})^2}{E_{ij}}\\]\n\nAnalysis of contingency table\n\\(O_{ij}\\) is observed count in row \\(i\\), column \\(j\\)\n\\(E_{ij}\\) is expected count in row \\(i\\), column \\(j\\)\nSignificance can be tested by comparing to a \\(\\chi^2\\)-distribution with \\(df=k-1\\) (\\(k\\) being the number of categories)."
  },
  {
    "objectID": "slides/04-ols-slides.html#correlation-is-not-causation",
    "href": "slides/04-ols-slides.html#correlation-is-not-causation",
    "title": "Lecture 04: Ordinary Least Squares",
    "section": "Correlation is not causation!",
    "text": "Correlation is not causation!\n\n\n\n\n\n\nAdapted from https://www.tylervigen.com/spurious-correlations"
  },
  {
    "objectID": "slides/04-ols-slides.html#a-general-formula",
    "href": "slides/04-ols-slides.html#a-general-formula",
    "title": "Lecture 04: Ordinary Least Squares",
    "section": "A general formula",
    "text": "A general formula\n\n\n\n\n\n\\[\nY = E[Y] + \\epsilon \\\\[6pt]\nE[Y] = \\beta X\n\\]\n\n\n\n\\[y_i = \\hat\\beta X + \\epsilon_i\\]\n\n\n\n\\[y_i = \\hat\\beta_0 + \\hat\\beta_1 x_i + \\ldots +  \\hat\\beta_n x_i + \\epsilon_i\\]\n\n\n\n\n\\(y_i\\) is the dependent variable, a sample from the population \\(Y\\)\n\n\\(X\\) is a set of independent variables \\(x_i, \\ldots, x_n\\), sometimes called the design matrix\n\\(\\hat\\beta\\) is a set of coefficients of relationship that estimate the true relationship \\(\\beta\\)\n\\(\\epsilon_i\\) is the residuals or errors"
  },
  {
    "objectID": "slides/04-ols-slides.html#simple-linear-regression",
    "href": "slides/04-ols-slides.html#simple-linear-regression",
    "title": "Lecture 04: Ordinary Least Squares",
    "section": "Simple Linear Regression",
    "text": "Simple Linear Regression\n\n\n\n\n\n\n\n\n\n‘Simple’ means one explanatory variable (speed)\n\\[y_i = \\hat\\beta_0 + \\hat\\beta_1 speed_i + \\epsilon_i\\]\n\n\\(\\hat\\beta_0\\) = -2.0107\n\\(\\hat\\beta_1\\) = 1.9362\n\nQuestion: How did we get these values?"
  },
  {
    "objectID": "slides/04-ols-slides.html#ordinary-least-squares",
    "href": "slides/04-ols-slides.html#ordinary-least-squares",
    "title": "Lecture 04: Ordinary Least Squares",
    "section": "Ordinary Least Squares",
    "text": "Ordinary Least Squares\n\n\n\n\nA method for estimating the coefficients, \\(\\beta\\), in a linear regression model by minimizing the Residual Sum of Squares, \\(SS_{R}\\).\n\\[SS_{R} = \\sum_{i=1}^{n} (y_{i}-\\hat{y}_i)^2\\]\nwhere \\(\\hat{y} \\approx E[Y]\\). To minimize this, we take its derivative with respect to \\(\\beta\\) and set it equal to zero.\n\\[\\frac{d\\, SS_{R}}{d\\, \\beta} = 0\\]\n\n\nSimple estimators for coefficients:\n\nSlope Ratio of covariance to variance \\[\\beta_{1} = \\frac{cov(x, y)}{var(x)}\\]\n\nIntercept Conditional difference in means \\[\\beta_{0} = \\bar{y} - \\beta_1 \\bar{x}\\]\nIf \\(\\beta_{1} = 0\\), then \\(\\beta_{0} = \\bar{y}\\)."
  },
  {
    "objectID": "slides/04-ols-slides.html#cars-model-again",
    "href": "slides/04-ols-slides.html#cars-model-again",
    "title": "Lecture 04: Ordinary Least Squares",
    "section": "🚗 Cars Model, again",
    "text": "🚗 Cars Model, again\n\n\n\n\n\n\n\n\n\n\n\n\nSlope\n\\[\\hat\\beta_{1} = \\frac{cov(x,y)}{var(x)} = \\frac{10.426}{5.3847} = 1.9362\\]\n\nIntercept\n\\[\n\\begin{align}\n\\hat\\beta_{0} &= \\bar{y} - \\hat\\beta_{1}\\bar{x} \\\\[6pt]\n              &= 10.54 - 1.9362 * 6.4821 \\\\[6pt]\n              &= -2.0107\n\\end{align}\n\\]"
  },
  {
    "objectID": "slides/04-ols-slides.html#multiple-linear-regression",
    "href": "slides/04-ols-slides.html#multiple-linear-regression",
    "title": "Lecture 04: Ordinary Least Squares",
    "section": "Multiple Linear Regression",
    "text": "Multiple Linear Regression\nOLS can be extended to models containing multiple explanatory variables using matrix algebra. Then the coefficient estimator is:\n\\[\\hat\\beta = (X^{T}X)^{-1}X^{T}y\\]\n\nWhere, if you squint a little\n\n\\(X^{T}X\\) ⇨ variance.\n\\(X^{T}y\\) ⇨ covariance."
  },
  {
    "objectID": "slides/04-ols-slides.html#regression-assumptions",
    "href": "slides/04-ols-slides.html#regression-assumptions",
    "title": "Lecture 04: Ordinary Least Squares",
    "section": "Regression assumptions",
    "text": "Regression assumptions\n\nWeak Exogeneity: the predictor variables have fixed values and are known.\n\nLinearity: the relationship between the predictor variables and the response variable is linear.\n\nConstant Variance: the variance of the errors does not depend on the values of the predictor variables. Also known as homoskedasticity.\n\nIndependence of errors: the errors are uncorrelated with each other.\n\nNo perfect collinearity: the predictors are not linearly correlated with each other."
  },
  {
    "objectID": "slides/05-distributions-slides.html#lecture-outline",
    "href": "slides/05-distributions-slides.html#lecture-outline",
    "title": "Lecture 05: Visualizing Distributions",
    "section": "📋 Lecture Outline",
    "text": "📋 Lecture Outline\n\nBar Chart\nHistogram\nProbability Density\nCumulative Density\nBoxplot"
  },
  {
    "objectID": "slides/05-distributions-slides.html#whats-it-for",
    "href": "slides/05-distributions-slides.html#whats-it-for",
    "title": "Lecture 05: Visualizing Distributions",
    "section": "What’s it for?",
    "text": "What’s it for?\nVisualize the amount of some variable across categories, represented using length or height of bars."
  },
  {
    "objectID": "slides/05-distributions-slides.html#a-rule-of-thumb",
    "href": "slides/05-distributions-slides.html#a-rule-of-thumb",
    "title": "Lecture 05: Visualizing Distributions",
    "section": "A rule of thumb 👍",
    "text": "A rule of thumb 👍\nOften easier to read when oriented horizontally."
  },
  {
    "objectID": "slides/05-distributions-slides.html#grouped-data",
    "href": "slides/05-distributions-slides.html#grouped-data",
    "title": "Lecture 05: Visualizing Distributions",
    "section": "Grouped data",
    "text": "Grouped data\nGrouped bar chart can represent higher dimensional data.\n\n\n\n\n\nAlthough this graph is not terribly informative…"
  },
  {
    "objectID": "slides/05-distributions-slides.html#whats-it-for-1",
    "href": "slides/05-distributions-slides.html#whats-it-for-1",
    "title": "Lecture 05: Visualizing Distributions",
    "section": "What’s it for?",
    "text": "What’s it for?\nVisualize the approximate distribution of a continuous random variable."
  },
  {
    "objectID": "slides/05-distributions-slides.html#bin-width",
    "href": "slides/05-distributions-slides.html#bin-width",
    "title": "Lecture 05: Visualizing Distributions",
    "section": "Bin Width",
    "text": "Bin Width\nObtained by counting the number of observations that fall into each interval or “bin.”"
  },
  {
    "objectID": "slides/05-distributions-slides.html#lookout",
    "href": "slides/05-distributions-slides.html#lookout",
    "title": "Lecture 05: Visualizing Distributions",
    "section": "⚠️ Lookout!",
    "text": "⚠️ Lookout!\nThe shape of the distribution depends on the bin width."
  },
  {
    "objectID": "slides/05-distributions-slides.html#a-rule-of-thumb-1",
    "href": "slides/05-distributions-slides.html#a-rule-of-thumb-1",
    "title": "Lecture 05: Visualizing Distributions",
    "section": "A rule of thumb 👍",
    "text": "A rule of thumb 👍\n\n\nGenerally, a bad idea to use stacked or dodged groupings in a single histogram.\n\n\n\n\n\n\n\nBetter to use facets."
  },
  {
    "objectID": "slides/05-distributions-slides.html#whats-it-for-2",
    "href": "slides/05-distributions-slides.html#whats-it-for-2",
    "title": "Lecture 05: Visualizing Distributions",
    "section": "What’s it for?",
    "text": "What’s it for?\nVisualize the approximate distribution of a continuous random variable."
  },
  {
    "objectID": "slides/05-distributions-slides.html#kernel-density-estimation-kde",
    "href": "slides/05-distributions-slides.html#kernel-density-estimation-kde",
    "title": "Lecture 05: Visualizing Distributions",
    "section": "Kernel Density Estimation (KDE)",
    "text": "Kernel Density Estimation (KDE)\n\n\nProcedure:\n\nDefine a kernel, often a normal distribution with mean equal to the observation.\nDefine bandwidth for scaling the kernel.\nSum the kernels.\n\n\n\n\n\n\n\n\nThe kernels in this figure are not to scale."
  },
  {
    "objectID": "slides/05-distributions-slides.html#grouped-data-1",
    "href": "slides/05-distributions-slides.html#grouped-data-1",
    "title": "Lecture 05: Visualizing Distributions",
    "section": "Grouped data",
    "text": "Grouped data\nThere’s not a simple answer for how to plot multiple KDE’s, but facets are your friend."
  },
  {
    "objectID": "slides/05-distributions-slides.html#whats-it-for-3",
    "href": "slides/05-distributions-slides.html#whats-it-for-3",
    "title": "Lecture 05: Visualizing Distributions",
    "section": "What’s it for?",
    "text": "What’s it for?\nVisualize the approximate distribution of a continuous random variable without having to specify a bandwidth."
  },
  {
    "objectID": "slides/05-distributions-slides.html#procedure",
    "href": "slides/05-distributions-slides.html#procedure",
    "title": "Lecture 05: Visualizing Distributions",
    "section": "Procedure",
    "text": "Procedure\n\n\nConsider this sample:\n(0.3, 2.0, 3.4, 1.2, 2.2, 1.9).\nTo calculate its eCDF, we divide the number of observations that are less than or equal to each unique value by the total sample size.\n0.0 -> 0/6 = 0.00\n0.3 -> 1/6 = 0.17\n1.2 -> 2/6 = 0.33 \n1.9 -> 3/6 = 0.50 \n2.0 -> 4/6 = 0.67 \n2.2 -> 5/6 = 0.83\n3.4 -> 6/6 = 1.00"
  },
  {
    "objectID": "slides/05-distributions-slides.html#lookout-1",
    "href": "slides/05-distributions-slides.html#lookout-1",
    "title": "Lecture 05: Visualizing Distributions",
    "section": "⚠️ Lookout!",
    "text": "⚠️ Lookout!\nThese are a little bit harder to interpret. Gives the probability of being less than or equal to x. E.g., the probability of being 28 years old or younger is 0.5."
  },
  {
    "objectID": "slides/05-distributions-slides.html#whats-it-for-4",
    "href": "slides/05-distributions-slides.html#whats-it-for-4",
    "title": "Lecture 05: Visualizing Distributions",
    "section": "What’s it for?",
    "text": "What’s it for?\nVisualize the approximate distribution of a continuous random variable using its quartiles.\n\n\n\n\n\nUseful for plotting distributions across multiple groups."
  },
  {
    "objectID": "slides/05-distributions-slides.html#quartiles",
    "href": "slides/05-distributions-slides.html#quartiles",
    "title": "Lecture 05: Visualizing Distributions",
    "section": "Quartiles",
    "text": "Quartiles\nWhen the data are ordered from smallest to largest, the quartiles divide them into four sets of more-or-less equal size. The second quartile is the median!"
  },
  {
    "objectID": "slides/05-distributions-slides.html#a-rule-of-thumb-2",
    "href": "slides/05-distributions-slides.html#a-rule-of-thumb-2",
    "title": "Lecture 05: Visualizing Distributions",
    "section": "A rule of thumb 👍",
    "text": "A rule of thumb 👍\nSometimes easier to read when oriented horizontally."
  },
  {
    "objectID": "slides/07-evaluation-slides.html#lecture-outline",
    "href": "slides/07-evaluation-slides.html#lecture-outline",
    "title": "Lecture 07: Evaluating Linear Models",
    "section": "📋 Lecture Outline",
    "text": "📋 Lecture Outline\n\n🧪 A simple experiment\nCompeting models\nModel interpretation\n\nFormula\nPrediction\nStandard error vs prediction interval\n\nModel evaluation\n\nModel complexity\nANOVA for models\nR-Squared\nt-test for coefficients\nDiagnostic plots"
  },
  {
    "objectID": "slides/07-evaluation-slides.html#a-simple-experiment",
    "href": "slides/07-evaluation-slides.html#a-simple-experiment",
    "title": "Lecture 07: Evaluating Linear Models",
    "section": "🧪 A simple experiment",
    "text": "🧪 A simple experiment\n\n\n\n\n\n\n\n\n We take ten cars, send each down a track, have them brake at the same point, and measure the distance it takes them to stop.\nQuestion: how far do you think it will take the next car to stop?"
  },
  {
    "objectID": "slides/07-evaluation-slides.html#competing-models",
    "href": "slides/07-evaluation-slides.html#competing-models",
    "title": "Lecture 07: Evaluating Linear Models",
    "section": "Competing Models",
    "text": "Competing Models\n\n\n\n\n\n\n\n\nE[Y]: mean distance\n\n\n\n\n\n\n\n\nE[Y]: some function of speed]"
  },
  {
    "objectID": "slides/07-evaluation-slides.html#model-formula",
    "href": "slides/07-evaluation-slides.html#model-formula",
    "title": "Lecture 07: Evaluating Linear Models",
    "section": "Model Formula",
    "text": "Model Formula\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\[\n\\begin{align}\ny_i &= E[Y] + \\epsilon_i \\\\\n\\epsilon &\\sim N(0,\\sigma) \\\\\n\\sigma &= 2.91\n\\end{align}\n\\]\n\n\n\\[\n\\begin{align}\nE[Y] &\\approx \\hat\\beta X \\\\\n\\hat\\beta_0 &= -2.01 \\\\\n\\hat\\beta_1 &= 1.94\n\\end{align}\n\\]\n\n\n\\(\\hat\\beta_{0}\\) is the value of \\(Y\\) where \\(X=0\\), here the total stopping distance when the car isn’t moving, which should be zero!!!\n\\(\\hat\\beta_{1}\\) is change in \\(Y\\) relative to change in \\(X\\), here the additional distance the car will travel for each unit increase in speed."
  },
  {
    "objectID": "slides/07-evaluation-slides.html#prediction",
    "href": "slides/07-evaluation-slides.html#prediction",
    "title": "Lecture 07: Evaluating Linear Models",
    "section": "Prediction",
    "text": "Prediction\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\[\n\\begin{align}\ny_i &= E[Y] + \\epsilon_i \\\\\n\\epsilon &\\sim N(0,\\sigma) \\\\\n\\sigma &= 2.91\n\\end{align}\n\\]\n\n\n\\[\n\\begin{align}\nE[Y] &\\approx \\hat\\beta X \\\\\n\\hat\\beta_0 &= -2.01 \\\\\n\\hat\\beta_1 &= 1.94\n\\end{align}\n\\]\n\n\nQuestion: if a car is going 8 m/s when it applies the brakes, how long will it take it to stop?\n\n\\[\nE[Y] = -2.01 + 1.94 \\cdot 8 = 13.48\n\\]"
  },
  {
    "objectID": "slides/07-evaluation-slides.html#prediction-interval",
    "href": "slides/07-evaluation-slides.html#prediction-interval",
    "title": "Lecture 07: Evaluating Linear Models",
    "section": "Prediction Interval",
    "text": "Prediction Interval\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\[\n\\begin{align}\ny_i &= E[Y] + \\epsilon_i \\\\\n\\epsilon &\\sim N(0,\\sigma) \\\\\n\\sigma &= 2.91\n\\end{align}\n\\]\n\n\n\\[\n\\begin{align}\nE[Y] &\\approx \\hat\\beta X \\\\\n\\hat\\beta_0 &= -2.01 \\\\\n\\hat\\beta_1 &= 1.94\n\\end{align}\n\\]\n\n\nQuestion: But, what about \\(\\epsilon\\)?!!!\nWe can visualize this with a prediction interval, or the range within which a future outcome is expected to fall with a certain probability for some value of \\(X\\). Can generalize this outside the range of the model, but with increasing uncertainty."
  },
  {
    "objectID": "slides/07-evaluation-slides.html#confidence-interval",
    "href": "slides/07-evaluation-slides.html#confidence-interval",
    "title": "Lecture 07: Evaluating Linear Models",
    "section": "Confidence Interval",
    "text": "Confidence Interval\n\n\n\n\n\n\n\n\n\n\n\\[\n\\begin{align}\ny_i &= E[Y] + \\epsilon_i \\\\\n\\epsilon &\\sim N(0,\\sigma) \\\\\n\\sigma &= 2.91\n\\end{align}\n\\]\n\n\n\\[\n\\begin{align}\nE[Y] &\\approx \\hat\\beta X \\\\\n\\hat\\beta_0 &= -2.01 \\\\\n\\hat\\beta_1 &= 1.94\n\\end{align}\n\\]\n\n\nQuestion: But, what if we’re wrong about \\(\\beta\\)?!!!\nWe can visualize this with a confidence interval, or the range within which the average outcome is expected to fall with a certain probability for some value of \\(X\\)."
  },
  {
    "objectID": "slides/07-evaluation-slides.html#model-complexity",
    "href": "slides/07-evaluation-slides.html#model-complexity",
    "title": "Lecture 07: Evaluating Linear Models",
    "section": "Model Complexity",
    "text": "Model Complexity\n\n\n\n\n\n\n\n\n\n⚖️ The error is smaller for the more complex model. This is a good thing, but what did it cost us? Need to weigh this against the increased complexity!"
  },
  {
    "objectID": "slides/07-evaluation-slides.html#anova-for-model",
    "href": "slides/07-evaluation-slides.html#anova-for-model",
    "title": "Lecture 07: Evaluating Linear Models",
    "section": "ANOVA for model",
    "text": "ANOVA for model\n\nProblemHypothesesStrategySum of SquaresF-statisticF-distribution\n\n\n\n\n\n\n\n\n\n\nWe have two models of our data, one simple, the other complex.\nQuestion: Does the complex (bivariate) model explain more variance than the simple (intercept) model? Does the difference arise by chance?\n\n\n\n\nThe null hypothesis:\n\n\\(H_0:\\) no difference in variance explained.\n\nThe alternate hypothesis:\n\n\\(H_1:\\) difference in variance explained.\n\n\n\nVariance Decomposition. Total variance in the dependent variable can be decomposed into the variance captured by the more complex model and the remaining (or residual) variance:\n\n\nDecompose the differences:\n\\((y_{i} - \\bar{y}) = (\\hat{y}_{i} - \\bar{y}) + (y_{i} - \\hat{y}_{i})\\)\nwhere\n\n\\((y_{i} - \\bar{y})\\) is the total error,\n\n\\((\\hat{y}_{i} - \\bar{y})\\) is the model error, and\n\\((y_{i} - \\hat{y}_{i})\\) is the residual error.\n\n\nSum and square the differences:\n\\(SS_{T} = SS_{M} + SS_{R}\\)\nwhere\n\n\\(SS_{T}\\): Total Sum of Squares\n\\(SS_{M}\\): Model Sum of Squares\n\\(SS_{R}\\): Residual Sum of Squares\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\[F = \\frac{\\text{model variance}}{\\text{residual variance}}\\]\nwhere\n\nModel variance = \\(SS_{M}/k\\) for \\(k\\) model parameters\nResidual variance = \\(SS_{R}/(n-k-1)\\) for \\(k\\) and \\(n\\) observations.\n\nQuestion: How probable is it that \\(F=\\) 19.07?\nWe can answer this by comparing the F-statistic to the F-distribution.\n\n\n\n\n\nSummary:\n\n\\(\\alpha = 0.05\\)\n\\(H_0:\\) no difference\n\\(p=\\) 0.0024\n\nTranslation: the null hypothesis is really, really unlikely. So, there must be some difference between models!"
  },
  {
    "objectID": "slides/07-evaluation-slides.html#r-squared",
    "href": "slides/07-evaluation-slides.html#r-squared",
    "title": "Lecture 07: Evaluating Linear Models",
    "section": "R-Squared",
    "text": "R-Squared\n\n\nCoefficient of Determination\n\\[R^2 = 1- \\frac{SS_{R}}{SS_{T}}\\]\n\nProportion of variance in \\(y\\) explained by the model \\(M\\).\nScale is 0 to 1. A value closer to 1 means that \\(M\\) explains more variance.\n\\(M\\) is evaluated relative to simple, intercept-only model."
  },
  {
    "objectID": "slides/07-evaluation-slides.html#t-test-for-coefficients",
    "href": "slides/07-evaluation-slides.html#t-test-for-coefficients",
    "title": "Lecture 07: Evaluating Linear Models",
    "section": "t-test for coefficients",
    "text": "t-test for coefficients\n\nProblemHypothesesStandard Errorst-statistict-test\n\n\n\n\n\n\n\n\n\n\nQuestion: Are the coefficient estimates significantly different than zero?\nTo answer this question, we need some measure of uncertainty for these estimates.\n\n\n\n\nThe null hypothesis:\n\n\\(H_0:\\) coefficient estimate is not different than zero.\n\nThe alternate hypothesis:\n\n\\(H_1:\\) coefficient estimate is different than zero.\n\n\n\n\n\n\n\n\n\n\n\nFor simple linear regression,\n\nthe standard error of the slope, \\(se(\\hat\\beta_1)\\), is the ratio of the average squared error of the model to the total squared error of the predictor.\nthe standard error of the intercept, \\(se(\\hat\\beta_0)\\), is \\(se(\\hat\\beta_1)\\) weighted by the average squared values of the predictor.\n\n\n\n\n\n\n\n\nThe t-statistic is the coefficient estimate divided by its standard error\n\\[t = \\frac{\\hat\\beta}{se(\\hat\\beta)}\\]\n\nThis can be compared to a t-distribution with \\(n-k-1\\) degrees of freedom (\\(n\\) observations and \\(k\\) independent predictors).\n\n\n\n\n\n\n\n\nSummary:\n\n\\(\\alpha = 0.05\\)\n\\(H_0\\): \\(\\beta=0\\)\np-values:\n\nintercept = 0.5263\nslope < 0.0024"
  },
  {
    "objectID": "slides/07-evaluation-slides.html#diagnostic-plots",
    "href": "slides/07-evaluation-slides.html#diagnostic-plots",
    "title": "Lecture 07: Evaluating Linear Models",
    "section": "Diagnostic Plots",
    "text": "Diagnostic Plots\n\n\n\n\n\nModel AssumptionsResidual vs FittedNormal Q-QResidual DensityCook’s Distance\n\n\n\nWeak Exogeneity: the predictor variables have fixed values and are known.\nLinearity: the relationship between the predictor variables and the response variable is linear.\nConstant Variance: the variance of the errors does not depend on the values of the predictor variables. Also known as homoscedasticity.\nIndependence of errors: the errors are uncorrelated with each other.\nNo perfect collinearity: the predictors are not linearly correlated with each other."
  },
  {
    "objectID": "slides/08-multiple-lm-slides.html#lecture-outline",
    "href": "slides/08-multiple-lm-slides.html#lecture-outline",
    "title": "Lecture 08: Multiple Linear Models",
    "section": "📋 Lecture Outline",
    "text": "📋 Lecture Outline\n\nSimple Linear Regression\nMultiple Linear Regression\nCollinearity\nMulticollinearity\nANOVA\nPartial Dependence"
  },
  {
    "objectID": "slides/08-multiple-lm-slides.html#simple-linear-regression",
    "href": "slides/08-multiple-lm-slides.html#simple-linear-regression",
    "title": "Lecture 08: Multiple Linear Models",
    "section": "Simple Linear Regression",
    "text": "Simple Linear Regression\n\n\n\n\n\n\n\n\n\n\nFrom James et al (2021). ISLR. 2nd Ed.\n\n\n\n\n\n  \n  \n  \n  \n    \n      sales\n      tv\n      radio\n      newspaper\n    \n  \n  \n    22.1\n230.1\n37.8\n69.2\n    10.4\n44.5\n39.3\n45.1\n    9.3\n17.2\n45.9\n69.3\n    18.5\n151.5\n41.3\n58.5\n    12.9\n180.8\n10.8\n58.4\n    7.2\n8.7\n48.9\n75.0\n    11.8\n57.5\n32.8\n23.5\n    13.2\n120.2\n19.6\n11.6\n    4.8\n8.6\n2.1\n1.0\n    10.6\n199.8\n2.6\n21.2\n    8.6\n66.1\n5.8\n24.2\n    17.4\n214.7\n24.0\n4.0\n    9.2\n23.8\n35.1\n65.9\n    9.7\n97.5\n7.6\n7.2\n    19.0\n204.1\n32.9\n46.0\n    22.4\n195.4\n47.7\n52.9\n    12.5\n67.8\n36.6\n114.0\n    24.4\n281.4\n39.6\n55.8\n    11.3\n69.2\n20.5\n18.3\n    14.6\n147.3\n23.9\n19.1\n    18.0\n218.4\n27.7\n53.4\n    12.5\n237.4\n5.1\n23.5\n    5.6\n13.2\n15.9\n49.6\n    15.5\n228.3\n16.9\n26.2\n    9.7\n62.3\n12.6\n18.3\n    12.0\n262.9\n3.5\n19.5\n    15.0\n142.9\n29.3\n12.6\n    15.9\n240.1\n16.7\n22.9\n    18.9\n248.8\n27.1\n22.9\n    10.5\n70.6\n16.0\n40.8\n    21.4\n292.9\n28.3\n43.2\n    11.9\n112.9\n17.4\n38.6\n    9.6\n97.2\n1.5\n30.0\n    17.4\n265.6\n20.0\n0.3\n    9.5\n95.7\n1.4\n7.4\n    12.8\n290.7\n4.1\n8.5\n    25.4\n266.9\n43.8\n5.0\n    14.7\n74.7\n49.4\n45.7\n    10.1\n43.1\n26.7\n35.1\n    21.5\n228.0\n37.7\n32.0\n    16.6\n202.5\n22.3\n31.6\n    17.1\n177.0\n33.4\n38.7\n    20.7\n293.6\n27.7\n1.8\n    12.9\n206.9\n8.4\n26.4\n    8.5\n25.1\n25.7\n43.3\n    14.9\n175.1\n22.5\n31.5\n    10.6\n89.7\n9.9\n35.7\n    23.2\n239.9\n41.5\n18.5\n    14.8\n227.2\n15.8\n49.9\n    9.7\n66.9\n11.7\n36.8\n    11.4\n199.8\n3.1\n34.6\n    10.7\n100.4\n9.6\n3.6\n    22.6\n216.4\n41.7\n39.6\n    21.2\n182.6\n46.2\n58.7\n    20.2\n262.7\n28.8\n15.9\n    23.7\n198.9\n49.4\n60.0\n    5.5\n7.3\n28.1\n41.4\n    13.2\n136.2\n19.2\n16.6\n    23.8\n210.8\n49.6\n37.7\n    18.4\n210.7\n29.5\n9.3\n    8.1\n53.5\n2.0\n21.4\n    24.2\n261.3\n42.7\n54.7\n    15.7\n239.3\n15.5\n27.3\n    14.0\n102.7\n29.6\n8.4\n    18.0\n131.1\n42.8\n28.9\n    9.3\n69.0\n9.3\n0.9\n    9.5\n31.5\n24.6\n2.2\n    13.4\n139.3\n14.5\n10.2\n    18.9\n237.4\n27.5\n11.0\n    22.3\n216.8\n43.9\n27.2\n    18.3\n199.1\n30.6\n38.7\n    12.4\n109.8\n14.3\n31.7\n    8.8\n26.8\n33.0\n19.3\n    11.0\n129.4\n5.7\n31.3\n    17.0\n213.4\n24.6\n13.1\n    8.7\n16.9\n43.7\n89.4\n    6.9\n27.5\n1.6\n20.7\n    14.2\n120.5\n28.5\n14.2\n    5.3\n5.4\n29.9\n9.4\n    11.0\n116.0\n7.7\n23.1\n    11.8\n76.4\n26.7\n22.3\n    12.3\n239.8\n4.1\n36.9\n    11.3\n75.3\n20.3\n32.5\n    13.6\n68.4\n44.5\n35.6\n    21.7\n213.5\n43.0\n33.8\n    15.2\n193.2\n18.4\n65.7\n    12.0\n76.3\n27.5\n16.0\n    16.0\n110.7\n40.6\n63.2\n    12.9\n88.3\n25.5\n73.4\n    16.7\n109.8\n47.8\n51.4\n    11.2\n134.3\n4.9\n9.3\n    7.3\n28.6\n1.5\n33.0\n    19.4\n217.7\n33.5\n59.0\n    22.2\n250.9\n36.5\n72.3\n    11.5\n107.4\n14.0\n10.9\n    16.9\n163.3\n31.6\n52.9\n    11.7\n197.6\n3.5\n5.9\n    15.5\n184.9\n21.0\n22.0\n    25.4\n289.7\n42.3\n51.2\n    17.2\n135.2\n41.7\n45.9\n    11.7\n222.4\n4.3\n49.8\n    23.8\n296.4\n36.3\n100.9\n    14.8\n280.2\n10.1\n21.4\n    14.7\n187.9\n17.2\n17.9\n    20.7\n238.2\n34.3\n5.3\n    19.2\n137.9\n46.4\n59.0\n    7.2\n25.0\n11.0\n29.7\n    8.7\n90.4\n0.3\n23.2\n    5.3\n13.1\n0.4\n25.6\n    19.8\n255.4\n26.9\n5.5\n    13.4\n225.8\n8.2\n56.5\n    21.8\n241.7\n38.0\n23.2\n    14.1\n175.7\n15.4\n2.4\n    15.9\n209.6\n20.6\n10.7\n    14.6\n78.2\n46.8\n34.5\n    12.6\n75.1\n35.0\n52.7\n    12.2\n139.2\n14.3\n25.6\n    9.4\n76.4\n0.8\n14.8\n    15.9\n125.7\n36.9\n79.2\n    6.6\n19.4\n16.0\n22.3\n    15.5\n141.3\n26.8\n46.2\n    7.0\n18.8\n21.7\n50.4\n    11.6\n224.0\n2.4\n15.6\n    15.2\n123.1\n34.6\n12.4\n    19.7\n229.5\n32.3\n74.2\n    10.6\n87.2\n11.8\n25.9\n    6.6\n7.8\n38.9\n50.6\n    8.8\n80.2\n0.0\n9.2\n    24.7\n220.3\n49.0\n3.2\n    9.7\n59.6\n12.0\n43.1\n    1.6\n0.7\n39.6\n8.7\n    12.7\n265.2\n2.9\n43.0\n    5.7\n8.4\n27.2\n2.1\n    19.6\n219.8\n33.5\n45.1\n    10.8\n36.9\n38.6\n65.6\n    11.6\n48.3\n47.0\n8.5\n    9.5\n25.6\n39.0\n9.3\n    20.8\n273.7\n28.9\n59.7\n    9.6\n43.0\n25.9\n20.5\n    20.7\n184.9\n43.9\n1.7\n    10.9\n73.4\n17.0\n12.9\n    19.2\n193.7\n35.4\n75.6\n    20.1\n220.5\n33.2\n37.9\n    10.4\n104.6\n5.7\n34.4\n    11.4\n96.2\n14.8\n38.9\n    10.3\n140.3\n1.9\n9.0\n    13.2\n240.1\n7.3\n8.7\n    25.4\n243.2\n49.0\n44.3\n    10.9\n38.0\n40.3\n11.9\n    10.1\n44.7\n25.8\n20.6\n    16.1\n280.7\n13.9\n37.0\n    11.6\n121.0\n8.4\n48.7\n    16.6\n197.6\n23.3\n14.2\n    19.0\n171.3\n39.7\n37.7\n    15.6\n187.8\n21.1\n9.5\n    3.2\n4.1\n11.6\n5.7\n    15.3\n93.9\n43.5\n50.5\n    10.1\n149.8\n1.3\n24.3\n    7.3\n11.7\n36.9\n45.2\n    12.9\n131.7\n18.4\n34.6\n    14.4\n172.5\n18.1\n30.7\n    13.3\n85.7\n35.8\n49.3\n    14.9\n188.4\n18.1\n25.6\n    18.0\n163.5\n36.8\n7.4\n    11.9\n117.2\n14.7\n5.4\n    11.9\n234.5\n3.4\n84.8\n    8.0\n17.9\n37.6\n21.6\n    12.2\n206.8\n5.2\n19.4\n    17.1\n215.4\n23.6\n57.6\n    15.0\n284.3\n10.6\n6.4\n    8.4\n50.0\n11.6\n18.4\n    14.5\n164.5\n20.9\n47.4\n    7.6\n19.6\n20.1\n17.0\n    11.7\n168.4\n7.1\n12.8\n    11.5\n222.4\n3.4\n13.1\n    27.0\n276.9\n48.9\n41.8\n    20.2\n248.4\n30.2\n20.3\n    11.7\n170.2\n7.8\n35.2\n    11.8\n276.7\n2.3\n23.7\n    12.6\n165.6\n10.0\n17.6\n    10.5\n156.6\n2.6\n8.3\n    12.2\n218.5\n5.4\n27.4\n    8.7\n56.2\n5.7\n29.7\n    26.2\n287.6\n43.0\n71.8\n    17.6\n253.8\n21.3\n30.0\n    22.6\n205.0\n45.1\n19.6\n    10.3\n139.5\n2.1\n26.6\n    17.3\n191.1\n28.7\n18.2\n    15.9\n286.0\n13.9\n3.7\n    6.7\n18.7\n12.1\n23.4\n    10.8\n39.5\n41.1\n5.8\n    9.9\n75.5\n10.8\n6.0\n    5.9\n17.2\n4.1\n31.6\n    19.6\n166.8\n42.0\n3.6\n    17.3\n149.7\n35.6\n6.0\n    7.6\n38.2\n3.7\n13.8\n    9.7\n94.2\n4.9\n8.1\n    12.8\n177.0\n9.3\n6.4\n    25.5\n283.6\n42.0\n66.2\n    13.4\n232.1\n8.6\n8.7\n  \n  \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFrom James et al (2021). ISLR. 2nd Ed.\n\n\n\n\n\n  \n  \n  \n  \n    \n      term\n      estimate\n      std.error\n      t.statistic\n      p.value\n    \n  \n  \n    (Intercept)\n7.033\n0.458\n15.36\n0\n    tv\n0.048\n0.003\n17.67\n0\n  \n  \n  \n\n\n\nNote the units! We interpret the slope coefficient here as saying, “For every $1000 spent on television advertising, sales increased by approximately 50 units.”\nQuestion What does the intercept mean?"
  },
  {
    "objectID": "slides/08-multiple-lm-slides.html#but-muh-data",
    "href": "slides/08-multiple-lm-slides.html#but-muh-data",
    "title": "Lecture 08: Multiple Linear Models",
    "section": "But, muh data…",
    "text": "But, muh data…\nQuestion What about the money invested in radio and newspaper advertising?\n\n\nTo answer this, we could try building three separate models:\n\nsales \\(\\sim\\) tv\nsales \\(\\sim\\) newspaper\nsales \\(\\sim\\) radio\n\nBut, this assumes that the coefficient estimates are independent of each other…\n\n\n\nSolution Make one big model!\n\nsales \\(\\sim\\) tv + newspaper + radio"
  },
  {
    "objectID": "slides/08-multiple-lm-slides.html#multiple-linear-regression",
    "href": "slides/08-multiple-lm-slides.html#multiple-linear-regression",
    "title": "Lecture 08: Multiple Linear Models",
    "section": "Multiple Linear Regression",
    "text": "Multiple Linear Regression\n\n\n\n\n\n\n\n\n\n\n\n\n\nFrom James et al (2021). ISLR. 2nd Ed.\n\n\nCoefficients are estimated in the same way, but the interpretation changes! Consider\n\\[\\beta_{radio}=0.189\\]\nRead: “For some amount of tv and newspaper advertising, spending an additional $1000 on radio advertising increases sales by approximately 189 units.”\n\n\n\n\n\n\n\n\n\n\n\n\nFrom James et al (2021). ISLR. 2nd Ed.\n\n\nThe estimate for newspaper is no longer significantly different than zero!\nQuestion Why might this be?"
  },
  {
    "objectID": "slides/08-multiple-lm-slides.html#collinearity",
    "href": "slides/08-multiple-lm-slides.html#collinearity",
    "title": "Lecture 08: Multiple Linear Models",
    "section": "Collinearity",
    "text": "Collinearity\n\n\n\n\n\n\n\n\n\n\n\n\nAnswer: newspaper and radio advertising are correlated (\\(\\rho\\) = 0.35).\nSo, newspaper advertising was never really increasing sales. It was just taking “credit” for radio’s contribution."
  },
  {
    "objectID": "slides/08-multiple-lm-slides.html#multicollinearity",
    "href": "slides/08-multiple-lm-slides.html#multicollinearity",
    "title": "Lecture 08: Multiple Linear Models",
    "section": "Multicollinearity",
    "text": "Multicollinearity\n\n\nProblem It is possible for multiple variables to correlate even if no single pair does.\n\nSolution Measure the Variance Inflation Factor for each covariate.\n\\[VIF(\\hat{B_j}) = \\frac{1}{1-R^{2}_{j}}\\]\nwhere \\(R^{2}_{j}\\) is the coefficient of determination for the model \\(x_j \\sim x_1 + x_2 + \\ldots + x_i\\).\n\nVariance Inflation inflates the standard errors, too!\n\\[se(\\hat{B_j}) = \\sqrt{Var(\\hat{B_j}) \\cdot VIF(\\hat{B_j})}\\]\n\n\n\n\n\n  \n  \n  \n  \n    \n      covariate\n      VIF\n    \n  \n  \n    tv\n1.005\n    radio\n1.145\n    newspaper\n1.145\n  \n  \n  \n\n\n\nVIF close to 1 means no multicollinearity.\nRule of Thumb VIF that exceeds 5 is too much! Or is it 10? 🤔\n\n\nStrategies for handling multicollinearity include:\n\nRemoving one or more correlated variables (simplest strategy)\nAggregating correlated variables (e.g., product or mean)\nUsing an interaction term (more later)\nUsing an ordination technique (more later)"
  },
  {
    "objectID": "slides/08-multiple-lm-slides.html#anova-for-model",
    "href": "slides/08-multiple-lm-slides.html#anova-for-model",
    "title": "Lecture 08: Multiple Linear Models",
    "section": "ANOVA for model",
    "text": "ANOVA for model\n\n\nWe have two nested models of our data:\n\n\\(S:\\) sales \\(\\sim\\) tv\n\\(C:\\) sales \\(\\sim\\) tv + newspaper + radio\n\nQuestion: Does the complex model (C) explain more variance than the simple model (S)? Does the difference arise by chance?\nHere are our hypotheses:\n\n\\(H_0:\\) no difference in variance explained.\n\\(H_1:\\) difference in variance explained.\n\n\n\n\n\n\n  \n  \n  \n  \n    \n      m\n      term\n      df.residual\n      RSS\n      df\n      sumsq\n      F\n      Pr(>F)\n    \n  \n  \n    S\nsales ~ tv\n198\n2102.5\n\n\n\n\n    C\nsales ~ tv + radio + newspaper\n196\n556.8\n2\n1546\n272\n< 2.2e-16\n  \n  \n  \n\n\n\n\n\\[F = \\frac{(RSS_{S} - RSS_{C})/(df_S - df_C)}{RSS_{C}/df_C}\\]"
  },
  {
    "objectID": "slides/08-multiple-lm-slides.html#partial-dependence-plot",
    "href": "slides/08-multiple-lm-slides.html#partial-dependence-plot",
    "title": "Lecture 08: Multiple Linear Models",
    "section": "Partial Dependence Plot",
    "text": "Partial Dependence Plot"
  },
  {
    "objectID": "slides/10-transforms-slides.html#lecture-outline",
    "href": "slides/10-transforms-slides.html#lecture-outline",
    "title": "Lecture 10: Transforming Variables",
    "section": "📋 Lecture Outline",
    "text": "📋 Lecture Outline\n\nQualitative variables\n\nDummy variables\nIntercept offset\nt-test\n\nCentering\nScaling\nInteractions\nPolynomial transformations for non-linearity\nLog transformations for non-linearity"
  },
  {
    "objectID": "slides/10-transforms-slides.html#qualitative-variables",
    "href": "slides/10-transforms-slides.html#qualitative-variables",
    "title": "Lecture 10: Transforming Variables",
    "section": "Qualitative Variables",
    "text": "Qualitative Variables\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  \n  \n  \n    \n      term\n      estimate\n      std.error\n      t.statistic\n      p.value\n    \n  \n  \n    \n      Sex Insensitive Model\n    \n    (Intercept)\n23.495\n0.809\n29.04\n0\n    education\n-0.839\n0.067\n-12.54\n0\n  \n  \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  \n  \n  \n    \n      term\n      estimate\n      std.error\n      t.statistic\n      p.value\n    \n  \n  \n    \n      Sex Insensitive Model\n    \n    (Intercept)\n23.495\n0.809\n29.036\n0\n    education\n-0.839\n0.067\n-12.540\n0\n    \n      Sex Sensitive Model\n    \n    (Intercept)\n-1.660\n0.441\n-3.761\n0\n    education\n0.674\n0.028\n23.732\n0\n    sex(male)\n16.556\n0.266\n62.311\n0"
  },
  {
    "objectID": "slides/10-transforms-slides.html#dummy-variables",
    "href": "slides/10-transforms-slides.html#dummy-variables",
    "title": "Lecture 10: Transforming Variables",
    "section": "Dummy Variables",
    "text": "Dummy Variables\n\n\n\n\nDummy variables code each observation:\n\\(x_i=\\left\\{\\begin{array}{c l} 1 & \\text{if i in category} \\\\ 0 & \\text{if i not in category} \\end{array}\\right.\\)\nwith zero being the reference class.\nFor m categories, requires m-1 dummy variables.\n\n\n\n\n\n  \n  \n  \n  \n    \n      Category\n      D1\n      D2\n      …\n      Dm-1\n    \n  \n  \n    D1\n\n1\n\n0\n\n0\n\n0\n\n    D2\n\n0\n\n1\n\n0\n\n0\n\n    ⋮\n\n⋮\n\n⋮\n\n⋮\n\n⋮\n\n    Dm-1\n\n0\n\n0\n\n0\n\n1\n\n    Dm\n\n0\n\n0\n\n0\n\n0\n\n  \n  \n  \n\n\n\n\n\n\n\n\n\n\nDummy variables code each observation:\n\\(sex_i=\\left\\{\\begin{array}{c l} 1 & \\text{if i-th person is male} \\\\ 0 & \\text{if i-th person is not male} \\end{array}\\right.\\)\nHere the reference class is female.\nThe sex variable has two categories, hence one dummy variable.\n\n\n\n\n\n  \n  \n  \n  \n    \n      sex\n      male\n    \n  \n  \n    male\n\n1\n\n    female\n\n0\n\n    ⋮\n\n⋮\n\n    male\n\n1\n\n    female\n\n0"
  },
  {
    "objectID": "slides/10-transforms-slides.html#intercept-offset",
    "href": "slides/10-transforms-slides.html#intercept-offset",
    "title": "Lecture 10: Transforming Variables",
    "section": "Intercept Offset",
    "text": "Intercept Offset\n\n\n\n\n\n\n\n\n\n\n\n\nSimple linear model\n\\(y_i = \\beta_0 + \\beta_1x_1 + \\epsilon_i\\)\n\n\nAdd dummy \\(D\\) with \\(\\gamma\\) offset\n\\(y_i = (\\beta_0 + \\gamma\\cdot D) + \\beta_1x_1 + \\epsilon_i\\)\n\n\nfor a binary class:\n\n\n\nModel for reference class\n\\(y_i = (\\beta_0 + \\gamma\\cdot 0) + \\beta_1x_1 + \\epsilon_i\\)\n\n\nModel for other class\n\\(y_i = (\\beta_0 + \\gamma\\cdot 1) + \\beta_1x_1 + \\epsilon_i\\)"
  },
  {
    "objectID": "slides/10-transforms-slides.html#t-test",
    "href": "slides/10-transforms-slides.html#t-test",
    "title": "Lecture 10: Transforming Variables",
    "section": "t-test",
    "text": "t-test\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  \n  \n  \n    \n      term\n      estimate\n      std.error\n      t.statistic\n      p.value\n    \n  \n  \n    \n      Sex Sensitive Model\n    \n    (Intercept)\n8.555\n0.191\n44.79\n0\n    sex(male)\n11.165\n0.270\n41.33\n0\n  \n  \n  \n\n\n\n\n\\(H_{0}\\): no difference in mean\nmodel: \\(income \\sim sex\\)\n\\(\\bar{y}_{F} = 8.555\\)\n\\(\\bar{y}_{M} = \\bar{y}_{F} + 11.165 = 19.72\\)"
  },
  {
    "objectID": "slides/10-transforms-slides.html#centering",
    "href": "slides/10-transforms-slides.html#centering",
    "title": "Lecture 10: Transforming Variables",
    "section": "Centering",
    "text": "Centering\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  \n  \n  \n    \n      term\n      estimate\n      std.error\n      t.statistic\n      p.value\n    \n  \n  \n    \n      Uncentered Model\n    \n    (Intercept)\n25.80\n5.917\n4.36\n0\n    Mom's IQ\n0.61\n0.059\n10.42\n0\n  \n  \n  \n\n\n\n\nModel child IQ as a function of mother’s IQ.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  \n  \n  \n    \n      term\n      estimate\n      std.error\n      t.statistic\n      p.value\n    \n  \n  \n    \n      Uncentered Model\n    \n    (Intercept)\n25.80\n5.917\n4.36\n0\n    Mom's IQ\n0.61\n0.059\n10.42\n0\n  \n  \n  \n\n\n\n\nModel child IQ as a function of mother’s IQ.\n\n\n\n\n\n\n\n\n\n\n\n\nCentering will make intercept more interpretable.\nTo center, subtract the mean:\n\\[\\text{Mom's IQ} - \\text{mean(Mom's IQ)}\\]\nThat gets us this model…\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  \n  \n  \n    \n      term\n      estimate\n      std.error\n      t.statistic\n      p.value\n    \n  \n  \n    \n      Uncentered Model\n    \n    (Intercept)\n25.80\n5.917\n4.36\n0\n    Mom's IQ\n0.61\n0.059\n10.42\n0\n    \n      Centered Model\n    \n    (Intercept)\n86.80\n0.877\n98.99\n0\n    Mom's IQ\n0.61\n0.059\n10.42\n0\n  \n  \n  \n\n\n\n\nNow we interpret the intercept as expected IQ of a child for a mother with mean IQ. (Notice change in standard error!)\n\n\n\n\n\n\nNo commitment here to IQ being meaningful. Just an example I’m using till I have time to find a better one."
  },
  {
    "objectID": "slides/10-transforms-slides.html#scaling",
    "href": "slides/10-transforms-slides.html#scaling",
    "title": "Lecture 10: Transforming Variables",
    "section": "Scaling",
    "text": "Scaling\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  \n  \n  \n    \n      term\n      estimate\n      std.error\n      t.statistic\n      p.value\n    \n  \n  \n    \n      Unscaled Model\n    \n    (Intercept)\n37.227\n1.599\n23.285\n0.000\n    Horsepower\n-0.032\n0.009\n-3.519\n0.001\n    Weight\n-3.878\n0.633\n-6.129\n0.000\n  \n  \n  \n\n\n\n\nModel fuel efficiency as a function of weight and horse power.\nQuestion Which covariate has a bigger effect?\n\n\n\n\n\n\n\n\n\n\n\n\nProblem Cannot directly compare coefficients on different scales.\nSolution Convert variable values to their z-scores:\n\\[z_i = \\frac{x_i-\\bar{x}}{\\sigma_{x}}\\]\nAll coefficients now give change in \\(y\\) for 1\\(\\sigma\\) change in \\(x\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  \n  \n  \n    \n      term\n      estimate\n      std.error\n      t.statistic\n      p.value\n    \n  \n  \n    \n      Unscaled Model\n    \n    (Intercept)\n37.227\n1.599\n23.285\n0.000\n    Horsepower\n-0.032\n0.009\n-3.519\n0.001\n    Weight\n-3.878\n0.633\n-6.129\n0.000\n    \n      Scaled Model\n    \n    (Intercept)\n20.091\n0.458\n43.822\n0.000\n    Horsepower\n-2.178\n0.619\n-3.519\n0.001\n    Weight\n-3.794\n0.619\n-6.129\n0.000"
  },
  {
    "objectID": "slides/10-transforms-slides.html#interactions",
    "href": "slides/10-transforms-slides.html#interactions",
    "title": "Lecture 10: Transforming Variables",
    "section": "Interactions",
    "text": "Interactions\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  \n  \n  \n    \n      term\n      estimate\n      std.error\n      t.statistic\n      p.value\n    \n  \n  \n    \n      Additive Model\n    \n    (Intercept)\n86.80\n0.877\n98.99\n0\n    Mom's IQ\n0.61\n0.059\n10.42\n0\n  \n  \n  \n\n\n\n\nQuestion What if the relationship between child’s IQ and mother’s IQ depends on the mother’s educational attainment?\n\n\n\n\n\n\n\n\n\n\n\n\nSimple formula becomes:\n\\[y_i = (\\beta_0 + \\gamma D) + (\\beta_1x_1 + \\omega D x_1) + \\epsilon_i\\]\nWith\n- \\(\\gamma\\) giving the change in \\(\\beta_0\\) and\n- \\(\\omega\\) giving the change in \\(\\beta_1\\).\nThat’s a change in intercept and a change in slope!\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  \n  \n  \n    \n      term\n      estimate\n      std.error\n      t.statistic\n      p.value\n    \n  \n  \n    \n      Additive Model\n    \n    (Intercept)\n86.797\n0.877\n98.993\n0.000\n    Mom's IQ\n0.610\n0.059\n10.423\n0.000\n    \n      Interactive Model\n    \n    (Intercept)\n85.407\n2.218\n38.502\n0.000\n    Mom's IQ\n0.969\n0.148\n6.531\n0.000\n    High School\n2.841\n2.427\n1.171\n0.242\n    Mom's IQ x HS\n-0.484\n0.162\n-2.985\n0.003"
  },
  {
    "objectID": "slides/10-transforms-slides.html#polynomial-transformations-for-non-linearity",
    "href": "slides/10-transforms-slides.html#polynomial-transformations-for-non-linearity",
    "title": "Lecture 10: Transforming Variables",
    "section": "Polynomial transformations for non-linearity",
    "text": "Polynomial transformations for non-linearity\n\n\n\n\n\nSimple Linear Model: \\(y_{i} = \\beta_{0} + \\beta_{1}X + \\epsilon_{i}\\)\n\\(R^2 = 0.3088\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuadratic Model: \\(y_{i} = \\beta_{0} + \\beta_{1}X + \\beta_{2}X^{2} + \\epsilon_{i}\\)\n\\(R^2=0.7636\\)"
  },
  {
    "objectID": "slides/10-transforms-slides.html#log-transformations",
    "href": "slides/10-transforms-slides.html#log-transformations",
    "title": "Lecture 10: Transforming Variables",
    "section": "Log transformations",
    "text": "Log transformations\n\n\n\n\n\n\n\n\n\n\n\nLogging a skewed variable normalizes it.\nThis is the inverse of exponentiation:\n\\[Y = log(exp(Y))\\]\nMay be applied to \\(X\\) and \\(Y\\).\nQuestion: but, whyyyyyy???"
  },
  {
    "objectID": "slides/10-transforms-slides.html#log-transformations-for-non-linearity",
    "href": "slides/10-transforms-slides.html#log-transformations-for-non-linearity",
    "title": "Lecture 10: Transforming Variables",
    "section": "Log transformations for non-linearity",
    "text": "Log transformations for non-linearity\n\n\n\n\n\nSimple Linear Model: \\(y_{i} = \\beta_{0} + \\beta_{1}X + \\epsilon_{i}\\)\n\\(R^2 = 0.8325\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLog Linear Model: \\(log(y_{i}) = \\beta_{0} + \\beta_{1}X + \\epsilon_{i}\\)\n\\(R^2 = 0.9407\\)"
  },
  {
    "objectID": "slides/10-transforms-slides.html#careful",
    "href": "slides/10-transforms-slides.html#careful",
    "title": "Lecture 10: Transforming Variables",
    "section": "⚠️ Careful!",
    "text": "⚠️ Careful!\nHave to take care how we interpret log-models!\n\\(\\beta_1\\) means…\n\n\n\nLinear Model\n\\(y_{i} = \\beta_{0} + \\beta_{1}X + \\epsilon_{i}\\)\nIncrease in Y for one unit increase in X.\n\nLog-Linear Model\n\\(log(y_{i}) = \\beta_{0} + \\beta_{1}X + \\epsilon_{i}\\)\nPercent increase in Y for one unit increase in X.\n\nLinear-Log Model\n\\(y_{i} = \\beta_{0} + \\beta_{1}log(X) + \\epsilon_{i}\\)\nIncrease in Y for percent increase in X.\n\nLog-Log Model\n\\(log(y_{i}) = \\beta_{0} + \\beta_{1}log(X) + \\epsilon_{i}\\)\nPercent increase in Y for percent increase in X."
  },
  {
    "objectID": "slides/11-maximum-likelihood-slides.html#lecture-outline",
    "href": "slides/11-maximum-likelihood-slides.html#lecture-outline",
    "title": "Lecture 11: Maximum Likelihood",
    "section": "📋 Lecture Outline",
    "text": "📋 Lecture Outline\n\nLimitations of OLS\nLikelihood\nLikelihood Estimation\nMaximum Likelihood Estimation (MLE)"
  },
  {
    "objectID": "slides/11-maximum-likelihood-slides.html#limitations-of-ols",
    "href": "slides/11-maximum-likelihood-slides.html#limitations-of-ols",
    "title": "Lecture 11: Maximum Likelihood",
    "section": "Limitations of OLS",
    "text": "Limitations of OLS\n\n\n\n\n\n\n\n\nThis is a made-up dataset.\n\n\nCan’t use OLS to model counts or binary outcomes. Inflexible approach to quantifying uncertainty about these types of outcome.\n\nModel evaluation with ANOVA is restricted to subsets."
  },
  {
    "objectID": "slides/11-maximum-likelihood-slides.html#likelihood",
    "href": "slides/11-maximum-likelihood-slides.html#likelihood",
    "title": "Lecture 11: Maximum Likelihood",
    "section": "Likelihood",
    "text": "Likelihood\n\n\n\n\n\n\n\n\nDefinition: Probability of data, \\(X\\), given model, \\(\\theta\\).\n\\[\\mathcal{L}(\\theta|X) = P(X|\\theta)\\]\nIn English: “The likelihood of the model given the data is equal to the probability of the data given the model.”\nAnswers the Question: how unlikely or strange is our data?"
  },
  {
    "objectID": "slides/11-maximum-likelihood-slides.html#likelihood-estimation",
    "href": "slides/11-maximum-likelihood-slides.html#likelihood-estimation",
    "title": "Lecture 11: Maximum Likelihood",
    "section": "Likelihood Estimation",
    "text": "Likelihood Estimation\n\n\n\n\nA Simple Experiment\n\n\n\n\n\nWe flip a coin \\(n = 100\\) times and count the number of heads.\nResult: \\(h = 56\\)\n\nSuppose our model is that the coin is fair.\n\nQuestion: What is \\(\\mathcal{L}(p=0.5|h=56)\\)?\n\nIn English: “How likely is it that the coin is fair given that heads came up 56 times?”\n\n⚠️ This is the probability that heads comes up 56/100 times assuming that the coin is fair.\n\n\n\n\n\n\nA Simple Experiment\n\n\n\n\n\nWe flip a coin \\(n = 100\\) times and count the number of heads.\nResult: \\(h = 56\\)\n\nSuppose our model is that the coin is fair.\n\nQuestion: What is \\(\\mathcal{L}(p=0.5|h=56)\\)?\n\nThis experiment is a series of Bernoulli trials, so we can use the binomial distribution to calculate \\(\\mathcal{L}(p|h)\\).\n\\[\\;\\;\\;\\;\\,\\mathcal{L}(p|h) = \\binom{n}{h}p^{h}(1-p)^{n-h}\\]\n\n\n\n\n\n\n\n\n\n\n\n\nSuppose our model is that the coin is fair.\n\nQuestion: What is \\(\\mathcal{L}(p=0.5|h=56)\\)?\n\nThis experiment is a series of Bernoulli trials, so we can use the binomial distribution to calculate \\(\\mathcal{L}(p|h)\\).\n\\[\n\\begin{align}\n\\mathcal{L}(p=0.5|h=56) &= \\binom{100}{56}0.5^{56}(1-0.5)^{100-56}\\\\\\\\\n\\mathcal{L}(p=0.5|h=56) &= 0.039\n\\end{align}\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\nSuppose our model is that the coin is biased.\n\nQuestion: What is \\(\\mathcal{L}(p=0.52|h=56)\\)?\n In English: “How likely is it that the coin is biased given that heads came up 56 times?”\n\n⚠️ This is the probability that heads comes up 56/100 times assuming that the coin is biased."
  },
  {
    "objectID": "slides/11-maximum-likelihood-slides.html#maximum-likelihood",
    "href": "slides/11-maximum-likelihood-slides.html#maximum-likelihood",
    "title": "Lecture 11: Maximum Likelihood",
    "section": "Maximum Likelihood",
    "text": "Maximum Likelihood\n\n\n\n\n\n\n\n\nQuestion: What value of \\(p\\) maximizes \\(\\mathcal{L}(p|h)\\)?\n\\[\\hat{p} = max\\, \\mathcal{L}(p|h)\\]\nIn English: “Given a set of models, choose the one that makes what we observe the most probable thing to observe.”\n\nIt’s a method for estimating the parameters of a model given some observed data."
  },
  {
    "objectID": "slides/11-maximum-likelihood-slides.html#maximum-likelihood-estimation",
    "href": "slides/11-maximum-likelihood-slides.html#maximum-likelihood-estimation",
    "title": "Lecture 11: Maximum Likelihood",
    "section": "Maximum Likelihood Estimation",
    "text": "Maximum Likelihood Estimation\n\n\n\n\nWhat if we have multiple observations?\n\\[X = [5, 7, 8, 2, 4]\\]\nQuestion: What is the probability that this sample comes from a normal distribution with a mean of 5 and a variance of 2?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe probability density for a given observation \\(x_i\\) given a normal distribution is:\n\\[N(x_i,\\mu,\\sigma) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\;exp\\left[-\\frac{1}{2}\\left(\\frac{x_i-\\mu}{\\sigma}\\right)^2\\right]\\]\nFor any set of values \\(X\\), the likelihood is the product of the individual densities:\n\\[\\mathcal{L}(\\mu, \\sigma^2|x_i)=\\prod_{i=1}^{n} N(x_i, \\mu, \\sigma^2)\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFor \\(X = [5, 7, 8, 2, 4]\\), we have\n\n\n\n\n\\[\n\\begin{align}\n\\mathcal{L}(\\mu, \\sigma^2|X) &= N(5) \\cdot N(7) \\cdot N(8) \\cdot N(2) \\cdot N(4)\\\\\\\\\n&= 1.78e-05\n\\end{align}\n\\] \nwhere \\(\\mu=5\\) and \\(\\sigma^2=2\\).\nAs the likelihood is often very small, a common strategy is to minimize the negative log likelihood rather than maximize the likelihood, (\\(min\\;\\mathrm{-}\\ell\\)) rather than (\\(max\\;\\mathcal{L}\\))."
  },
  {
    "objectID": "slides/12-logistic-regression-slides.html#lecture-outline",
    "href": "slides/12-logistic-regression-slides.html#lecture-outline",
    "title": "Lecture 12: Logistic Regression",
    "section": "📋 Lecture Outline",
    "text": "📋 Lecture Outline\n\nGeneral Linear Model\nGeneralized Linear Model\n\nDistribution function\nLinear predictor\nLink function\n\nGaussian outcomes\nBernoulli outcomes\nProportional outcomes\nDeviance\nInformation Criteria\nANOVE aka Likelihood Ratio Test"
  },
  {
    "objectID": "slides/12-logistic-regression-slides.html#general-linear-models",
    "href": "slides/12-logistic-regression-slides.html#general-linear-models",
    "title": "Lecture 12: Logistic Regression",
    "section": "General Linear Models",
    "text": "General Linear Models\n\\[y = E(Y) + \\epsilon\\] \\[E(Y) = \\mu = \\beta X\\]\n\n\\(E(Y)\\) is the expected value (equal to the conditional mean, \\(\\mu\\))\n\n\\(\\beta X\\) is the linear predictor\n\nError is normal, \\(\\epsilon \\sim N(0, \\sigma^2)\\)"
  },
  {
    "objectID": "slides/12-logistic-regression-slides.html#generalized-linear-models",
    "href": "slides/12-logistic-regression-slides.html#generalized-linear-models",
    "title": "Lecture 12: Logistic Regression",
    "section": "Generalized Linear Models",
    "text": "Generalized Linear Models\n\\[y = E(Y) + \\epsilon\\] \\[E(Y) = g(\\mu) = \\beta X\\]\n\n\\(g\\) is the link function, makes the relationship linear\n\\(\\beta X\\) is the linear predictor\nError is exponential, \\(\\epsilon \\sim Exponential\\)"
  },
  {
    "objectID": "slides/12-logistic-regression-slides.html#generalized-linear-models-1",
    "href": "slides/12-logistic-regression-slides.html#generalized-linear-models-1",
    "title": "Lecture 12: Logistic Regression",
    "section": "Generalized Linear Models",
    "text": "Generalized Linear Models\n\n\n\n\nExponentialLinear predictorLink function\n\n\n\nA family of distributions:\n\nNormal - continuous and unbounded\nGamma - continuous and non-negative\nBinomial - binary (yes/no) or proportional (0-1)\nPoisson - count data\n\nDescribes the distribution of the response\nTwo parameters: mean and variance\nVariance is a function of the mean:\n\n\\[Var(\\epsilon) = \\phi \\mu\\]\nwhere \\(\\phi\\) is a scaling parameter, assumed to be equal to 1, meaning the variance is assumed to be equal to the mean.\n\n\n\nIncorporates information about independent variables\n\nCombination of \\(x\\) variables and associated coefficients\n\n\\[\\beta X = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\ldots + \\beta_n x_n\\]\n\nCommonly denoted with Greek letter “eta”, as in \\(\\eta = \\beta X\\)\n\n\n\n\n\\(g()\\) modifies relationship between predictors and expected value.\n\nMakes this relationship linear\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDistribution\nName\nLink\nMean\n\n\n\n\nNormal\nIdentity\n\\(\\beta X = \\mu\\)\n\\(\\mu = \\beta X\\)\n\n\nGamma\nInverse\n\\(\\beta X = \\mu^{-1}\\)\n\\(\\mu = (\\beta X)^{-1}\\)\n\n\nPoisson\nLog\n\\(\\beta X = ln\\,(\\mu)\\)\n\\(\\mu = exp\\, (\\beta X)\\)\n\n\nBinomial\nLogit\n\\(\\beta X = ln\\, \\left(\\frac{\\mu}{1-\\mu}\\right)\\)\n\\(\\mu = \\frac{1}{1+exp\\, (-\\beta X)}\\)"
  },
  {
    "objectID": "slides/12-logistic-regression-slides.html#gaussian-response",
    "href": "slides/12-logistic-regression-slides.html#gaussian-response",
    "title": "Lecture 12: Logistic Regression",
    "section": "Gaussian response",
    "text": "Gaussian response\n\n\n\n\n\n\n\n\n\n\n\n\nAssume length is Gaussian with\n\\(Var(\\epsilon) = \\sigma^2\\)\n\\(E(Y) = \\mu = \\beta X\\)\nQuestion What is the probability that we observe these data given a model with parameters \\(\\beta\\) and \\(\\sigma^2\\)?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArchaic dart points. Sample from Fort Hood, Texas"
  },
  {
    "objectID": "slides/12-logistic-regression-slides.html#log-odds",
    "href": "slides/12-logistic-regression-slides.html#log-odds",
    "title": "Lecture 12: Logistic Regression",
    "section": "Log Odds",
    "text": "Log Odds\n\nLocation of residential features at the Snodgrass sites\n\n\n\n\n\n\n\n\n\n\nInside wall\nOutside wall\nTotal\n\n\n\n\nCount\n38\n53\n91\n\n\nProbability\n  0.42 = \\(\\frac{38}{91}\\)\n  0.58 = \\(\\frac{53}{91}\\)\n1\n\n\nOdds\n  0.72 = \\(\\frac{0.42}{0.58}\\)\n  1.40 = \\(\\frac{0.58}{0.42}\\)\n\n\n\nLog Odds\n-0.33 = log(0.72)\n  0.33 = log(1.40)\n\n\n\n\n\nWhy Log Odds?\nBecause the distribution of odds can be highly skewed, and taking the log normalizes it (makes it more symmetric)."
  },
  {
    "objectID": "slides/12-logistic-regression-slides.html#bernoulli-response",
    "href": "slides/12-logistic-regression-slides.html#bernoulli-response",
    "title": "Lecture 12: Logistic Regression",
    "section": "Bernoulli response",
    "text": "Bernoulli response\n\n\n\n\n\n\n\n\n\nLocation inside or outside of the inner wall at the Snodgrass site is a Bernoulli variable and has expectation \\(E(Y) = p\\) where\n\\[p = \\frac{1}{1 + exp(-\\beta X)}\\] This defines a logistic curve or sigmoid, with \\(p\\) being the probability of success. This constrains the estimate \\(E(Y)\\) to be in the range 0 to 1.\n\n\n\n\n\n\n\n\n\n\nTaking the log of \\(p\\) gives us\n\\[log(p) = log\\left(\\frac{p}{1 - p}\\right) = \\beta X\\]\nThis is known as the “logit” or log odds.\nQuestion What is the probability that we observe these data (these inside features) given a model with parameters \\(\\beta\\)?\n\n\n\n\n\n\n\n\n\n\nEstimated coefficients:\n\\(\\beta_0 = -8.6631\\)\n\\(\\beta_1 = 0.0348\\)\nFor these, the log Likelihood is\n\\(\\mathcal{l} = -28.8641\\)\n\n\n\n\n\nData from the Snodgrass site in Butler County, MS"
  },
  {
    "objectID": "slides/12-logistic-regression-slides.html#interpretation",
    "href": "slides/12-logistic-regression-slides.html#interpretation",
    "title": "Lecture 12: Logistic Regression",
    "section": "Interpretation",
    "text": "Interpretation\n\n\n\n\n\n\n\n\n\nEstimated coefficients:\n\\(\\beta_0 = -8.6631\\)\n\\(\\beta_1 = 0.0348\\)\nNote that these coefficient estimates are log-odds! To get the odds, we take the exponent.\n\\(\\beta_0 = exp(-8.6631) = 0.0002\\)\n\\(\\beta_1 = exp(0.0348) = 1.0354\\)\nFor a one unit increase in area, the odds of being in the inside wall increase by 1.0354.\n\n\n\n\n\n\n\n\n\n\nEstimated coefficients:\n\\(\\beta_0 = -8.6631\\)\n\\(\\beta_1 = 0.0348\\)\nTo get the probability, we can use the mean function (also known as the inverse link):\n\\[p = \\frac{1}{1+exp(-\\beta X)}\\]\nFor a house structure with an area of 300 square feet, the estimated probability that it occurs inside the inner wall is 0.8538.\n\n\n\n\n\nData from the Snodgrass site in Butler County, MS"
  },
  {
    "objectID": "slides/12-logistic-regression-slides.html#proportional-response",
    "href": "slides/12-logistic-regression-slides.html#proportional-response",
    "title": "Lecture 12: Logistic Regression",
    "section": "Proportional response",
    "text": "Proportional response\n\n\n\n\n\n\n\n\n\nProportion of Roman pottery is a binomial variable and has expectation \\(E(Y) = p\\) where\n\\[p = \\frac{1}{1 + exp(-\\beta X)}\\] This defines a logistic curve or sigmoid, with \\(p\\) being the proportion of successful Bernoulli trials. This constrains the estimate \\(E(Y)\\) to be in the range 0 to 1.\n\n\n\n\n\n\n\n\n\n\nTaking the log of \\(p\\) gives us\n\\[log(p) = log\\left(\\frac{p}{1 - p}\\right) = \\beta X\\]\nThis is known as the “logit” or log odds.\nQuestion What is the probability that we observe these data (these proportions) given a model with parameters \\(\\beta\\)?\n\n\n\n\n\n\n\n\n\n\nEstimated coefficients:\n\\(\\beta_0 = -1.1818\\)\n\\(\\beta_1 = -0.0121\\)\nFor these, the log Likelihood is\n\\(\\mathcal{l} = -4.1148\\)\n\n\n\n\n\nData from Fulford and Hodder (1974). A Regression Analysis of Some Late Romano-British Pottery: A Case Study"
  },
  {
    "objectID": "slides/12-logistic-regression-slides.html#deviance",
    "href": "slides/12-logistic-regression-slides.html#deviance",
    "title": "Lecture 12: Logistic Regression",
    "section": "Deviance",
    "text": "Deviance\n\nMeasure of goodness of fit, so smaller is better\nGives the difference in log-Likelihood between a model \\(M\\) and a saturated model \\(M_S\\)\n\n\\(M_S\\) has a parameter for each observation (i.e., zero degrees of freedom)\n\n\n\\[D = -2\\,\\Big[\\,log\\,\\mathcal{L}(M_1) - log\\,\\mathcal{L}(M_S)\\,\\Big]\\]\n\nResidual deviance = deviance of proposed model\nNull deviance = deviance of null model (ie, intercept-only model)"
  },
  {
    "objectID": "slides/12-logistic-regression-slides.html#information-criteria",
    "href": "slides/12-logistic-regression-slides.html#information-criteria",
    "title": "Lecture 12: Logistic Regression",
    "section": "Information Criteria",
    "text": "Information Criteria\n\nA function of the deviance, so smaller is better\nTwo penalties:\n\n\\(n\\) = number of observations\n\\(p\\) = number of parameters\n\nAkaike Information Criteria\n\n\\(AIC = D + 2p\\)\n\nBayesian Information Criteria\n\n\\(BIC = D + (p \\cdot log(n))\\)"
  },
  {
    "objectID": "slides/12-logistic-regression-slides.html#anova",
    "href": "slides/12-logistic-regression-slides.html#anova",
    "title": "Lecture 12: Logistic Regression",
    "section": "ANOVA",
    "text": "ANOVA\n\nAnalysis of Deviance aka Likelihood Ratio Test\nFor Binomial and Poisson models\nTest statistic is the logged ratio of likelihoods between proposed, \\(M_1\\), and null, \\(M_0\\), models\n\n\\[\n\\begin{aligned}\n\\chi^2 &= -2\\,log\\,\\frac{\\mathcal{L}(M_0)}{\\mathcal{L}(M_1)}\\\\\\\\\n&= -2\\,\\Big[\\,log\\,\\mathcal{L}(M_0) - log\\,\\mathcal{L}(M_1)\\,\\Big]\n\\end{aligned}\n\\]\n\nCompare to a \\(\\chi^2\\) distribution with \\(k\\) degrees of freedom - asks what the probability is of getting a value greater than the observed \\(\\chi^2\\) value.\nNull hypothesis is no difference in log-Likelihood."
  },
  {
    "objectID": "slides/13-poisson-regression-slides.html#lecture-outline",
    "href": "slides/13-poisson-regression-slides.html#lecture-outline",
    "title": "Lecture 13: Poisson Regression",
    "section": "📋 Lecture Outline",
    "text": "📋 Lecture Outline\n\nGaussian v Poisson\nAssumptions\nGaussian outcomes\nPoisson outcomes\nOffset\nDispersion"
  },
  {
    "objectID": "slides/13-poisson-regression-slides.html#gaussian-v-poisson",
    "href": "slides/13-poisson-regression-slides.html#gaussian-v-poisson",
    "title": "Lecture 13: Poisson Regression",
    "section": "Gaussian v Poisson",
    "text": "Gaussian v Poisson"
  },
  {
    "objectID": "slides/13-poisson-regression-slides.html#assumptions",
    "href": "slides/13-poisson-regression-slides.html#assumptions",
    "title": "Lecture 13: Poisson Regression",
    "section": "Assumptions",
    "text": "Assumptions\n\n\nGaussian\n\nLinearity\nHomoscedasticity\nNormality\nIndependence\n\n\nPoisson\n\nLog-linearity\nMean = Variance\nPoisson\nIndependence"
  },
  {
    "objectID": "slides/13-poisson-regression-slides.html#gaussian-response",
    "href": "slides/13-poisson-regression-slides.html#gaussian-response",
    "title": "Lecture 13: Poisson Regression",
    "section": "Gaussian response",
    "text": "Gaussian response\n\n\n\n\n\n\n\n\n\n\n\n\nAssume length is Gaussian with\n\\(Var(\\epsilon) = \\sigma^2\\)\n\\(E(Y) = \\mu = \\beta X\\)\nQuestion What is the probability that we observe these data given a model with parameters \\(\\beta\\) and \\(\\sigma^2\\)?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArchaic dart points. Sample from Fort Hood, Texas"
  },
  {
    "objectID": "slides/13-poisson-regression-slides.html#poisson-glm",
    "href": "slides/13-poisson-regression-slides.html#poisson-glm",
    "title": "Lecture 13: Poisson Regression",
    "section": "Poisson GLM",
    "text": "Poisson GLM\n\n\n\n\n\n\n\n\n\n\n\n\nCounts arise from a Poisson process with expectation \\(E(Y) = \\lambda\\) and\n\\[log\\,\\lambda = \\beta X\\]\nBy taking the log, this constrains the expected count to be greater than zero.\n\n\n\n\n\n\n\n\n\n\nEstimated coefficients:\n\\(\\beta_0 = 0.7074\\)\n\\(\\beta_1 = 1.2442\\)\n⚠️ Coefficients are on the log scale! To get counts, need the exponent.\n\\(\\beta_0 = exp(0.7074) = 2.0286\\)\n\\(\\beta_1 = exp(1.2442) = 3.4701\\)\nFor a one unit increase in elevation, the count of sites increases by 3.4701."
  },
  {
    "objectID": "slides/13-poisson-regression-slides.html#a-count-relative-to-what",
    "href": "slides/13-poisson-regression-slides.html#a-count-relative-to-what",
    "title": "Lecture 13: Poisson Regression",
    "section": "A count relative to what?",
    "text": "A count relative to what?\nSurvey blocks? Need to account for area in our sampling strategy!"
  },
  {
    "objectID": "slides/13-poisson-regression-slides.html#offset",
    "href": "slides/13-poisson-regression-slides.html#offset",
    "title": "Lecture 13: Poisson Regression",
    "section": "Offset",
    "text": "Offset\n\n\n\n\n\n\n\n\n\n\n\n\nModel the density\n\\[log\\;(\\lambda_i/area_i) = \\beta X\\] Equivalent to\n\\[log\\;(\\lambda_i) = \\beta X + log\\;(area_i)\\]\nStill linear! Still modeling counts!\n\n\n\n\n\n\n\n\n\n\nEstimated coefficients:\n\\(\\beta_0 = 1.3899\\)\n\\(\\beta_1 = 1.0537\\)\nFor these, the log Likelihood is\n\\(\\mathcal{l} = -444.0432\\)"
  },
  {
    "objectID": "slides/13-poisson-regression-slides.html#over-dispersion",
    "href": "slides/13-poisson-regression-slides.html#over-dispersion",
    "title": "Lecture 13: Poisson Regression",
    "section": "Over-dispersion",
    "text": "Over-dispersion\n\nFor exponential family of distributions, variance is a function of the mean:\n\n\\[Var(\\epsilon) = \\phi \\mu\\]\nwhere \\(\\phi\\) is a scaling parameter, assumed to be equal to 1, meaning the variance is assumed to be equal to the mean.\n\nWhen \\(\\phi > 1\\), this is called over-dispersion. When \\(\\phi < 1\\), it’s under-dispersion."
  },
  {
    "objectID": "slides/13-poisson-regression-slides.html#check-for-over-dispersion",
    "href": "slides/13-poisson-regression-slides.html#check-for-over-dispersion",
    "title": "Lecture 13: Poisson Regression",
    "section": "Check for over-dispersion",
    "text": "Check for over-dispersion\n\n\nRule of thumb: compare model’s residual deviance to its degrees of freedom. Values greater than one indicate over-dispersion.\n\n\n\nFor our site count model, that’s\n\\(D = 288.1438\\)\n\\(df = 98\\)\n\\(D/df = 2.9402\\)\n\n\nCan also test for dispersion using a simple linear model where\n\\[Var(\\epsilon) = \\mu + \\alpha \\mu\\]\nIf variance is equal to the mean, then \\(\\alpha = 0\\).\n\n\n\n\n\n\n\n  \n  \n    \n      estimate\n      statistic\n      null\n      p.value\n    \n  \n  \n    1.8372\n5.1420\n0.0000\n0.0000"
  },
  {
    "objectID": "slides/13-poisson-regression-slides.html#accounting-for-dispersion",
    "href": "slides/13-poisson-regression-slides.html#accounting-for-dispersion",
    "title": "Lecture 13: Poisson Regression",
    "section": "Accounting for dispersion",
    "text": "Accounting for dispersion\n\n\n\n\n\n\n\n\n\n\n\nTwo strategies:\n\nquasi-Poisson\nnegative binomial\n\n⚠️ Trade-offs! QP doesn’t use MLE. NB can’t be fit with stats::glm().\n\n\n\n\n \n\n\n (1)\n\n  \n      \n    Est. \n    S.E. \n    t \n    p \n  \n \n\n  \n    (Intercept) \n    1.314 \n    0.120 \n    10.986 \n    <0.001 \n  \n  \n    elevation \n    1.077 \n    0.076 \n    14.197 \n    <0.001"
  }
]