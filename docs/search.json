[
  {
    "objectID": "classes.html",
    "href": "classes.html",
    "title": "Classes",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nDate\n\n\nTitle\n\n\nDescription\n\n\n\n\n\n\nJan 10, 2023\n\n\nWeek 01: Introduction\n\n\nWhy statistics? Why R? Why Quarto? Making a webpage and a plot in just seconds!\n\n\n\n\nJan 17, 2023\n\n\nWeek 02: Probability as a Model\n\n\nA simple experiment. Random variables. Probability distributions. Probability as a model.\n\n\n\n\nJan 24, 2023\n\n\nWeek 03: Statistical Inference\n\n\nTesting hypotheses, visualizing distributions, reading and writing tabular data, and learning how to work with it in R.\n\n\n\n\nJan 31, 2023\n\n\nWeek 04: Ordinary Least Squares\n\n\nCalculating and testing bivariate statistics, including correlation and covariance. Visualizing probability densities. Fitting linear models using ordinary least squares. And some simple table indexing in R.\n\n\n\n\nFeb 7, 2023\n\n\nWeek 05: Visualizing Distributions\n\n\nA review and a deep dive into visualizing distributions in R, including bar chart, histogram, kernel density, cumulative density, and boxplot. Examples are given using base R {graphics} and {ggplot2}.\n\n\n\n\n\nFeb 14, 2023\n\n\nWeek 06: Working with Tables\n\n\nA review and a deep dive into working with rectangular data in R.\n\n\n\n\nFeb 21, 2023\n\n\nWeek 07: Evaluating Linear Models\n\n\n(Stats) Learn how to interpret linear models, make predictions, and use standard tests and diagnostics for evaluation, including making diagnostic plots. (R) Model summaries. Diagnostic plots. Prediction and plotting.\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "classes/01-intro.html#lab-exercises",
    "href": "classes/01-intro.html#lab-exercises",
    "title": "Week 01: Introduction",
    "section": "üî¨ Lab Exercises",
    "text": "üî¨ Lab Exercises\nWeek 01 Lab"
  },
  {
    "objectID": "classes/02-probability.html#lab-exercises",
    "href": "classes/02-probability.html#lab-exercises",
    "title": "Week 02: Probability as a Model",
    "section": "üî¨ Lab Exercises",
    "text": "üî¨ Lab Exercises\nWeek 02 Lab"
  },
  {
    "objectID": "classes/03-inference.html#lab-exercises",
    "href": "classes/03-inference.html#lab-exercises",
    "title": "Week 03: Statistical Inference",
    "section": "üî¨ Lab Exercises",
    "text": "üî¨ Lab Exercises\nWeek 03 Lab"
  },
  {
    "objectID": "classes/04-ols.html#lab-exercises",
    "href": "classes/04-ols.html#lab-exercises",
    "title": "Week 04: Ordinary Least Squares",
    "section": "üî¨ Lab Exercises",
    "text": "üî¨ Lab Exercises\nWeek 04 Lab"
  },
  {
    "objectID": "classes/05-distributions.html#lab-exercises",
    "href": "classes/05-distributions.html#lab-exercises",
    "title": "Week 05: Visualizing Distributions",
    "section": "üî¨ Lab Exercises",
    "text": "üî¨ Lab Exercises\nWeek 05 Lab"
  },
  {
    "objectID": "classes/06-dataframes.html",
    "href": "classes/06-dataframes.html",
    "title": "Week 06: Working with Tables",
    "section": "",
    "text": "No lecture this week."
  },
  {
    "objectID": "classes/06-dataframes.html#lab-exercises",
    "href": "classes/06-dataframes.html#lab-exercises",
    "title": "Week 06: Working with Tables",
    "section": "üî¨ Lab Exercises",
    "text": "üî¨ Lab Exercises\nWeek 06 Lab"
  },
  {
    "objectID": "classes/07-evaluation.html#lab-exercises",
    "href": "classes/07-evaluation.html#lab-exercises",
    "title": "Week 07: Evaluating Linear Models",
    "section": "üî¨ Lab Exercises",
    "text": "üî¨ Lab Exercises\nWeek 07 Lab"
  },
  {
    "objectID": "help.html",
    "href": "help.html",
    "title": "Looking for help",
    "section": "",
    "text": "Wisdom of the Ancients (xkcd 979)."
  },
  {
    "objectID": "help.html#the-google-search-paradox",
    "href": "help.html#the-google-search-paradox",
    "title": "Looking for help",
    "section": "The Google Search Paradox",
    "text": "The Google Search Paradox\nAs you make your first tentative forays into the R programming environment, you will on occasion experience the jarring dislocation of an R error, a typically bright red eruption of your R console, perhaps symbolic of your code exploding before your eyes. Here is one of the more infamous errors you are likely to encounter:\n\nobject of type 'closure' is not subsettable\nNever mind what this particular error means.1 The point is that it can be terribly frustrating when you encounter it or one of its kin.1¬†Though check out Jenny Bryan‚Äôs talk at the 2020 RStudio Conference: https://rstudio.com/resources/rstudioconf-2020/object-of-type-closure-is-not-subsettable/\nTroubleshooting these errors can often be an additional source of anxiety and frustration, especially early on, as you simply lack the words required to describe your problem accurately and, thus, to formulate the question whose answer you so desperately need. I like to refer to this unhappy circumstance as the Google Search Paradox because you will inevitably find yourself staring at an empty search bar, wondering what words to give to Google. It‚Äôs also a bit like Meno‚Äôs Paradox, or the Paradox of Inquiry. For if you could properly state your question, it‚Äôs probably the case that you already know the answer. So, you either know and thus don‚Äôt need to ask, or need to ask, but don‚Äôt know how.\nOf course, the situation is not nearly so dire as this. In truth, you always know at least a little about your problem - you do have the error itself after all! - and can thus Google your way through to an answer - eventually, anyway. But life is fleeting, as they say, and time is short, so you should probably avoid the brute force approach, relying instead on searching efficiently. To help you with that (and to help you get better with that), this page provides a brief annotated list of where to look for answers, starting from within R itself!"
  },
  {
    "objectID": "help.html#r-helpers",
    "href": "help.html#r-helpers",
    "title": "Looking for help",
    "section": "R helpers",
    "text": "R helpers\nTypically, though not always, R code will have lots of supporting documentation. These come in two varieties: function help pages and vignettes. If you are having trouble a single function to work properly, you may find its help page more useful. If you are having trouble getting through some analysis and you cannot pinpoint the exact reason for your trouble, the vignettes are probably where you should look. There are a couple of ways to access this documentation.\n\nFrom within R, you can use the help() and ?... functions to access short-form documentation. Examples include help(\"plot\") and ?plot. This will call up the function‚Äôs documentation page and display it in your computer‚Äôs graphical device.\nhelp.search(...) and ??... both provide means of searching through help pages to find multiple functions with the same name (and potentially the same or similar uses). Simply replace the ellipses (three dots) with a character string and these functions will return all help pages with that string. So if you want to carry out a cluster analysis, typing ??cluster will search for any functions in your installed packages that use the word cluster.\nThe rdrr.io website provides access to all function help pages online. If you Google an R function, a link to its documentation on this website is typically the first that you will see. For the best search results, I recommend Googling ‚ÄúR <package name> <function name>.‚Äù\nFrom within R, you can also access the vignettes using some combination of vignette(), browseVignettes(), and RShowDoc().\n\nThe function vignette() with no argument specified will bring up a list of all available vignettes, organized by package. If you want the vignettes for a particular R package, you can also type vignette(package = ...), for example, vignette(package = \"grid\") will bring up the vignettes for the grid package.\n\nbrowseVignettes() will open a locally hosted HTML page in your browser with links to all available R vignettes. This is actually quite helpful, and you should give it a try when you get a chance. Just browsing through these vignettes will give you a great feel for all that you can do in R.\n\nRShowDoc() is mostly for opening a single vignette. This is usefully paired with vignette(), which will give you the name of the vignette and package, so that you can, for example, call RShowDoc(what = \"plotExample\", package = \"grid\"). This will bring up the ‚ÄúplotExample‚Äù vignette from the grid package.\n\nPackage authors have lots of resources for sharing their documentation now, including websites designed specifically to present both function help pages and vignettes. Here is an example of the website for the colorspace package.\nFinally, you can access all available documentation for official R packages by navigating the Comprehensive R Archive Network (CRAN) website, here: https://cran.r-project.org/."
  },
  {
    "objectID": "help.html#rstudio-helpers",
    "href": "help.html#rstudio-helpers",
    "title": "Looking for help",
    "section": "RStudio helpers",
    "text": "RStudio helpers\nWhile RStudio provides loads of support to R users, here we mention some of the more important ones.\n\nRStudio How To Articles provide loads of how-to guides for working with R and RStudio. This is a very comprehensive suite of useful documentation.\nRStudio Cheatsheets strive to communicate package information in a single, concise poster format with lots of visual queues and simple definitions. These can be really helpful when you need a quick refresher on the use of some bit of code.\nRStudio Community is an online forum where individuals ask and answer questions about R and RStudio. They have a very strict code of conduct for their members that emphasizes mutual respect and inclusivity, so you will generally find the discussions here much more friendly and supportive. Use of this forum is highly recommended.\nRStudio Education is a very, very recent development by RStudio (it came online in 2020), and it is simply amazing as a resource for not only learning R itself, but also learning how to teach R. Please note that, with the exception of number 4, these RStudio help tools can be accessed within the RStudio IDE under the Help tab."
  },
  {
    "objectID": "help.html#r-community-helpers",
    "href": "help.html#r-community-helpers",
    "title": "Looking for help",
    "section": "R Community helpers",
    "text": "R Community helpers\nThe R community refers to R users who are actively communicating with and supporting other R users. As there are lots and lots of engaged R users these days, and more and more every day, the community is definitely thriving. There is also an expanding ethos within this community driven largely by RStudio and its code of conduct, so you will generally find R users to be a friendly bunch (if a little hoity-toity). So, let‚Äôs talk about where you can engage with this community. We have already mentioned one, RStudio Community, but here we will list some more.\n\nStack Overflow is a forum for programmers in all programming languages to ask and answer questions, much like RStudio Community. It‚Äôs just been around longer (2008 to be exact), which means its code of conduct has evolved over time to address a number of unanticipated issues. The consequence is that answers to questions will run the gamut from being respectful and clear to downright insulting. Still, it is a rich resource for addressing your R coding issues. And it has gotten a lot better.\nROpenSci is an R programming community focused on promoting open and reproducible research in science. They have a forum much like RStudio Community, a blog with helpful news and overviews of the packages in their ecosystem, and a rich suite of webpages for their supported R packages, which you can explore here.\nR-bloggers is a clearinghouse for R related content, basically an aggregator of content from individual blogs. It is worth perusing every now and then to pick up the occasional gem of R understanding.\nThe #rstats Twitter community is something. Use this if you use Twitter, I guess‚Ä¶\nThe rstats subreddit is a helpful community of Redditors that are pretty good about answering questions you might have."
  },
  {
    "objectID": "help.html#other-resources",
    "href": "help.html#other-resources",
    "title": "Looking for help",
    "section": "Other resources",
    "text": "Other resources\n\nThe UCLA Institute for Digital Research & Education offers Statistical Consulting geared toward R. This is a tremendous resource for both R and statistics and is highly recommended."
  },
  {
    "objectID": "help.html#reproducible-examples",
    "href": "help.html#reproducible-examples",
    "title": "Looking for help",
    "section": "Reproducible examples",
    "text": "Reproducible examples\nOthers have likely asked the same question you want to ask, so you will not always need to make a post yourself. But, in the off chance that you do find yourself confronted with a question never asked before, you need to make sure you provide R users with all the information and resources they need to help troubleshoot your code and to do so with the least effort possible. This involves providing a ‚Äúreproducible example‚Äù or reprex. There are two essential ingredients to a reprex:\n\nIt needs to be reproducible, obviously. That means you need to make sure you provide everything needed to reproduce your error as is, for instance, all library() calls in your code.\nIt needs to be minimal. In other words, do not include anything extraneous or burdensome, like a 400 MB data object. A much smaller R object should suffice.\n\nA lot has been written about how to put together a reprex, so rather than belabor the point here, it is perhaps best to direct you to Jenny Bryan‚Äôs reprex package, which will walk you through the process of submitting a help request on the various forums mentioned above."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Quantitative Analysis of Archaeological Data",
    "section": "",
    "text": "This is a Github page setup to host lectures and other content for the University of Utah course ANTH 5850: Quantitative Analysis of Archaeological Data (affectionately referred to as ‚Äúquad‚Äù). As its name suggests, this class offers students quantitative tools and techniques for working with archaeological data. Those tools include, first and foremost, the language of statistics, but also importantly the statistical programming language R, and finally the mark-up language Markdown (via Quarto), which aids in literate programming (think science communication). Obviously, no one can become fluent in a language - much less three languages! - with just four months of exposure. For that, there is no substitute for immersion, for living and working with these languages and the people who speak them, meaning scientists. This course is merely designed to get you started on that process and to hopefully make it smoother for you as you go. I think the word for it is a ‚Äúsurvey‚Äù course.\nOn this website, you‚Äôll find course lecture slides and labs. These are organized by class meetings, which you can find a link to in the navbar. The site was built using the open-source scientific and technical publishing system, Quarto, which you‚Äôll also learn about in this course! The source code for the website, along with the lecture slides and lab exercises, can be found at the associated Github repository."
  },
  {
    "objectID": "index.html#inspiration",
    "href": "index.html#inspiration",
    "title": "Quantitative Analysis of Archaeological Data",
    "section": "Inspiration?",
    "text": "Inspiration?\nI can‚Äôt take credit for all of the content in this course. The lecture slides, in particular, are adapted from the lectures of Dr.¬†Simon Brewer in the Department of Geography at the University of Utah. The R labs, at least the parts of them concerned with data science rather than statistics, draw heavily on the very popular book R for Data Science (2e) by Hadley Wickham and Garrett Grolemund.\nIt probably goes without saying, of course, but those folks are way smarter than I could ever hope to be, so any errors or confusions that occur here are definitely, one-hundred percent, without a doubt my own."
  },
  {
    "objectID": "index.html#reuse",
    "href": "index.html#reuse",
    "title": "Quantitative Analysis of Archaeological Data",
    "section": "Reuse",
    "text": "Reuse\n\nText and figures are licensed under Creative Commons Attribution CC BY 4.0. Any computer code (R, HTML, CSS, etc.) in slides and worksheets, including in slide and worksheet sources, is also licensed under MIT. Note that figures in slides may be pulled in from external sources and may be licensed under different terms. For such images, image credits are available in the slide notes, accessible via pressing the letter ‚Äòp‚Äô."
  },
  {
    "objectID": "labs/01-intro-lab.html",
    "href": "labs/01-intro-lab.html",
    "title": "Lab 01: Introduction",
    "section": "",
    "text": "In this lab, you will learn\n\nhow to use RStudio\nhow to make a plot with R\nhow to do math in R, create objects, use functions, etc.,\nhow to create an R Project folder\nhow to make a website (what?!) with Quarto\nand, you‚Äôll also learn the ends and outs of a typical workflow in R\n\nNo additional packages required this week.\n\n\n\ncars\n\nIncludes measurements of car speed and stopping distance.\npackage: datasets\n\nreference: https://stat.ethz.ch/R-manual/R-devel/library/datasets/html/cars.html"
  },
  {
    "objectID": "labs/01-intro-lab.html#working-in-rstudio",
    "href": "labs/01-intro-lab.html#working-in-rstudio",
    "title": "Lab 01: Introduction",
    "section": "Working in RStudio",
    "text": "Working in RStudio\n\n\n\n\nIf you are going to do anything with R, RStudio is hands-down the best place to do it. RStudio is an open-source integrated development environment (or IDE) that makes programming in R simpler, more efficient, and most importantly, more reproducible. Some of its more user-friendly features are syntax highlighting (it displays code in different colors depending on what it is or does, which makes it easier for you to navigate the code that you‚Äôve written), code completion (it will try to guess what code you are attempting to write and write it for you), and keyboard shortcuts for the more repetitive tasks.\nPane layout\nWhen you first open RStudio, you should see three window panes: the Console, the Environment, and the Viewer. If you open an R script, a fourth Source pane will also open. The default layout of these panes is shown in the figure above.\n\n\nSource. The Source pane provides basic text editing functionality, allowing you to create and edit R scripts. Importantly, you cannot execute the code in these scripts directly, but you can save the scripts that you write as simple text files. A dead give away that you have an R script living on your computer is the .R extension, for example, my_script.R.\n\n\nConsole. The Console pane, as its name suggests, provides an interface to the R console, which is where your code actually gets run. While you can type R code directly into the console, you can‚Äôt save the R code you write there into an R script like you can with the Source editor. That means you should reserve the console for non-essential tasks, meaning tasks that are not required to replicate your results.\n\nEnvironment. The Environment pane is sort of like a census of your digital zoo, providing a list of its denizens, i.e., the objects that you have created during your session. This pane also has the History tab, which shows the R code you have sent to the console in the order that you sent it.\n\n\nViewer. The Viewer pane is a bit of a catch-all, including a Files tab, a Plots tab, a Help tab, and a Viewer tab.\n\nThe Files tab works like a file explorer. You can use it to navigate through folders and directories. By default, it is set to your working directory.\nThe Plots tab displays any figures you make with R.\nThe Help tab is where you can go to find helpful R documentation, including function pages and vignettes.\nThe actual Viewer tab provides a window to visualize R Markdown.\n\n\n\nLet‚Äôs try out a few bits of code just to give you a sense of the difference between Source and Console.\n\nAs you work through this lab, you can practice running code in the Console, but make sure to do the actual exercises in an R script.\n\nExercises\n\nFirst, let‚Äôs open a new R script. To open an R script in RStudio, just click File > New File > R Script (or hit Ctrl + Shift + N, Cmd + Shift + N on Mac OS).\nCopy this code into the console and hit Enter.\n\n\nrep(\"Boba Fett\", 5)\n\n\nNow, copy that code into the R script you just opened and hit Enter again. As you see, the code does not run. Instead, the cursor moves down to the next line. To actually run the code, put the cursor back on the line with the code, and hit Ctrl + Enter (CMD + Enter on Mac OS)."
  },
  {
    "objectID": "labs/01-intro-lab.html#make-your-first-plot",
    "href": "labs/01-intro-lab.html#make-your-first-plot",
    "title": "Lab 01: Introduction",
    "section": "Make Your First Plot!",
    "text": "Make Your First Plot!\nTo ease you into working with R, let‚Äôs visualize some data to answer a simple question: Do fast moving objects take longer to slow down than slow moving objects? Don‚Äôt worry about understanding all of this! It‚Äôs just to give you a feel for the sort of graphics you can make with R. We‚Äôll actually spend all of the next lab learning how to make even better graphics.\nThe data\nTo answer that question, we‚Äôll use the cars data.frame that comes pre-loaded with R. A data.frame is simply an R object that stores tabular data, with rows for each observation and columns for each variable. Let‚Äôs have a look at the first n rows of this table, specifically the first 5 rows. We can do this using the function head().\n\nhead(cars, n = 5)\n\n  speed dist\n1     4    2\n2     4   10\n3     7    4\n4     7   22\n5     8   16\n\n\nNote that, in this case, each row represents a car. The first column, or variable, records the speed (in miles per hour) each car was traveling when it applied its brakes, and the second column provides measures of the distances (in feet) that each took to stop.\nThe plot() function\nThe base R graphics package provides a generic function for plotting, which - as you might have guessed - is called plot(). To see how it works, try running this code:\n\nplot(cars)\n\n\n\n\nCustomizing your plot\nWith the plot() function, you can do a lot of customization to the resulting graphic. For instance, you can modify all of the following:\n\n\npch will change the point type,\n\nmain will change the main plot title,\n\nxlab and ylab will change the x and y axis labels,\n\ncex will change the size of shapes within the plot region,\n\npch will change the type of point used (you can use triangles, squares, or diamonds, among others),\n\ncol changes the color of the point (or its border), and\n\nbg changes the color of the point fill (depending on the type of point it is)\n\nFor instance, try running this code:\n\nplot(\n  cars,\n  pch = 21,\n  bg = \"darkorange\",\n  col = \"darkred\",\n  cex = 2\n)\n\n\n\n\nExercises\n\nComplete the following line of code to preview only the first three rows of the cars table.\n\n\nhead(cars, n = )\n\n\nModify the code below to change the size (cex) of the points from 2 to 1.5.\n\n\nplot(\n  cars,\n  pch = 21,\n  bg = \"darkorange\",\n  col = \"darkred\",\n  cex = 2\n)\n\n\nWhat does this plot tell us about the relationship between car speed and stopping distance? Is it positive or negative? Or is there no relationship at all? If there is a relationship, what might explain it?\nComplete the code below to add ‚ÄúStopping distance for cars‚Äù as the main title.\n\n\nplot(\n  cars,\n  pch = 21,\n  bg = \"darkorange\",\n  col = \"darkred\",\n  cex = 1,\n  main = \n)\n\n\nComplete the code below to add ‚ÄúSpeed (mph)‚Äù as the x-axis label and ‚ÄúDistance (ft)‚Äù as the y-axis label.\n\n\nplot(\n  cars,\n  pch = 21,\n  bg = \"darkorange\",\n  col = \"darkred\",\n  cex = 2,\n  main = \"Stopping distance for cars\",\n  xlab = ,\n  ylab = \n)"
  },
  {
    "objectID": "labs/01-intro-lab.html#r-basics",
    "href": "labs/01-intro-lab.html#r-basics",
    "title": "Lab 01: Introduction",
    "section": "R Basics",
    "text": "R Basics\n\n\n\n\nR is a calculator\nYou can just do math with it:\n\n300 * (2/25)\n\n[1] 24\n\n3^2 + 42\n\n[1] 51\n\nsin(17)\n\n[1] -0.961\n\n\nObjects and Functions\nBut, R is more than just a calculator. There are a lot of things you can make with R, and a lot of things you can do with it. The things that you make are called objects and the things that you do things with are called functions. Any complex statistical operation you want to conduct in R will almost certainly involve the use of one or more functions.\nCalling functions\nTo use a function, we call it like this:\n\nfunction_name(arg1 = value1, arg2 = value2, ...)\n\nTry calling the seq() function.\n\nseq(from = 1, to = 5)\n\n[1] 1 2 3 4 5\n\n\nAs you can see, this generates a sequence of numbers starting at 1 and ending at 5. There are two things to note about this. First, we do not have to specify the arguments explicitly, but they must be in the correct order:\n\nseq(1, 5) \n\n[1] 1 2 3 4 5\n\nseq(5, 1)\n\n[1] 5 4 3 2 1\n\n\nSecond, the seq() function has additional arguments you can specify, like by and length. While we do not have to specify these because they have default values, you can change one or the other (but not at the same time!):\n\nseq(1, 5, by = 2)\n\n[1] 1 3 5\n\nseq(1, 5, length = 3)\n\n[1] 1 3 5\n\n\nCreating objects\nTo make an object in R, you use the arrow, <-, like so:\n\nobject_name <- value\n\nTry creating an object with value 5.137 and assigning it to the name bob, like this:\n\nbob <- 5.137\n\nThere are three things to note here. First, names in R must start with a letter and can only contain letters, numbers, underscores, and periods.\n\n# Good\nwinter_solder <- \"Buckey\"\nobject4 <- 23.2\n\n# Bad\nwinter soldier <- \"Buckey\" # spaces not allowed\n4object <- 23.2            # cannot start with a number\n\nSecond, when you create an object with <-, it ends up in your workspace or environment (you can see it in the RStudio environment pane). Finally, it is worth noting that the advantage of creating objects is that we can take the output of one function and pass it to another.\n\nx <- seq(1, 5, length = 3)\n\nlogx <- log(x)\n\nexp(logx)\n\n[1] 1 3 5\n\n\nExercises\n\nUse seq() to generate a sequence of numbers from 3 to 12.\nUse seq() to generate a sequence of numbers from 3 to 12 with length 25.\nWhy doesn‚Äôt this code work?\n\n\nseq(1, 5, by = 2, length = 10)\n\n\nUse <- to create an object with value 25 and assign it to a name of your choice.\nNow try to create another object with a different value and name.\nWhat is wrong with this code?\n\n\n2bob <- 10"
  },
  {
    "objectID": "labs/01-intro-lab.html#workflow",
    "href": "labs/01-intro-lab.html#workflow",
    "title": "Lab 01: Introduction",
    "section": "Workflow",
    "text": "Workflow\n\n\n\n\nAs you work more and more in R, you will learn that statistical analyses tend to involve the same basic set of tasks:\n\n\nimporting data,\n\nwrangling data to get it into a format necessary for analysis,\n\nexploring data with some simple descriptive statistics,\n\n\nanalyzing data with models to investigate potential trends or relationships, and\n\nsummarizing the results.\n\nAt various stages, you will also spend considerable time\n\n\nvisualizing the data and the results, either to explore the data further or to help communicate the results to others.\n\nA lot of the output of this process, we will also want to save for later, perhaps to include in a publication (like a figure or model summary), but maybe also to avoid repetition of difficult and time-consuming tasks, so the workflow will also involve\n\n\nexporting refined data and models.\n\nTo make this more concrete, let‚Äôs try out an example, working with the cars data again. As we go through this, try running all the code in the console.\nAn Example\nSuppose we return to the question we asked in the plotting section: Does the speed a car is going when it applies its brakes determine the distance it takes the car to stop? Obviously, the answer is Yes, but let‚Äôs pretend we don‚Äôt know the answer, so we can walk through the process of answering the question anyway.\nImport\nFirst, we need some data. In this case, we do not actually need to import the cars dataset because it is already available to us in R, so let‚Äôs just pretend like we did.\nExplore\nNow, let‚Äôs explore the data. Always, always, always, the best way to explore data is to visualize data! We already did this once, but it can‚Äôt hurt to try it again!\n\nplot(\n  cars,\n  pch = 21,\n  bg = adjustcolor(\"gray\", 0.75),\n  col = \"gray25\",\n  cex = 1.5\n)\n\n\n\n\nThen, we can do things like calculate the mean stopping distance.\n\nmean(cars$dist)\n\n[1] 43\n\n\nNote that we use the $ operator to pull the distance (dist) values from the cars table and supply it to the mean() function. Don‚Äôt worry too much about wrapping your head around that idea as we will talk about it more in another lab. We can also make a histogram to explore the distribution of stopping distances:\n\nhist(cars$dist)\n\n\n\n\nWhat does this tell you about car stopping distances? Is it clustered? Random?\nWrangle\nMaybe we think that one really long distance is exceptional, perhaps owing to measurement error, and we want to remove it from our analysis. In that case, we want to subset the data, including only distance values less than some amount, say 100 ft.\n\ncars <- subset(cars, dist < 100)\n\nThis is data wrangling, preparing the data for analysis.\nAnalyze\nNow, finally, we might want to answer our question directly by modeling the relationship between car speeds and stopping distances. Here, our hypothesis is that there is no relationship. This is called the null hypothesis. If we can show that this hypothesis is very likely false, then we can with some confidence accept the alternative hypothesis, namely, that there is a relationship. To test the null hypothesis, we can construct a simple linear model. In R, we do this:\n\ndistance_model <- lm(dist ~ speed, data = cars)\n\nsummary(distance_model)\n\n\nCall:\nlm(formula = dist ~ speed, data = cars)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-26.79  -9.15  -1.67   8.01  43.05 \n\nCoefficients:\n            Estimate Std. Error t value        Pr(>|t|)    \n(Intercept)  -14.002      6.295   -2.22           0.031 *  \nspeed          3.640      0.392    9.29 0.0000000000033 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 14.1 on 47 degrees of freedom\nMultiple R-squared:  0.647, Adjusted R-squared:  0.64 \nF-statistic: 86.3 on 1 and 47 DF,  p-value: 0.00000000000326\n\n\nWoah! That‚Äôs a lot to digest. For now, just note that the asterisks (*) imply that that there is very likely a relationship between speed and distance. But, what is that relationship? Or, what does it look like? Well, let‚Äôs try to visualize that in R, too.\n\nplot(\n  cars,\n  pch = 21,\n  bg = adjustcolor(\"gray\", 0.75),\n  col = \"gray25\",\n  cex = 1.5,\n  xlab = \"Speed (mph)\",\n  ylab = \"Distance (ft)\"\n)\n\nabline(\n  distance_model, \n  col = \"#A20000\",\n  lwd = 2\n)\n\ntitle(\n  \"Distance Model\",\n  line = 0.3, \n  adj = 0, \n  cex = 1.5\n)\n\n\n\n\nExport\nNow, if we feel it necessary, we can save our model, so we can inspect it again later.\n\nsave(distance_model, file = \"distance_model.Rds\")\n\nAnd that‚Äôs it! Now, all we have to do is write this up and publish it! Easy peasy."
  },
  {
    "objectID": "labs/01-intro-lab.html#r-projects",
    "href": "labs/01-intro-lab.html#r-projects",
    "title": "Lab 01: Introduction",
    "section": "R Projects",
    "text": "R Projects\n\n\n\n\nYOU CANNOT EAT R CODE. Believe me. You can‚Äôt. Eventually, you‚Äôll have to close out of R, turn off your computer, walk away, and do whatever it is that you do to maintain your existence. That means you need some way to save your progress and you need some place to save it. R has a few built-in tools for this, and they are really convenient, at least early on. However, you will be much better off if you get into the habit of using RStudio Projects. What is an R Project? Basically, it‚Äôs a folder on your computer that you can use to organize all the data, code, figures, texts, and analyses associated with a single scientific research project.\nWhen you open your project in RStudio, it will establish your project folder as your working directory by default. The advantage of this is that you can access R scripts and data using relative file paths rather than specifying the full path from your computer‚Äôs root directory. Why is this advantageous? Because you can copy the project folder to any computer you want and your relative file paths will just work!\nExercises\n\nBefore we setup your project, let‚Äôs turn off some of R‚Äôs default settings.\n\nIn RStudio, go to Tools > Global Options‚Ä¶.\nIn the dialog box that appears, navigate to the General section, and under Workspace, make sure ‚ÄúRestore .RData into workspace at startup‚Äù is unchecked.\nThen, for ‚ÄúSave workspace to .Rdata on exit‚Äù, select Never.\nHit ‚ÄúApply‚Äù, then hit ‚ÄúOK.‚Äù\n\n\n\nNow, we are going to create a new project for you for this class. You will use this folder to save all your lab and homework exercises, required datasets, and figures. To do that, follow these steps:\n\nIn RStudio, go to File > New Project‚Ä¶.\nIn the dialog box that appears, select New Directory, then New Project.\nPut ‚Äúqaad‚Äù as the Directory name.\nThen Browse to a location on your computer where you would like to keep this project and hit ‚ÄúOK.‚Äù\nMake sure ‚ÄúCreate a git repository‚Äù and ‚ÄúUse renv with this project‚Äù are unchecked.\n\nThen click ‚ÄúCreate Project.‚Äù This will restart RStudio with your project loaded. You can confirm this by looking at the top left of the RStudio window. It should say ‚Äúqaad - RStudio‚Äù now. If you look in the File pane (bottom-right), you will also see a file called ‚Äúqaad.Rproj.‚Äù\n\n\nOnce you have your project folder setup, have a look at the Files pane again. You should see a button that says ‚ÄúNew Folder.‚Äù Click that, and in the dialog box that appears, enter ‚ÄúR‚Äù and hit ‚ÄúOK.‚Äù You should now see a folder in your project directory called ‚ÄúR.‚Äù This is where you will keep all the files with your R code in it. Repeat this process to add ‚Äúdata‚Äù, ‚Äúfigures‚Äù, and ‚Äú_misc‚Äù folders to your project. The ‚Äú_misc‚Äù folder is short for miscellaneous. This folder is not strictly necessary but I find it helpful. It‚Äôs like that drawer in the kitchen where random stuff goes. It might not be clean or orderly, but at least your kitchen is!\nJust to check that everything is working, minimize RStudio and navigate to the location of your R Project on your computer. Do you see the folders you have created and the ‚Äúqaad.Rproj‚Äù file?"
  },
  {
    "objectID": "labs/01-intro-lab.html#quarto",
    "href": "labs/01-intro-lab.html#quarto",
    "title": "Lab 01: Introduction",
    "section": "Quarto",
    "text": "Quarto\n\n\n\n\n\nFigure¬†1: Artwork from ‚ÄúHello, Quarto‚Äù keynote by Julia Lowndes and Mine √áetinkaya-Rundel, presented at RStudio Conference 2022. Illustrated by Allison Horst.\n\n\nQuarto offers a unified framework for statistical programming and science communication by letting you write and run R code alongside text to explain your methods to others. The text you write is formatted using Markdown syntax (the same syntax you would use on, for example, a Reddit post). The basis for creating documents using Quarto is a test-based file format with the extension ‚Äú.qmd‚Äù, short for Quarto Markdown. In just about every one of these documents you come across, you will find three major components:\n\na YAML header surrounded at the top and bottom by three hyphens, ---,\nR code chunks surrounded at the top and bottom by three back ticks, ```, and\ntext formatted with markdown syntax like # heading 1 and _italics_.\n\nHere is an example:\n---\ntitle: \"Week 01\"\nauthor: \"Kenneth Blake Vernon\"\ndate: \"1/10/2023\"\nformat: html\nexecute:\n  echo: true\n---\n\n## Lab Exercises\n\n### Plot\n\n1. Complete the following line of code to preview only the first three rows of the `cars` table.\n\n```{r}\n\nhead(cars, n = )\n\n```\n\n***\n\n## Homework Exercises\n\n1. \nLet‚Äôs start with some simple markdown formatting and work our way back to the YAML.\nMarkdown formatting\nMarkdown is a lightweight markup language for formatting plain text and is designed to be easy to read and write. The markdown you will use most often includes all of the following (borrowed from here:\nText formatting \n------------------------------------------------------------\n\n*italic*  or _italic_\n**bold**   __bold__\n`code`\nsuperscript^2^ and subscript~2~\n\nHeadings\n------------------------------------------------------------\n\n# 1st Level Header\n\n## 2nd Level Header\n\n### 3rd Level Header\n\nLists\n------------------------------------------------------------\n\n*   Bulleted list item 1\n\n*   Bulleted list item 2\n\n    * Nested list item 2a\n\n    * Nested list item 2b\n\n1.  Numbered list item 1\n\n1.  Numbered list item 2. The numbers are incremented automatically in the output.\n\n1.  Numbered list item 3. \n\nLinks and images\n------------------------------------------------------------\n\n<http://example.com>\n\n[linked phrase](http://example.com)\n\n![optional caption text](path/to/img.png)\nYAML\n‚ÄòYAML‚Äô is a recursive acronym that means ‚ÄúYAML Ain‚Äôt Markup Language.‚Äù You don‚Äôt actually need to know that. I just think it‚Äôs funny. The YAML controls features of the whole document, specifying, for instance, the title and author. It looks like this:\n---\ntitle: \"Week 01\"\nauthor: \"Kenneth Blake Vernon\"\ndate: \"1/11/2022\"\nformat: html\n---\nNotice the three dashes --- at the top and bottom. These must be there in order for Quarto to recognize it as the YAML. You should always include this at the beginning of the document. There‚Äôs A LOT you can specify in the YAML. In fact, you can specify basically anything you want, but being quite new to Quarto, I don‚Äôt think that would be helpful. For now, let me draw your attention to the format field. Quarto (with the power of a utility known as Pandoc) can generate a wide variety of output formats, including Word documents, PDFs, revealjs slides (presentations, what the slides in tihs class are built with), and even Powerpoint (if you really insist on it). In this class, we‚Äôll stick with the default HTML output, so the only thing you will need to specify in the YAML is the title, author, and date.\nBy the way, the HTML output is the same stuff that a website is built on. In fact, when you open the resulting HTML file, it will open in your browser.\nR code chunks\nAll R code that you want to run needs to be ‚Äúfenced‚Äù by three back ticks ```. You also need to tell Quarto that it‚Äôs R code and not, say, Python or SQL. To do that, you add {r} after the first set of back ticks. Altogether, it should look like this:\n```{r}\n\n1+1\n\n```\nInstead of typing this every time, you can use Ctrl + Alt + I (or CMD) in RStudio, and it will automatically generate a code chunk in your qmd document. You can run the code in these chunks like you would code in an R script, by placing the cursor over it and hitting Ctrl + Enter. You can specify options for code chunks in R Markdown that will affect the way that they behave or are displayed. You can find a complete list of chunk options at http://yihui.name/knitr/options/. Here are a few examples:\n\n\neval: false prevents code from being evaluated.\n\necho: false hides the code but not the results of running the code.\n\nwarning: false prevents R messages and warnings from appearing in the knitted document.\n\nHere is how it would look to specify these in a code chunk:\n```{r}\n#| echo: false\n#| warning: false\n\n1+1\n\n```\nYou can also set these globally, applying them to all code chunks, by specifying them in the execute field (for code execution) in the YAML at the top of your qmd document. It would look like this:\n---\ntitle: \"Week 01\"\nauthor: \"Kenneth Blake Vernon\"\ndate: \"1/11/2022\"\nformat: html\nexecute:\n  echo: false\n  warning: false\n---\nThere are loads more of these options, some of the more important ones involve figures you generate with these documents. Rather than overload you with all of those now, we‚Äôll try to go over some of those here or there in future labs and homework exercises.\nExercises\n\nLet‚Äôs create a new qmd document in RStudio. To do that, follow these steps:\n\nGo to File > New File > Quarto Document‚Ä¶.\nIn the dialog box that appears, put ‚ÄúANTH 5580 (QAAD) Week 01‚Äù as the Title.\nPut your name as Author.\nHit ‚ÄúOK‚Äù.\n\n\n\nRStudio will open a new qmd document for you. Notice that it sets up the YAML for you. Let‚Äôs copy the template we will use for these course assignments. To do that, follow these steps:\n\nScroll up to the section above with the example of a qmd document. You can now copy and paste this into your qmd document (I recommend typing it out by hand, so you can get a feel for it, but that‚Äôs not necessary).\n\n\nNotice that this template has two level two headers, ‚ÄúLab Exercises‚Äù and ‚ÄúHomework Exercises.‚Äù These are the two major assignments you will have to complete each week. You will enter all your answers in a qmd document with this format and submit it via Canvas. To make sure your code is actually working, you can ‚Äúrender‚Äù the document and see if it completes without error. This is partly what I will do each week when grading your assignments. To keep these things organized, each exercise section in the lab should have its own level three header, like ### Plot for this week. Since there is no R related homework assignment for this week, you can just delete that section from this qmd document. Before continuing, save your qmd document to the R folder in your course project directory.\nNow, go back through this lab and re-do the exercises by adding them to this qmd document. Make sure to save that again, then submit it on the Canvas course page. Again, go ahead and render the document, too, just to make sure everything is working. This is the process that you will go through each week!"
  },
  {
    "objectID": "labs/01-intro-lab.html#homework",
    "href": "labs/01-intro-lab.html#homework",
    "title": "Lab 01: Introduction",
    "section": "Homework",
    "text": "Homework\nThere is no R related homework assignment for this week. Please fill out the pre-course self-assessment survey on Canvas."
  },
  {
    "objectID": "labs/02-probability-lab.html",
    "href": "labs/02-probability-lab.html",
    "title": "Lab 02: Statistical Graphics",
    "section": "",
    "text": "This lab will guide you through the process of\n\nloading (or ‚Äúattaching‚Äù) R packages with library()\n\ngenerating summary statistics for your data\nvisualizing data with the grammar of graphics and ggplot()\n\naesthetic mappings\ngeometric objects\nfacets\nscales\nthemes\n\n\n\nWe will be using the following packages:\n\narchdata\nggplot2\npalmerpenguins\nskimr\nviridis\n\n\n\npenguins\n\nIncludes measurements for penguin species, island in Palmer Archipelago, size (flipper length, body mass, bill dimensions), and sex.\npackage: palmerpenguins\n\nreference: https://allisonhorst.github.io/palmerpenguins/reference/penguins.html\n\n\n\n\nDartPoints\n\nIncludes measurements of 91 Archaic dart points recovered during surface surveys at Fort Hood, Texas.\npackage: archdata\n\nreference: https://cran.r-project.org/web/packages/archdata/archdata.pdf"
  },
  {
    "objectID": "labs/02-probability-lab.html#the-library",
    "href": "labs/02-probability-lab.html#the-library",
    "title": "Lab 02: Statistical Graphics",
    "section": "The Library",
    "text": "The Library\nR is an extensible programming language, meaning you can write R code to extend the functionality of base R. To share that code, R users will often bundle it into a package, a collection of functions, data, and documentation. You can think of packages as apps, but apps specifically designed for R. To make the functionality a package offers available in R, you have to load them in with the library() function (the technical term is attach).\nYou should always, always, always load all the packages you use at the beginning of a document. That way, people who read your code know exactly what packages you are using all at once and right away. To make this really, really explicit, I prefer to set this off with its own section that I call the ‚ÄúR Preamble.‚Äù In a Quarto document, it looks like this:\n## R Preamble\n\n```{r packages}\n#| warning: false\n\nlibrary(archdata)\nlibrary(ggplot2)\nlibrary(palmerpenguins)\nlibrary(skimr)\nlibrary(viridis)\n\n```\nOf course, these aren‚Äôt just automatically on your computer, so you have to install the packages first. Then you can open them in R. To do that, you use the function install.packages(). For the packages used today, you can use this call just once like so:\ninstall.packages(\n  c(\"archdata\", \"ggplot2\", \"palmerpenguins\", \"skimr\", \"viridis\")\n)\nNote that you only need to run this once, so don‚Äôt put this as a line in your Quarto document, which you might render multiple times. Just run it in the console.\nExercises\n\nOpen a new Quarto document and add the R Preamble with an R code chunk with the library() calls that load the R packages required for this lab.\nNow actually run each library() call. You can do that by either highlighting them and hitting Ctrl + Enter (Cmd + Enter) or by clicking the green arrow that appears in the top right of the code chunk."
  },
  {
    "objectID": "labs/02-probability-lab.html#summary-statistics",
    "href": "labs/02-probability-lab.html#summary-statistics",
    "title": "Lab 02: Statistical Graphics",
    "section": "Summary Statistics",
    "text": "Summary Statistics\n\n\n\n\n\nFigure¬†1: Artwork by Allison Horst.\n\n\nLet‚Äôs use R to describe some properties of a sample of penguins from Palmer Station in Antarctica. These data became available in R when you loaded the palmerpenguins package. They aren‚Äôt currently visible in your environment (for complicated reasons), but trust me, they‚Äôre there. The name of the dataset is penguins, so you can call it that way.\n\nhead(penguins, n = 5)\n\n# A tibble: 5 √ó 8\n  species island    bill_length_mm bill_depth_mm flipper_l‚Ä¶¬π body_‚Ä¶¬≤ sex    year\n  <fct>   <fct>              <dbl>         <dbl>       <int>   <int> <fct> <int>\n1 Adelie  Torgersen           39.1          18.7         181    3750 male   2007\n2 Adelie  Torgersen           39.5          17.4         186    3800 fema‚Ä¶  2007\n3 Adelie  Torgersen           40.3          18           195    3250 fema‚Ä¶  2007\n4 Adelie  Torgersen           NA            NA            NA      NA <NA>   2007\n5 Adelie  Torgersen           36.7          19.3         193    3450 fema‚Ä¶  2007\n# ‚Ä¶ with abbreviated variable names ¬π‚Äãflipper_length_mm, ¬≤‚Äãbody_mass_g\n\n\n\n\nCentral tendency\nThe central tendency is, as its name suggests, a value around which other values tend to cluster. There are two primary measures of central tendency: the mean and the median. As you may recall, the mean or average of a sample is simply the sum of a finite set of values, \\(x_i\\), divided by the number of values, \\(n\\).\n\\[\\bar{x} = \\frac{1}{n} \\sum_{i=1}^{n} x_i\\]\nRemember that this is only an estimate of the central tendency of a population, \\(\\mu\\)! In R, you can calculate the mean by hand if you like, but it‚Äôs probably easier to use the built-in R function, mean(). Let‚Äôs use this to calculate the mean bill length of penguins.\n\nmean(penguins$bill_length_mm)\n\n[1] NA\n\n\nWhoops! Need to set na.rm = TRUE to ignore missing or NA values.\n\nmean(penguins$bill_length_mm, na.rm = TRUE)\n\n[1] 43.9\n\n\nAn important limitation of the mean is its sensitivity to outliers - you know, like rich people. If you calculate the mean household income in the United States, for example, the incomes of obscenely wealthy individuals like Jeff Bezos and Elon Musk will pull that measure up, thus painting a much rosier picture of the US than the reality the rest of us live in. When there are extreme outliers, it is often advisable to use the median because it is less sensitive to outliers as it is the ‚Äúmiddle‚Äù number or value that evenly divides the sample in half.\n\nmedian(penguins$bill_length_mm, na.rm = TRUE)\n\n[1] 44.5\n\n\n\n\n\n\n\n\nNote\n\n\n\n\n\nBy the way, did you notice the weird use of the dollar sign $? This is used to index data and pull out a specific element by its name. For example, when we run the code snippet penguins$bill_length_mm, we are asking R to pull the variable bill_length_mm from the penguins table and give us its values. One important implication of this sort of indexing is that we can assign the variable to its own object outside of the table, e.g.\nbill_length <- penguins$bill_length_mm\nYou‚Äôll actually learn more about this, in particular how to work with tabular data, in the next lab.\n\n\n\nDispersion\nDispersion describes the spread of data around its central tendency. Are the values tightly clustered around the central tendency or highly dispersed? Is there, in other words, a lot of variability? This is what dispersion seeks to characterize. As with the central tendency, it has two primary measures: variance and standard deviation. The variance of a sample is the mean squared error.\n\\[s^{2} = \\frac{1}{n-1} \\sum_{i=1}^{n} (x_i - \\bar{x})^{2}\\]\nThis is an estimate of the population variance, \\(\\sigma^2\\). To calculate the variance of a sample with R, use var().\n\nvar(penguins$body_mass_g, na.rm = TRUE)\n\n[1] 643131\n\n\nWhoa! That‚Äôs a really big number. Do penguins get that big? The answer, of course, is No.¬†The number is large because variance is squared error, so this is in units of squared-grams, \\(g^2\\), not grams, \\(g\\). Why do we square it? Well, if you think about it, some of the errors will be negative and some will be positive (some penguins will be larger than average and some smaller), but on balance - on average - the sum of the errors will tend towards zero, which is not terribly informative of the spread of the data. Or, put that another way, what we want with a measure of dispersion is not just difference but distance from the mean, and distance is always positive. Squaring the errors is one way of ensuring that the values are positive (similar to taking the absolute value).\nThere is one small catch to squaring though. It makes it a really weird measure to think about - like, what is a square gram? Hard to say. That‚Äôs why it is common to take the square root of the variance, to get the measure back into units of the data. This value is known as the standard deviation, \\(s\\). You can calculate it with the sd() function.\n\nsd(penguins$body_mass_g, na.rm = TRUE)\n\n[1] 802\n\n\nThat‚Äôs about 1.8 pounds (if you prefer Imperial units).\nTable summaries\nTo generate summary statistics for all the variables in your data, base R provides a really nice summary() function that you can apply to a table like so:\n\nsummary(penguins)\n\n      species          island    bill_length_mm bill_depth_mm \n Adelie   :152   Biscoe   :168   Min.   :32.1   Min.   :13.1  \n Chinstrap: 68   Dream    :124   1st Qu.:39.2   1st Qu.:15.6  \n Gentoo   :124   Torgersen: 52   Median :44.5   Median :17.3  \n                                 Mean   :43.9   Mean   :17.1  \n                                 3rd Qu.:48.5   3rd Qu.:18.7  \n                                 Max.   :59.6   Max.   :21.5  \n                                 NA's   :2      NA's   :2     \n flipper_length_mm  body_mass_g       sex           year     \n Min.   :172       Min.   :2700   female:165   Min.   :2007  \n 1st Qu.:190       1st Qu.:3550   male  :168   1st Qu.:2007  \n Median :197       Median :4050   NA's  : 11   Median :2008  \n Mean   :201       Mean   :4202                Mean   :2008  \n 3rd Qu.:213       3rd Qu.:4750                3rd Qu.:2009  \n Max.   :231       Max.   :6300                Max.   :2009  \n NA's   :2         NA's   :2                                 \n\n\nAs you can see, this prints out summary statistics for the variables in your data. However, the printout is not easy to read and it provides a somewhat limited set of summary statistics. As an alternative, you might try the skim() function from the skimr package.\n\nNote that I have applied css styling to this table output to make it more compact and fit on the screen. Yours will look slightly different.\n\nskim(penguins)\n\n\nData summary\n\n\nName\npenguins\n\n\nNumber of rows\n344\n\n\nNumber of columns\n8\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n3\n\n\nnumeric\n5\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\nspecies\n0\n1.00\nFALSE\n3\nAde: 152, Gen: 124, Chi: 68\n\n\nisland\n0\n1.00\nFALSE\n3\nBis: 168, Dre: 124, Tor: 52\n\n\nsex\n11\n0.97\nFALSE\n2\nmal: 168, fem: 165\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\nbill_length_mm\n2\n0.99\n43.9\n5.46\n32.1\n39.2\n44.5\n48.5\n59.6\n‚ñÉ‚ñá‚ñá‚ñÜ‚ñÅ\n\n\nbill_depth_mm\n2\n0.99\n17.1\n1.97\n13.1\n15.6\n17.3\n18.7\n21.5\n‚ñÖ‚ñÖ‚ñá‚ñá‚ñÇ\n\n\nflipper_length_mm\n2\n0.99\n200.9\n14.06\n172.0\n190.0\n197.0\n213.0\n231.0\n‚ñÇ‚ñá‚ñÉ‚ñÖ‚ñÇ\n\n\nbody_mass_g\n2\n0.99\n4201.8\n801.95\n2700.0\n3550.0\n4050.0\n4750.0\n6300.0\n‚ñÉ‚ñá‚ñÜ‚ñÉ‚ñÇ\n\n\nyear\n0\n1.00\n2008.0\n0.82\n2007.0\n2007.0\n2008.0\n2009.0\n2009.0\n‚ñá‚ñÅ‚ñá‚ñÅ‚ñá\n\n\n\n\n\n\n\nAs you can see, there are three major sections of this printout: (i) Data Summary, (ii) Factor variables, and (iii) Numeric variables. The Data Summary gives you an overview of your table, with counts of the number of rows and columns, as well as counts of the different types of variables (factor, numeric, etc). The section on factor variables gives you counts for each level of the factor (for example, counts of the different species of penguins), as well as information about missing data. Finally, the section on numeric variables gives you information on missing data, as well as measures of dispersion and central tendency, including the mean, median (p50), and standard deviation (sd), the range or minimum and maximum values (p0 and p100), and the inner quartiles (p25 and p75).11¬†You‚Äôll learn more about quartiles and how to visualize distributions in the next lab.\nExercises\n\nHave a look at the penguins data again. Use head() to preview the first 15 rows.\nWith the penguins data, calculate all of the following:\n\nmedian body mass\nmean bill depth\nvariance in bill depth\nstandard deviation in bill length"
  },
  {
    "objectID": "labs/02-probability-lab.html#the-grammar-of-graphics",
    "href": "labs/02-probability-lab.html#the-grammar-of-graphics",
    "title": "Lab 02: Statistical Graphics",
    "section": "The Grammar of Graphics",
    "text": "The Grammar of Graphics\nIt‚Äôs easy to imagine how you would go about with pen and paper drawing a bar chart of, say, the number of penguins in each species in the penguins dataset. But, what if you had to dictate the steps to make that graph to another person, one you can‚Äôt see or physically interact with? All you can do is use words to communicate the graphic you want. How would you do it? The challenge here is that you and your illustrator must share a coherent vocabulary for describing graphics. That way you can unambiguously communicate your intent. That‚Äôs essentially what the grammar of graphics is, a language with a set of rules (a grammar) for specifying each component of a graphic.\nNow, if you squint just right, you can see that R has a sort of grammar built-in with the base graphics package. To visualize data, it provides the default plot() function, which you learned about in the last lab. This is a workhorse function in R that will give you a decent visualization of your data fast, with minimal effort. It does have its limitations though. For starters, the default settings are, shall we say, less than appealing. I mean, they‚Äôre fine if late-nineties styles are your thing, but less than satisfying if a more modern look is what you‚Äôre after.2 Second, taking fine-grained control over graphics generated with plot() can be quite frustrating, especially when you want to have a faceted figure (a figure with multiple plot panels).2¬†But, you know, opinions, everyone has them, and there‚Äôs no accounting for taste.\nThat‚Äôs where the ggplot2 package comes in. It provides an elegant implementation of the grammar of graphics, one with more modern aesthetics and with a more standardized framework for fine-tuning figures, so that‚Äôs what we‚Äôll be using here. From time to time, I‚Äôll try to give you examples of how to do things with the plot() function, too, so you can speak sensibly to the die-hard holdouts, but we‚Äôre going to focus on learning ggplot.\n\n\n\n\n\n\nTip\n\n\n\n\n\nThere are several excellent sources of additional information on statistical graphics in R and statistical graphics in general that I would recommend.\n\nThe website for the ggplot2 package: https://ggplot2.tidyverse.org/. This has loads of articles and references that will answer just about any question you might have.\n\nThe R graph gallery website: https://r-graph-gallery.com/. This has straightforward examples of how to make all sorts of different plot visualizations, both with base R and ggplot.\n\nClaus Wilke‚Äôs free, online book Fundamentals of Data Visualization, which provides high-level rules or guidelines for generating statistical graphics in a way that clearly communicates its meaning or intent and is visually appealing.\n\nThe free, online book ggplot2: Elegant Graphics for Data Analysis (3ed) by Hadley Wickham, Danielle Navarro, and Thomas Lin Pedersen. This is a more a deep dive into the grammar of graphics than a cookbook, but it also has lots of examples of making figures with ggplot2.\n\n\n\n\nSo, to continue our analogy above, we‚Äôre going to treat R like our illustrator, and ggplot2 is the language we are going to speak to R to visualize our data. So, how do we do that? Well, let‚Äôs start with the basics. Suppose we want to know if there‚Äôs some kind of relationship (an allometric relationship) among the Palmer Station penguins between their body mass and bill length. Here‚Äôs how we would visualize that.\n\nggplot(data = penguins) + \n  geom_point(\n    mapping = aes(x = body_mass_g, y = bill_length_mm)\n  )\n\n\n\n\nHere, we have created what is known as a scatterplot, a representation of the raw data as points on a Cartesian grid. There are several things to note about the code used to generate this plot.\n\nFirst, it begins with a call to the ggplot() function. This takes a data argument. In this case, we say that we want to make a plot to visualize the penguins data.\nThe next function call is geom_point(). This is a way of specifying the geometry we want to plot. Here we chose points, but we could have used another choice (lines, for example, or polygons).\nNote that the geom_point() call takes a mapping argument. You use this to specify how variables in your data are mapped to properties of the graphic. Here, we chose to map the body_mass_g variable to the x-coordinates and the bill_length_mm variable to the y-coordinates. Importantly, we use the aes() function to supply an aesthetic to the mapping parameter. This is always the case.\nThe final thing to point out here is that we combined or connected these arguments using the plus-sign, +. You should read this literally as addition, as in ‚Äúmake this ggplot of the penguins data and add a point geometry to it.‚Äù Be aware that the use of the plus-sign in this way is unique to the ggplot2 package and won‚Äôt work with other graphical tools in R.\n\nWe can summarize these ideas with a simple template. All that is required to make a graph in R is to replace the elements in the bracketed sections with a dataset, a geometry function, and an aesthetic mapping.\nggplot(data = <DATA>) + \n  <GEOM_FUNCTION>(mapping = aes(<MAPPINGS>))\nOne of the great things about ggplot, something that makes it stand out compared to alternative graphics engines in R, is that you can assign plots to a variable and call it in different places, or modify it as needed.\npenguins_plot <- ggplot(data = penguins) + \n  geom_point(\n    mapping = aes(x = body_mass_g, y = bill_length_mm)\n  )\n\npenguins_plot\nExercises\n\nRecreate the scatterplot above, but switch the axes. Put bill length on the x-axis and body mass on the y-axis.\nNow create a scatterplot of bill length (on the y-axis) by bill depth (on the x-axis)."
  },
  {
    "objectID": "labs/02-probability-lab.html#aesthetics",
    "href": "labs/02-probability-lab.html#aesthetics",
    "title": "Lab 02: Statistical Graphics",
    "section": "Aesthetics",
    "text": "Aesthetics\nIn the plot above, we only specified the position of the points (the x- and y-coordinates) in the aesthetic mapping, but there are many aesthetics (see the figure below), and we can map the same or other variables to those.\n\n\nFigure¬†2: Commonly used aesthetics. Figure from Claus O. Wilke. Fundamentals of Data Visualization. O‚ÄôReilly, 2019.\n\n\nConsider, for example, the fact that there are three penguin species in our dataset: Adelie, Gentoo, and Chinstrap. Do we think the relationship between body mass and bill length holds for all of them? Let‚Äôs add penguin species to our aesthetic mapping (specifically to the color parameter) and see what happens.\n\nggplot(data = penguins) + \n  geom_point(\n    mapping = aes(x = body_mass_g, y = bill_length_mm, color = species)\n  )\n\n\n\n\nNotice that ggplot2 automatically assigns a unique color to each species and adds a legend to the right that explains each color. In this way, the color doesn‚Äôt just change the look of the figure. It conveys information about the data. Rather than mapping a variable in the data to a specific aesthetic, though, we can also define an aesthetic manually for the geometry as a whole. In this case, the aesthetics do not convey information about the data. They merely change the look of the figure. The key to doing this is to move the specification outside the aes(), but still inside the geom_point() function.\n\nggplot(data = penguins) + \n  geom_point(\n    mapping = aes(x = body_mass_g, y = bill_length_mm),\n    shape = 21,\n    size = 4,\n    color = \"darkred\",\n    fill = \"darkgoldenrod1\"\n  )\n\n\n\n\nNotice that we specified the shape with a number. R has 25 built-in shapes that you can specify with a number, as shown in the figure below. Some important differences in these shapes concern the border and fill colors. The hollow shapes (0-14) have a border that you specify with color, the solid shapes (15-20) have a border and fill, both specified with color, and the filled shapes (21-24) have separate border and fill colors, specified with color and fill respectively.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\n\n\nNote that you can use hexadecimal codes like #004F2D instead of ‚Äúforestgreen‚Äù to specify a color. This also allows you to specify a much wider range of colors. See https://htmlcolorcodes.com/ for one way of exploring colors.\n\n\n\nExercises\n\nChange the code below to map the species variable to the x-axis (in addition to the color).\n\nggplot(data = penguins) + \n  geom_point(\n    mapping = aes(x = body_mass_g, y = bill_length_mm, color = species)\n  )\n\nWhat does this do to the position of the points?\nChange the code below to map the species variable to the shape aesthetic (in addition to the color).\n\n# hint: use shape = ...\nggplot(data = penguins) + \n  geom_point(\n    mapping = aes(x = body_mass_g, y = bill_length_mm, color = species)\n  )\n\nChange the code below to map the species variable to the size aesthetic (replacing color).\n\n# hint: use size = ...\nggplot(data = penguins) + \n  geom_point(\n    mapping = aes(x = body_mass_g, y = bill_length_mm, color = species)\n  )\n\nFor the following code, change the color, size, and shape aesthetics for the entire geometry (do not map them to the data).\n\nggplot(data = penguins) + \n  geom_point(\n    mapping = aes(x = body_mass_g, y = bill_length_mm),\n    color = , # <------- insert value here\n    size = ,  # <------- \n    shape =   # <------- \n  )"
  },
  {
    "objectID": "labs/02-probability-lab.html#geometries",
    "href": "labs/02-probability-lab.html#geometries",
    "title": "Lab 02: Statistical Graphics",
    "section": "Geometries",
    "text": "Geometries\nHave a look at these two plots.\n\n\n\n\n\n\n\n\n\n\nBoth represent the same data and the same x and y variables, but they do so in very different ways. That difference concerns their different geometries. As their name suggests, these are geometrical objects used to represent the data. To change the geometry, simply change the geom_*() function. For example, to create the plots above, use the geom_point() and geom_smooth() functions.\n# left\nggplot(data = penguins) + \n  geom_point(\n    mapping = aes(x = body_mass_g, y = bill_length_mm)\n  )\n\n# right\nggplot(data = penguins) + \n  geom_smooth(\n    mapping = aes(x = body_mass_g, y = bill_length_mm)\n  )\nWhile every geometry function takes a mapping argument, not every aesthetic works (or is needed) for every geometry. For example, there‚Äôs no shape aesthetic for lines, but there is a linetype. Conversely, points have a shape, but not a linetype.\n\nggplot(data = penguins) + \n  geom_smooth(\n    mapping = aes(x = body_mass_g, y = bill_length_mm, linetype = species),\n  )\n\n\n\n\nOne really important thing to note here is that you can add multiple geometries to the same plot to represent the same data. Simply add them together with +.\n\nggplot(data = penguins) + \n  geom_smooth(\n    mapping = aes(x = body_mass_g, y = bill_length_mm, linetype = species),\n  ) + \n  geom_point(\n    mapping = aes(x = body_mass_g, y = bill_length_mm, color = species)\n  )\n\n\n\n\nThat‚Äôs a hideous figure, though it should get the point across. While layering in this way is a really powerful tool for visualizing data, it does have one important drawback. Namely, it violates the DRY principle (Don‚Äôt Repeat Yourself), as it specifies the x and y variables twice. This makes it harder to make changes, forcing you to edit the same aesthetic parameters in multiple locations. To avoid this, ggplot2 allows you to specify a common set of aesthetic mappings in the ggplot() function itself. These will then apply globally to all the geometries in the figure.\nggplot(\n  data = penguins,\n  mapping = aes(x = body_mass_g, y = bill_length_mm)\n) +\n  geom_smooth(mapping = aes(linetype = species)) +\n  geom_point(mapping = aes(color = species))\nNotice that you can still specify specific aesthetic mappings in each geometry function. These will apply only locally to that specific geometry rather than globally to all geometries in the plot. In the same way, you can specify different data for each geometry.\n\nggplot(\n  data = penguins,\n  mapping = aes(x = body_mass_g, y = bill_length_mm)\n) +\n  geom_smooth(data = filter(penguins, species == \"Adelie\")) +\n  geom_point(mapping = aes(color = species))\n\n\n\n\nSome of the more important geometries you are likely to use include:\n\ngeom_point()\ngeom_line()\ngeom_segment()\ngeom_polygon()\ngeom_boxplot()\ngeom_histogram()\ngeom_density()\n\nWe‚Äôll actually cover those last three in the section on plotting distributions. For a complete list of available geometries, see the layers section of the ggplot2 website reference page."
  },
  {
    "objectID": "labs/02-probability-lab.html#facets",
    "href": "labs/02-probability-lab.html#facets",
    "title": "Lab 02: Statistical Graphics",
    "section": "Facets",
    "text": "Facets\nSometimes mapping variables to aesthetics can generate a lot of noise and clutter, making it hard to read or interpret a figure. One way to handle this is to split your plot into multiple plots or facets based on levels of a categorical variable like species. To do this for one categorical variable, you use the facet_wrap() function.\n\nggplot(data = penguins) +\n  geom_point(\n    mapping = aes(x = body_mass_g, y = bill_length_mm)\n  ) +\n  facet_wrap(vars(species))\n\n\n\n\nPlacing species in the vars() function tells facet_wrap() to ‚Äúsplit the plot by species.‚Äù If you want to split the plot by two categorical variables, like species and sex, use the facet_grid() function.\n\nggplot(data = penguins) + \n  geom_point(\n    mapping = aes(x = body_mass_g, y = bill_length_mm)\n  ) +\n  facet_grid(rows = vars(species), cols = vars(sex))\n\n\n\n\nEvidently, there are some penguins for whom the sex is unknown. To remove these penguins from the dataset, you can use the na.omit() function.\n\nggplot(data = na.omit(penguins)) + \n  geom_point(\n    mapping = aes(x = body_mass_g, y = bill_length_mm)\n  ) +\n  facet_grid(rows = vars(species), cols = vars(sex))\n\n\n\n\nExercises\n\nUse facet_wrap() to split the following scatterplot of the penguins data by sex.\n\nggplot(data = na.omit(penguins)) + \n  geom_point(\n    mapping = aes(x = body_mass_g, y = bill_length_mm)\n  ) + \n  facet_wrap() # <------- insert value here\n\nNow, map the species to the color aesthetic for the point geometry.\nUse facet_grid() to split the following scatterplot of the penguins data by species and island.\n\nggplot(data = na.omit(penguins)) + \n  geom_point(\n    mapping = aes(x = body_mass_g, y = bill_length_mm)\n  ) + \n  facet_grid() # <------- insert value here\n\nWhat does this tell you about how species are distributed across islands?"
  },
  {
    "objectID": "labs/02-probability-lab.html#scales",
    "href": "labs/02-probability-lab.html#scales",
    "title": "Lab 02: Statistical Graphics",
    "section": "Scales",
    "text": "Scales\nScales provide the basic structure that determines how data values get mapped to visual properties in a graph. The most obvious example is the axes because these determine where things will be located in the graph, but color scales are also important if you want your figure to provide additional information about your data. Here, we will briefly cover two aspects of scales that you will often want to change: axis labels and color palettes, in particular palettes that are colorblind safe.\nLabels\nBy default, ggplot2 uses the names of the variables in the data to label the axes. This, however, can lead to poor graphics as naming conventions in R are not the same as those you might want to use to visualize your data. Fortunately, ggplot2 provides tools for renaming the axis and plot titles. The one you are likely to use most often is probably the labs() function. Here is a standard usage:\n\nggplot(data = penguins) + \n  geom_point(\n    mapping = aes(x = body_mass_g, y = bill_length_mm)\n  ) +\n  labs(\n    x = \"Body mass (g)\",\n    y = \"Bill length (mm)\",\n    title = \"Palmer Penguins\"\n  )\n\n\n\n\nColor Palettes\nWhen you map a variable to an aesthetic property, ggplot2 will supply a default color palette. This is fine if you are just wanting to explore the data yourself, but when it comes to publication-ready graphics, you should be a little more thoughtful. The main reason for this is that you want to make sure your graphics are accessible. For instance, the default ggplot2 color palette is not actually colorblind safe. To address this shortcoming, you can specify colorblind safe color palettes using the scale_color_viridis() function from the viridis package.3 It works like this:3¬†When it comes to colors in R, the sky is the limit. As far as I am aware, the most comprehensive list of palettes for use in R is provided by the paletteer package, which attempts to collect all of the palettes scattered across the R ecosystem into one place. See also this beautiful website for creating custom color palettes: https://coolors.co/.\nggplot(data = penguins) + \n  geom_point(\n    mapping = aes(x = body_mass_g, y = bill_length_mm, color = species)\n  ) +\n  labs(\n    x = \"Body mass (g)\",\n    y = \"Bill length (mm)\",\n    title = \"Palmer Penguins\"\n  ) +\n  scale_color_viridis(\n    option = \"viridis\", \n    discrete = TRUE\n  )\n\n\n\n\n\n\n\n\n\n\nFor comparison, I‚Äôm showing the viridis scale next to the default ggplot2 scale, so you can see the difference. Two things to note about scale_color_viridis(). First, you choose a specific colorblind safe palette with the option parameter. In this case, I chose viridis, but there are others, including magma, cividis, and inferno, to name a few. Second, if the variable is continuous rather than discrete, you will have to set discrete = FALSE in the function, otherwise it will throw an error.\nExercises\n\nUsing the penguins dataset, plot body mass (y variable) by bill length (x variable) and change the axis labels to reflect this.\nUsing the penguins dataset, plot bill length (y variable) by bill depth (x variable) and change the axis labels to reflect this.\n\nUsing the code below, try out these different colorblind safe palettes from the viridis package:\n\nmagma\ncividis\ninferno\n\n\n\nggplot(data = penguins) + \n  geom_point(\n    mapping = aes(x = body_mass_g, y = bill_length_mm, color = species)\n  ) +\n  labs(\n    x = \"Body mass (g)\",\n    y = \"Bill length (mm)\",\n    title = \"Palmer Penguins\"\n  ) +\n  scale_color_viridis(\n    option = , # <------- insert value here\n    discrete = TRUE\n  )"
  },
  {
    "objectID": "labs/02-probability-lab.html#themes",
    "href": "labs/02-probability-lab.html#themes",
    "title": "Lab 02: Statistical Graphics",
    "section": "Themes",
    "text": "Themes\nTo control the display of non-data elements in a figure, you can specify a theme. This is done with the theme() function. Using this can get pretty complicated, pretty quick, as there are many many elements of a figure that can be modified, so rather than elaborate on it in detail, I want to draw your attention to pre-defined themes that you can use to modify your plots in a consistent way.\nHere is an example of the black and white theme, which removes filled background grid squares, leaving only the grid lines.\n\nggplot(data = penguins) + \n  geom_point(\n    mapping = aes(x = body_mass_g, y = bill_length_mm, color = species)\n  ) +\n  labs(\n    x = \"Body mass (g)\",\n    y = \"Bill length (mm)\",\n    title = \"Palmer Penguins\"\n  ) +\n  theme_bw()\n\n\n\n\nExercises\n\nComplete the code below, trying out each separate theme:\n\ntheme_minimal()\ntheme_classic()\ntheme_void()\n\n\n\nggplot(data = penguins) + \n  geom_point(\n    mapping = aes(x = body_mass_g, y = bill_length_mm, color = species)\n  ) +\n  labs(\n    x = \"Body mass (g)\",\n    y = \"Bill length (mm)\",\n    title = \"Palmer Penguins\"\n  ) +\n  theme_    # <------- complete function call to change theme"
  },
  {
    "objectID": "labs/02-probability-lab.html#homework",
    "href": "labs/02-probability-lab.html#homework",
    "title": "Lab 02: Statistical Graphics",
    "section": "Homework",
    "text": "Homework\n\n\nLoad data. For this homework exercise, we‚Äôll work with the DartPoints dataset from the archdata package. To load that dataset, use data(DartPoints).\n\nSummary statistics. Let‚Äôs summarize these data now.\n\nUse head() to print out the first 10 rows of the table.\nUse mean() and median() on the Length, Width, Thickness, and Weight variables. (Hint: use DartPoints$<VARIABLE> as in DartPoints$Length.)\nUse var() and sd() on the same.\nNow use skim() to summarize the DartPoints data.\n\n\n\nGraphics. And now to visualize them.\n\nUse ggplot() to make a scatterplot showing dart point length as a function of weight. (Hint: use geom_point().)\nIs there a trend?\nMap the dart point Name (this is the dart point type) to the color aesthetic. (Hint: this should go inside the aes() mapping!)\nDo you see any meaningful differences between dart point types?\nChange the size of all points to 2.5. (Hint: this should go outside the aes() mapping but inside geom_point()!)\nUse scale_color_viridis() to make the color scale colorblind safe. Feel free to use whichever palette you prefer. (Hint: dart point type is a categorical variable, so you need to set discrete = TRUE!)\nTry out facet_wrap() on the dart point Name variable.\nDoes this make it easier or harder to see differences between types?"
  },
  {
    "objectID": "labs/03-inference-lab.html",
    "href": "labs/03-inference-lab.html",
    "title": "Lab 03: Statistical Inference",
    "section": "",
    "text": "This lab will guide you through the process of\n\nmaking a tidy data.frame\nvisualizing distributions with histograms\nrunning a t-test\nperforming an ANOVA\n\n\narchdata\nggplot2\npalmerpenguins\nskimr\n\nMake sure to load these into your R session with library(). This should always go at the start of your document!\n\nlibrary(archdata)\nlibrary(ggplot2)\nlibrary(palmerpenguins)\nlibrary(skimr)\n\n\n\n\npenguins\n\nIncludes measurements for penguin species, island in Palmer Archipelago, size (flipper length, body mass, bill dimensions), and sex.\npackage: palmerpenguins\n\nreference: https://allisonhorst.github.io/palmerpenguins/reference/penguins.html\n\n\n\n\nDartPoints\n\nIncludes measurements of 91 Archaic dart points recovered during surface surveys at Fort Hood, Texas.\npackage: archdata\n\nreference: https://cran.r-project.org/web/packages/archdata/archdata.pdf\n\n\n\n\ntitanic\n\nProvides information on the fate of passengers of the Titanic, including economic status, sex, age, and survival.\npackage: none\nreference: https://wilkelab.org/SDS375/\n\nnote: we‚Äôre going to download this one rather than get it from a package\n\n\n\nSnodgrass\n\nIncludes measurements of size, location, and contents of 91 pit houses at the Snodgrass site in Butler County, Missouri.\nreference: https://cran.r-project.org/web/packages/archdata/archdata.pdf"
  },
  {
    "objectID": "labs/03-inference-lab.html#data-frames",
    "href": "labs/03-inference-lab.html#data-frames",
    "title": "Lab 03: Statistical Inference",
    "section": "Data Frames",
    "text": "Data Frames\n\n\n\n\n\nFigure¬†1: Elements of a Data Frame.\n\n\nConsider this research scenario: you have a museum collection of projectile points that you want to use for an analysis, maybe you want to know whether East Gate and Rose Spring points are actually the same type of point, commonly referred to using the portmanteau Rosegate. Following tradition, you think maybe it‚Äôs the size and shape that will you help finally put this old question to rest, so for each individual point, you set about measuring its length, width, and height.\nIn this course, we‚Äôll refer to each individual measure that you make as a value, each set of such measures for an individual point we will call (somewhat awkwardly) an observation, and each individual type of measurement will be a variable. It is almost certainly the case that you will be collecting this data and storing it in something like a spreadsheet or table, like that shown in Figure¬†1. You will sometimes hear data scientists refer to data stored in this way as rectangular data. All they mean by that is that the data come as collections of values organized into rows and columns of equal length. While data may admit of many different ways of being organized into rows and columns, here we will focus on a rectangular format known as tidy data. As defined by Hadley Wickham in R for Data Science (2e), tidy data must follow three simple rules:\n\nEach variable must have its own column,\nEach observation must have its own row, and\nEach value must have its own cell.\n\nIt is important to note, as Wickham cautions, that the opposite of tidy data is not necessarily messy data, for data can come in many formats (sound and video, for example). However, when we want to conduct some statistical analysis, and especially when we want to conduct such an analysis in R, we will almost certainly want our data to be tidy.\nCreating tables\nIn R, tabular datasets are known as data frames. To create a data frame, we use the eponymous data.frame() function. Here, for example, is how we would create the table in Figure¬†1 above:\n\nprojectiles <- data.frame(\n  type = c(\"Elko\", \"Rosegate\", \"DSN\", \"Elko\", \"Clovis\"),\n  length = c(2.03, 1.4, 1.9, 2.1, 3.3),\n  width = c(0.8, 0.4, 0.3, 0.7, 0.95),\n  height = c(3.23, 2.4, 1.29, 2.7, 4.15)\n)\n\nprojectiles\n\n      type length width height\n1     Elko   2.03  0.80   3.23\n2 Rosegate   1.40  0.40   2.40\n3      DSN   1.90  0.30   1.29\n4     Elko   2.10  0.70   2.70\n5   Clovis   3.30  0.95   4.15\n\n\nNote that the values (or measurements) contained in each variable are wrapped in the c() function (short for concatenate). This variable tells R that all these values belong to this one variable. Each variable in turn is an argument to data.frame() having the form <variable> = c(<value-1>, <value-2>, ..., <value-n>).\nGetting basic meta-data from tables\nWhen you want to know what variables a table includes, you can use the names() function.\n\nnames(projectiles)\n\n[1] \"type\"   \"length\" \"width\"  \"height\"\n\n\nIf you want to know how many variables or observations the table has, you can use nrow() and ncol() respectively.\n\n# number of observations\nnrow(projectiles)\n\n[1] 5\n\n# number of variables\nncol(projectiles)\n\n[1] 4\n\n\nExercises\n\nGet the names of the variables in the penguins table with names().\nHow many observations and variables are in this dataset? Hint: use nrow() and ncol().\nExtract the bill_length_mm variable from this table and assign it to an object called bill_length. Do the same for bill_depth_mm and call it bill_depth. You can use either <table>[[<variable>]] or <table>$<variable>."
  },
  {
    "objectID": "labs/03-inference-lab.html#histograms",
    "href": "labs/03-inference-lab.html#histograms",
    "title": "Lab 03: Statistical Inference",
    "section": "Histograms",
    "text": "Histograms\nA histogram is an excellent aid for visualizing the distribution of numerical data. Making one involves ‚Äúbinning‚Äù a continuous variable, counting the number of its values that fall into each bin, then drawing a rectangle for each bin whose height is proportional to the count for that bin. A good example of this is the distribution of a population over age ranges, like the age distribution of the passengers on the ill-fated voyage of the Titanic. Before we get to what the histogram looks like for this macabre example, let‚Äôs first have a look at the raw data:11¬†These data come from Claus Wilke‚Äôs Fundamentals of Data Visualization.\n\n\n\n\n\n\nWarning\n\n\n\n\n\nTo be able to work through the examples in this section, you need to run the code below. We‚Äôll explain what this code does in another lab.\n\ntitanic <- read.csv(\"https://raw.githubusercontent.com/wilkelab/SDS375/master/datasets/titanic.csv\")\n\n\n\n\n\nhead(titanic)\n\n  class   age    sex survived\n1   1st 29.00 female survived\n2   1st  2.00 female     died\n3   1st 30.00   male     died\n4   1st 25.00 female     died\n5   1st  0.92   male survived\n6   1st 47.00   male survived\n\n\nYou see that our data has an age for each individual passenger. If we bin those ages into 5 year intervals (0-5, 5-10, 10-15, and so on) and count the number of passengers that fall into each bin or age range, we get a summary table that looks like this.\n\n\n\n\n  \n  \n\nAge Range\n      Count\n       \n      Age Range\n      Count\n    \n\n\n0‚Äì5\n36\n   \n41‚Äì45\n54\n\n\n6‚Äì10\n19\n   \n46‚Äì50\n50\n\n\n11‚Äì15\n18\n   \n51‚Äì55\n26\n\n\n16‚Äì20\n99\n   \n56‚Äì60\n22\n\n\n21‚Äì25\n139\n   \n61‚Äì65\n16\n\n\n26‚Äì30\n121\n   \n66‚Äì70\n3\n\n\n31‚Äì35\n76\n   \n71‚Äì75\n3\n\n\n36‚Äì40\n74\n   \n76‚Äì80\n0\n\n\n\n\n\n\nWe can actually visualize this distribution with a histogram using ggplot() and geom_histogram(). Importantly, we pass ggplot() the raw tidy data, not the summary table. Here is how that looks:\n\nggplot(titanic) + \n  geom_histogram(\n    aes(age)\n  )\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\nNotice that ggplot2 provides an informative message that the default number of bins used in this plot is 30, which makes the age interval for each bin about 2.5 years. We can (and SHOULD!) change this by specifying a different number of bins with bins, as the number of bins can dramatically change the interpretation of the distribution.\n\nggplot(titanic) +\n  geom_histogram(\n    aes(age),\n    bins = 10\n  )\n\n\n\n\n\n\n\nUnfortunately, the default histogram produced by ggplot2 doesn‚Äôt do a great job of conveying the relationship between the axis text and the ranges represented by the widths of the rectangles. For instance, zero falls at the center of the first histogram, which would seem to suggest that the range of values represented by that rectangle includes negative ages. What it is really saying is that zero falls in the range of that bin, but that is not obvious. One solution to this issue is to set the binwidth (or bin width, as in the range of values for that bin) and the bin boundary (or where the left side of the rectangle is positioned relative to the bin range). That‚Äôs a tad confusing, I know, but the basic idea is that if you set the width to 5 and the boundary to 0, then the bin for the range 0-5 will start at 0, and the bin for the range 5-10 will start at 5, and the bin for 10-15 will start at 10, and so on. Finding the best options for visualizing your data will, of course, involve some trial and error.\n\nggplot(titanic) +\n  geom_histogram(\n    aes(age),\n    binwidth = 5,\n    boundary = 0\n  )\n\n\n\n\n\n\n\nAs always, you can change the fill and outline color for these plots by supplying those parameters to the geometry. We can also clean up the labels with labs() and change the theme if we like (and we do). And since the heights of these bars are what we care about, we can turn off the light vertical grid lines that ggplot2 adds by default by specifying this as a theme() option.\n\nggplot(titanic) +\n  geom_histogram(\n    aes(age),\n    binwidth = 5,\n    boundary = 0,\n    fill = \"#56B4E9\",\n    color = \"#01587A\"\n  ) +\n  labs(\n    x = \"Age (years)\",\n    y = \"Count\",\n    title = \"Age Distribution\",\n    subtitle = \"Passengers of the Titanic\"\n  ) +\n  theme_minimal(14) +\n  theme(\n    panel.grid.major.x = element_blank(),\n    panel.grid.minor.x = element_blank()\n  )\n\n\n\n\n\n\n\nOne last point before moving on. You may have noticed that the data contains a binary variable survived. Do you think maybe the age distribution of those that survived differs from the distribution for those that did not survive? Do you think maybe they were older? Younger? Or is there no difference? Let‚Äôs explore this a little by visualizing the different distributions using facet_wrap().\n\nggplot(titanic) +\n  geom_histogram(\n    aes(age),\n    binwidth = 5,\n    boundary = 0,\n    fill = \"#56B4E9\",\n    color = \"#01587A\"\n  ) +\n  labs(\n    x = \"Age (years)\",\n    y = \"Count\",\n    title = \"Age Distribution\",\n    subtitle = \"Passengers of the Titanic\"\n  ) +\n  facet_wrap(vars(survived)) + # <----- adding facets here\n  theme_minimal(14) +\n  theme(\n    panel.grid.major.x = element_blank(),\n    panel.grid.minor.x = element_blank()\n  )\n\n\n\n\n\n\n\nHmmmm ü§î. What do you think this means? And how might it help to compare this to the total distribution?\nExercises\n\nCreate a histogram of penguin bill length using the penguins dataset. Then do all of the following:\n\nChange the number of bins (try two different options).\nTry specifying the bin width and boundary.\nChange the fill and outline color.\nReset the labels to be more informative.\nChange the theme and remove the vertical grid lines.\nFacet the distributions by penguin species.\n\n\nRepeat (1), but use the DartPoints dataset from the archdata package, creating a histogram of dart length (Length in the table) and facet by dart type (Name).\n\nDoes it look like the dart types might differ in length? Or maybe they‚Äôre all basically the same?"
  },
  {
    "objectID": "labs/03-inference-lab.html#t-test",
    "href": "labs/03-inference-lab.html#t-test",
    "title": "Lab 03: Statistical Inference",
    "section": "t-test",
    "text": "t-test\nWe use a t-test (technically, Welch‚Äôs two-sample t-test) to evaluate whether two samples come from the same population. This test starts by computing the \\(t\\) statistic, or the standardized difference in sample means:\n\\[t = \\frac{\\bar{x}_1 - \\bar{x}_2}{s_{\\bar{x}}}\\]\nwhere \\(s_{\\bar{x}}\\) is the standard error of the sample mean. This is then compared to a t-distribution to determine the probability of that difference - the more improbable, the less likely the samples come from the same distribution. Why is that? Well, if you think about it, we are starting from the assumption that the null hypothesis is true. Then we are asking, if the null hypothesis is true, how likely is that we would get this difference in sample means? And, if it‚Äôs extremely unlikely, that would presumably count against the null model being true.\nTo perform this test in R, we use the t.test() function, providing it with two samples. Suppose, for example, that we have two samples of projectile points, namely their lengths in millimeters, and we want to answer this question: Are these samples of the same point type (the same population)? Here are our two samples:\n\nsample1 <- c(59.10, 61.97, 56.23, 53.83, 60.24, 44.27, 61.41, 55.07, 55.01, 50.56)\nsample2 <- c(58.42, 68.09, 60.85, 61.60, 57.25, 63.08, 58.57, 57.34, 64.60, 60.49)\n\nmean(sample1) - mean(sample2)\n\n[1] -5.26\n\n\nBefore we run the test, let‚Äôs make sure we have our hypotheses clear.\n\n\n\\(H_0\\): There is NO difference in mean length (\\(\\bar{x}_1 = \\bar{x}_2\\))\n\n\\(H_1\\): There is a difference in mean length (\\(\\bar{x}_1 \\neq \\bar{x}_2\\))\n\nWe also need to specify our critical value. Here, we‚Äôll stick with a common standard of 0.05:\n\n\\(\\alpha = 0.05\\)\n\nNow, we can run our test!\n\nt.test(sample1, sample2)\n\n\n    Welch Two Sample t-test\n\ndata:  sample1 and sample2\nt = -3, df = 15, p-value = 0.02\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -9.598 -0.922\nsample estimates:\nmean of x mean of y \n     55.8      61.0 \n\n\nNotice the output that R has provided here. You have the name of the test: Welch Two Sample t-test (this is a version of Student‚Äôs t-test for two independent samples). It gives you the t-statistic, the degrees of freedom (df), and the p-value. It also states the alternative hypothesis and gives you the mean of each sample. In this case, \\(p < \\alpha\\). Hence, we reject the null. A significant difference exists between the means of these two samples.\nIf your samples are in a data.frame, you can also call t.test() using R‚Äôs formula syntax. So, assume your data above are in a table like so:\n\nn <- length(sample1)\n\nsamples <- data.frame(\n  sample = c(rep(1, n), rep(2, n)),\n  length = c(sample1, sample2)\n)\n\nhead(samples)\n\n  sample length\n1      1   59.1\n2      1   62.0\n3      1   56.2\n4      1   53.8\n5      1   60.2\n6      1   44.3\n\n\nThen you would run the t.test() this way:\n\nt.test(length ~ sample, data = samples)\n\n\n    Welch Two Sample t-test\n\ndata:  length by sample\nt = -3, df = 15, p-value = 0.02\nalternative hypothesis: true difference in means between group 1 and group 2 is not equal to 0\n95 percent confidence interval:\n -9.598 -0.922\nsample estimates:\nmean in group 1 mean in group 2 \n           55.8            61.0 \n\n\nThe formula in this example is length ~ sample. The tilde expresses a relation of dependence. In this case, we want to know whether the length of a point differs in some meaningful way between samples. So, here we are, in effect, asking R to run a t-test comparing the mean length in each sample using the samples dataset. You will notice that the results are the same, we just called the function in a slightly different way because of how we had the data stored.\nExercises\n\nPerform a t-test on these two samples of dart length (drawn from the DartPoints dataset).\n\nCalculate the mean for each sample.\n\nBe sure to specify the null and alternate hypotheses.\n\nState the critical value.\n\nRun the test and print the results.\n\n\n\n\ndarl <- c(42.8,40.5,37.5,40.3,30.6,41.8,40.3,48.5,47.7,33.6,32.4,42.2,33.5,\n          41.8,38,35.5,31.2,34.5,33.1,32,38.1,47.6,42.3,38.3,50.6,54.2,44.2,40)\ntravis <- c(56.5,54.6,46.3,57.6,49.1,64.6,69,40.1,41.5,46.3,39.6)\n\n\nBased on the test, do you accept or reject the null hypothesis? Why?"
  },
  {
    "objectID": "labs/03-inference-lab.html#anova",
    "href": "labs/03-inference-lab.html#anova",
    "title": "Lab 03: Statistical Inference",
    "section": "ANOVA",
    "text": "ANOVA\nLike the t-test, the ANOVA is used to test for a difference between samples, to see whether they come from the same population. Unlike the t-test, however, the ANOVA can be used on more than two samples. It does that by decomposing the total variance into within and between group variance. The ratio of those standardized variances defines the F-statistic:\n\\[F = \\frac{\\text{between-group variance}}{\\text{within-group variance}}\\]\nBy comparing the value of \\(F\\) to the F-distribution, we can evaluate the probability of getting that value. A highly improbable F-statistic means that at least one group or sample comes from a different population. To perform this test in R, we use the aov() function.\nTo illustrate this method, let‚Äôs return to the DartPoints dataset, which contains various measures for five different dart point types (Darl, Ensor, Pedernales, Travis, and Wells). We‚Äôll try to answer the question: Are these samples from the same dart point type (the same population)? Less formally, we want to know whether the types we have lumped these darts into are really all that different. As always, we first specify our null and alternate hypotheses and the critical value we will use to determine whether to reject the null.\n\n\n\\(H_0\\): There is no difference in length between groups.\n\n\\(H_1\\): At least one group differs in length.\n\nAnd our critical value, again, will be 0.05.\n\n\\(\\alpha = 0.05\\)\n\nNow, we can conduct our test.\n\ndata(DartPoints)\n\naov_test <- aov(Length ~ Name, data = DartPoints)\n\nThere are two things to note here. First, we are assigning the output of this ANOVA test to an object. Second, we call the test using a formula, in this case Length ~ Name. The full call to aov() you can read as saying, ‚ÄúPerform an analysis of variance comparing the lengths for each sample in this dataset.‚Äù\nANOVA Table\nIf we print out the result of the aov() call, it looks like this:\n\naov_test\n\nCall:\n   aov(formula = Length ~ Name, data = DartPoints)\n\nTerms:\n                Name Residuals\nSum of Squares  5532      9067\nDeg. of Freedom    4        86\n\nResidual standard error: 10.3\nEstimated effects may be unbalanced\n\n\nWhile this printout offers some important information, a much more useful ummary of the test is provided by an ANOVA table. We can generate one of these by running the summary() function on our test.\n\nsummary(aov_test)\n\n            Df Sum Sq Mean Sq F value      Pr(>F)    \nName         4   5532    1383    13.1 0.000000022 ***\nResiduals   86   9067     105                        \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThis table has two rows, one for the between-group variance and one for the within-group variance respectively. As you can see, R refers to these, somewhat cryptically, as the Name (the grouping variable in the data) and Residuals. The reason for this concerns the fact that the aov() function is actually fitting a linear model, but let‚Äôs leave that detail to the side. For now, just note that the columns are, in order,\n\n\nDf = the degrees of freedom,\n\n\nSum Sq = the sum of squares,\n\n\nMean Sq = the mean sum of squares (the sum of squares divided by the degrees of freedom),\n\n\nF value = the F-statistic (the ratio of the mean sum of squares), and\n\n\nPr(>F) = the p-value, formally the probability of getting a value of the statistic greater than observed (determined by comparing the F-statistic to the F-distribution).\n\nIn this case, \\(p < \\alpha\\), so we reject the null hypothesis, meaning that at least one of these groups is different.\nExercises\n\nPerform an ANOVA on the DartPoints dataset to see if at least one dart type differs signficantly in its Width.\n\nBe sure to specify the null and alternate hypotheses.\nState the critical value.\nRun the test and print the results.\nBe sure to assign the test to an object.\nProvide a summary() of the test.\n\n\nBased on the test, do you accept or reject the null hypothesis? Why?"
  },
  {
    "objectID": "labs/03-inference-lab.html#homework",
    "href": "labs/03-inference-lab.html#homework",
    "title": "Lab 03: Statistical Inference",
    "section": "Homework",
    "text": "Homework\n\nLoad the Snodgrass dataset from the archdata package using data(Snodgrass). This dataset includes measurements of pithouses found in a small village affiliated with maize farmers in Missouri about 650 years ago.\nGenerate a summary table using the skim() function from skimr.\nFor practice, let‚Äôs get some of those summary statistics manually. Calculate the mean and standard deviation of the inside floor area of each pithouse using mean() and sd(). You‚Äôll need to pull the Area variable out of the table with Snodgrass$Area. Hint: if you were to calculate the variance, you would use var(Snodgrass$Area).\nUse ggplot() and geom_histogram() to plot a histogram showing the distribution of floor area. Make sure to do all of the following (only need to make one graph):\n\nChange the default number of bins (or optionally, you can specify the breaks).\nChange the fill and outline color.\nUpdate the labels and the plot title.\nChoose an appropriate theme and remove the vertical grid lines.\nFacet by the Inside variable. Then try faceting using the Segment variable.\nDo the distributions look different?\n\n\nSome of these pithouses occur inside a walled-in portion of the village. This information is provided by the Inside variable in the dataset. Run a t-test to determine whether the mean floor area of pithouses inside the walled-in area differs significantly from the mean floor area of pithouses outside the wall.\n\nThese data are in a table, so make sure to use the formula notation.\nState the null and alternative hypotheses.\nSpecify the critical value.\nRun the test and print the result.\nDoes the test confirm or refute the null hypothesis? Why?\nHow would you interpret this result?\n\n\nThe pithouses were initially divided into three groups or ‚Äúsegments‚Äù (Segment in the table) based on their location: segment one includes those inside the wall, segment two those to the north and west of the wall, and segment three those to the east and south of the wall. Run an ANOVA test to determine whether mean floor area for these groups is significantly different.\n\nState the null and alternative hypotheses.\nSpecify the critical value.\nRun the test and print the result.\nBe sure to assign the result to an object.\nProvide a summary() of the test.\nDoes the test confirm or refute the null hypothesis? Why?\nHow would you interpret this result?"
  },
  {
    "objectID": "labs/04-ols-lab.html",
    "href": "labs/04-ols-lab.html",
    "title": "Lab 04: Ordinary Least Squares",
    "section": "",
    "text": "This lab will guide you through the process of\n\nindexing data.frames with base R\nvisualizing distributions with density plots\ncalculating covariance\ncalculating correlation and evaluating with the t-test\nbuilding a simple linear model\n\nthe formula notation\nthe lm() function\nthe model summary()\n\n\n\n\nWe will be using the following packages:\n\narchdata\nggplot2\npalmerpenguins\nviridis\n\n\nlibrary(archdata)\nlibrary(ggplot2)\nlibrary(palmerpenguins)\nlibrary(viridis)\n\n\n\n\ncars\n\nIncludes measurements of car speed and stopping distance.\npackage: datasets\n\nreference: https://stat.ethz.ch/R-manual/R-devel/library/datasets/html/cars.html\n\n\n\n\nDartPoints\n\nIncludes measurements of 91 Archaic dart points recovered during surface surveys at Fort Hood, Texas.\npackage: archdata\n\nreference: https://cran.r-project.org/web/packages/archdata/archdata.pdf\n\n\n\n\nOxfordPots\n\nIncludes percentages of late Romano-British Oxford Pottery on 30 sites, along with their distance from Oxford.\nreference: https://cran.r-project.org/web/packages/archdata/archdata.pdf\n\n\n\n\npenguins\n\nIncludes measurements for penguin species, island in Palmer Archipelago, size (flipper length, body mass, bill dimensions), and sex.\npackage: palmerpenguins\n\nreference: https://allisonhorst.github.io/palmerpenguins/reference/penguins.html\n\n\n\n\ntitanic\n\nProvides information on the fate of passengers of the Titanic, including economic status, sex, age, and survival.\npackage: none\nreference: https://wilkelab.org/SDS375/\n\nnote: we‚Äôre going to download this one rather than get it from a package"
  },
  {
    "objectID": "labs/04-ols-lab.html#indexing-tables-with-base-r",
    "href": "labs/04-ols-lab.html#indexing-tables-with-base-r",
    "title": "Lab 04: Ordinary Least Squares",
    "section": "Indexing tables with base R",
    "text": "Indexing tables with base R\n\n\n\n\n\nFigure¬†1: Elements of a Data Frame.\n\n\nIt will often be the case that you do not need all the data in a table, but only a subset of rows and columns. To get the rows and columns you want, though, you need to be able to, as it were, point to them or point R to them. Another word for this is indexing.\nLet‚Äôs start with the use of square brackets, [,]. The basic idea here is that you can take a table and index it by row and column by appending the square brackets to it. The basic syntax is this:\ntable[row,column]\nAs an example, let‚Äôs say we are working with our simple projectile point table:\n\nprojectiles <- data.frame(\n  type = c(\"Elko\", \"Rosegate\", \"DSN\", \"Elko\", \"Clovis\"),\n  length = c(2.03, 1.4, 1.9, 2.1, 3.3),\n  width = c(0.8, 0.4, 0.3, 0.7, 0.95),\n  height = c(3.23, 2.4, 1.29, 2.7, 4.15)\n)\n\nprojectiles\n\n      type length width height\n1     Elko   2.03  0.80   3.23\n2 Rosegate   1.40  0.40   2.40\n3      DSN   1.90  0.30   1.29\n4     Elko   2.10  0.70   2.70\n5   Clovis   3.30  0.95   4.15\n\n\nAnd maybe we want the value at the 3rd row and 2nd column, so we‚Äôre wanting the length of that particular desert side-notched (or DSN). Here is one way to do that with just the numeric position (or coordinates) of that value:\n\nprojectiles[3,2]\n\n[1] 1.9\n\n\nWhile we did specify both a row and a column in this example, that is not required.\n\nprojectiles[3,]\n\n  type length width height\n3  DSN    1.9   0.3   1.29\n\nprojectiles[,2]\n\n[1] 2.03 1.40 1.90 2.10 3.30\n\n\n\n\n\n\n\n\nDanger\n\n\n\n\n\nDid you notice the different outputs? projectiles[3,] returns a data.frame, but projectiles[,2] returns a vector. This is a ‚Äúgotcha‚Äù in R, a little bit of unexpected behavior. The most common situation in which indexing returns a vector from a data.frame is when a single variable is selected. Sometimes getting just the variable is intentional (see below), but other times it is not, so it‚Äôs worth being aware of.\n\n\n\nWe can also subset multiple rows and columns, though this requires that we use vectors of data, and not just single values. A useful tool in this regard is the colon, :, which allows you to create a sequence of integers, starting with the number on the left and proceeding by one to the number on the right.\n\n1:3\n\n[1] 1 2 3\n\n\nNow, we can use this syntax to index the last four rows of our table and the first three columns.\n\nprojectiles[2:5, 1:3]\n\n      type length width\n2 Rosegate    1.4  0.40\n3      DSN    1.9  0.30\n4     Elko    2.1  0.70\n5   Clovis    3.3  0.95\n\n\nIf we want to get rows or columns that are not next to each other in the table, we can use the c() function, as in concatenate.\n\nc(1,2,4)\n\n[1] 1 2 4\n\n\nWhen applied to the projectiles table, we get the following.\n\nprojectiles[c(2,4), c(1,2,4)]\n\n      type length height\n2 Rosegate    1.4    2.4\n4     Elko    2.1    2.7\n\n\nImportantly, you can also index columns by name.\n\nprojectiles[1:3, c(\"type\", \"length\")]\n\n      type length\n1     Elko   2.03\n2 Rosegate   1.40\n3      DSN   1.90\n\n\nOne advantage of using names rather than numbers is that it is much more readable as it is not immediately obvious with numbers what columns you are actually selecting. More importantly, though, using names is more robust. Were the length column for whatever reason to move to the right of the height column, its numeric position in the table would be 4, not 2. So, using projectiles[,2] will work to index the length variable only if length is at that position. Using projectiles[,\"length\"] to index it will work either way, though, regardless of the numeric position of that variable.\nSo, that‚Äôs pretty much the basics of indexing rectangular data with base R. Before moving on, though, let‚Äôs talk about one additional thing you might want to do with a data.frame, and that‚Äôs extract an entire variable or column. There are two primary ways to achieve this. You can use double brackets, <table>[[<variable>]], or you can use the dollar-sign operator, <table>$<variable>.\n\nprojectiles[[\"type\"]]\n\n[1] \"Elko\"     \"Rosegate\" \"DSN\"      \"Elko\"     \"Clovis\"  \n\nprojectiles$length\n\n[1] 2.03 1.40 1.90 2.10 3.30\n\n\nNote that you can and often will want to assign these to their own objects, so you can use them again later.\n\np_type <- projectiles[[\"type\"]]\n\np_length <- projectiles$length\n\nAnd, if you want, you can index specific values in the vector as you would rows in the table.\n\nprojectiles$length\n\n[1] 2.03 1.40 1.90 2.10 3.30\n\nprojectiles$length[c(1,2,4)]\n\n[1] 2.03 1.40 2.10\n\n\nExercises\n\nGet the names of the variables in the penguins table with names().\nHow many observations and variables are in this dataset? Hint: use nrow() and ncol().\nTry indexing multiple rows and columns of the penguins data using the square brackets with row numbers and column names, for example, penguins[1:25, c(\"species\", \"island\", \"body_mass_g\")]. Try doing this a couple of different ways.\nExtract the bill_length_mm variable from this table and assign it to an object called bill_length. Do the same for bill_depth_mm and call it bill_depth. You can use either <table>[[<variable>]] or <table>$<variable>."
  },
  {
    "objectID": "labs/04-ols-lab.html#density-plots",
    "href": "labs/04-ols-lab.html#density-plots",
    "title": "Lab 04: Ordinary Least Squares",
    "section": "Density Plots",
    "text": "Density Plots\nHere you will learn how to generate a ‚Äúdensity‚Äù plot or a plot of the underlying probability density of a variable using ggplot() and geom_density(). This is similar to a histogram in that it seeks to show the distribution of a continuous random variable. It differs, however, in the way it goes about doing that. To see how they differ, let‚Äôs first have a look at these two plots of the titanic data. The one on the left is a histogram, the one on the right a density plot.\n\n\n\n\n\n\nWarning\n\n\n\n\n\nTo be able to work through the examples in this section, you need to run the code below. We‚Äôll explain what this code does in another lab.\n\ntitanic <- read.csv(\"https://raw.githubusercontent.com/wilkelab/SDS375/master/datasets/titanic.csv\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe big difference here is that histograms discretize a sample using bins, counts the number of observations that fall into each bin, and then represents those counts using the heights of the rectangles. A density plot uses a method known as kernel density estimate (or KDE) to estimate the density of each observation and represents it using the height of a smooth and continuous curve. The KDE approach works like this. First, you set each data point as the mean of a distribution, typically the normal or Gaussian distribution (also called the kernel). Each distribution is assumed to have the same varianc eor standard deviation (called the bandwidth), which is set to some arbitrary value. The heights of the kernels are then summed to produce a curve like the one above.\nAs with the histogram, we specify a density geometry for ggplot using geom_density().\n\nggplot(titanic, aes(age)) + \n  geom_density()\n\n\n\n\nAgain, we can specify different aesthetics like fill and color and update the labels with labs().\n\nggplot(titanic, aes(age)) + \n  geom_density(\n    fill = \"#A8BFF0\", \n    color = \"#183C8C\"\n  ) + \n  labs(\n    x = \"Age\",\n    y = \"Density\",\n    title = \"Age Distribution\",\n    subtitle = \"Passengers of the Titanic\"\n  )\n\n\n\n\nWe can also map these aesthetics to other variables like the sex of the passenger.\n\nggplot(titanic, aes(age, fill = sex)) + \n  geom_density() + \n  labs(\n    x = \"Age\",\n    y = \"Density\",\n    title = \"Age Distribution\",\n    subtitle = \"Passengers of the Titanic\"\n  )\n\n\n\n\nAnd, we can change the default fill colors using scale_fill_manual(), too.\n\nggplot(titanic, aes(age, fill = sex)) + \n  geom_density() + \n  scale_fill_manual(\n    values = c(\"#A8BFF0\", \"#FFE66D\")\n  ) +\n  labs(\n    x = \"Age\",\n    y = \"Density\",\n    title = \"Age Distribution\",\n    subtitle = \"Passengers of the Titanic\"\n  )\n\n\n\n\nIn this case, however, it‚Äôs hard to see where the two distributions overlap, which makes it hard to compare them. One way to fix this is to change the opacity of the fill color using the alpha() function (note that the alpha or transparency of a color can range from 0 to 1.).\n\nggplot(titanic, aes(age, fill = sex)) + \n  geom_density() + \n  scale_fill_manual(\n    values = alpha(c(\"#A8BFF0\", \"#FFE66D\"), 0.5)\n  ) +\n  labs(\n    x = \"Age\",\n    y = \"Density\",\n    title = \"Age Distribution\",\n    subtitle = \"Passengers of the Titanic\"\n  )\n\n\n\n\nThis is still a little hard to read, so let‚Äôs try faceting instead of alpha. Let‚Äôs also drop the background vertical grid lines using the theme() function. At the same time, we‚Äôll go ahead and drop the label ‚Äúsex‚Äù from the legend as that should be obvious from the key. We do that by setting name = NULL in scale_fill_manual().\n\nggplot(titanic, aes(age, fill = sex)) + \n  geom_density() + \n  scale_fill_manual(\n    name = NULL,\n    values = c(\"#A8BFF0\", \"#FFE66D\")\n  ) +\n  labs(\n    x = \"Age\",\n    y = \"Density\",\n    title = \"Age Distribution\",\n    subtitle = \"Passengers of the Titanic\"\n  ) +\n  facet_wrap(~sex) +\n  theme_minimal() +\n  theme(\n    panel.grid.major.x = element_blank(),\n    panel.grid.minor.x = element_blank(),\n  )\n\n\n\n\nNotice that the facet labels are redundant with the legend key here, so let‚Äôs remove those, too. We do that by setting the theme arguments strip.background and strip.text to element_blank(). Finally, we can move the legend to the bottom of the plot and make it horizontal with legend.position and legend.direction respectively.\n\nggplot(titanic, aes(age, fill = sex)) + \n  geom_density() + \n  scale_fill_manual(\n    name = NULL,\n    values = c(\"#A8BFF0\", \"#FFE66D\")\n  ) +\n  labs(\n    x = \"Age\",\n    y = \"Density\",\n    title = \"Age Distribution\",\n    subtitle = \"Passengers of the Titanic\"\n  ) +\n  facet_wrap(~sex) +\n  theme_minimal() +\n  theme(\n    legend.position = \"bottom\",\n    legend.direction = \"horizontal\",\n    panel.grid.major.x = element_blank(),\n    panel.grid.minor.x = element_blank(),\n    strip.background = element_blank(),\n    strip.text = element_blank()\n  )\n\n\n\n\nExercises\n\nMake a kernel density plot of penguin bill length using ggplot() and geom_density(). Then make all of the following changes:\n\nMap penguin species to the fill aesthetic.\nUpdate the axis labels and plot title using labs().\nUse scale_fill_viridis to use colorblind safe colors for the fill. Note! Species is a discrete or categorical variable, so make sure to set discrete = TRUE!\nUse facet_wrap() to facet by species.\nChoose a suitable theme, like theme_minimal().\nRemove vertical grid lines.\nChange the legend position to bottom and make it horizontal.\nRemove strip text and background."
  },
  {
    "objectID": "labs/04-ols-lab.html#bivariate-statistics",
    "href": "labs/04-ols-lab.html#bivariate-statistics",
    "title": "Lab 04: Ordinary Least Squares",
    "section": "Bivariate Statistics",
    "text": "Bivariate Statistics\nBivariate statistics provide simple measures of the relationship between two variables. Here we will learn how to calculate two such statistics in R: covariance and correlation. These allow us to describe the direction of the relationship (is it positive or negative?) and the strength of the relationship (is it strong or weak?). In this case, we‚Äôll investigate the relationship between penguin body mass and bill length. We‚Äôll be asking this question: Is there a relationship between bill length and body mass? Is it positive or negative?\nBefore we do that, however, it is useful to visualize our data. Since we are concerned with a potential relationship, we will use a scatterplot, or a cloud of points arrayed along the dimensions of two variables, in this case body mass and bill length.\n\nggplot(penguins, aes(x = body_mass_g, y = bill_length_mm)) + \n  geom_point(\n    shape = 21,\n    fill = \"#A8BFF0\",\n    color = \"#15357A\",\n    size = 2\n  ) +\n  labs(\n    x = \"Body Mass (g)\",\n    y = \"Bill Length (mm)\",\n    title = \"Palmer Penguins\"\n  ) +\n  theme_bw()\n\n\n\n\nWhat does this tell you about the relationship between these variables?\nCovariance\nCovariance provides a measure of the extent to which two variables vary together. The sign of the covariance reflects a positive or negative trend, but not magnitude. To calculate this value in R, use the cov() function.\n\nbill_length <- penguins$bill_length_mm\nbody_mass_g <- penguins$body_mass_g\n\ncov(bill_length, body_mass_g, use = \"complete.obs\") # complete.obs means ignore NA values\n\n[1] 2606\n\n\nThis is a positive number, meaning the relationship between bill length and body mass is positive (the one tends to increase as the other increases). The size of the number by itself is unhelpful, however, and cannot be used to infer anything about the strength of the relationship. That is because covariance is sensitive to the unit of measure. If, for example, we convert body_mass from grams to kilograms, we will get a different covariance statistic.\n\n# convert to kilograms by dividing by 1000\nbody_mass_kg <- body_mass_g/1000\n\ncov(bill_length, body_mass_kg, use = \"complete.obs\")\n\n[1] 2.61\n\n\nCorrelation\nTo remove units of measure and prevent resulting changes in the magnitude of the covariance, we can scale the covariance by the standard deviations of the samples. The resulting value is known as Pearson‚Äôs Correlation Coefficient, which ranges from -1 to 1.\n\ncor(bill_length, body_mass_g, use = \"complete.obs\")\n\n[1] 0.595\n\n\nJust to demonstrate that this isn‚Äôt sensitive to units of measure, let‚Äôs see what happens when use body mass measures in kilograms.\n\ncor(bill_length, body_mass_kg, use = \"complete.obs\")\n\n[1] 0.595\n\n\nThere‚Äôs no change! In either case, the resulting coefficient is greater than zero, suggesting a positive trend, but is this value significantly different than zero? To answer that question, we can convert this coefficient to a t-statistic and compare it to a t-distribution. This is done with the cor.test() function. For this test, we have the following hypotheses:\n\n\n\\(H_0\\): the coefficient is equal to zero\n\n\\(H_1\\): the coefficient is not equal to zero\n\nAnd, of course, we must stipulate a critical value. In this case, we will stick with tradition:\n\\(\\alpha = 0.05\\)\nSo, now, here is our test:\n\ncor.test(bill_length, body_mass_g, use = \"complete.obs\")\n\n\n    Pearson's product-moment correlation\n\ndata:  bill_length and body_mass_g\nt = 14, df = 340, p-value <0.0000000000000002\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.522 0.660\nsample estimates:\n  cor \n0.595 \n\n\nIn this case, you see that \\(p < \\alpha\\), hence we reject the null hypothesis, meaning our coefficient estimate is significantly different than zero. There is, in other words, a significant positive relationship between body mass and bill length among the Palmer penguins.\nExercises\n\nUsing the penguins dataset, do all of the following:\n\ncalculate the covariance between bill length and bill depth,\ncalculate Pearson‚Äôs Correlation Coefficient for bill length and bill depth,\ndo a correlation test to determine whether the coefficient is significantly different than zero, and\nbe sure to state your null and alternative hypotheses, as well as the critical value!\n\n\nWhat does the correlation test tell you about the relationship between bill length and bill depth?"
  },
  {
    "objectID": "labs/04-ols-lab.html#linear-models",
    "href": "labs/04-ols-lab.html#linear-models",
    "title": "Lab 04: Ordinary Least Squares",
    "section": "Linear Models",
    "text": "Linear Models\nIn this section, we will learn how to fit a linear model to our data. We will look, specifically, at a scenario involving an experiment with cars recorded in the cars dataset. We want to know what kind of relationship there is between the distance (in feet) a car travels after the brakes are applied and the speed (in miles per hour) the car was going when the brakes were applied. We will be doing this by fitting a linear model with the lm() function. Here are our hypotheses:\n\n\n\\(H_0\\): there is no relationship between speed and distance.\n\n\\(H_1\\): there is a relationship between speed and distance.\n\nModel formula\nFirst, however, let‚Äôs discuss the formula syntax that the lm() function uses. You were already introduced to this with the t.test(), but let‚Äôs go into a little more detail now. To fit a model, we must first specify a formula. This involves three components: a predictor variable, the tilde ~, and a response variable. The syntax is this:\n<response> ~ <predictor> or <dependent> ~ <independent>\nIn the case of the cars data, that‚Äôs:\ndist ~ speed\nThis can be read as saying, in effect, ‚Äúdistance as a function of speed.‚Äù Note that you do not have to put the variables in quotes or anything like that. It‚Äôs just the names of the variables separated by a tilde.\nModel fitting\nIn addition to specifyfing the formula, we must also tell the lm() function what data set our observations are coming from. We do this by specifying the data argument. The whole function call looks like this:\n\ncars_model <- lm(dist ~ speed, data = cars)\n\ncars_model\n\n\nCall:\nlm(formula = dist ~ speed, data = cars)\n\nCoefficients:\n(Intercept)        speed  \n     -17.58         3.93  \n\n\nHere, the model estimates a coefficient for both the intercept and the relationship between speed and distance.\nModel summary\nA more informative report of the model is provided by the summary() function. In addition to reporting on the model coefficients, this will also conduct a t-test on each coefficient, evaluating whether they are significantly different than zero.\n\nsummary(cars_model)\n\n\nCall:\nlm(formula = dist ~ speed, data = cars)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-29.07  -9.53  -2.27   9.21  43.20 \n\nCoefficients:\n            Estimate Std. Error t value        Pr(>|t|)    \n(Intercept)  -17.579      6.758   -2.60           0.012 *  \nspeed          3.932      0.416    9.46 0.0000000000015 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 15.4 on 48 degrees of freedom\nMultiple R-squared:  0.651, Adjusted R-squared:  0.644 \nF-statistic: 89.6 on 1 and 48 DF,  p-value: 0.00000000000149\n\n\nWe‚Äôll go over this summary() in more detail later. For now, note that it reports the coefficient ‚ÄúEstimate‚Äù, the t-statistic (or ‚Äút value‚Äù) for each coefficient estimate, and the p-value for the respective t-tests. In each case, the null hypothesis is that the coefficient is zero. A small p-value then gives us reason to reject the null and accept the coefficient estimate as significant. In this case, the p-value is very small, so we can accept both the intercept and speed coefficients. This tells us (as you might expect) that there is a significant positive relationship between the speed the car was going when it applied the brakes and the distance it traveled after applying the brakes.\nExercises\n\nUsing the penguins dataset, build a linear model of the relationship between bill length and bill depth.\nWhat are the coefficients reported by this model? Specifically, the intercept and the coefficient of relationship between bill length and bill depth.\nApply the summary() function to your model. Are the coefficients significant?"
  },
  {
    "objectID": "labs/04-ols-lab.html#homework",
    "href": "labs/04-ols-lab.html#homework",
    "title": "Lab 04: Ordinary Least Squares",
    "section": "Homework",
    "text": "Homework\n\nLoad the following datasets from the archdata package using data().\n\nDartPoints\nOxfordPots\n\n\nPractice extracting variables from these tables.\n\nFrom each, remove one variable and assign it to an object with an informative name.\nCalculate the mean and variance for each variable.\n\n\nUsing the DartPoints dataset, make a kernel density plot of dart Length to visualize its distribution. Make sure to do all of the following:\n\nMap the dart Name (or type) to the fill aesthetic.\nUpdate the axis labels and plot title using labs().\nUse scale_fill_viridis to use colorblind safe colors for the fill. Note! Species is a discrete or categorical variable, so make sure to set discrete = TRUE!\nUse facet_wrap() to facet by Name (or type).\nChoose a suitable theme, like theme_minimal().\nRemove vertical grid lines.\nChange the legend position to bottom and make it horizontal.\nRemove strip text and background.\n\n\nUsing the DartPoints dataset, calculate the covariance and correlation between dart length and width.\n\nThen conduct a correlation test to evaluate the significance of Pearson‚Äôs Correlation Coefficient.\nBe sure to state the null and alternative hypotheses, as well as the critical value.\nIs the coefficient significant?\nWhat does this mean about the relationship between dart length and width?\n\n\nUsing the DartPoints dataset, build a linear model showing the relationship (if any) between the length and width of dart points. Be sure to do all of the following:\n\nTo visualize the relationship, make a scatter plot of dart Length and Width using ggplot() and geom_point(). Hint: your aesthetic mapping should be aes(x = Width, y = Length).\nUse the correct formula syntax. In this case, the dependent variable is Length and the independent variable is Width.\nUse summary() to report the model.\nAre the coefficient estimates significant?\nWhat does this mean about the relationship between the length and width of dart points? Hint: it‚Äôs called allometry.\n\n\nUsing the OxfordPots dataset, build a linear model showing the relationship (if any) between the percentage of Oxford Pots found on an archaeological site and the distance of that site from the city of Oxford. Be sure to do all of the following:\n\nTo visualize the relationship, make a scatter plot of the proportion of Oxford pots and distance to Oxford using ggplot() and geom_point(). Hint: your aesthetic mapping should be aes(x = OxfordDst, y = OxfordPct).\nUse the correct formula syntax. In this case, the dependent variable is OxfordPct and the independent variable is OxfordDst.\nUse summary() to report the model.\nAre the coefficient estimates significant?\nWhat does this mean about the relationship between the proportion of Oxford pots on an archaeological site and distance from Oxford?"
  },
  {
    "objectID": "labs/05-distributions-lab.html",
    "href": "labs/05-distributions-lab.html",
    "title": "Lab 05: Visualizing Distributions",
    "section": "",
    "text": "This lab will guide you through the process of\n\nvisualizing amounts with bar charts\nvisualizing distributions with\n\nhistograms\nprobability density plots\ncumulative distribution plots\nboxplots\n\n\nbase R alternatives to ggplot\n\nWe will be using the following packages:\n\narchdata\npalmerpenguins\ntidyverse\nviridis\n\n\nlibrary(archdata)\nlibrary(palmerpenguins)\nlibrary(tidyverse)\nlibrary(viridis)\n\n\n\n\n\n\n\nNote\n\n\n\nYou do not have to explicitly attach the {graphics} package, as it comes pre-loaded every time you open a new R session. That‚Äôs partly what it means for it to be a ‚Äúbase‚Äù R package.\n\n\n\n\n\npenguins\n\nIncludes measurements for penguin species, island in Palmer Archipelago, size (flipper length, body mass, bill dimensions), and sex.\npackage: palmerpenguins\n\nreference: https://allisonhorst.github.io/palmerpenguins/reference/penguins.html\n\n\n\n\nDartPoints\n\nIncludes measurements of 91 Archaic dart points recovered during surface surveys at Fort Hood, Texas.\npackage: archdata\n\nreference: https://cran.r-project.org/web/packages/archdata/archdata.pdf\n\n\n\n\ntitanic\n\nProvides information on the fate of passengers of the Titanic, including economic status, sex, age, and survival.\npackage: none\nreference: https://wilkelab.org/SDS375/\n\nnote: we‚Äôre going to download this one rather than get it from a package"
  },
  {
    "objectID": "labs/05-distributions-lab.html#bar-charts",
    "href": "labs/05-distributions-lab.html#bar-charts",
    "title": "Lab 05: Visualizing Distributions",
    "section": "Bar charts",
    "text": "Bar charts\nBar charts or bar plots use the length or height of bars to represent the amount of some variable across categories or groups. With ggplot2, you can create a bar chart with geom_bar(). As an example, we‚Äôll use the penguins data set.\n\npenguins\n\n# A tibble: 344 √ó 8\n   species island    bill_length_mm bill_depth_mm flipper_‚Ä¶¬π body_‚Ä¶¬≤ sex    year\n   <fct>   <fct>              <dbl>         <dbl>      <int>   <int> <fct> <int>\n 1 Adelie  Torgersen           39.1          18.7        181    3750 male   2007\n 2 Adelie  Torgersen           39.5          17.4        186    3800 fema‚Ä¶  2007\n 3 Adelie  Torgersen           40.3          18          195    3250 fema‚Ä¶  2007\n 4 Adelie  Torgersen           NA            NA           NA      NA <NA>   2007\n 5 Adelie  Torgersen           36.7          19.3        193    3450 fema‚Ä¶  2007\n 6 Adelie  Torgersen           39.3          20.6        190    3650 male   2007\n 7 Adelie  Torgersen           38.9          17.8        181    3625 fema‚Ä¶  2007\n 8 Adelie  Torgersen           39.2          19.6        195    4675 male   2007\n 9 Adelie  Torgersen           34.1          18.1        193    3475 <NA>   2007\n10 Adelie  Torgersen           42            20.2        190    4250 <NA>   2007\n# ‚Ä¶ with 334 more rows, and abbreviated variable names ¬π‚Äãflipper_length_mm,\n#   ¬≤‚Äãbody_mass_g\n\n\n\nggplot(penguins) +\n  geom_bar(\n    aes(x = species)\n  )\n\n\n\n\nYou can reorder the species variable based on their frequencies (with the most frequent first, the least frequent last) using fct_infreq(), short for factor infrequent. The word ‚Äòfactor‚Äô here refers to the way that R represents categorical or grouping variables.\n\nggplot(penguins) +\n  geom_bar(\n    aes(x = fct_infreq(species))\n  )\n\n\n\n\nIt will sometimes be preferable to orient bar charts horizontally. The simplest way to do that is to pass the factor or grouping variable to the ‚Äúy‚Äù argument in the aes(). Notice that you have to change the labels, too, since the count will now be on the x-axis, species on the y-axis.\n\nggplot(penguins) +\n  geom_bar(\n    aes(y = fct_infreq(species)),\n  )\n\n\n\n\nYou will notice now that the Adelie penguins, the most frequent species in our data, appear at the bottom. This is because the ordering starts at the origin, as it did when these were arrayed on the x-axis. In this case and in many other cases, it will make sense to re-order these with the most frequent category or species at the top and the least frequent on the bottom. To do that, we use fct_rev(), short for factor reverse, as in ‚Äúreverse the order of this factor.‚Äù\n\nggplot(penguins) +\n  geom_bar(\n    aes(y = fct_rev(fct_infreq(species)))\n  )\n\n\n\n\nNow, let‚Äôs update the default fill color and theme. We‚Äôll also remove most of the grid lines, as they do not contribute to interpreting the data.\n\nggplot(penguins) +\n  geom_bar(\n    aes(y = fct_rev(fct_infreq(species))),\n    fill = \"#6B8F7E\"\n  ) +\n  labs(\n    x = \"Count\", \n    y = NULL, \n    title = \"Palmer Penguins\"\n  ) +\n  theme_bw() +\n  theme(\n    panel.grid.minor = element_blank(), # remove all minor grid lines\n    panel.grid.major.y = element_blank() # remove major grid lines only on the y-axis\n  )\n\n\n\n\nExercises\n\nCreate a bar chart of the counts of dart point types using the DartPoints dataset (the types are stored in the Name variable). Remember to load that data into R with data(\"DartPoints\"). Then do all of the following:\n\nRe-orient the figure horizontally.\nOrder the types by their frequency, with the most frequent on the top, the least frequent on the bottom.\nAdd appropriate labels.\nChange the theme and remove unnecessary grid lines."
  },
  {
    "objectID": "labs/05-distributions-lab.html#histograms",
    "href": "labs/05-distributions-lab.html#histograms",
    "title": "Lab 05: Visualizing Distributions",
    "section": "Histograms",
    "text": "Histograms\nA histogram is an excellent aid for visualizing the distribution of numerical data. Making one involves ‚Äúbinning‚Äù a continuous variable, counting the number of its values that fall into each bin, then drawing a rectangle for each bin whose height is proportional to the count for that bin. A good example of this is the distribution of a population over age ranges, like the age distribution of the passengers on the ill-fated voyage of the Titanic. Before we get to what the histogram looks like for this macabre example, let‚Äôs first have a look at the raw data:11¬†These data come from Claus Wilke‚Äôs Fundamentals of Data Visualization.\n\n\n\n\n\n\nWarning\n\n\n\n\n\nTo be able to work through the examples in this section, you need to run the code below. We‚Äôll explain what this code does in another lab.\n\ntitanic <- read.csv(\"https://raw.githubusercontent.com/wilkelab/SDS375/master/datasets/titanic.csv\")\n\n\n\n\n\nhead(titanic)\n\n  class   age    sex survived\n1   1st 29.00 female survived\n2   1st  2.00 female     died\n3   1st 30.00   male     died\n4   1st 25.00 female     died\n5   1st  0.92   male survived\n6   1st 47.00   male survived\n\n\nYou see that our data has an age for each individual passenger. If we bin those ages into 5 year intervals (0-5, 5-10, 10-15, and so on) and count the number of passengers that fall into each bin or age range, we get a summary table that looks like this.\n\n\n\n\n  \n  \n\nAge Range\n      Count\n       \n      Age Range\n      Count\n    \n\n\n0‚Äì5\n36\n   \n41‚Äì45\n54\n\n\n6‚Äì10\n19\n   \n46‚Äì50\n50\n\n\n11‚Äì15\n18\n   \n51‚Äì55\n26\n\n\n16‚Äì20\n99\n   \n56‚Äì60\n22\n\n\n21‚Äì25\n139\n   \n61‚Äì65\n16\n\n\n26‚Äì30\n121\n   \n66‚Äì70\n3\n\n\n31‚Äì35\n76\n   \n71‚Äì75\n3\n\n\n36‚Äì40\n74\n   \n76‚Äì80\n0\n\n\n\n\n\n\nHere‚Äôs how those values look as bins:\n\n\n\n\n\nTo construct the actual histogram, we use ggplot() and geom_histogram(). Importantly, we pass ggplot() the raw tidy data, not the summary table, and in this case, we are going to use the default number of bins. Here is how that looks:\n\nggplot(titanic) + \n  geom_histogram(\n    aes(age)\n  )\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nNotice that ggplot2 provides an informative message that the default number of bins used in this plot is 30, which makes the age interval for each bin about 2.5 years. We can (and SHOULD!) change this by specifying a different number of bins with bins, as the number of bins can dramatically change the interpretation of the distribution.\n\nggplot(titanic) +\n  geom_histogram(\n    aes(age),\n    bins = 10\n  )\n\n\n\n\nUnfortunately, the default histogram produced by ggplot2 doesn‚Äôt do a great job of conveying the relationship between the axis text and the ranges represented by the widths of the rectangles. For instance, zero falls at the center of the first histogram, which would seem to suggest that the range of values represented by that rectangle includes negative ages. What it is really saying is that zero falls in the range of that bin, but that is not obvious. One solution to this issue is to set the binwidth (or bin width, as in the range of values for that bin) and the bin boundary (or where the left side of the rectangle is positioned relative to the bin range). That‚Äôs a tad confusing, I know, but the basic idea is that if you set the width to 5 and the boundary to 0, then the bin for the range 0-5 will start at 0, and the bin for the range 5-10 will start at 5, and the bin for 10-15 will start at 10, and so on. Finding the best options for visualizing your data will, of course, involve some trial and error.\n\nggplot(titanic) +\n  geom_histogram(\n    aes(age),\n    binwidth = 5,\n    boundary = 0\n  )\n\n\n\n\nAs always, you can change the fill and outline color for these plots by supplying those parameters to the geometry. We can also clean up the labels with labs() and change the theme if we like (and we do). And since the heights of these bars are what we care about, we can turn off the light vertical grid lines that ggplot2 adds by default by specifying this as a theme() option.\n\nggplot(titanic) +\n  geom_histogram(\n    aes(age),\n    binwidth = 5,\n    boundary = 0,\n    fill = \"#6B8F7E\",\n    color = \"#4C6257\",\n  ) +\n  labs(\n    x = \"Age (years)\",\n    y = \"Count\",\n    title = \"Passengers of the Titanic\"\n  ) +\n  theme_minimal() +\n  theme(\n    panel.grid.major.x = element_blank(),\n    panel.grid.minor = element_blank()\n  )\n\n\n\n\nOne last point before moving on. You may have noticed that the data contains a binary variable survived. Do you think maybe the age distribution of those that survived differs from the distribution for those that did not survive? Do you think maybe they were older? Younger? Or is there no difference? Let‚Äôs explore this a little by visualizing the different distributions using facet_wrap().\n\nggplot(titanic) +\n  geom_histogram(\n    aes(age),\n    binwidth = 5,\n    boundary = 0,\n    fill = \"#6B8F7E\",\n    color = \"#4C6257\",\n  ) +\n  labs(\n    x = \"Age (years)\",\n    y = \"Count\",\n    title = \"Passengers of the Titanic\"\n  ) +\n  facet_wrap(vars(survived)) + # <----- adding facets here\n  theme_minimal() +\n  theme(\n    panel.grid.major.x = element_blank(),\n    panel.grid.minor.x = element_blank()\n  )\n\n\n\n\nHmmmm ü§î. What do you think this means? And how might it help to compare this to the total distribution?\nExercises\n\nCreate a histogram of penguin bill length using the penguins dataset from the palmerpenguins package. Then do all of the following:\n\nChange the number of bins (try two different options).\nTry specifying the bin width and boundary.\nChange the fill and outline color.\nReset the labels to be more informative.\nChange the theme and remove the vertical grid lines.\n\n\nRepeat (1), but use the DartPoints dataset from the {archdata} package, creating a histogram of dart length (Length in the table).\n\nDoes it look like the dart types might differ in length? Or maybe they‚Äôre all basically the same?"
  },
  {
    "objectID": "labs/05-distributions-lab.html#density-plots",
    "href": "labs/05-distributions-lab.html#density-plots",
    "title": "Lab 05: Visualizing Distributions",
    "section": "Density Plots",
    "text": "Density Plots\nHere you will learn how to generate a ‚Äúdensity‚Äù plot or a plot of the underlying probability density of a variable using ggplot() and geom_density(). This is similar to a histogram in that it seeks to show the distribution of a continuous random variable. It differs, however, in the way it goes about doing that. To see how they differ, let‚Äôs first have a look at these two plots of the titanic data. The one on the left is a histogram, the one on the right a density plot.\n\n\n\n\n\n\nWarning\n\n\n\n\n\nTo be able to work through the examples in this section, you need to run the code below. We‚Äôll explain what this code does in another lab.\n\ntitanic <- read.csv(\"https://raw.githubusercontent.com/wilkelab/SDS375/master/datasets/titanic.csv\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe big difference here is that histograms discretize a sample using bins, counts the number of observations that fall into each bin, and then represents those counts using the heights of the rectangles. A density plot uses a method known as kernel density estimate (or KDE) to estimate the density of each observation and represents it using the height of a smooth and continuous curve. The KDE approach works like this. First, you define the kernel, often the Gaussian distribution with constant variance but a mean defined by each observation. Then you define a bandwidth, which is used to scale each kernel. The heights of the kernels are then summed to produce a curve like the one above.\n\n\n\n\n\nAs with the histogram, we specify a density geometry for ggplot using geom_density(). We can also update the fill and outline colors, remove unnecessary grid lines, specify the labels, and choose a simpler theme.\n\nggplot(titanic) + \n  geom_density(\n    aes(age),\n    fill = \"#6B8F7E\",\n    color = \"#4C6257\"\n  ) +\n  theme_minimal() +\n  theme(\n    panel.grid.major.x = element_blank(),\n    panel.grid.minor = element_blank()\n  )\n\n\n\n  labs(\n    x = \"Age (years)\",\n    y = \"Density\",\n    title = \"Passengers of the Titanic\"\n  )\n\n$x\n[1] \"Age (years)\"\n\n$y\n[1] \"Density\"\n\n$title\n[1] \"Passengers of the Titanic\"\n\nattr(,\"class\")\n[1] \"labels\"\n\n\nWe can also map these aesthetics to other variables like the sex of the passenger. And, we can change the default fill colors using scale_fill_manual(), too.\n\nggplot(titanic) + \n  geom_density(\n    aes(age, fill = sex)\n  ) + \n  scale_fill_manual(\n    values = c(\"#A8BFF0\", \"#FFE66D\")\n  ) +\n  labs(\n    x = \"Age (years)\",\n    y = \"Density\",\n    title = \"Passengers of the Titanic\"\n  )\n\n\n\n\nIn this case, however, it‚Äôs hard to see where the two distributions overlap, which makes it hard to compare them. One way to fix this is to change the opacity of the fill color using the alpha() function (note that the alpha or transparency of a color can range from 0 to 1.).\n\nggplot(titanic) + \n  geom_density(\n    aes(age, fill = sex)\n  ) + \n  scale_fill_manual(\n    values = alpha(c(\"#A8BFF0\", \"#FFE66D\"), 0.5)\n  ) +\n  labs(\n    x = \"Age\",\n    y = \"Density\",\n    title = \"Passengers of the Titanic\"\n  )\n\n\n\n\nThis is still a little hard to read, so let‚Äôs try faceting instead. Let‚Äôs also drop the background vertical grid lines using the theme() function. At the same time, we‚Äôll go ahead and drop the label ‚Äúsex‚Äù from the legend as that should be obvious from the key. We do that by setting name = NULL in scale_fill_manual().\n\nggplot(titanic, aes(age, fill = sex)) + \n  geom_density() + \n  scale_fill_manual(\n    name = NULL,\n    values = c(\"#A8BFF0\", \"#FFE66D\")\n  ) +\n  labs(\n    x = \"Age\",\n    y = \"Density\",\n    title = \"Age Distribution\",\n    subtitle = \"Passengers of the Titanic\"\n  ) +\n  facet_wrap(~sex) +\n  theme_minimal() +\n  theme(\n    panel.grid.major.x = element_blank(),\n    panel.grid.minor.x = element_blank(),\n  )\n\n\n\n\nNotice that the facet labels are redundant with the legend key here, so let‚Äôs remove those, too. We do that by setting the theme arguments strip.background and strip.text to element_blank(). Finally, we can move the legend to the bottom of the plot and make it horizontal with legend.position and legend.direction respectively.\n\nggplot(titanic, aes(age, fill = sex)) + \n  geom_density() + \n  scale_fill_manual(\n    name = NULL,\n    values = c(\"#A8BFF0\", \"#FFE66D\")\n  ) +\n  labs(\n    x = \"Age\",\n    y = \"Density\",\n    title = \"Age Distribution\",\n    subtitle = \"Passengers of the Titanic\"\n  ) +\n  facet_wrap(~sex) +\n  theme_minimal() +\n  theme(\n    legend.position = \"bottom\",\n    legend.direction = \"horizontal\",\n    panel.grid.major.x = element_blank(),\n    panel.grid.minor.x = element_blank(),\n    strip.background = element_blank(),\n    strip.text = element_blank()\n  )\n\n\n\n\nExercises\n\nMake a kernel density plot of penguin bill length using ggplot() and geom_density(). Then make all of the following changes:\n\nMap penguin species to the fill aesthetic.\nUpdate the axis labels and plot title using labs().\nUse scale_fill_viridis to use colorblind safe colors for the fill. Note! Species is a discrete or categorical variable, so make sure to set discrete = TRUE!\nUse facet_wrap() to facet by species.\nChoose a suitable theme, like theme_minimal().\nRemove vertical grid lines.\nChange the legend position to bottom and make it horizontal.\nRemove strip text and background.\n\n\n\nDo the same as (1), but for dart point length, and substitute dart point type for species."
  },
  {
    "objectID": "labs/05-distributions-lab.html#cumulative-distribution-plots",
    "href": "labs/05-distributions-lab.html#cumulative-distribution-plots",
    "title": "Lab 05: Visualizing Distributions",
    "section": "Cumulative Distribution Plots",
    "text": "Cumulative Distribution Plots\nWhen constructed from a sample, the cumulative distribution is technically referred to as the empirical cumulative distribution function (or eCDF). It has one critical advantage over histograms and probability density plots, namely, that you don‚Äôt have to specify a binwidth or bandwidth. That‚Äôs because you first order the data from smallest to largest value, then count the number of observations that are equal to or less than each unique value, and increment the cumulative proportion of observations by that amount.\nAs a simple example, consider this vector or sample: (0.3, 2.0, 3.4, 1.2, 2.2, 1.9).\nRearranging it smallest to largest value, we get: (0.3, 1.2, 1.9, 2.0, 2.2, 3.4).\nNow, for each unique value, we count the number of observations that are less than or equal to it, so\n0.0 -> none of them, so 0\n0.3 -> just 0.3, so 1\n1.2 -> 0.3 and 1.2, so 2\n1.9 -> 0.3, 1.2, and 1.9, so 3\n2.0 -> 0.3, 1.2, 1.9, and 2.0, so 4\n2.2 -> 0.3, 1.2, 1.9, 2.0, and 2.2, so 5\n3.4 -> all of them, so 6\nAs proportions of the total sample, which has six observations, that‚Äôs\n0.0 -> 0/6 = 0.00\n0.3 -> 1/6 = 0.17\n1.2 -> 2/6 = 0.33 \n1.9 -> 3/6 = 0.50 \n2.0 -> 4/6 = 0.67 \n2.2 -> 5/6 = 0.83\n3.4 -> 6/6 = 1.00\nNow, we can plot that.\n\n\n\n\n\nSo, fewer assumptions here, but it‚Äôs also a smidge harder to interpret since it gives you the probability of being less than or equal to x, for example, the probability of being less than or equal to 1.2 is 0.33.\nUnfortunately, plotting the eCDF of, for example, the ages of passengers on the Titanic, is not straightforward with ggplot2 because you have to use what is known as a stat_*() function, in this case, stat_ecdf(), rather than the more familiar geometry functions. This is also an example of when it would be useful to have major grid lines along both axes, so we will only remove the minor ones.\n\nggplot(titanic) +\n  stat_ecdf(\n    aes(age),\n    geom = \"step\",\n    color = \"#4C6257\",\n    linewidth = 1.2\n  ) +\n  theme_minimal() +\n  theme(\n    panel.grid.minor = element_blank()\n  ) +\n  labs(\n    x = \"Age (years)\",\n    y = \"Probability\",\n    title = \"Passengers of the Titanic\"\n  )\n\n\n\n\nYou can facet these, too, if you so desire.\n\nggplot(titanic) +\n  stat_ecdf(\n    aes(age, color = class),\n    geom = \"step\",\n    linewidth = 1.2\n  ) +\n  scale_color_manual(\n    name = \"Class\",\n    values = c(\"#942911\", \"#37423D\", \"#0094C6\")\n  ) +\n  theme_minimal() +\n  theme(\n    panel.grid.minor = element_blank(),\n    legend.justification = c(\"right\", \"bottom\"),\n    legend.position = c(0.98, 0.05)\n  ) +\n  labs(\n    x = \"Age (years)\",\n    y = \"Probability\",\n    title = \"Passengers of the Titanic\"\n  )\n\n\n\n\nTwo things to note here. First, there‚Äôs the interpretation. Consider the age 40. If you follow that vertical grid line up to where it intersect the line for each class, you will see that only about 50% of first class passengers were 40 years old or younger, but for second and third class, that number is closer to 80 or even 85%. The implication here is that first class passengers on the Titanic were generally older than second and third class passengers.\nThe second thing to note is that I moved the legend into the plot area by specifying the legend justification and the legend position. The position may consist of character strings like ‚Äútop‚Äù or ‚Äúbottom‚Äù or a vector of x,y coordinates (both ranging from 0 to 1, zero for left and bottom, one for top and right). The justification determines how the legend is oriented relative to the position coordinates. In this case, the bottom right corner of the legend will be at x = 0.98 and y = 0.05. This usually requires some trial and error before finding a position you like.\nExercises\n\nMake an eCDF plot of penguin bill length using ggplot() and stat_ecdf(). Then make all of the following changes:\n\nMap penguin species to the color aesthetic.\nUpdate the axis labels and plot title using labs().\nUse scale_color_viridis to use colorblind safe colors for the fill. Note! Species is a discrete or categorical variable, so make sure to set discrete = TRUE!\nChoose a suitable theme, like theme_minimal().\nRemove minor grid lines on each axis.\nMove the legend into the plot panel.\n\n\nDo the same as (1), but for dart point length, and substitute dart point type for species."
  },
  {
    "objectID": "labs/05-distributions-lab.html#boxplots",
    "href": "labs/05-distributions-lab.html#boxplots",
    "title": "Lab 05: Visualizing Distributions",
    "section": "Boxplots",
    "text": "Boxplots\nIn this section we‚Äôll learn how to make boxplots, which provide a simple but effective way of representing the distribution of a variable. For one or two variables, it‚Äôs often better to use a density plot, but if you‚Äôre comparing the distributions of lots of variables or lots of samples of the same variable across multiple categories, a density plot can get crowded quick. That‚Äôs when it‚Äôs useful to turn to boxplots, so we‚Äôll focus on that here.\n\nset.seed(42)\n\ny <- rnorm(250)\n\nlabels <- tibble(\n  y = c(boxplot.stats(y)$stats, max(y)),\n  x = 0.5,\n  label = c(\"1.5 x IQR\", \"first quartile\", \"median\", \"third quartile\", \"1.5 x IQR\", \"outlier\")\n)\n\nggplot(tibble(x = 0, y), aes(x, y)) + \n  stat_boxplot(geom ='errorbar', width = 0.33) +\n  geom_boxplot(\n    fill = \"#6B8F7E\", \n    width = 0.6,\n    outlier.size = 4,\n    notch = TRUE,\n    notchwidth = 0.75\n  ) +\n  geom_text(\n    data = labels,\n    aes(x, y, label = label), \n    hjust = 0,\n    size = 11/.pt\n  ) +\n  geom_point(\n    data = tibble(x = runif(length(y), -1.2, -0.5), y = y),\n    aes(x, y),\n    size = 3,\n    color = \"#4C6257\",\n    alpha = 0.85\n  ) +\n  coord_cartesian(xlim = c(-2.2, 3)) +\n  theme_void()\n\n\n\n\nAs you can see, the boxplot shows the distribution of a variable using a five-number summary, which includes all of the following:\n\nMinimum: the lowest value excluding outliers\nMaximum: the greatest value excluding outliers\nMedian: the middle value that separates the data in half (also called the second quartile)\nFirst quartile: the middle value of the lower half of the data, meaning 75% of the data fall above it and %25 below it (also called the lower quartile)\nThird quartile: the middle value of the upper half of the data, meaning 25% of the data fall above it and 75% below it (also called the upper quartile)\n\nBy extension, this includes the\n\nInterquartile Range: the distance between the first and third quartile, which includes 50% of the data.\n\nNotice that the minimum and maximum values are connected to the first and third quartiles by lines commonly referred to as ‚Äúwhiskers.‚Äù These are drawn to the largest and smallest values that fall within 1.5 * IQR of the first and third quartiles, respectively. Observations that fall above or below the whiskers are considered ‚Äúoutliers‚Äù.\nTo make a boxplot with ggplot, we need our table of data (as always), which we pass to the ggplot() function. We then specify the boxplot geometry with geom_boxplot().\n\nggplot(penguins) +\n  geom_boxplot(\n    aes(x = species, y = bill_length_mm),\n    fill = \"#6B8F7E\"\n  )\n\n\n\n\nNotice that we specified that the distribution of bill length should range over the y-axis. This makes the boxplots display vertically in the graph. We can change them to horizontal by having the variables distribution range over the x-axis.\n\nggplot(penguins) +\n  geom_boxplot(\n    aes(x = bill_length_mm, y = species),\n    fill = \"#6B8F7E\"\n  )\n\n\n\n\nNow, let‚Äôs update the axis labels and add a title. In this case, we‚Äôll drop the y-axis label because it should be obvious from the values there that these are different species. While we‚Äôre at it, let‚Äôs also change the theme settings to minimal and remove the horizontal grid lines. And, we‚Äôll add perpendicular lines at the end of the whiskers with stat_boxplot(geom = \"errorbar\"). Note that we move the aes() call up to ggplot(), so that these arguments can be shared between the stat and the geometry.\n\nggplot(penguins, aes(x = bill_length_mm, y = species)) +\n  stat_boxplot(geom ='errorbar', width = 0.33) +\n  geom_boxplot(fill = \"#6B8F7E\") +\n  labs(\n    x = \"Bill Length (mm)\",\n    y = NULL,\n    title = \"Palmer Penguins\"\n  ) +\n  theme_bw() +\n  theme(\n    panel.grid.major.y = element_blank(),\n    panel.grid.minor = element_blank(),\n  )\n\n\n\n\nExercises\n\nMake a boxplot showing the distribution of penguin body mass by island. Do all of the following:\n\nPosition the boxes horizontally.\nChange the fill color to a color of your choice.\nUpdate the labels and add a title.\nChange the theme to one of your choice.\nRemove the horizontal grid lines.\nAdd perpendicular lines to the whiskers.\n\n\nDo the same as (1), but for dart point length, substituting dart type for island."
  },
  {
    "objectID": "labs/05-distributions-lab.html#base-r-graphics",
    "href": "labs/05-distributions-lab.html#base-r-graphics",
    "title": "Lab 05: Visualizing Distributions",
    "section": "Base R {graphics}\n",
    "text": "Base R {graphics}\n\nWhile we have focused on visualizing distributions using ggplot2, it‚Äôs also useful to learn how to construct them with the {graphics} package from base R. The latter is not something you would want to rely on to create publication-quality figures, but it does provide simple and powerful tools for visually exploring your data. This means we will not spend much time learning the ends and outs of manipulating base R graphics to try and make them look similar to those generated with ggplot(). Believe me when I say that would be a painful experience. You will be best served only turning to {graphics} for rough and ready exploratory visualizations and not for the sort of fine-tuning you would do with ggplot2 to get a figure ready for publication. With that, then, here are the base R alternatives.\nFair warning, we‚Äôre going to make sure we have plenty of margin space for these plots, so labels and whatnot don‚Äôt get cutoff. We set the margins using the par() function (for graphical parameters) and pass a vector of length 4 to the mar (for margins) argument, one value for the size of each margin.\nFirst, we‚Äôll get the default margins.\n\ndefault_margins <- par()$mar\n\nbig_margins <- c(6, 8, 4, 1)\n\nBar chart\nThe base R equivalent of ggplot() + geom_bar() is the barplot() function. There are many differences between these two methods, but the primary one is that barplot() requires you to compute the counts for each category beforehand. To do that, we‚Äôll use the handy - though misleadingly named - table() function.\n\nspecies <- table(penguins$species)\n\nspecies\n\n\n   Adelie Chinstrap    Gentoo \n      152        68       124 \n\nbarplot(\n  height = sort(species, decreasing = TRUE),\n  col = \"#6B8F7E\",\n  xlab = NULL,\n  ylab = \"Count\"\n)\n\n\n\n\nNotice that we re-ordered the species variable using the sort() function with decreasing = TRUE rather than fct_infreq(). This will work only on the summarized data generated by the table() function, not the raw data passed to geom_bar().\nIf you want to make this horizontal, just specify horiz = TRUE. You‚Äôll also want to change the sorting to increasing, to get the most frequent species on top. And use las = 1 to rotate the axis text so that they are horizontal, too.\n\npar(mar = big_margins)\n\nbarplot(\n  height = sort(species, decreasing = FALSE),\n  horiz = TRUE,\n  las = 1,\n  col = \"#6B8F7E\",\n  xlab = \"Count\",\n  ylab = NULL\n)\n\n\n\n\nHistogram\nThe base R equivalent of ggplot() + geom_histogram() is the extremely compact hist() function. It looks like this.\n\n# reset margins\npar(mar = default_margins)\n\nhist(\n  titanic$age,\n  breaks = seq(0, 75, by = 5),\n  xlab = \"Age (years)\",\n  ylab = \"Count\",\n  main = \"Passengers of the Titanic\",\n  col = \"#6B8F7E\",\n  border = \"#4C6257\"\n)\n\n\n\n\nTwo important things to note here. First, you have to pass the histogram function the variable age, so you have to pull out of the data.frame with titanic$age. Second, you use breaks instead of bins or binwidth to define the bins.\nProbability density\nThe base R equivalent of ggplot() + geom_density() uses the density() and plot() functions. It looks like this.\n\nkde <- density(titanic$age)\n\nplot(\n  kde,\n  xlab = \"Age (years)\",\n  ylab = \"Density\",\n  main = \"Passengers of the Titanic\"\n)\n\n\n\n\nNote that the density() function does not like missing values, so if there are any NA values in your data, you will need to include the argument na.rm = TRUE (this tells the function to ignore missing values).\nTo control the fill and outline color, it‚Äôs necessary to add the KDE as an additional layer to the plot with the polygon() function.\n\nkde <- density(titanic$age)\n\nplot(\n  kde,\n  xlab = \"Age (years)\",\n  ylab = \"Density\",\n  main = \"Passengers of the Titanic\"\n)\n\npolygon(\n  kde,\n  col = \"#6B8F7E\",\n  border = \"#4C6257\"\n)\n\n\n\n\nAnd that‚Äôs about as far as I want to take you with that.\nCumulative distribution\nThe base R equivalent of ggplot() + stat_ecdf() uses the ecdf() and plot() functions. It looks like this.\n\necdf_function <- ecdf(titanic$age)\n\nplot(\n  ecdf_function,\n  col = \"#4C6257\",\n  xlab = \"Age (years)\",\n  ylab = \"Density\",\n  main = \"Passengers of the Titanic\"\n)\n\n\n\n\nThat‚Äôs a little weird. To get this into a proper stewise line, we need to do a little work. Specifically, we need to sort the unique age values of passengers on the Titanic. Then we need to feed those to the ecdf_function() we created with ecdf(). We also need to specify that we want the stepwise plot type with type = \"s\". Ugh‚Ä¶ üòï\n\nx <- sort(unique(titanic$age))\n\nplot(\n  x,\n  ecdf_function(x),\n  type = \"s\",\n  col = \"#4C6257\",\n  xlab = \"Age (years)\",\n  ylab = \"Density\",\n  main = \"Passengers of the Titanic\"\n)\n\n\n\n\nMother of dragons‚Ä¶ üêâ\nBoxplot\nThe base R equivalent of ggplot() + geom_boxplot() is, thankfully, boxplot(). It looks like this.\n\nboxplot(\n  penguins$bill_length_mm ~ penguins$species,\n  col = \"#4C6257\",\n  xlab = NULL,\n  ylab = \"Bill Length (mm)\",\n  main = \"Palmer Penguins\"\n)\n\n\n\n\nNotice that I used the formula notation. This is to specify the grouping structure for the boxes, so a separate box for each penguin species.\nTo make that figure horizontal, use horizontal = TRUE.\n\npar(mar = big_margins)\n\nboxplot(\n  penguins$bill_length_mm ~ penguins$species,\n  horizontal = TRUE,\n  las = 1,\n  col = \"#4C6257\",\n  xlab = \"Bill Length (mm)\",\n  ylab = NULL,\n  main = \"Palmer Penguins\"\n)\n\n\n\n\nHey, in the base R bar chart above, didn‚Äôt we specify a horizontal orientation with horiz = TRUE?\nYeah, yeah you did‚Ä¶ ü§∑\nExercises\nFor each of the following, make sure to change the fill (col) and outline (border) colors and update the labels.\n\nCreate a bar chart of the count of dart point types using table() and the barplot() function. Give the plot a horizontal orientation.\nCreate a histogram of penguin bill length using the hist() function.\nCreate a histogram of dart point length using the hist() function.\nCreate a probability density plot of penguin bill length using the density() and plot() functions. (Note that the density() function does not like missing values, so if there are any NA values in your data, you will need to include the argument na.rm = TRUE (this tells the function to ignore missing values).)\nCreate a probability density plot of dart point length using the density() and plot() functions.\nCreate an eCDF plot of penguin bill length using the ecdf() and plot() functions. Make sure the plot is stepwise.\nCreate an eCDF plot of dart point length using the ecdf() and plot() functions. Make sure the plot is stepwise.\nCreate a boxplot of penguin bill length grouped by species using the boxplot() function. Give the plot a horizontal orientation.\nCreate a boxplot of dart point length grouped by dart point type using the boxplot() function. Give the plot a horizontal orientation."
  },
  {
    "objectID": "labs/06-dataframes-lab.html",
    "href": "labs/06-dataframes-lab.html",
    "title": "Lab 06: Working with Tables",
    "section": "",
    "text": "This lab will guide you through the process of\n\nconstructing data.frames and tibbles\nworking with the pipe\nworking with columns (variables) in a table\nworking with rows (observations) in a table\nworking with grouped data in a table\n\n\narchdata\npalmerpenguins\ndplyr\n\n\nlibrary(archdata)\nlibrary(palmerpenguins)\nlibrary(dplyr)\n\n\n\n\npenguins\n\nIncludes measurements for penguin species, island in Palmer Archipelago, size (flipper length, body mass, bill dimensions), and sex.\npackage: palmerpenguins\n\nreference: https://allisonhorst.github.io/palmerpenguins/reference/penguins.html\n\n\n\n\nDartPoints\n\nIncludes measurements of 91 Archaic dart points recovered during surface surveys at Fort Hood, Texas.\npackage: archdata\n\nreference: https://cran.r-project.org/web/packages/archdata/archdata.pdf"
  },
  {
    "objectID": "labs/06-dataframes-lab.html#tables",
    "href": "labs/06-dataframes-lab.html#tables",
    "title": "Lab 06: Working with Tables",
    "section": "Tables",
    "text": "Tables\n\n\n\n\n\nFigure¬†1: Elements of a Data Frame.\n\n\nConsider this research scenario: you have a museum collection of projectile points that you want to use for an analysis, maybe you want to know whether East Gate and Rose Spring points are actually the same type of point, commonly referred to using the portmanteau Rosegate. Following tradition, you think maybe it‚Äôs the size and shape that will you help finally put this old question to rest, so for each individual point, you set about measuring its length, width, and height.\nIn this course, we‚Äôll refer to each individual measure that you make as a value, each set of such measures for an individual point we will call (somewhat awkwardly) an observation, and each individual type of measurement will be a variable. It is almost certainly the case that you will be collecting this data and storing it in something like a spreadsheet or table, like that shown in Figure¬†1. You will sometimes hear data scientists refer to data stored in this way as rectangular data. All they mean by that is that the data come as collections of values organized into rows and columns. Crucially, all the columns must have the same length, so you can describe the data as having dimensions, specifically length (the number of observations or rows) and width (the number of variables or columns).\nCreating tables\nIn R, tabular datasets are known as data frames. To create a data frame, we use the eponymous data.frame() function. Here, for example, is how we would create the table in Figure¬†1 above:\n\nprojectiles_df <- data.frame(\n  type = c(\"Elko\", \"Rosegate\", \"DSN\", \"Elko\", \"Clovis\"),\n  length = c(2.03, 1.4, 1.9, 2.1, 3.3),\n  width = c(0.8, 0.4, 0.3, 0.7, 0.95),\n  height = c(3.23, 2.4, 1.29, 2.7, 4.15)\n)\n\nprojectiles_df\n\n      type length width height\n1     Elko   2.03  0.80   3.23\n2 Rosegate   1.40  0.40   2.40\n3      DSN   1.90  0.30   1.29\n4     Elko   2.10  0.70   2.70\n5   Clovis   3.30  0.95   4.15\n\n\nNote that the values (or measurements) contained in each variable are wrapped in the c() function (short for concatenate). This variable tells R that all these values belong to this one variable. Each variable in turn is an argument to data.frame() having the form <variable> = c(<value-1>, <value-2>, ..., <value-n>).\nGetting basic meta-data from tables\nWhen you want to know what variables a table includes, you can use the names() function.\n\nnames(projectiles_df)\n\n[1] \"type\"   \"length\" \"width\"  \"height\"\n\n\nIf you want to know how many variables or observations the table has, you can use nrow() and ncol() respectively.\n\n# number of observations\nnrow(projectiles_df)\n\n[1] 5\n\n# number of variables\nncol(projectiles_df)\n\n[1] 4\n\n\nPretty tibbles\nAs an alternative to the default data.frame, you can also use a tibble, which is a standard format used by packages within the tidyverse, including the tibble package that defines it. In the grand scheme of things, the differences between tibbles and data.frames are quite small, mostly relating to their nicer (some might say prettier) print methods. It has some other nice features, but most of them are not things that should concern a beginning R user, so we won‚Äôt dwell on them here.\nYou create a tibble just like you would a data.frame, but with the tibble() function.\n\nprojectiles_tbl <- tibble(\n  type = c(\"Elko\", \"Rosegate\", \"DSN\", \"Elko\", \"Clovis\"),\n  length = c(2.03, 1.4, 1.9, 2.1, 3.3),\n  width = c(0.8, 0.4, 0.3, 0.7, 0.95),\n  height = c(3.23, 2.4, 1.29, 2.7, 4.15)\n)\n\nNow you can see how the print method for tibbles differs from that of data.frames.\n\nprojectiles_df\n\n      type length width height\n1     Elko   2.03  0.80   3.23\n2 Rosegate   1.40  0.40   2.40\n3      DSN   1.90  0.30   1.29\n4     Elko   2.10  0.70   2.70\n5   Clovis   3.30  0.95   4.15\n\nprojectiles_tbl\n\n# A tibble: 5 √ó 4\n  type     length width height\n  <chr>     <dbl> <dbl>  <dbl>\n1 Elko       2.03  0.8    3.23\n2 Rosegate   1.4   0.4    2.4 \n3 DSN        1.9   0.3    1.29\n4 Elko       2.1   0.7    2.7 \n5 Clovis     3.3   0.95   4.15\n\n\nThere are a few big differences here:\n\nBy default tibble() will only print the first ten rows of data. This saves you from having to use the head() function. It will also tell you how many rows were not printed at the bottom, assuming there are more than ten. With data.frame(), the default print will print the maximum number of rows defined in your default options.\n\ntibble() will only print columns that fit on your screen. It will then tell you how many columns were not printed.\n\ntibble() provides information about the dimensions of your table: the number of rows and columns.\n\ntibble() provides information about the type of data each variable is: factor (fct), character, numeric (dbl for ‚Äúdouble‚Äù or int for ‚Äúinteger‚Äù).\n\nWhile the choice between tibbles and data.frames is largely a stylistic one, I would encourage you to use tibbles as much as possible. It‚Äôll make things easier for you in the long run. You can always convert between them if you want with as_tibble() and as.data.frame().\n\n# turn a tibble into a data.frame\nprojectiles_df <- as.data.frame(projectiles_tbl)\n\n# turn a data.frame into a tibble\nprojectiles_tbl <- as_tibble(projectiles_df)\n\nIn what follows, we are going to learn how to perform some tasks that almost always arise when working with tables using a powerful set of tools provided by the dplyr package, which is part of the tidyverse.\nExercises\n\nGet the names of the variables in the penguins table with names().\nHow many observations and variables are in this dataset? Hint: use nrow() and ncol().\nLoad the DartPoints dataset and convert the data.frame to a tibble with as_tibble(). Assign it to a new object called ‚Äúdarts‚Äù using the arrow <-. Then remove the original DartPoints dataframe from your environment with remove(DartPoints)."
  },
  {
    "objectID": "labs/06-dataframes-lab.html#the-pipe",
    "href": "labs/06-dataframes-lab.html#the-pipe",
    "title": "Lab 06: Working with Tables",
    "section": "The Pipe",
    "text": "The Pipe\n\n\nFigure¬†2: The Treachery of Images (1929) by Rene Magritte.\n\n\nBefore we get to the ways in which you can shape and otherwise manipulate tables in R, though, let‚Äôs introduce a style of coding referred to as piping first, which allows you to express a series of operations in a clear and concise way. To motivate the use of pipes, consider this extremely common scenario: you want to select some specific variables in your table, filter the observations to include a subset based on some condition, then add a new variable by transforming or mutating another one. Your code might look like this (we will explain how to use these functions in just a moment):\n\nbob <- select(penguins, species, island, bill_length_mm, body_mass_g)\ntom <- filter(bob, sex == \"male\")\nsue <- mutate(tom, bill_length_cm = bill_length_mm/10)\n\nremove(bob, tom)\n\nYou will sometimes see people write the same series of function calls this way:\n\nsue <- mutate(filter(select(penguins, species, island, bill_length_mm, body_mass_g), island == \"Torgersen\"), bill_length_cm = bill_length_mm/10)\n\nThis is called nesting. It‚Äôs primary advantage is that it does not require saving intermediate objects (like bob and tom) and thus polluting your R environment with unnecessary copies of your data. Nesting function calls in this way is fine as long as it remains relatively shallow, but the deeper the nest (like 2 or 3 functions deep), the harder it is to read, as you must follow results from the inside out and bottom up, as opposed to left to right and top down, which is more natural.\nTo avoid deep nests and temporary assignments, you can use pipes. This involves calling the special operator, |>, consisting of a vertical bar and the greater-than sign that was introduced to base R in 2021 with version 4.1.0. If you squint (and depending on the font you are using), it looks a bit like an arrow pointing to the right. Using the pipe-operator looks like this:\n\nsue <- penguins |> \n  select(species, sex, body_mass_g) |> \n  filter(sex == \"male\") |> \n  mutate(bill_length_cm = bill_length_mm/10)\n\nIf you were to transpose this code into a natural language like English, it would sound something like this,\n\nTake the penguins table, then select the variables species, sex, and body mass, then filter the males, then create a new variable by converting bill length from millimeters to centimeters.\n\nThe ‚Äòthen‚Äô in each case is the pipe operator. Crucially, the pipe assumes that the data being transformed is the first argument to the function, which is true for all the functions in the tidyverse, as they were all designed very intentionally to work with the pipe. Unfortunately, many parts of R were written long before the pipe was ever introduced, so they may not work as easily with it.\n\n\n\n\n\n\nTip: keyboard shortcut\n\n\n\nThis is easily one of the most powerful tools for working with data in R, which is probably why it is so popular these days. RStudio gives a keyboard shortcut to make inserting this operator into your code easier. It‚Äôs Ctrl + Shift + M on Windows and Cmd + Shift + M on Mac. To ensure that this keyboard shortcut inserts the base R pipe operator, go to Tools > Global Options‚Ä¶ > Code > Editing and check that the option ‚ÄúUse native pipe operator, |> (requires R 4.1+)‚Äù is selected."
  },
  {
    "objectID": "labs/06-dataframes-lab.html#columns",
    "href": "labs/06-dataframes-lab.html#columns",
    "title": "Lab 06: Working with Tables",
    "section": "Columns",
    "text": "Columns\nNow that we have the pipe, let‚Äôs look at different ways of working with variables or columns in a table. Some of the most common tasks involve selecting a subset of variables, relocating them, renaming them, and creating new variables. As mentioned before, the focus here will be on dplyr functions, but we will also take some time to introduce base R methods where appropriate.\npull()\nTo pull a variable out of a table and return it as a vector, you can pass the table along with the name of the target variable to the pull() function from dplyr.\n\nprojectiles_tbl |> pull(type)\n\n[1] \"Elko\"     \"Rosegate\" \"DSN\"      \"Elko\"     \"Clovis\"  \n\nprojectiles_tbl |> pull(length)\n\n[1] 2.03 1.40 1.90 2.10 3.30\n\n\nThe base R way of doing this relies on double brackets or the dollar sign, as in <table>[[\"<variable>\"]] and <table>$<variable>.\n\nprojectiles_tbl$type\n\n[1] \"Elko\"     \"Rosegate\" \"DSN\"      \"Elko\"     \"Clovis\"  \n\nprojectiles_tbl[[\"length\"]]\n\n[1] 2.03 1.40 1.90 2.10 3.30\n\n\nAs you can see, using the dplyr function is quite a bit easier to read and intuit what you are doing. It also plays nice with the pipe.\nselect()\nIf you want to select one or more variables but return the result as a table, pass the table along with the names of the desired variables to select(). As an example, suppose (for reasons) that we want a smaller table that only has the species and bill_length_mm variables. To get that from the larger table, I simply do this\n\npenguins |> select(species, bill_length_mm)\n\n# A tibble: 344 √ó 2\n   species bill_length_mm\n   <fct>            <dbl>\n 1 Adelie            39.1\n 2 Adelie            39.5\n 3 Adelie            40.3\n 4 Adelie            NA  \n 5 Adelie            36.7\n 6 Adelie            39.3\n 7 Adelie            38.9\n 8 Adelie            39.2\n 9 Adelie            34.1\n10 Adelie            42  \n# ‚Ä¶ with 334 more rows\n\n\nrename()\nTo rename variables in a table, use the rename() function. This is similar to pull() and select() in that you pass it column names, but you are also going to signal new names for those columns using the equal sign. The idea here is to do rename(<new name> = <old name>). Here, for example, is how you might remove the units of measure from the variable names in the penguins table.\n\npenguins |> names()\n\n[1] \"species\"           \"island\"            \"bill_length_mm\"   \n[4] \"bill_depth_mm\"     \"flipper_length_mm\" \"body_mass_g\"      \n[7] \"sex\"               \"year\"             \n\npenguins |> \n  rename(\n    bill_length = bill_length_mm,\n    bill_depth = bill_depth_mm,\n    flipper_length = flipper_length_mm,\n    body_mass = body_mass_g\n  ) |> \n  names()\n\n[1] \"species\"        \"island\"         \"bill_length\"    \"bill_depth\"    \n[5] \"flipper_length\" \"body_mass\"      \"sex\"            \"year\"          \n\n\n\n\n\n\n\n\nA good rule of üëç\n\n\n\nHere are a few good rules of thumb for variable names in tables (and for all names in R!).\n\nUse all lower-case characters. This makes them easier to type.\nAvoid special characters. They require special treatment when included.\nUse underscores _ instead of spaces for similar reasons.\nTry to keep the names as simple as possible but‚Ä¶\nAir on the side of longer names that clearly communicate the meaning of each variable.\n\n\n\nHere is one trick you might find useful. Sometimes you will find yourself wanting to rename all the variables in a consistent way, for instance, by making them all lower case. This is something I find myself doing quite often actually. Base R has this nice function called tolower() that converts capitalized letters to lowercase. For instance, tolower(\"ABC\") returns abc. There‚Äôs a corresponding toupper() that does the opposite. You can apply these functions to every column in one go with rename_with().\n\npenguins |> \n  rename_with(toupper) |> \n  names()\n\n[1] \"SPECIES\"           \"ISLAND\"            \"BILL_LENGTH_MM\"   \n[4] \"BILL_DEPTH_MM\"     \"FLIPPER_LENGTH_MM\" \"BODY_MASS_G\"      \n[7] \"SEX\"               \"YEAR\"             \n\n\nrelocate()\nYou will sometimes find it helpful to reorder your variables. To do that, you can use relocate(), again passing it the column names. By default, it moves variables to the front, but you can also move them before or after specific variables with the .before and .after arguments.\n\npenguins |> names()\n\n[1] \"species\"           \"island\"            \"bill_length_mm\"   \n[4] \"bill_depth_mm\"     \"flipper_length_mm\" \"body_mass_g\"      \n[7] \"sex\"               \"year\"             \n\npenguins |> relocate(sex, year) |> names()\n\n[1] \"sex\"               \"year\"              \"species\"          \n[4] \"island\"            \"bill_length_mm\"    \"bill_depth_mm\"    \n[7] \"flipper_length_mm\" \"body_mass_g\"      \n\npenguins |> relocate(sex, year, .after = island) |> names()\n\n[1] \"species\"           \"island\"            \"sex\"              \n[4] \"year\"              \"bill_length_mm\"    \"bill_depth_mm\"    \n[7] \"flipper_length_mm\" \"body_mass_g\"      \n\n\nmutate()\nOne of the most important and powerful functions provided by dplyr is mutate(). This allows you to add or change variables, often based on the values of other variables in your table. For instance, you can create a new variable that codes bill length in centimeters rather than millimeters. You just divide millimeters by 10.\n\npenguins |> \n  mutate(bill_length_cm = bill_length_mm/10) |> \n  select(species, bill_length_mm, bill_length_cm)\n\n# A tibble: 344 √ó 3\n   species bill_length_mm bill_length_cm\n   <fct>            <dbl>          <dbl>\n 1 Adelie            39.1           3.91\n 2 Adelie            39.5           3.95\n 3 Adelie            40.3           4.03\n 4 Adelie            NA            NA   \n 5 Adelie            36.7           3.67\n 6 Adelie            39.3           3.93\n 7 Adelie            38.9           3.89\n 8 Adelie            39.2           3.92\n 9 Adelie            34.1           3.41\n10 Adelie            42             4.2 \n# ‚Ä¶ with 334 more rows\n\n\nYou can refer to as many variables in the table as you like, too. For instance, you can calculate the ratio of bill length to flipper length.\n\npenguins |> \n  mutate(bill_to_flipper = bill_length_mm/flipper_length_mm) |> \n  select(species, bill_length_mm, flipper_length_mm, bill_to_flipper)\n\n# A tibble: 344 √ó 4\n   species bill_length_mm flipper_length_mm bill_to_flipper\n   <fct>            <dbl>             <int>           <dbl>\n 1 Adelie            39.1               181           0.216\n 2 Adelie            39.5               186           0.212\n 3 Adelie            40.3               195           0.207\n 4 Adelie            NA                  NA          NA    \n 5 Adelie            36.7               193           0.190\n 6 Adelie            39.3               190           0.207\n 7 Adelie            38.9               181           0.215\n 8 Adelie            39.2               195           0.201\n 9 Adelie            34.1               193           0.177\n10 Adelie            42                 190           0.221\n# ‚Ä¶ with 334 more rows\n\n\nWhen you work with mutate(), keep in mind that data.frames and tibbles are rectangular. That means each variable in the table must have the same number of values (equal to the number of rows or observations in the table). In the penguins table, there are 344 observations, so any new variable that you add with mutate() must have 344 values (it must have that length). If the variable you want to add does not have the same length, you will receive an error like the following:\n\npenguins |> mutate(id = 1:5)\n\nError in `mutate()`:\n! Problem while computing `id = 1:5`.\n‚úñ `id` must be size 344 or 1, not 5.\n\n\nNote, too, that you can refer to new variables you have created in the same mutate() call.\n\npenguins |> \n  select(species, island) |> \n  mutate(\n    id = 1:n(),\n    id2 = id * 2\n  )\n\n# A tibble: 344 √ó 4\n   species island       id   id2\n   <fct>   <fct>     <int> <dbl>\n 1 Adelie  Torgersen     1     2\n 2 Adelie  Torgersen     2     4\n 3 Adelie  Torgersen     3     6\n 4 Adelie  Torgersen     4     8\n 5 Adelie  Torgersen     5    10\n 6 Adelie  Torgersen     6    12\n 7 Adelie  Torgersen     7    14\n 8 Adelie  Torgersen     8    16\n 9 Adelie  Torgersen     9    18\n10 Adelie  Torgersen    10    20\n# ‚Ä¶ with 334 more rows\n\n\nExercises\n\nRemind yourself of what variables the penguins table has with names().\nNow, extract a variable of your choice with pull().\nSubset the data by choosing only four variables of your choice with select().\nUse relocate() to move the species and island variables after bill_length_mm.\nUse mutate() on the penguins data to create variables that\n\nrepresent body mass in kilograms,\nrepresent bill depth in centimeters,\ncompare bill length to bill depth to see whether the former is greater than (>) the latter.\n\n\nUsing the darts tibble (formerly the DartPoints data.frame), rename the variables so that they are all lower case, rename the ‚Äúname‚Äù variable to ‚Äútype‚Äù, and subset to the type, length, width, and thickness variables. Reassign this to darts, so you can save the result. Hint:\n\n\ndarts <- darts |> \n  rename_with() |> \n  rename() |> \n  selet()\n\n\nUse mutate() to add a variable to darts that is the ratio of point length to width."
  },
  {
    "objectID": "labs/06-dataframes-lab.html#rows",
    "href": "labs/06-dataframes-lab.html#rows",
    "title": "Lab 06: Working with Tables",
    "section": "Rows",
    "text": "Rows\narrange()\nSometimes it can be useful to sort a table, so you can more easily navigate the information it contains. To do this, dplyr provides the arrange() function. This sorts observations based on a supplied variable or variables. By default, it sorts observations in ascending order. If you provide multiple variables, it will sort the first variable first, then sort the second variable within that variable, and so on. In effect, it breaks ties in the sorting process.\n\npenguins |> arrange(species, island)\n\n# A tibble: 344 √ó 8\n   species island bill_length_mm bill_depth_mm flipper_len‚Ä¶¬π body_‚Ä¶¬≤ sex    year\n   <fct>   <fct>           <dbl>         <dbl>         <int>   <int> <fct> <int>\n 1 Adelie  Biscoe           37.8          18.3           174    3400 fema‚Ä¶  2007\n 2 Adelie  Biscoe           37.7          18.7           180    3600 male   2007\n 3 Adelie  Biscoe           35.9          19.2           189    3800 fema‚Ä¶  2007\n 4 Adelie  Biscoe           38.2          18.1           185    3950 male   2007\n 5 Adelie  Biscoe           38.8          17.2           180    3800 male   2007\n 6 Adelie  Biscoe           35.3          18.9           187    3800 fema‚Ä¶  2007\n 7 Adelie  Biscoe           40.6          18.6           183    3550 male   2007\n 8 Adelie  Biscoe           40.5          17.9           187    3200 fema‚Ä¶  2007\n 9 Adelie  Biscoe           37.9          18.6           172    3150 fema‚Ä¶  2007\n10 Adelie  Biscoe           40.5          18.9           180    3950 male   2007\n# ‚Ä¶ with 334 more rows, and abbreviated variable names ¬π‚Äãflipper_length_mm,\n#   ¬≤‚Äãbody_mass_g\n\n\nYou can use the desc() function to sort descending.\n\npenguins |> arrange(desc(species))\n\n# A tibble: 344 √ó 8\n   species island bill_length_mm bill_depth_mm flipper_len‚Ä¶¬π body_‚Ä¶¬≤ sex    year\n   <fct>   <fct>           <dbl>         <dbl>         <int>   <int> <fct> <int>\n 1 Gentoo  Biscoe           46.1          13.2           211    4500 fema‚Ä¶  2007\n 2 Gentoo  Biscoe           50            16.3           230    5700 male   2007\n 3 Gentoo  Biscoe           48.7          14.1           210    4450 fema‚Ä¶  2007\n 4 Gentoo  Biscoe           50            15.2           218    5700 male   2007\n 5 Gentoo  Biscoe           47.6          14.5           215    5400 male   2007\n 6 Gentoo  Biscoe           46.5          13.5           210    4550 fema‚Ä¶  2007\n 7 Gentoo  Biscoe           45.4          14.6           211    4800 fema‚Ä¶  2007\n 8 Gentoo  Biscoe           46.7          15.3           219    5200 male   2007\n 9 Gentoo  Biscoe           43.3          13.4           209    4400 fema‚Ä¶  2007\n10 Gentoo  Biscoe           46.8          15.4           215    5150 male   2007\n# ‚Ä¶ with 334 more rows, and abbreviated variable names ¬π‚Äãflipper_length_mm,\n#   ¬≤‚Äãbody_mass_g\n\n\ndistinct()\nObservations will sometimes be duplicated in your data. The simplest way to remove them is with the distinct() function. For demonstration purposes, suppose you have this simple dataset.\n\ntbl <- tibble(\n  x = c(1, 1, 1, 3, 2), \n  y = c(\"a\", \"a\", \"a\", \"b\", \"c\"),\n  z = c(TRUE, TRUE, FALSE, TRUE, TRUE)\n)\n\ntbl\n\n# A tibble: 5 √ó 3\n      x y     z    \n  <dbl> <chr> <lgl>\n1     1 a     TRUE \n2     1 a     TRUE \n3     1 a     FALSE\n4     3 b     TRUE \n5     2 c     TRUE \n\n\nFor whatever reason, your first observation or row has been duplicated (there are two rows with the values x=1, y=a, and z=TRUE). To remove that row, do this:\n\ntbl |> distinct()\n\n# A tibble: 4 √ó 3\n      x y     z    \n  <dbl> <chr> <lgl>\n1     1 a     TRUE \n2     1 a     FALSE\n3     3 b     TRUE \n4     2 c     TRUE \n\n\nYou may also want distinct combinations of a specific variable or variables.\n\ntbl |> distinct(x, y)\n\n# A tibble: 3 √ó 2\n      x y    \n  <dbl> <chr>\n1     1 a    \n2     3 b    \n3     2 c    \n\n\nfilter()\nSubsetting data by filtering observations is a little bit more involved than simply selecting variables, but intuitively, you are simply asking for those observations that satisfy a certain condition. Getting filter() to return those observations requires that you pass it an expression containing a comparison operator. The expression is then evaluated by R for its truth or falsity, with observations that evaluate to TRUE being returned, observations that evaluate to FALSE being ignored. Let‚Äôs walk through an example, then try to break down what is happening in a little more detail. Suppose we want only those observations of penguins residing on Biscoe Island. Here is how we would go about collecting those observations from our penguins data.frame.\n\npenguins |> filter(island == \"Biscoe\")\n\n# A tibble: 168 √ó 8\n   species island bill_length_mm bill_depth_mm flipper_len‚Ä¶¬π body_‚Ä¶¬≤ sex    year\n   <fct>   <fct>           <dbl>         <dbl>         <int>   <int> <fct> <int>\n 1 Adelie  Biscoe           37.8          18.3           174    3400 fema‚Ä¶  2007\n 2 Adelie  Biscoe           37.7          18.7           180    3600 male   2007\n 3 Adelie  Biscoe           35.9          19.2           189    3800 fema‚Ä¶  2007\n 4 Adelie  Biscoe           38.2          18.1           185    3950 male   2007\n 5 Adelie  Biscoe           38.8          17.2           180    3800 male   2007\n 6 Adelie  Biscoe           35.3          18.9           187    3800 fema‚Ä¶  2007\n 7 Adelie  Biscoe           40.6          18.6           183    3550 male   2007\n 8 Adelie  Biscoe           40.5          17.9           187    3200 fema‚Ä¶  2007\n 9 Adelie  Biscoe           37.9          18.6           172    3150 fema‚Ä¶  2007\n10 Adelie  Biscoe           40.5          18.9           180    3950 male   2007\n# ‚Ä¶ with 158 more rows, and abbreviated variable names ¬π‚Äãflipper_length_mm,\n#   ¬≤‚Äãbody_mass_g\n\n\nHere we supplied this key information to filter():\nisland == \"Biscoe\"\nWhat does this expression mean exactly? In effect, it is directing filter() to scan through our data, specifically the island column, and select only those rows where the value is Biscoe. The so-called comparison operator here is the double equal sign, ==. This is importantly different than the single equal sign, =, which is used inside a function as part of a key=value or argument=value pair. R provides several helpful comparison operators:\n\n\n== for equals in the sense of a perfect match,\n\n\n!= for not equals,\n\n\n> for greater than,\n\n\n>= for greater than or equal to,\n\n\n< for less than, and\n\n\n<= for less than or equal to.\n\nYou can use the first two, == and !=, for comparisons with either character or numeric variables, but the rest apply only to the latter. Let‚Äôs run through a few more examples:\nFilter penguins with body mass greater than 3500 grams.\n\npenguins |> filter(body_mass_g > 3500)\n\n# A tibble: 264 √ó 8\n   species island    bill_length_mm bill_depth_mm flipper_‚Ä¶¬π body_‚Ä¶¬≤ sex    year\n   <fct>   <fct>              <dbl>         <dbl>      <int>   <int> <fct> <int>\n 1 Adelie  Torgersen           39.1          18.7        181    3750 male   2007\n 2 Adelie  Torgersen           39.5          17.4        186    3800 fema‚Ä¶  2007\n 3 Adelie  Torgersen           39.3          20.6        190    3650 male   2007\n 4 Adelie  Torgersen           38.9          17.8        181    3625 fema‚Ä¶  2007\n 5 Adelie  Torgersen           39.2          19.6        195    4675 male   2007\n 6 Adelie  Torgersen           42            20.2        190    4250 <NA>   2007\n 7 Adelie  Torgersen           37.8          17.3        180    3700 <NA>   2007\n 8 Adelie  Torgersen           38.6          21.2        191    3800 male   2007\n 9 Adelie  Torgersen           34.6          21.1        198    4400 male   2007\n10 Adelie  Torgersen           36.6          17.8        185    3700 fema‚Ä¶  2007\n# ‚Ä¶ with 254 more rows, and abbreviated variable names ¬π‚Äãflipper_length_mm,\n#   ¬≤‚Äãbody_mass_g\n\n\nFilter penguins with beaks longer than 39 millimeters.\n\npenguins |> filter(bill_length_mm > 39)\n\n# A tibble: 260 √ó 8\n   species island    bill_length_mm bill_depth_mm flipper_‚Ä¶¬π body_‚Ä¶¬≤ sex    year\n   <fct>   <fct>              <dbl>         <dbl>      <int>   <int> <fct> <int>\n 1 Adelie  Torgersen           39.1          18.7        181    3750 male   2007\n 2 Adelie  Torgersen           39.5          17.4        186    3800 fema‚Ä¶  2007\n 3 Adelie  Torgersen           40.3          18          195    3250 fema‚Ä¶  2007\n 4 Adelie  Torgersen           39.3          20.6        190    3650 male   2007\n 5 Adelie  Torgersen           39.2          19.6        195    4675 male   2007\n 6 Adelie  Torgersen           42            20.2        190    4250 <NA>   2007\n 7 Adelie  Torgersen           41.1          17.6        182    3200 fema‚Ä¶  2007\n 8 Adelie  Torgersen           42.5          20.7        197    4500 male   2007\n 9 Adelie  Torgersen           46            21.5        194    4200 male   2007\n10 Adelie  Biscoe              40.6          18.6        183    3550 male   2007\n# ‚Ä¶ with 250 more rows, and abbreviated variable names ¬π‚Äãflipper_length_mm,\n#   ¬≤‚Äãbody_mass_g\n\n\nMultiple conditions\nOften enough, we will want to combine a number of these simple conditions into one complex expression. In R, this is done with Boolean operators:\n\n\n& for and,\n\n\n| for or, and\n\n\n! for not.\n\nTo demonstrate the underlying logic of these Boolean operators, consider these shapes and colors. You can think of each of A, B, and C as its own observation or row in a data.frame that includes two variables color and shape.\n\n\n\n\n\n\n\n\n  \n  \n\nBoolean\n      Filter\n      Result\n    \n\n\nx\n\ncolor == \"yellow\"\n\nA, B\n\n\ny\n\nshape == \"circle\"\n\nB, C\n\n\nx & y\n\ncolor == \"yellow\" & shape == \"circle\"\n\nB\n\n\nx | y\n\ncolor == \"yellow\" | shape == \"circle\"\n\nA, B, C\n\n\nx & !y\n\ncolor == \"yellow\" & shape != \"circle\"\n\nA\n\n\n!x & y\n\ncolor != \"yellow\" & shape == \"circle\"\n\nC\n\n\n!(x & y)\n\n!(color == \"yellow\" & shape == \"circle\")\n\nA, C\n\n\n!(x | y)\n\n!(color == \"yellow\" | shape == \"circle\")\n\nNULL\n\n\n\n\n\n\nAnd here is an example with our penguins data, where we ask R to return those observations in which (a) penguins reside on Biscoe Island and (b) their bills are longer than 39 millimeters.\n\npenguins |> filter(island == \"Biscoe\" & bill_length_mm > 39)\n\n# A tibble: 143 √ó 8\n   species island bill_length_mm bill_depth_mm flipper_len‚Ä¶¬π body_‚Ä¶¬≤ sex    year\n   <fct>   <fct>           <dbl>         <dbl>         <int>   <int> <fct> <int>\n 1 Adelie  Biscoe           40.6          18.6           183    3550 male   2007\n 2 Adelie  Biscoe           40.5          17.9           187    3200 fema‚Ä¶  2007\n 3 Adelie  Biscoe           40.5          18.9           180    3950 male   2007\n 4 Adelie  Biscoe           39.6          17.7           186    3500 fema‚Ä¶  2008\n 5 Adelie  Biscoe           40.1          18.9           188    4300 male   2008\n 6 Adelie  Biscoe           42            19.5           200    4050 male   2008\n 7 Adelie  Biscoe           41.4          18.6           191    3700 male   2008\n 8 Adelie  Biscoe           40.6          18.8           193    3800 male   2008\n 9 Adelie  Biscoe           41.3          21.1           195    4400 male   2008\n10 Adelie  Biscoe           41.1          18.2           192    4050 male   2008\n# ‚Ä¶ with 133 more rows, and abbreviated variable names ¬π‚Äãflipper_length_mm,\n#   ¬≤‚Äãbody_mass_g\n\n\nNote that filter() let‚Äôs you separate conditions with a comma, which it interprets as conjunction, represented by the &.\n\npenguins |> \n  filter(\n    island == \"Biscoe\", \n    species == \"Adelie\", \n    body_mass_g < 3500\n  )\n\n# A tibble: 13 √ó 8\n   species island bill_length_mm bill_depth_mm flipper_len‚Ä¶¬π body_‚Ä¶¬≤ sex    year\n   <fct>   <fct>           <dbl>         <dbl>         <int>   <int> <fct> <int>\n 1 Adelie  Biscoe           37.8          18.3           174    3400 fema‚Ä¶  2007\n 2 Adelie  Biscoe           40.5          17.9           187    3200 fema‚Ä¶  2007\n 3 Adelie  Biscoe           37.9          18.6           172    3150 fema‚Ä¶  2007\n 4 Adelie  Biscoe           35            17.9           190    3450 fema‚Ä¶  2008\n 5 Adelie  Biscoe           34.5          18.1           187    2900 fema‚Ä¶  2008\n 6 Adelie  Biscoe           36.5          16.6           181    2850 fema‚Ä¶  2008\n 7 Adelie  Biscoe           35.7          16.9           185    3150 fema‚Ä¶  2008\n 8 Adelie  Biscoe           36.4          17.1           184    2850 fema‚Ä¶  2008\n 9 Adelie  Biscoe           35.5          16.2           195    3350 fema‚Ä¶  2008\n10 Adelie  Biscoe           37.7          16             183    3075 fema‚Ä¶  2009\n11 Adelie  Biscoe           37.9          18.6           193    2925 fema‚Ä¶  2009\n12 Adelie  Biscoe           38.1          17             181    3175 fema‚Ä¶  2009\n13 Adelie  Biscoe           39.7          17.7           193    3200 fema‚Ä¶  2009\n# ‚Ä¶ with abbreviated variable names ¬π‚Äãflipper_length_mm, ¬≤‚Äãbody_mass_g\n\n\nExercises\n\nTry all of the following with filter():\n\nFilter penguins that reside on Torgersen island.\nFilter penguins that have a flipper length greater than 185 mm.\nFilter penguins that reside on Torgersen island and have a body mass less than 3500 g."
  },
  {
    "objectID": "labs/06-dataframes-lab.html#groups",
    "href": "labs/06-dataframes-lab.html#groups",
    "title": "Lab 06: Working with Tables",
    "section": "Groups",
    "text": "Groups\nYour data will often include natural groupings (like species in the penguins table or dart types in the darts table), and you will want to summarize information about each of those groups. The way that you do this with dplyr involves the use of group_by() and summarize().\n\npenguins |> group_by(species)\n\n# A tibble: 344 √ó 8\n# Groups:   species [3]\n   species island    bill_length_mm bill_depth_mm flipper_‚Ä¶¬π body_‚Ä¶¬≤ sex    year\n   <fct>   <fct>              <dbl>         <dbl>      <int>   <int> <fct> <int>\n 1 Adelie  Torgersen           39.1          18.7        181    3750 male   2007\n 2 Adelie  Torgersen           39.5          17.4        186    3800 fema‚Ä¶  2007\n 3 Adelie  Torgersen           40.3          18          195    3250 fema‚Ä¶  2007\n 4 Adelie  Torgersen           NA            NA           NA      NA <NA>   2007\n 5 Adelie  Torgersen           36.7          19.3        193    3450 fema‚Ä¶  2007\n 6 Adelie  Torgersen           39.3          20.6        190    3650 male   2007\n 7 Adelie  Torgersen           38.9          17.8        181    3625 fema‚Ä¶  2007\n 8 Adelie  Torgersen           39.2          19.6        195    4675 male   2007\n 9 Adelie  Torgersen           34.1          18.1        193    3475 <NA>   2007\n10 Adelie  Torgersen           42            20.2        190    4250 <NA>   2007\n# ‚Ä¶ with 334 more rows, and abbreviated variable names ¬π‚Äãflipper_length_mm,\n#   ¬≤‚Äãbody_mass_g\n\n\nNotice that the data are not changed, but the tibble now includes information about the groupings (Groups: species [3]). Now, if you apply summarize() to it, it will know that you want summaries for each group.\n\npenguins |> \n  group_by(species) |> \n  summarize(\n    mean_bill_length = mean(bill_length_mm, na.rm = TRUE),\n    var_bill_length = var(bill_length_mm, na.rm = TRUE)\n  )\n\n# A tibble: 3 √ó 3\n  species   mean_bill_length var_bill_length\n  <fct>                <dbl>           <dbl>\n1 Adelie                38.8            7.09\n2 Chinstrap             48.8           11.2 \n3 Gentoo                47.5            9.50\n\n\nNote that you can summarize by multiple groups.\n\npenguins |> \n  group_by(species, island) |> \n  summarize(\n    mean_bill_length = mean(bill_length_mm, na.rm = TRUE),\n    var_bill_length = var(bill_length_mm, na.rm = TRUE)\n  )\n\n# A tibble: 5 √ó 4\n# Groups:   species [3]\n  species   island    mean_bill_length var_bill_length\n  <fct>     <fct>                <dbl>           <dbl>\n1 Adelie    Biscoe                39.0            6.15\n2 Adelie    Dream                 38.5            6.08\n3 Adelie    Torgersen             39.0            9.15\n4 Chinstrap Dream                 48.8           11.2 \n5 Gentoo    Biscoe                47.5            9.50\n\n\nA handy little function you will often want to use in summaries is n(). This counts the number of observations in each group.\n\npenguins |> \n  group_by(species, island) |> \n  summarize(\n    count = n(),\n    mean_bill_length = mean(bill_length_mm, na.rm = TRUE),\n    var_bill_length = var(bill_length_mm, na.rm = TRUE)\n  )\n\n# A tibble: 5 √ó 5\n# Groups:   species [3]\n  species   island    count mean_bill_length var_bill_length\n  <fct>     <fct>     <int>            <dbl>           <dbl>\n1 Adelie    Biscoe       44             39.0            6.15\n2 Adelie    Dream        56             38.5            6.08\n3 Adelie    Torgersen    52             39.0            9.15\n4 Chinstrap Dream        68             48.8           11.2 \n5 Gentoo    Biscoe      124             47.5            9.50\n\n\nNow, suppose you wanted to get the three penguins in each species with the longest bills. To do that, you can apply the slice_max() function to the grouped penguins table.\n\npenguins |> \n  group_by(species) |> \n  slice_max(bill_length_mm, n = 3)\n\n# A tibble: 9 √ó 8\n# Groups:   species [3]\n  species   island    bill_length_mm bill_depth_mm flipper‚Ä¶¬π body_‚Ä¶¬≤ sex    year\n  <fct>     <fct>              <dbl>         <dbl>     <int>   <int> <fct> <int>\n1 Adelie    Torgersen           46            21.5       194    4200 male   2007\n2 Adelie    Torgersen           45.8          18.9       197    4150 male   2008\n3 Adelie    Biscoe              45.6          20.3       191    4600 male   2009\n4 Chinstrap Dream               58            17.8       181    3700 fema‚Ä¶  2007\n5 Chinstrap Dream               55.8          19.8       207    4000 male   2009\n6 Chinstrap Dream               54.2          20.8       201    4300 male   2008\n7 Gentoo    Biscoe              59.6          17         230    6050 male   2007\n8 Gentoo    Biscoe              55.9          17         228    5600 male   2009\n9 Gentoo    Biscoe              55.1          16         230    5850 male   2009\n# ‚Ä¶ with abbreviated variable names ¬π‚Äãflipper_length_mm, ¬≤‚Äãbody_mass_g\n\n\nYou can also get a random sample from each group (known as a stratified random sample) with slice_sample().\n\npenguins |> \n  group_by(species) |> \n  slice_sample(n = 3)\n\n# A tibble: 9 √ó 8\n# Groups:   species [3]\n  species   island bill_length_mm bill_depth_mm flipper_le‚Ä¶¬π body_‚Ä¶¬≤ sex    year\n  <fct>     <fct>           <dbl>         <dbl>        <int>   <int> <fct> <int>\n1 Adelie    Dream            42.3          21.2          191    4150 male   2007\n2 Adelie    Biscoe           38.1          16.5          198    3825 fema‚Ä¶  2009\n3 Adelie    Dream            36            17.9          190    3450 fema‚Ä¶  2007\n4 Chinstrap Dream            48.5          17.5          191    3400 male   2007\n5 Chinstrap Dream            52            20.7          210    4800 male   2008\n6 Chinstrap Dream            51.3          19.9          198    3700 male   2007\n7 Gentoo    Biscoe           48.4          14.6          213    5850 male   2007\n8 Gentoo    Biscoe           46.4          15.6          221    5000 male   2008\n9 Gentoo    Biscoe           45.2          16.4          223    5950 male   2008\n# ‚Ä¶ with abbreviated variable names ¬π‚Äãflipper_length_mm, ¬≤‚Äãbody_mass_g\n\n\nSome other useful slice_*() functions include the following\n\n\nslice_head(n) takes the first n rows in each group.\n\nslice_tail(n) takes the last n rows in each group.\n\nslice_min(x, n) takes the n rows with the smallest values of x.\n\nslice_max(x, n) takes the n rows with the largest value of x.\n\nslice_sample(n) takes n random rows in each group.\n\nExercises\n\nGroup the penguins data by species and summarize the mean flipper length.\nGroup the penguins data by island and count the number of penguins on each. Hint: you can use n() to do this.\nGroup the penguins data by sex and summarize the mean and variance in body mass.\nGroup the penguins data by sex and species and summarize the mean and variance in body mass.\nGroup the darts data by type and summarize the mean length, width, and height.\nGroup the darts data by type and take a random sample of length five from each group.\nRemove any potential duplicates from the darts data with distinct()."
  },
  {
    "objectID": "labs/07-evaluation-lab.html",
    "href": "labs/07-evaluation-lab.html",
    "title": "Lab 07: Evaluating Linear Models",
    "section": "",
    "text": "This lab will guide you through the process of\n\nimporting and exporting data tables\nVisualizing a linear model\n\nUsing AB lines\nUsing predictions\nAdding prediction and confidence intervals\n\n\nEvaluating a linear model\n\ninterpreting model summaries in R\nt-tests\nANOVA\n\n\nDiagnostic plots with plot() and check_model()\n\nResidual Histogram\nRaw Residuals v Fitted\nStandardized Residuals v Fitted\nResiduals v Leverage (and Cook‚Äôs Distance)\nQ-Q Plot\n\n\n\nWe will be using the following packages:\n\narchdata\npalmerpenguins\nperformance\nskimr\ntidyverse\n\nYou‚Äôll want to install performance with install.packages(\"performance\").\n\nlibrary(archdata)\nlibrary(palmerpenguins)\nlibrary(performance)\nlibrary(skimr)\nlibrary(tidyverse)\n\n\n\n\ncars\n\nIncludes measurements of car speed and stopping distance.\npackage: datasets\n\nreference: https://stat.ethz.ch/R-manual/R-devel/library/datasets/html/cars.html\n\n\n\n\nDartPoints\n\nIncludes measurements of 91 Archaic dart points recovered during surface surveys at Fort Hood, Texas.\npackage: archdata\n\nreference: https://cran.r-project.org/web/packages/archdata/archdata.pdf\n\n\n\n\nOxfordPots\n\nIncludes percentages of late Romano-British Oxford Pottery on 30 sites, along with their distance from Oxford.\nreference: https://cran.r-project.org/web/packages/archdata/archdata.pdf\n\n\n\n\npenguins\n\nIncludes measurements for penguin species, island in Palmer Archipelago, size (flipper length, body mass, bill dimensions), and sex.\npackage: palmerpenguins\n\nreference: https://allisonhorst.github.io/palmerpenguins/reference/penguins.html"
  },
  {
    "objectID": "labs/07-evaluation-lab.html#readwrite-data",
    "href": "labs/07-evaluation-lab.html#readwrite-data",
    "title": "Lab 07: Evaluating Linear Models",
    "section": "Read/Write Data",
    "text": "Read/Write Data\n\n\n\n\n\nFigure¬†1: Elements of a Data Frame.\n\n\nThis sections covers how to import data into and export data out of R, with a focus on rectangular data or tables. While many formats exist for representing rectangular data in plain-text file, the most common is perhaps common-separated values. You can spot this format by looking for files on your computer with the .csv extension. If we had saved the data represented in Figure¬†1 to a csv file, the plain text of that file would look like this:\ntype, length, width, height\nElko, 2.03, 0.8, 3.23 \nRosegate, 1.4, 0.4, 2.4\nDSN, 1.9, 0.3, 1.29\nElko, 2.1, 0.7, 2.7\nClovis, 3.3, 0.95, 4.15 \nwith the first row typically assumed to contain the column names, also known as the header. To prove this to yourself, you can open a .csv file using a text editor like Notepad.\nBase R provides two functions for reading and writing these sorts of files: read.csv() and write.csv(). These have some unfortunate default behavior, however, so we are going to focus on their tidyverse equivalents. Those are read_csv() and write_csv() from the readr package. There are three main differences between these functions:\n\nthe tidy functions cannot be used without loading readr,\n\nread_csv() reads the data in as a tibble rather than a data.frame ,and\n\nwrite_csv() does not by default add a column with row names or row ids to the exported csv file.\n\n\nis a cost of doing business, but (2) and (3) more than make up for that.\n\nFile Paths\nTo read and write data into R, you have to point R to where that data lives on your computer, meaning you have to specify a path to the file that holds the data. Suppose, for example, the penguins.csv file lives on Leslie Knope‚Äôs computer in this location:\nC:/Users/Leslie_Knope/qaad-course/data/penguins.csv\nThis is a file path. The ‚Äú/‚Äù denotes containment, meaning the thing to the right is within the thing to the left, so the penguins.csv file is in the data folder, which is in the qaad-course folder, which is in the Leslie_Knope folder, which is in the Users folder, which is in the C: folder. I have called them ‚Äúfolders‚Äù here, of course, but you will also often hear them referred to as ‚Äúdirectories.‚Äù\nTo read the penguins data into R from the location specified by this file path, you simply feed the file path as a character string to read_csv() like so:\n\npenguins <- read_csv(\"C:/Users/Leslie_Knope/qaad-course/data/penguins.csv\")\n\nNote that you will usually want to assign the table to an object, so that you can reference it later on during your R session.\nIf you have the data in R and you want to write to this location, it works much the same way. The only difference is that you have to tell write_csv() what table you are wanting to write to disc, hence:\n\nwrite_csv(penguins, \"C:/Users/Leslie_Knope/qaad-course/data/penguins.csv\")\n\nAbsolute v Relative Paths\nThe path listed above is known as an absolute path because it starts at the top most directory in the file path, namely ‚ÄúC:‚Äù, and works out from there to the exact location of the file. This top level directory is also known as the root directory. That‚Äôs ‚ÄúC:‚Äù for Windows systems. For MacOS the root is, well, ‚Äúroot‚Äù.\nWhenever you open R, your session happens somewhere on your computer, meaning it has some sense of its location within your file system. This is known as the working directory. For instance, when Leslie opens an R session, it might open in her ‚Äúqaad-course‚Äù folder. The location of her working directory is, thus, ‚ÄúC:/Users/Leslie_Knope/qaad-course.‚Äù Because of this, she does not have to specify the entire file path to the penguins file. All she needs to specify is ‚Äúdata/penguins.csv,‚Äù as in read_csv(\"data/penguins.csv\"). This is known as a relative file path, as it is relative to the working directory.\nOf course, R doesn‚Äôt just open anywhere on your computer. By default, it will always open in the same place (usually the Documents folder on Windows). But you can then point it to a specific directory on your computer where you would like to work. The traditional way of doing this is with the function setwd(), as in\n\nsetwd(\"C:/Users/Leslie_Knope/qaad-course/data/penguins.csv\")\n\nThis is a brittle method, however, as it requires specifying the working directory using an absolute file path. So, if Leslie changes the location of qaad-course on her computer, if she moves it to, say, the parks-and-rec folder, it will no longer work. And, if Leslie wants to share her qaad-course folder - all the work and data and R code - with someone else, so they save that folder onto their own computer, again the call to setwd() with the absolute path will not work. The actual path could be ‚ÄúC:/Users/Gerry/‚Ä¶‚Äù or ‚ÄúC:/Users/Jerry/‚Ä¶‚Äù or something to that effect, and yet the call to setwd() uses ‚ÄúC:/Users/Leslie_Knope/‚Ä¶‚Äù.\nThe alternative to setting the working directory with setwd() is to use R projects. Fortunately, you have already done this for this class! Whenever you open an R project, the R session will automatically be set to the directory where that R project lives, whether that is on Leslie‚Äôs computer or Gerry‚Äôs or Andy‚Äôs or April‚Äôs. Doesn‚Äôt matter. Then you can use relative paths as you please, as long as you keep all the necessary data and code together in that project folder, which is what you should do!\nThere are some extra advantages to using projects, but we‚Äôll leave it at that for now.\nExercises\n\nMake sure your working directory is set to the R project folder for the course!\nUse write_csv() to write the cars data to a file called cars.csv in your project‚Äôs data folder.\nRemove the cars data from your environment with remove(cars). If you look in the environment pane of RStudio (in the top right corner), you should see that the cars table has been removed.\nNow read cars.csv back into R with read_csv() and assign it to cars. Check to make sure the table is back in your environment."
  },
  {
    "objectID": "labs/07-evaluation-lab.html#visualize-model",
    "href": "labs/07-evaluation-lab.html#visualize-model",
    "title": "Lab 07: Evaluating Linear Models",
    "section": "Visualize Model",
    "text": "Visualize Model\nHere we are going to use ordinary least squares to build a simple linear model of penguin bill length as a function of flipper length. To aid in the interpretation of this model, it is useful to visualize the relationship or trend it suggests (if it does suggest one!). Before building that model, however, you should, as always, make sure to visualize your data! In this case, we make a simple scatter plot.\n\n# clean up data, remove rows with missing data\npenguins <- na.omit(penguins)\n\nggplot(penguins, aes(flipper_length_mm, bill_length_mm)) +\n  geom_point(size = 2) +\n  labs(\n    x = \"Flipper Length (mm)\",\n    y = \"Bill Length (mm)\"\n  ) +\n  theme_minimal()\n\n\n\n\nWhat is the relationship here? Let‚Äôs see if a linear model can help us out.\n\npenguins_model <- lm(bill_length_mm ~ flipper_length_mm, data = penguins)\n\nsummary(penguins_model)\n#> \n#> Call:\n#> lm(formula = bill_length_mm ~ flipper_length_mm, data = penguins)\n#> \n#> Residuals:\n#>    Min     1Q Median     3Q    Max \n#> -8.637 -2.698 -0.579  2.066 19.095 \n#> \n#> Coefficients:\n#>                   Estimate Std. Error t value            Pr(>|t|)    \n#> (Intercept)        -7.2186     3.2717   -2.21               0.028 *  \n#> flipper_length_mm   0.2548     0.0162   15.69 <0.0000000000000002 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 4.15 on 331 degrees of freedom\n#> Multiple R-squared:  0.427,  Adjusted R-squared:  0.425 \n#> F-statistic:  246 on 1 and 331 DF,  p-value: <0.0000000000000002\n\nThere are two ways to visualize this model:\n\nWith abline. Use the estimated coefficients (the slope and intercept) to construct a formula that will calculate values of y across the range of x. The formula has the form: \\(y \\sim a + bx\\), where is \\(a\\) is the intercept and \\(b\\) is the slope, hence abline.\nWith predict. Use the model to estimate values of \\(y\\) for specified values of \\(x\\) and construct a line from those values.\n\nOK. There‚Äôs actually three ways to do it. The most direct way that ggplot() offers uses geom_smooth(), but we‚Äôll save that one for another time. The point here isn‚Äôt just to learn how to visualize a linear model, but to learn what it is that we are visualizing and what it means. So, let‚Äôs try an example of each, so you can get a feel for how to do this.\nABline\nggplot() has a geometry for plotting AB lines. As you might have guessed, it‚Äôs geom_abline(). All we need to do is extract the values of the coefficients from the model and feed these to the slope and intercept parameters, respectively. To do that, we will use the coefficients() function. This provides a named vector that we can use to get our estimates. Notice that we use <vector>[[<variable]] like we do with tables, only this time we are extracting a single value.\n\nbetas <- coefficients(penguins_model)\n\nbetas\n#>       (Intercept) flipper_length_mm \n#>            -7.219             0.255\n\nintercept <- betas[[\"(Intercept)\"]]\nslope <- betas[[\"flipper_length_mm\"]]\n\nNow, we can plot our model over the data. This is always useful, as you can see how the model compares to the actual observations.\n\nggplot(penguins, aes(flipper_length_mm, bill_length_mm)) +\n  geom_point(size = 2) +\n  labs(\n    x = \"Flipper Length (mm)\",\n    y = \"Bill Length (mm)\"\n  ) +\n  theme_minimal() +\n  geom_abline(\n    slope = slope,\n    intercept = intercept,\n    color = \"darkred\",\n    linewidth = 1\n  )\n\n\n\n\nWith this method, we simply supply the coefficients. ggplot() then uses those to estimate values of y for each value of x shown within the range of x represented by the plot. Notice that the line continues across the full range of the graph. This shows that the model assumes the relationship is linear, meaning in this case that it will always increase to the right (to infinity) and always decrease to the left (to negative infinity)\nPredict\nWe can also generate values of y manually with the predict() function. The key here is to supply it with our model, which it will then use to make predictions.\n\npenguins <- penguins |> mutate(estimates = predict(penguins_model))\n\nggplot(penguins) +\n  geom_point(\n    aes(flipper_length_mm, bill_length_mm),\n    size = 2\n  ) +\n  labs(\n    x = \"Flipper Length (mm)\",\n    y = \"Bill Length (mm)\"\n  ) +\n  theme_minimal() +\n  geom_line(\n    aes(flipper_length_mm, estimates),\n    color = \"darkred\",\n    size = 1\n  )\n\n\n\n\nThis is very similar to our abline graph above, but with the important difference that the trend line or modeled relationship does not extend across the entire graph.\nNote that you can use predict() to estimate the value of the response at specific values of the independent variable. To do that, you simply feed the predict() function, specifically its newdata parameter, a table with the values of the independent variable that interest you. For example, suppose you wanted to know what bill length this model would expect for a penguin having a body mass of, say, 4,500 grams. We can figure that out this way:\n\nnew_data <- tibble(flipper_length_mm = 200)\n\npredict(penguins_model, newdata = new_data)\n#>    1 \n#> 43.7\n\nIf you like, you can also do that for multiple values like so:\n\nnew_data <- tibble(flipper_length_mm = c(190, 195, 200, 205))\n\npredict(penguins_model, newdata = new_data)\n#>    1    2    3    4 \n#> 41.2 42.5 43.7 45.0\n\nIntervals\nYou can use predict() to calculate the prediction interval for these estimates by specifying interval = \"prediction\". Note, too, that we ask it to provide that interval at level = 0.95 to ensure the prediction interval is estimated at the 95% level. Recall that this is the interval within which we expect the value of Y to fall with 95% probability for each value of X.\n\npredict(\n  penguins_model, \n  newdata = new_data,\n  interval = \"prediction\",\n  level = 0.95\n)\n#>    fit  lwr  upr\n#> 1 41.2 33.0 49.4\n#> 2 42.5 34.3 50.6\n#> 3 43.7 35.6 51.9\n#> 4 45.0 36.8 53.2\n\nIf we set interval = \"confidence\", we can get the confidence interval, or the interval within which we expect the average value of Y to fall with 95% probability for each value of X.\n\npredict(\n  penguins_model, \n  newdata = new_data,\n  interval = \"confidence\",\n  level = 0.95\n)\n#>    fit  lwr  upr\n#> 1 41.2 40.6 41.8\n#> 2 42.5 42.0 43.0\n#> 3 43.7 43.3 44.2\n#> 4 45.0 44.6 45.5\n\nWe can actually add these to our model using the function geom_ribbon() like so.\n\n\nconfidence <- predict(\n  penguins_model, \n  interval = \"confidence\",\n  level = 0.95\n)\n\n# coerce to a table\nconfidence <- as_tibble(confidence)\n\n# add the X values\nconfidence <- confidence |> mutate(flipper_length_mm = penguins$flipper_length_mm)\n\nggplot() +\n  geom_ribbon(\n    data = confidence,\n    aes(x = flipper_length_mm, ymin = lwr, ymax = upr),\n    fill = \"gray85\"\n  ) +\n  geom_line(\n    data = confidence, \n    aes(flipper_length_mm, fit),\n    color = \"darkred\",\n    linewidth = 1\n  ) +\n  geom_point(\n    data = penguins,\n    aes(flipper_length_mm, bill_length_mm),\n    size = 2\n  ) +\n  theme_minimal() +\n  labs(\n    x = \"Flipper Length (mm)\",\n    y = \"Bill Length (mm)\"\n  )\n\n\n\n\nThe ribbon geometry is, in effect, a polygon defined by an upper and lower line (ymax and ymin, respectively). Note that we added the ribbon before adding the trend line. If we did the reverse, the ribbon would be plotted over the trend line, thus obscuring it.\nExercises\n\nBuild a model of penguin flipper length by body mass.\n\nMake sure to visualize your data first! Make a scatter plot!\n\n\nNow plot the modeled relationship between flipper length and body mass.\n\nUse coefficients() and geom_abline().\nUse predict() and geom_line().\nAdd the confidence interval to the second plot using geom_ribbon()."
  },
  {
    "objectID": "labs/07-evaluation-lab.html#model-summary",
    "href": "labs/07-evaluation-lab.html#model-summary",
    "title": "Lab 07: Evaluating Linear Models",
    "section": "Model Summary",
    "text": "Model Summary\nLet‚Äôs return to the summary() of our model and discuss it in a little more detail.\n\nsummary(penguins_model)\n#> \n#> Call:\n#> lm(formula = bill_length_mm ~ flipper_length_mm, data = penguins)\n#> \n#> Residuals:\n#>    Min     1Q Median     3Q    Max \n#> -8.637 -2.698 -0.579  2.066 19.095 \n#> \n#> Coefficients:\n#>                   Estimate Std. Error t value            Pr(>|t|)    \n#> (Intercept)        -7.2186     3.2717   -2.21               0.028 *  \n#> flipper_length_mm   0.2548     0.0162   15.69 <0.0000000000000002 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 4.15 on 331 degrees of freedom\n#> Multiple R-squared:  0.427,  Adjusted R-squared:  0.425 \n#> F-statistic:  246 on 1 and 331 DF,  p-value: <0.0000000000000002\n\nThere are four major components of the summary: (i) the function call, (ii) the residual distribution, (iii) the coefficients table, and (iv) the summary statistics. Let‚Äôs work through each one of these separately.\nThe Call. This refers to your ‚Äúcall‚Äù to the lm() function, which includes the model specification (meaning, the formula used to specify the model) and the data used to fit that model specification.\nThe Residual Distribution. This refers to the distribution of the residuals, here represented using quartiles and the minimum and maximum values. You can visualize the quartiles for the residuals really quickly with a boxplot.\n\nboxplot(residuals(penguins_model), ylab = \"Raw Residuals\")\n\n\n\n\nThe thing to keep in mind here is that the linear model fit with OLS assumes that the residuals are normally distributed, \\(\\epsilon \\approx N(0, \\sigma)\\) with a mean of zero and standard deviation, \\(\\sigma\\). Crucially, in a normal distribution, the median value is equal to the mean. In the boxplot, that means the dark line representing the median should be really close to zero. The first and third quartiles should also be equidistant from the median. In the boxplot, that means the median bar should be about halfway between top (3Q) and bottom (1Q) of the box. If it‚Äôs not, that means your residuals are not normally distributed. The minimum and maximum values kinda have this flavor, but not to the same degree.\nThe Coefficients Table. The number of rows in the coefficients table is equal to the number of variables in the model specification plus one. In this case, that‚Äôs two rows, one for the intercept and one for the flipper length variable. There are also five columns, one for the variables and the other four listing their coefficient estimates, the standard errors of those estimates, the t-statistics associated with the estimates, and the p-values for those t-statistics. If you just want the coefficients, as noted above, you can use the coefficients() function to get them. You can also get the variable names with variable.names().\n\nvariable.names(penguins_model)\n#> [1] \"(Intercept)\"       \"flipper_length_mm\"\n\ncoefficients(penguins_model)\n#>       (Intercept) flipper_length_mm \n#>            -7.219             0.255\n\nNote that the t-statistic is just the ratio of the coefficient estimate to its standard error:\n\\[t = \\frac{\\beta_i}{se(\\beta_i)}\\]\nIf you multiply the standard error of the coefficients by $$1.96, you get the 95% confidence interval for the coefficient estimate. You can extract these confidence intervals from the linear model with the confint() function.\n\nconfint(penguins_model)\n#>                     2.5 % 97.5 %\n#> (Intercept)       -13.655 -0.783\n#> flipper_length_mm   0.223  0.287\n\nYou can also visualize those intervals using a dot and whisker plot using geom_errorbar() like so:\n\n# getting the standard errors is a smidge awkward\nbetas <- tibble(\n  x = coefficients(penguins_model),\n  y = variable.names(penguins_model),\n  se = coefficients(summary(penguins_model))[, \"Std. Error\"]\n)\n\nggplot(betas, aes(x, y)) +\n  geom_vline(\n    xintercept = 0, \n    linewidth = 1, \n    color = \"darkred\"\n  ) +\n  geom_point(size = 3) +\n  geom_errorbar(\n    aes(xmin = x - (1.96*se), xmax = x + (1.96*se)),\n    width = 0.33 # height of whiskers\n  ) +\n  labs(\n    x = \"Estimate\",\n    y = NULL,\n    title = \"Model Coefficients\"\n  ) +\n  theme_minimal()\n\n\n\n\nTwo things to note here. First, the width of the confidence interval tells you something about how certain we are about the estimate, wider intervals mean less certainty, narrow intervals more certainty. Second, you can visually inspect whether the 95% confidence intervals overlap with zero. In this case, they do not. This tells you that it is extremely improbable that the coefficients could equal zero, since there is only a 2.5% chance or less of the coefficient being zero for each estimate. So, the dot and whisker plot with the coefficient estimates and their standard errors is effectively a visual representation of the t-test.\nSummary statistics. The last element of the summary() is a set of statistics. This includes the residual standard error or the standard deviation in the error. The formula for this statistic is\n\\[\n\\begin{aligned}\n\\sigma^2_\\epsilon &= \\frac{1}{n-k}\\sum \\epsilon^2 \\\\\nse_\\epsilon &= \\sqrt{\\sigma^2}\n\\end{aligned}\n\\] where \\(n-k\\) is the degrees of freedom, in this case the number of observation \\(n\\) minus the number of model parameters \\(k\\) (meaning \\(\\beta_0\\) and \\(\\beta_1\\)). In R, you can calculate this standard error like so:\n\ne <- residuals(penguins_model)\ndf <- summary(penguins_model)$df[2]\n\nsqrt(sum(e^2)/df)\n#> [1] 4.15\n\nThe set of summary statistics for a linear model also includes the R-squared value, which - if you recall - is the ratio of the Model Sum of Squares \\(SS_M\\) to the Total Sum of Squares \\(SS_T\\) (defined by the null or intercept-only model). This provides a measure of the proportion of the total variance explained by the model. You will also see here the so-called Adjusted R-squared. This is simply the R-squared statistic weighted by the complexity of the model, which is important because R-squared tends to increase with increases in model complexity, meaning each time you add an independent variable to a model it‚Äôs going to capture some amount of the total variance in the dependent variable.\nFinally, there is the F-statistic, which is similar to the R-squared value, but is estimated by the ratio of the model variance to the residual variance. The p-value is obtained by comparing the F-statistic to the F-distribution with \\(k-1=1\\) and \\(n-k=331\\) degrees of freedom. These are the summary statistics provided by the ANOVA, so let‚Äôs dive into that in the next section.\nExercises\n\nBuild a model of penguin flipper length by body mass.\n\nMake sure to visualize your data first! Make a scatter plot!\n\n\nRun summary() on your model.\n\nDo the residuals appear to be normally distributed?\nWhat is the R-Squared value?\nDoes body mass explain much of the variance in flipper length?\nHow would you interpret this result? What is the relationship between flipper length and body mass?"
  },
  {
    "objectID": "labs/07-evaluation-lab.html#anova",
    "href": "labs/07-evaluation-lab.html#anova",
    "title": "Lab 07: Evaluating Linear Models",
    "section": "ANOVA",
    "text": "ANOVA\nA really powerful way to evaluate a linear model is to submit it to an ANOVA test. If you recall, an ANOVA for comparing the distribution of a variable across groups compares the ratio of the between-group variance to the within-group variance. This is known as an F-statistic. It gives us a sense of how much variance is coming from the groups themselves versus how much is coming from the individual observations within each group. If more of the variance is coming from the groups and not the individual observations, that indicates that at least one of the groups is not like the others. Crucially, the F-statistic can be compared to an F-distribution to determine whether the statistic is likely under the null hypothesis that the groups are not different.\nAn ANOVA applied to a statistical model works in much the same way. The only catch is that we are not comparing the variance between groups but between models. More precisely, we are comparing the variance captured by the model to the remaining or residual variance. That is,\n\\[F = \\frac{\\text{model variance}}{\\text{residual variance}}\\]\nwhere model variance is equal to \\(SS_M/k\\) and residual variance is equal to \\(SS_R/(n-k-1)\\) (or the square of the residual standard error). The sum of these two squared errors (\\(SS_M\\) and \\(SS_R\\)) is actually the total sum of squares \\(SS_T\\), which is defined by the mean or intercept-only model (sometimes referred to as the null model). Because the model includes at least one more parameter than the null model, an ANOVA applied to such a model can be thought of as answering the following\nQuestion: does the model capture a sufficient amount of the variance to make its increased complexity worthwhile?\nAgain, taking the ratio of the variance explained by the model to the residual variance provides an estimate of the F-statistic, which can be compared to the F-distribution to determine how likely it is under the null hypothesis that the more complex model does not explain more variance than the null model.\nTo conduct this test in R, it‚Äôs quite simple. Simply supply a fitted linear model to the aov() function. First, however, make sure to visualize your data! Note that summarizing the ANOVA with summary() prints an ANOVA table.\n\npenguins_model_anova <- aov(penguins_model)\n\nsummary(penguins_model_anova)\n#>                    Df Sum Sq Mean Sq F value              Pr(>F)    \n#> flipper_length_mm   1   4235    4235     246 <0.0000000000000002 ***\n#> Residuals         331   5694      17                                \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nThe Mean Squared Error (Mean Sq) is calculated by dividing the Sum of Squares (Sum Sq) by the degrees of freedom (Df), the latter being a function of the number of independent variables or predictors in the model and the number of observations. MSE is, thus, another way of referring to the model and residual variance. The F-statistic is then calculated by dividing the Model variance or MSE (in this case flipper_length_mm) by the Residual variance or MSE (Residuals), in this case \\(F = 4235/17 = 246.2\\). By comparing this statistic to an F-distribution with those degrees of freedom, we get a very small p-value, much less than the standard critical value of 0.05. We can thus reject the null hypothesis and conclude that the variance explained by this model is indeed worth the price of increased complexity.\nExercises\n\nUse aov() to conduct an ANOVA on the penguins model you created in the previous section.\n\nBe sure to state your null and alternative hypotheses.\nSpecify your critical value, too.\n\n\nSummarize the penguins model again.\nNow summarize the ANOVA with summary().\n\nDoes the model provide a significant improvement over the null model?\nDid you get the same F-statistic and p-value as in the model summary?"
  },
  {
    "objectID": "labs/07-evaluation-lab.html#diagnostic-plots",
    "href": "labs/07-evaluation-lab.html#diagnostic-plots",
    "title": "Lab 07: Evaluating Linear Models",
    "section": "Diagnostic Plots",
    "text": "Diagnostic Plots\nWhenever you build a model, it is critically important to visualize the model and its assumptions as this will give you some indication about whether those assumptions have been met. Here, we‚Äôll visualize our penguins model, starting with the residuals.\nResiduals\nOne important assumption of OLS regression is that the errors are normally distributed. A simple histogram of the residuals will give us some indication of that. To get the residuals in our model, we can use the residuals() function.\n\npenguin_fit <- tibble(residuals = residuals(penguins_model))\n\nggplot(penguin_fit, aes(residuals)) +\n  geom_histogram() +\n  labs(\n    x = \"Residual Bill Length (mm)\",\n    y = \"Count\"\n  )\n\n\n\n\nDo these look normally distributed to you? Do they have the approximate shape of a bell curve? If a visual check does not suffice, you can always try a Shapiro-Wilk test for normality. To do that in R, you can use the shapiro.test() function. Note that the null hypothesis for this test is that the variable is not normally distributed.\n\nshapiro.test(residuals(penguins_model))\n#> \n#>  Shapiro-Wilk normality test\n#> \n#> data:  residuals(penguins_model)\n#> W = 1, p-value = 0.00000005\n\nAdditional Diagnostics\nBase R provides a really useful plot() method for linear models. You just feed this function your linear model and tell it which kind of plot you want to make. Here is a list of options, which you can supply to the which parameter:\n\nResiduals vs Fitted plot\nNormal Q-Q\nScale-Location\nCook‚Äôs Distance\nResiduals vs Leverage\nCook‚Äôs Distance vs Leverage\n\nHere is an example of the Q-Q Plot for our penguins model.\n\nplot(penguins_model, which = 2)\n\n\n\n\nThe plot() function in this case is extremely utilitarian. If you just want a quick visual diagnostic for your own edification, I recommend using this. However, if you want to present all these plots together in a clean way that easily communicates the assumptions being tested by each, I recommend using the check_model() function from the performance package.\n\ncheck_model(\n  penguins_model, \n  check = c(\"linearity\", \"homogeneity\", \"outliers\", \"qq\")\n)\n\n\n\n\nWith this, we can hazard all the following conclusions:\n\nThe relationship is roughly linear, though you can see some wiggle.\nThe model departs from homoscedasticity.\nNo observations have an undue influence on the model.\nLooks like the residuals might be right skewed. (compare to histogram above)\n\nNote that none of the assumptions are met perfectly. This will never be the case, not with real world data. It should also clue you in to the fact that model evaluation is never certain. It always involves some risks that you might be wrong about your model.\nExercises\n\nExtract the residuals from your model of flipper length by body mass and visualize their distribution with a histogram.\n\nDo the residuals look normally distributed?\nUse the Shapiro Wilk test to verify this.\n\n\nExplore the model with base plot().\nNow, use check_model().\n\nWhat do these plots tell you about the model?"
  },
  {
    "objectID": "labs/07-evaluation-lab.html#homework",
    "href": "labs/07-evaluation-lab.html#homework",
    "title": "Lab 07: Evaluating Linear Models",
    "section": "Homework",
    "text": "Homework\n\nLoad the following datasets from the archdata package using data().\n\nDartPoints\nOxfordPots\n\n\nPractice writing these to disc and reading them back into R.\n\nFor each dataset, use write_csv() to write the table to a csv file. Put it in the data folder in your QAAD course project folder!\nRemove the table from your environment with remove(), e.g., remove(DartPoints). (This is just so you can see it returning to your environment in the next step, but also to practice using remove().)\nRead the table back into R with read_csv(). Make sure to assign it to an object! A good idea is to avoid using upper-case letters as much as possible!\n\n\nUsing the DartPoints dataset, build a linear model showing the relationship (if any) between the length and width of dart points. Be sure to do all of the following:\n\nUse skim() to quickly summarize your data.\nVisualize the data with a scatter plot!\nState your null and alternative hypotheses.\nBuild a model with lm().\nUse summary() to report the model.\nDoes the model refute the null hypothesis?\nConduct an ANOVA of the model with aov().\nUse summary() to print the ANOVA table.\nUse coefficients() and geom_abline() to visualize the modeled relationship. Be sure to plot this over the data!\nAdd a confidence interval with geom_ribbon().\nUse check_model() to visually inspect the model.\nDoes the model satisfy the assumptions of linear regression?\n\n\nUsing the OxfordPots dataset, build a linear model showing the relationship (if any) between the percentage of Oxford Pots found on an archaeological site and the distance of that site from the city of Oxford. Be sure to do all of the following:\n\nUse skim() to quickly summarize your data.\nVisualize the data with a scatter plot!\nState your null and alternative hypotheses.\nBuild a model with lm().\nUse summary() to report the model.\nDoes the model refute the null hypothesis?\nConduct an ANOVA of this with aov().\nUse summary() to print the ANOVA table.\nUse predict() and geom_line() to visualize the modeled relationship. Be sure to plot this over the data!\nAdd a confidence interval with geom_ribbon().\nUse check_model() to visually inspect the model.\nDoes the model satisfy the assumptions of linear regression?"
  },
  {
    "objectID": "slides/01-intro-slides.html#lecture-outline",
    "href": "slides/01-intro-slides.html#lecture-outline",
    "title": "Lecture 01: Introduction",
    "section": "üìã Lecture Outline",
    "text": "üìã Lecture Outline\n\nCourse Mechanics\n\nüß± Structure\n\nüéØ Objectives\n\nüèã Expectations\n\nü§ù Ethics\n\nüíæ Software\n\n\nCourse Content\n\nWhy statistics?\n\nWhat is an archaeological population?\n\nA note about terminology and notation\n\nStatistical programming with \n\nLiterate programming with Quarto"
  },
  {
    "objectID": "slides/01-intro-slides.html#course-structure",
    "href": "slides/01-intro-slides.html#course-structure",
    "title": "Lecture 01: Introduction",
    "section": "üß± Course Structure",
    "text": "üß± Course Structure\n\nMeetings are online every Tuesday from 2:00 to 5:00 PM MST.\n\nMeeting structure:\n\nhomework review and lecture (80 minutes),\n\nbreak (10 minutes), and\n\nlab (90 minutes).\n\n\nCourse work:\n\nlab and homework exercises due every Monday before class by 9:00 PM MST, and a\n\nterm project.\n\n\nAll course materials will be made available on the course website.\n\nAll graded materials will be submitted through Canvas."
  },
  {
    "objectID": "slides/01-intro-slides.html#course-objectives",
    "href": "slides/01-intro-slides.html#course-objectives",
    "title": "Lecture 01: Introduction",
    "section": "üéØ Course Objectives",
    "text": "üéØ Course Objectives\n\nStudents will develop programming skills by learning how to:\n\nimport and export data,\nwrangle (or prepare) data for analysis,\nexplore and visualize data, and\nbuild models of data and evaluate them.\n\n\n\nAnd students will gain statistical understanding by learning how to:\n\nformulate questions and alternative hypotheses,\nidentify and explain appropriate statistical tools,\nreport the results of analysis using scientific standards, and\ncommunicate the analysis to a general audience."
  },
  {
    "objectID": "slides/01-intro-slides.html#course-expectations",
    "href": "slides/01-intro-slides.html#course-expectations",
    "title": "Lecture 01: Introduction",
    "section": "üèã Course Expectations",
    "text": "üèã Course Expectations\nLearning is a lot like moving to a new city. You get lost, you get frustrated, you even get embarrassed! But gradually, over time, you come to know your way around. Unfortunately, you‚Äôll only have four months in this new city, so we need to be realistic about what we can actually achieve here.\n\n\nYou won‚Äôt become fluent in R, markdown, or statistics, but‚Ä¶\n\n\nyou will gain some sense of the way things tend to go with those languages."
  },
  {
    "objectID": "slides/01-intro-slides.html#course-ethics",
    "href": "slides/01-intro-slides.html#course-ethics",
    "title": "Lecture 01: Introduction",
    "section": "ü§ù Course Ethics",
    "text": "ü§ù Course Ethics\nAll course policies and other University requirements can be found in the course syllabus. They are very, very thorough, so rather than enumerate them all, let‚Äôs just summarize them this way:\n\n\nThere are many ways to be a bully. Don‚Äôt be any of them.\n\nAnd if you see someone getting bullied, do something about it."
  },
  {
    "objectID": "slides/01-intro-slides.html#software",
    "href": "slides/01-intro-slides.html#software",
    "title": "Lecture 01: Introduction",
    "section": "üíæ Software",
    "text": "üíæ Software\nThe primary statistical tools for this class are\n\n Programming Language\nRStudio\nQuarto\n\nWe will go over how to install each of these during our first lab."
  },
  {
    "objectID": "slides/01-intro-slides.html#why-statistics",
    "href": "slides/01-intro-slides.html#why-statistics",
    "title": "Lecture 01: Introduction",
    "section": "Why statistics?",
    "text": "Why statistics?\n\n\n\n\n\n\n\n\n\n We want to understand something about a population.\n\nWe can never observe the entire population, so we draw a sample.\n\n\nWe then use a model to describe the sample.\n\n\nBy comparing that model to a null model, we can infer something about the population."
  },
  {
    "objectID": "slides/01-intro-slides.html#what-population-does-archaeology-study",
    "href": "slides/01-intro-slides.html#what-population-does-archaeology-study",
    "title": "Lecture 01: Introduction",
    "section": "What population does archaeology study?",
    "text": "What population does archaeology study?"
  },
  {
    "objectID": "slides/01-intro-slides.html#a-note-on-terminology-and-notation",
    "href": "slides/01-intro-slides.html#a-note-on-terminology-and-notation",
    "title": "Lecture 01: Introduction",
    "section": "A note on terminology and notation",
    "text": "A note on terminology and notation\n\n\n\nA statistic is a property of a sample.\n‚ÄúWe measured the heights of 42 actors who auditioned for the role of Aragorn and took the average.‚Äù\nA parameter is a property of a population.\n‚ÄúHuman males have an average height of 1.74 meters (5.7 feet).‚Äù\nNote: Parameters are usually capitalized.\n\n\n\n\n\n  \n  \n  \n  \n    \n      \n      population\n      sample\n    \n  \n  \n    Size\nN\nn\n    Mean\nŒº\nxÃÑ\n    Standard Deviation\nœÉ\ns\n    Proportion\nP\np\n    Correlation\nœÅ\nr"
  },
  {
    "objectID": "slides/01-intro-slides.html#why-fa-brands-r-project-colorsteelblue",
    "href": "slides/01-intro-slides.html#why-fa-brands-r-project-colorsteelblue",
    "title": "Lecture 01: Introduction",
    "section": "Why ?",
    "text": "Why ?\nWhy R?\n\n\n\n\nIt‚Äôs free!It‚Äôs transferrable!It‚Äôs efficient!It‚Äôs extensible!It‚Äôs pretty figures!It‚Äôs reproducible!It‚Äôs a community!\n\n\nR is free software under the terms of the Free Software Foundation‚Äôs GNU General Public License.\n\n\nR will run on any system: Mac OS, Windows, or Linux.\n\n\nR lets you exploit the awesome computing powers of the modern world. It also provides an elegant and concise syntax for writing complex statistical operations.\n\n\n\n\n\nR users can write add-on packages that provide additional functionality. Here are a few of my favorites.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR offers a lot of tools to produce really, really impressive graphics. For example, here is a simple plot of a normal distribution:\n\n\n\n\n\n\n\n\n\n\n\nR facilitates reproducible research in two ways. First, it forces you to declare explicitly each step in your analysis.\n\n# take the mean\nmean(my_data)\n\n# take the standard deviation\nsd(my_data)\n\nSecond, it makes R code shareable. In the simplest case, we use R scripts, but we can also use Quarto, a much more flexible tool for writing, running, and explaining R code.\n\n\nR is also an incredibly active and growing community."
  },
  {
    "objectID": "slides/01-intro-slides.html#literate-programming-with-markdown-fa-brands-markdown",
    "href": "slides/01-intro-slides.html#literate-programming-with-markdown-fa-brands-markdown",
    "title": "Lecture 01: Introduction",
    "section": "Literate programming with markdown ",
    "text": "Literate programming with markdown \n\nMarkdown is a lightweight markup language for creating formatted text using a plain-text editor. From the Wikipedia page.\n\n\n\nINPUT\n\n\nThis is a sentence in Markdown, containing `code`, **bold text**, and *italics*.\n\n\nOUTPUT\n\nThis is a sentence in Markdown, containing code, bold text, and italics."
  },
  {
    "objectID": "slides/01-intro-slides.html#quarto-markdown-r",
    "href": "slides/01-intro-slides.html#quarto-markdown-r",
    "title": "Lecture 01: Introduction",
    "section": "Quarto = Markdown + R",
    "text": "Quarto = Markdown + R\nQuarto allows you to run code and format text in one document.\n\n\nINPUT\n\nThis is an example of Quarto with markdown __syntax__ \nand __R code__.\n\n```{r}\n#| fig-width: 4\n#| fig-asp: 1\n#| fig-align: center\n\nfit <- lm(dist ~ speed, data = cars)\n\npar(pty = \"s\")\n\nplot(cars, pch = 19, col = 'darkgray')\nabline(fit, lwd = 2)\n```\n\nOUTPUT\n\nThis is an example of Quarto with markdown syntax and R code."
  },
  {
    "objectID": "slides/02-probability-slides.html#lecture-outline",
    "href": "slides/02-probability-slides.html#lecture-outline",
    "title": "Lecture 02: Probability as a Model",
    "section": "üìã Lecture Outline",
    "text": "üìã Lecture Outline\n\nWhy statistics?\nüß™ A simple experiment\nSome terminology\nüé∞ Random variables\nüé≤ Probability\nüìä Probability Distribution\nProbability Distribution as a Model\nProbability Distribution as a Function\nProbability Mass Functions\nProbability Density Functions\nüöó Cars Model\nBrief Review\nA Simple Formula\nA Note About Complexity"
  },
  {
    "objectID": "slides/02-probability-slides.html#why-statistics",
    "href": "slides/02-probability-slides.html#why-statistics",
    "title": "Lecture 02: Probability as a Model",
    "section": "Why statistics?",
    "text": "Why statistics?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe want to understand something about a population.\nWe can never observe the entire population, so we draw a sample.\nWe then use a model to describe the sample.\nBy comparing that model to a null model, we can infer something about the population.\n\n\nHere, we‚Äôre going to focus on statistical description, aka models."
  },
  {
    "objectID": "slides/02-probability-slides.html#a-simple-experiment",
    "href": "slides/02-probability-slides.html#a-simple-experiment",
    "title": "Lecture 02: Probability as a Model",
    "section": "üß™ A simple experiment",
    "text": "üß™ A simple experiment\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe take ten cars, send each down a track, have them brake at the same point, and measure the distance it takes them to stop.\n\n\nQuestion: how far do you think it will take the next car to stop?\n\n\nQuestion: what distance is the most probable?\nBut, how do we determine this?"
  },
  {
    "objectID": "slides/02-probability-slides.html#some-terminology",
    "href": "slides/02-probability-slides.html#some-terminology",
    "title": "Lecture 02: Probability as a Model",
    "section": "Some terminology",
    "text": "Some terminology"
  },
  {
    "objectID": "slides/02-probability-slides.html#random-variables",
    "href": "slides/02-probability-slides.html#random-variables",
    "title": "Lecture 02: Probability as a Model",
    "section": "üé∞ Random Variables",
    "text": "üé∞ Random Variables\nTwo types of random variable:\n\n\nDiscrete random variables often take only integer (non-decimal) values.\n\nExamples: number of heads in 10 tosses of a fair coin, number of victims of the Thanos snap, number of projectile points in a stratigraphic level, number of archaeological sites in a watershed.\n\nContinuous random variables take real (decimal) values.\n\nExamples: cost in property damage of a superhero fight, kilocalories per kilogram, kilocalories per hour, ratio of isotopes\n\nNote: for continuous random variables, the sample space is infinite!"
  },
  {
    "objectID": "slides/02-probability-slides.html#probability",
    "href": "slides/02-probability-slides.html#probability",
    "title": "Lecture 02: Probability as a Model",
    "section": "üé≤ Probability",
    "text": "üé≤ Probability\nLet \\(X\\) be the number of heads in two tosses of a fair coin. What is the probability that \\(X=1\\)?"
  },
  {
    "objectID": "slides/02-probability-slides.html#probability-distribution-as-a-model",
    "href": "slides/02-probability-slides.html#probability-distribution-as-a-model",
    "title": "Lecture 02: Probability as a Model",
    "section": "Probability Distribution as a Model",
    "text": "Probability Distribution as a Model\n\n\n\nHas two components:\n\nCentral-tendency or ‚Äúfirst moment‚Äù\n\nPopulation mean (\\(\\mu\\)). Gives the expected value of an experiment, \\(E[X] = \\mu\\).\n\nSample mean (\\(\\bar{x}\\)). Estimate of \\(\\mu\\) based on a sample from \\(X\\) of size \\(n\\).\n\n\n\n\nDispersion or ‚Äúsecond moment‚Äù\n\nPopulation variance (\\(\\sigma^2\\)). The expected value of the squared difference from the mean.\n\nSample variance (\\(s^2\\)). Estimate of \\(\\sigma^2\\) based on a sample from \\(X\\) of size \\(n\\).\n\nStandard deviation (\\(\\sigma\\)) or \\(s\\) is the square root of the variance."
  },
  {
    "objectID": "slides/02-probability-slides.html#probability-distribution-as-a-function",
    "href": "slides/02-probability-slides.html#probability-distribution-as-a-function",
    "title": "Lecture 02: Probability as a Model",
    "section": "Probability Distribution as a Function",
    "text": "Probability Distribution as a Function\nThese can be defined using precise mathematical functions:\n\nA probability mass function (PMF) for discrete random variables.\n\nExamples: Bernoulli, Binomial, Negative Binomial, Poisson\n\nStraightforward probability interpretation.\n\n\n\n\nA probability density function (PDF) for continuous random variables.\n\nExamples: Normal, Chi-squared, Student‚Äôs t, and F\n\nHarder to interpret probability:\n\nWhat is the probability that a car takes 10.317 m to stop? What about 10.31742 m?\n\nBetter to consider probability across an interval.\n\n\nRequires that the function integrate to one (probability is the area under the curve)."
  },
  {
    "objectID": "slides/02-probability-slides.html#bernoulli",
    "href": "slides/02-probability-slides.html#bernoulli",
    "title": "Lecture 02: Probability as a Model",
    "section": "Bernoulli",
    "text": "Bernoulli\n\n\n\nDf. distribution of a binary random variable (‚ÄúBernoulli trial‚Äù) with two possible values, 1 (success) and 0 (failure), with \\(p\\) being the probability of success. E.g., a single coin flip.\n\\[f(x,p) = p^{x}(1-p)^{1-x}\\]\nMean: \\(p\\)\nVariance: \\(p(1-p)\\)"
  },
  {
    "objectID": "slides/02-probability-slides.html#binomial",
    "href": "slides/02-probability-slides.html#binomial",
    "title": "Lecture 02: Probability as a Model",
    "section": "Binomial",
    "text": "Binomial\n\n\n\nDf. distribution of a random variable whose value is equal to the number of successes in \\(n\\) independent Bernoulli trials. E.g., number of heads in ten coin flips.\n\\[f(x,p,n) = \\binom{n}{x}p^{x}(1-p)^{1-x}\\]\nMean: \\(np\\)\nVariance: \\(np(1-p)\\)"
  },
  {
    "objectID": "slides/02-probability-slides.html#poisson",
    "href": "slides/02-probability-slides.html#poisson",
    "title": "Lecture 02: Probability as a Model",
    "section": "Poisson",
    "text": "Poisson\n\n\n\nDf. distribution of a random variable whose value is equal to the number of events occurring in a fixed interval of time or space. E.g., number of orcs passing through the Black Gates in an hour.\n\\[f(x,\\lambda) = \\frac{\\lambda^{x}e^{-\\lambda}}{x!}\\]\nMean: \\(\\lambda\\)\nVariance: \\(\\lambda\\)"
  },
  {
    "objectID": "slides/02-probability-slides.html#normal-gaussian",
    "href": "slides/02-probability-slides.html#normal-gaussian",
    "title": "Lecture 02: Probability as a Model",
    "section": "Normal (Gaussian)",
    "text": "Normal (Gaussian)\n\n\n\nDf. distribution of a continuous random variable that is symmetric from positive to negative infinity. E.g., the height of actors who auditioned for the role of Aragorn.\n\\[f(x,\\mu,\\sigma) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\;exp\\left[-\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^2\\right]\\]\nMean: \\(\\mu\\)\nVariance: \\(\\sigma^2\\)"
  },
  {
    "objectID": "slides/02-probability-slides.html#cars-model",
    "href": "slides/02-probability-slides.html#cars-model",
    "title": "Lecture 02: Probability as a Model",
    "section": "üöó Cars Model",
    "text": "üöó Cars Model\n\n\n\n\n\nLet‚Äôs use the Normal distribution to describe the cars data.\n\n\\(Y\\) is stopping distance for population\n\\(Y\\) is normally distributed, \\(Y \\sim N(\\mu, \\sigma)\\)\nExperiment is a random sample of size \\(n\\) from \\(Y\\) with \\(y_1, y_2, ..., y_n\\) observations.\nSample statistics (\\(\\bar{y}, s\\)) approximate population parameters (\\(\\mu, \\sigma\\)).\n\n\n\nSample statistics:\n\nMean (\\(\\bar{y}\\)) = 10.54 m\n\n\n\nSample statistics:\n\nMean (\\(\\bar{y}\\)) = 10.54 m\n\nThis is our approximate expectation\n\n\\(E[Y] = \\mu \\approx \\bar{y}\\)\n\n\n\nSample statistics:\n\nMean (\\(\\bar{y}\\)) = 10.54 m\n\nBut, there‚Äôs error, \\(\\epsilon\\), in this estimate.\n\n\\(\\epsilon_i = y_i - \\bar{y}\\)\n\n\n\nSample statistics:\n\nMean (\\(\\bar{y}\\)) = 10.54 m\n\nThe average squared error is the variance:\n\n\\(s^2 = \\frac{1}{n-1}\\sum \\epsilon_{i}^{2}\\)\n\n\n\nSample statistics:\n\nMean (\\(\\bar{y}\\)) = 10.54 m\nS.D. (\\(s\\)) = 5.353 m\n\nThis is our uncertainty, how big we think any given error will be.\n\n\nSample statistics:\n\nMean (\\(\\bar{y}\\)) = 10.54 m\nS.D. (\\(s\\)) = 5.353 m\n\nSo, here is our probability model.\n\\[Y \\sim N(\\bar{y}, s)\\] This is only an estimate of \\(N(\\mu, \\sigma)\\)!\n\n\nSample statistics:\n\nMean (\\(\\bar{y}\\)) = 10.54 m\nS.D. (\\(s\\)) = 5.353 m\n\nWith it, we can say, for example, that the probability that a random draw from this distribution falls within one standard deviation (dashed lines) of the mean (solid line) is 68.3%."
  },
  {
    "objectID": "slides/02-probability-slides.html#a-simple-formula",
    "href": "slides/02-probability-slides.html#a-simple-formula",
    "title": "Lecture 02: Probability as a Model",
    "section": "A Simple Formula",
    "text": "A Simple Formula\n\n\n\n\n\nThis gives us a simple formula\n\\[y_i = \\bar{y} + \\epsilon_i\\] where\n\n\\(y_i\\): stopping distance for car \\(i\\), data\n\\(\\bar{y} \\approx E[Y]\\): expectation, predictable\n\\(\\epsilon_i\\): error, unpredictable\n\n\n\nThis gives us a simple formula\n\\[y_i = \\bar{y} + \\epsilon_i\\]\nIf we subtract the mean, we have a model of the errors centered on zero:\n\\[\\epsilon_i = 0 + (y_i - \\bar{y})\\]\n\n\nThis gives us a simple formula\n\\[y_i = \\bar{y} + \\epsilon_i\\]\nIf we subtract the mean, we have a model of the errors centered on zero:\n\\[\\epsilon_i = 0 + (y_i - \\bar{y})\\]\nThis means we can construct a probability model of the errors centered on zero."
  },
  {
    "objectID": "slides/02-probability-slides.html#probability-model-of-errors",
    "href": "slides/02-probability-slides.html#probability-model-of-errors",
    "title": "Lecture 02: Probability as a Model",
    "section": "Probability Model of Errors",
    "text": "Probability Model of Errors\n\n\nNote that the mean changes, but the variance stays the same."
  },
  {
    "objectID": "slides/02-probability-slides.html#summary",
    "href": "slides/02-probability-slides.html#summary",
    "title": "Lecture 02: Probability as a Model",
    "section": "Summary",
    "text": "Summary\nNow our simple formula is this:\n\\[y_i = \\bar{y} + \\epsilon_i\\] \\[\\epsilon \\sim N(0, s) \\]\n\nAgain, \\(\\bar{y} \\approx E[Y] = \\mu\\).\nFor any future outcome:\n\nThe expected value is deterministic\nThe error is stochastic\n\n\nMust assume that the errors are iid!\n\nindependent = they do not affect each other\nidentically distributed = they are from the same probability distribution\n\nThe distribution is now a model of the errors!"
  },
  {
    "objectID": "slides/03-inference-slides.html#lecture-outline",
    "href": "slides/03-inference-slides.html#lecture-outline",
    "title": "Lecture 03: Statistical Inference",
    "section": "üìã Lecture Outline",
    "text": "üìã Lecture Outline\n\nWhy statistics?\nStatistical Inference\nSimple Example\nHypotheses\nTests\nRejecting the null hypothesis\nStudent‚Äôs t-test\nANOVA"
  },
  {
    "objectID": "slides/03-inference-slides.html#why-statistics",
    "href": "slides/03-inference-slides.html#why-statistics",
    "title": "Lecture 03: Statistical Inference",
    "section": "Why statistics?",
    "text": "Why statistics?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe want to understand something about a population.\nWe can never observe the entire population, so we draw a sample.\nWe then use a model to describe the sample.\nBy comparing that model to a null model, we can infer something about the population.\n\n\nHere, we‚Äôre going to focus on statistical inference."
  },
  {
    "objectID": "slides/03-inference-slides.html#simple-example",
    "href": "slides/03-inference-slides.html#simple-example",
    "title": "Lecture 03: Statistical Inference",
    "section": "Simple Example",
    "text": "Simple Example\n\n\n\n\n\n\n\n\n\nTwo samples of length (mm, n=300).\n\nQuestion: Are these samples of the same type? The same population?"
  },
  {
    "objectID": "slides/03-inference-slides.html#sample-distribution",
    "href": "slides/03-inference-slides.html#sample-distribution",
    "title": "Lecture 03: Statistical Inference",
    "section": "Sample Distribution",
    "text": "Sample Distribution\nDo these really represent different types?\n\n\n\n\n\n\n\n\nTwo models:\n\nsame type (same population)\n\ndifferent types (different populations)\n\nNote that:\n\nthese models are mutually exclusive and\n\nthe second model is more complex."
  },
  {
    "objectID": "slides/03-inference-slides.html#hypotheses",
    "href": "slides/03-inference-slides.html#hypotheses",
    "title": "Lecture 03: Statistical Inference",
    "section": "Hypotheses",
    "text": "Hypotheses\nThe two models represent our hypotheses."
  },
  {
    "objectID": "slides/03-inference-slides.html#testing-method",
    "href": "slides/03-inference-slides.html#testing-method",
    "title": "Lecture 03: Statistical Inference",
    "section": "Testing Method",
    "text": "Testing Method\n\n\n\n\n\n\nProcedure:\n\nTake sample(s).\nCalculate test statistic.\n\nCompare to test probability distribution.\nGet p-value.\nCompare to critical value.\nAccept (or reject) null hypothesis."
  },
  {
    "objectID": "slides/03-inference-slides.html#average-of-differences",
    "href": "slides/03-inference-slides.html#average-of-differences",
    "title": "Lecture 03: Statistical Inference",
    "section": "Average of differences",
    "text": "Average of differences\n\n\n\n\n\n\nSuppose we take two samples from the same population, calculate the difference in their means, and repeat this 1,000 times.\nQuestion: What will the average difference be between the sample means?"
  },
  {
    "objectID": "slides/03-inference-slides.html#probability-of-differences",
    "href": "slides/03-inference-slides.html#probability-of-differences",
    "title": "Lecture 03: Statistical Inference",
    "section": "Probability of differences",
    "text": "Probability of differences\n\n\n\nIf we convert these differences into a probability distribution, we can estimate the probability of any given difference.\nThe p-value represents how likely it is that the difference we see arises by chance."
  },
  {
    "objectID": "slides/03-inference-slides.html#model-of-differences",
    "href": "slides/03-inference-slides.html#model-of-differences",
    "title": "Lecture 03: Statistical Inference",
    "section": "Model of Differences",
    "text": "Model of Differences\n\n\n\n\n\n\nIn classical statistics, we use a model of that distribution to estimate the probability of a given difference. Here we are using \\(N(0,\\) 0.69\\()\\), where the probability of getting a difference \\(\\pm\\) 1 mm (\\(\\pm2s\\)) or greater is 0.05."
  },
  {
    "objectID": "slides/03-inference-slides.html#rejecting-the-null-hypothesis",
    "href": "slides/03-inference-slides.html#rejecting-the-null-hypothesis",
    "title": "Lecture 03: Statistical Inference",
    "section": "Rejecting the Null Hypothesis",
    "text": "Rejecting the Null Hypothesis\n\n\n\nQuestion: How do we decide?\nDefine a critical limit (\\(\\alpha\\))\n\nMust be determined prior to the test!\nIf \\(p < \\alpha\\), reject. ‚Üê THIS IS THE RULE!\nGenerally, \\(\\alpha = 0.05\\)"
  },
  {
    "objectID": "slides/03-inference-slides.html#why-the-critical-limit",
    "href": "slides/03-inference-slides.html#why-the-critical-limit",
    "title": "Lecture 03: Statistical Inference",
    "section": "Why the critical limit?",
    "text": "Why the critical limit?\nBecause we might be wrong! But what kind of wrong?\n\n\n\nWith \\(\\alpha=0.05\\), we are saying, ‚ÄúIf we run this test 100 times, only 5 of those tests should result in a Type 1 Error.‚Äù"
  },
  {
    "objectID": "slides/03-inference-slides.html#students-t-test",
    "href": "slides/03-inference-slides.html#students-t-test",
    "title": "Lecture 03: Statistical Inference",
    "section": "Student‚Äôs t-test",
    "text": "Student‚Äôs t-test\n\nProblemHypothesesDifferencet-statisticComplexityt-distribution\n\n\n\n\n\n\n\nWe have two samples of projectile points, each consisting of 300 measurements of length (mm).\nQuestion: Are these samples of the same point type? The same population?\n\n\n\n\nThe null hypothesis:\n\\(H_0: \\mu_1 = \\mu_2\\)\nThe alternate hypothesis:\n\\(H_1: \\mu_1 \\neq \\mu_2\\)\nThis is a two-sided t-test as the difference can be positive or negative.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion: Is this difference (-1.82) big enough to reject the null?\n\n\n\n\nA t-statistic standardizes the difference in means using the standard error of the sample mean.\n\\[t = \\frac{\\bar{x}_1 - \\bar{x}_2}{s_{\\bar{x}}}\\]\n\n\n\nFor our samples, \\(t =\\) -3.61. This is a model of our data!\nQuestion: How probable is this estimate?\nWe can answer this by comparing the t-statistic to the t-distribution.\n\n\nBut first, we need to evaluate how complex the t-statistic is. We do this by estimating the degrees of freedom, or the number of values that are free to vary after calculating a statistic. In this case, we have two samples with 300 observations each, hence:\n\ndf = 530\n\nCrucially, this affects the shape of the t-distribution and, thus, determines the location of the critical value we use to evaluate the null hypothesis.\n\n\n\n\n\n\n\n\n\n\n\nSummary:\n\n\\(\\alpha = 0.05\\)\n\n\\(H_{0}: \\mu_1 = \\mu_2\\)\n\n\\(p =\\) 0.0003\n\nTranslation: the null hypothesis is really, really unlikely. So, there must be some difference in the mean!"
  },
  {
    "objectID": "slides/03-inference-slides.html#anova",
    "href": "slides/03-inference-slides.html#anova",
    "title": "Lecture 03: Statistical Inference",
    "section": "ANOVA",
    "text": "ANOVA\n\n\n\n\nProblemHypothesesStrategySum of SquaresF-statisticF-distribution\n\n\n\n\n\n\n\nWe have five samples of points, each consisting of 100 measurements of length (mm).\nQuestion: Are these samples of the same point type? The same population?\nAnalysis of Variance (ANOVA) is like a t-test but for more than two samples.\n\n\n\n\nThe null hypothesis:\n\\(H_0:\\) no difference between groups\nThe alternate hypothesis:\n\\(H_1:\\) at least one group is different\n\n\n\n\nVariance Decomposition. When group membership is known, the contribution of any value \\(x_{ij}\\) to the variance can be split into two parts:\n\\[(x_{ij} - \\bar{x}) = (\\bar{x}_{j} - \\bar{x}) + (x_{ij} - \\bar{x}_{j})\\]\nwhere\n\n\\(i\\) is the \\(i\\)th observation,\n\n\\(j\\) is the \\(j\\)th group,\n\n\\(\\bar{x}\\) is the between-group mean, and\n\n\\(\\bar{x_{j}}\\) is the within-group mean (of group \\(j\\)).\n\n\n\n\n\n\n\n\n\n\n\n\n\nSum and square the differences for all \\(n\\) observation and \\(m\\) groups gives us:\n\\[SS_{T} = SS_{B} + SS_{W}\\]\nwhere\n\n\\(SS_{T}\\): Total Sum of Squares\n\\(SS_{B}\\): Between-group Sum of Squares\n\\(SS_{W}\\): Within-group Sum of Squares\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRatio of variances:\n\\[F = \\frac{\\text{between-group variance}}{\\text{within-group variance}}\\]\nwhere\n\nBetween-group variance = \\(SS_{B}/df_{B}\\) and \\(df_{B}=m-1\\).\nWithin-group variance = \\(SS_{W}/df_{W}\\) and \\(df_{W}=m(n-1)\\).\n\nQuestion: Here, \\(F=\\) 5.63. How probable is this estimate?\nWe can answer this question by comparing the F-statistic to the F-distribution.\n\n\n\n\n\n\n\n\n\n\n\nSummary:\n\n\\(\\alpha = 0.05\\)\n\\(H_{0}:\\) no difference\n\\(p=\\) 0.003\n\nTranslation: the null hypothesis is really, really unlikely, so there must be some difference between groups!"
  },
  {
    "objectID": "slides/04-ols-slides.html#lecture-outline",
    "href": "slides/04-ols-slides.html#lecture-outline",
    "title": "Lecture 04: Ordinary Least Squares",
    "section": "üìã Lecture Outline",
    "text": "üìã Lecture Outline\n\nüß™ A simple experiment\nCompeting models\nModel complexity\nBivariate statistics (covariance and correlation)\nA general formula\nSimple Linear Regression\nOrdinary Least Squares (OLS)\nMultiple Linear Regression\nüöó Cars Model, again\nRegression assumptions"
  },
  {
    "objectID": "slides/04-ols-slides.html#a-simple-experiment",
    "href": "slides/04-ols-slides.html#a-simple-experiment",
    "title": "Lecture 04: Ordinary Least Squares",
    "section": "üß™ A simple experiment",
    "text": "üß™ A simple experiment\n\n\n\n\n\n\n\n\n We take ten cars, send each down a track, have them brake at the same point, and measure the distance it takes them to stop.\nQuestion: how far do you think it will take the next car to stop?"
  },
  {
    "objectID": "slides/04-ols-slides.html#competing-models",
    "href": "slides/04-ols-slides.html#competing-models",
    "title": "Lecture 04: Ordinary Least Squares",
    "section": "Competing Models",
    "text": "Competing Models\n\n\n\n\n\n\n\nE[Y]: mean distance\n\n\n\n\n\n\nE[Y]: some function of speed"
  },
  {
    "objectID": "slides/04-ols-slides.html#model-complexity",
    "href": "slides/04-ols-slides.html#model-complexity",
    "title": "Lecture 04: Ordinary Least Squares",
    "section": "Model Complexity",
    "text": "Model Complexity\n\n\n\n\n\n\n\n\n\n‚öñÔ∏è The error is smaller for the more complex model. This is a good thing, but what did it cost us? Need to weigh this against the increased complexity!"
  },
  {
    "objectID": "slides/04-ols-slides.html#bivariate-statistics",
    "href": "slides/04-ols-slides.html#bivariate-statistics",
    "title": "Lecture 04: Ordinary Least Squares",
    "section": "Bivariate statistics",
    "text": "Bivariate statistics\n\n\n\n\nExplore the relationship between two variables:"
  },
  {
    "objectID": "slides/04-ols-slides.html#covariance",
    "href": "slides/04-ols-slides.html#covariance",
    "title": "Lecture 04: Ordinary Least Squares",
    "section": "Covariance",
    "text": "Covariance\n\n\nThe extent to which two variables vary together.\n\\[cov(x, y) = \\frac{1}{n-1} \\sum_{i=1}^{n}  (x_i - \\bar{x})(y_i - \\bar{y})\\]\n\nSign reflects positive or negative trend, but magnitude depends on units (e.g., \\(cm\\) vs \\(km\\)).\n\nVariance is the covariance of a variable with itself."
  },
  {
    "objectID": "slides/04-ols-slides.html#correlation",
    "href": "slides/04-ols-slides.html#correlation",
    "title": "Lecture 04: Ordinary Least Squares",
    "section": "Correlation",
    "text": "Correlation\n\n\nPearson‚Äôs Correlation Coefficient\n\\[r = \\frac{cov(x,y)}{s_{x}s_y}\\]\n\nScales covariance (from -1 to 1) using standard deviations, \\(s\\), thus making magnitude independent of units.\nSignificance can be tested by converting to a t-statistic and comparing to a t-distribution with \\(df=n-2\\)."
  },
  {
    "objectID": "slides/04-ols-slides.html#non-linear-correlation",
    "href": "slides/04-ols-slides.html#non-linear-correlation",
    "title": "Lecture 04: Ordinary Least Squares",
    "section": "Non-linear correlation",
    "text": "Non-linear correlation\n\n\nSpearman‚Äôs Rank Correlation Coefficient\n\\[\\rho = \\frac{cov\\left(R(x),\\; R(y) \\right)}{s_{R(x)}s_{R(y)}}\\]\n\nPearson‚Äôs correlation but with ranks (R).\n\nThis makes it a robust estimate, less sensitive to outliers."
  },
  {
    "objectID": "slides/04-ols-slides.html#correlation-between-categories",
    "href": "slides/04-ols-slides.html#correlation-between-categories",
    "title": "Lecture 04: Ordinary Least Squares",
    "section": "Correlation between categories",
    "text": "Correlation between categories\n\n\nFor counts or frequencies\n\\[\\chi^2 = \\sum \\frac{(O_{ij}-E_{ij})^2}{E_{ij}}\\]\n\nAnalysis of contingency table\n\\(O_{ij}\\) is observed count in row \\(i\\), column \\(j\\)\n\\(E_{ij}\\) is expected count in row \\(i\\), column \\(j\\)\nSignificance can be tested by comparing to a \\(\\chi^2\\)-distribution with \\(df=k-1\\) (\\(k\\) being the number of categories)."
  },
  {
    "objectID": "slides/04-ols-slides.html#correlation-is-not-causation",
    "href": "slides/04-ols-slides.html#correlation-is-not-causation",
    "title": "Lecture 04: Ordinary Least Squares",
    "section": "Correlation is not causation!",
    "text": "Correlation is not causation!\n\n\n\n\n\n\nAdapted from https://www.tylervigen.com/spurious-correlations"
  },
  {
    "objectID": "slides/04-ols-slides.html#a-general-formula",
    "href": "slides/04-ols-slides.html#a-general-formula",
    "title": "Lecture 04: Ordinary Least Squares",
    "section": "A general formula",
    "text": "A general formula\n\n\n\n\n\n\\[\nY = E[Y] + \\epsilon \\\\[6pt]\nE[Y] = \\beta X\n\\]\n\n\n\n\\[y_i = \\hat\\beta X + \\epsilon_i\\]\n\n\n\n\\[y_i = \\hat\\beta_0 + \\hat\\beta_1 x_i + \\ldots +  \\hat\\beta_n x_i + \\epsilon_i\\]\n\n\n\n\n\\(y_i\\) is the dependent variable, a sample from the population \\(Y\\)\n\n\\(X\\) is a set of independent variables \\(x_i, \\ldots, x_n\\), sometimes called the design matrix\n\\(\\hat\\beta\\) is a set of coefficients of relationship that estimate the true relationship \\(\\beta\\)\n\\(\\epsilon_i\\) is the residuals or errors"
  },
  {
    "objectID": "slides/04-ols-slides.html#simple-linear-regression",
    "href": "slides/04-ols-slides.html#simple-linear-regression",
    "title": "Lecture 04: Ordinary Least Squares",
    "section": "Simple Linear Regression",
    "text": "Simple Linear Regression\n\n\n\n\n\n\n\n\n\n‚ÄòSimple‚Äô means one explanatory variable (speed)\n\\[y_i = \\hat\\beta_0 + \\hat\\beta_1 speed_i + \\epsilon_i\\]\n\n\\(\\hat\\beta_0\\) = -2.0107\n\\(\\hat\\beta_1\\) = 1.9362\n\nQuestion: How did we get these values?"
  },
  {
    "objectID": "slides/04-ols-slides.html#ordinary-least-squares",
    "href": "slides/04-ols-slides.html#ordinary-least-squares",
    "title": "Lecture 04: Ordinary Least Squares",
    "section": "Ordinary Least Squares",
    "text": "Ordinary Least Squares\n\n\n\n\nA method for estimating the coefficients, \\(\\beta\\), in a linear regression model by minimizing the Residual Sum of Squares, \\(SS_{R}\\).\n\\[SS_{R} = \\sum_{i=1}^{n} (y_{i}-\\hat{y}_i)^2\\]\nwhere \\(\\hat{y} \\approx E[Y]\\). To minimize this, we take its derivative with respect to \\(\\beta\\) and set it equal to zero.\n\\[\\frac{d\\, SS_{R}}{d\\, \\beta} = 0\\]\n\n\nSimple estimators for coefficients:\n\nSlope Ratio of covariance to variance \\[\\beta_{1} = \\frac{cov(x, y)}{var(x)}\\]\n\nIntercept Conditional difference in means \\[\\beta_{0} = \\bar{y} - \\beta_1 \\bar{x}\\]\nIf \\(\\beta_{1} = 0\\), then \\(\\beta_{0} = \\bar{y}\\)."
  },
  {
    "objectID": "slides/04-ols-slides.html#cars-model-again",
    "href": "slides/04-ols-slides.html#cars-model-again",
    "title": "Lecture 04: Ordinary Least Squares",
    "section": "üöó Cars Model, again",
    "text": "üöó Cars Model, again\n\n\n\n\n\n\n\n\n\n\n\n\nSlope\n\\[\\hat\\beta_{1} = \\frac{cov(x,y)}{var(x)} = \\frac{10.426}{5.3847} = 1.9362\\]\n\nIntercept\n\\[\n\\begin{align}\n\\hat\\beta_{0} &= \\bar{y} - \\hat\\beta_{1}\\bar{x} \\\\[6pt]\n              &= 10.54 - 1.9362 * 6.4821 \\\\[6pt]\n              &= -2.0107\n\\end{align}\n\\]"
  },
  {
    "objectID": "slides/04-ols-slides.html#multiple-linear-regression",
    "href": "slides/04-ols-slides.html#multiple-linear-regression",
    "title": "Lecture 04: Ordinary Least Squares",
    "section": "Multiple Linear Regression",
    "text": "Multiple Linear Regression\nOLS can be extended to models containing multiple explanatory variables using matrix algebra. Then the coefficient estimator is:\n\\[\\hat\\beta = (X^{T}X)^{-1}X^{T}y\\]\n\nWhere, if you squint a little\n\n\\(X^{T}X\\) ‚á® variance.\n\\(X^{T}y\\) ‚á® covariance."
  },
  {
    "objectID": "slides/04-ols-slides.html#regression-assumptions",
    "href": "slides/04-ols-slides.html#regression-assumptions",
    "title": "Lecture 04: Ordinary Least Squares",
    "section": "Regression assumptions",
    "text": "Regression assumptions\n\nWeak Exogeneity: the predictor variables have fixed values and are known.\n\nLinearity: the relationship between the predictor variables and the response variable is linear.\n\nConstant Variance: the variance of the errors does not depend on the values of the predictor variables. Also known as homoskedasticity.\n\nIndependence of errors: the errors are uncorrelated with each other.\n\nNo perfect collinearity: the predictors are not linearly correlated with each other."
  },
  {
    "objectID": "slides/05-distributions-slides.html#lecture-outline",
    "href": "slides/05-distributions-slides.html#lecture-outline",
    "title": "Lecture 05: Visualizing Distributions",
    "section": "üìã Lecture Outline",
    "text": "üìã Lecture Outline\n\nBar Chart\nHistogram\nProbability Density\nCumulative Density\nBoxplot"
  },
  {
    "objectID": "slides/05-distributions-slides.html#whats-it-for",
    "href": "slides/05-distributions-slides.html#whats-it-for",
    "title": "Lecture 05: Visualizing Distributions",
    "section": "What‚Äôs it for?",
    "text": "What‚Äôs it for?\nVisualize the amount of some variable across categories, represented using length or height of bars."
  },
  {
    "objectID": "slides/05-distributions-slides.html#a-rule-of-thumb",
    "href": "slides/05-distributions-slides.html#a-rule-of-thumb",
    "title": "Lecture 05: Visualizing Distributions",
    "section": "A rule of thumb üëç",
    "text": "A rule of thumb üëç\nOften easier to read when oriented horizontally."
  },
  {
    "objectID": "slides/05-distributions-slides.html#grouped-data",
    "href": "slides/05-distributions-slides.html#grouped-data",
    "title": "Lecture 05: Visualizing Distributions",
    "section": "Grouped data",
    "text": "Grouped data\nGrouped bar chart can represent higher dimensional data.\n\nAlthough this graph is not terribly informative‚Ä¶"
  },
  {
    "objectID": "slides/05-distributions-slides.html#whats-it-for-1",
    "href": "slides/05-distributions-slides.html#whats-it-for-1",
    "title": "Lecture 05: Visualizing Distributions",
    "section": "What‚Äôs it for?",
    "text": "What‚Äôs it for?\nVisualize the approximate distribution of a continuous random variable."
  },
  {
    "objectID": "slides/05-distributions-slides.html#bin-width",
    "href": "slides/05-distributions-slides.html#bin-width",
    "title": "Lecture 05: Visualizing Distributions",
    "section": "Bin Width",
    "text": "Bin Width\nObtained by counting the number of observations that fall into each interval or ‚Äúbin.‚Äù"
  },
  {
    "objectID": "slides/05-distributions-slides.html#lookout",
    "href": "slides/05-distributions-slides.html#lookout",
    "title": "Lecture 05: Visualizing Distributions",
    "section": "‚ö†Ô∏è Lookout!",
    "text": "‚ö†Ô∏è Lookout!\nThe shape of the distribution depends on the bin width."
  },
  {
    "objectID": "slides/05-distributions-slides.html#a-rule-of-thumb-1",
    "href": "slides/05-distributions-slides.html#a-rule-of-thumb-1",
    "title": "Lecture 05: Visualizing Distributions",
    "section": "A rule of thumb üëç",
    "text": "A rule of thumb üëç\n\n\nGenerally, a bad idea to use stacked or dodged groupings in a single histogram.\n\n\n\n\n\n\n\nBetter to use facets."
  },
  {
    "objectID": "slides/05-distributions-slides.html#whats-it-for-2",
    "href": "slides/05-distributions-slides.html#whats-it-for-2",
    "title": "Lecture 05: Visualizing Distributions",
    "section": "What‚Äôs it for?",
    "text": "What‚Äôs it for?\nVisualize the approximate distribution of a continuous random variable."
  },
  {
    "objectID": "slides/05-distributions-slides.html#kernel-density-estimation-kde",
    "href": "slides/05-distributions-slides.html#kernel-density-estimation-kde",
    "title": "Lecture 05: Visualizing Distributions",
    "section": "Kernel Density Estimation (KDE)",
    "text": "Kernel Density Estimation (KDE)\n\n\nProcedure:\n\nDefine a kernel, often a normal distribution with mean equal to the observation.\nDefine bandwidth for scaling the kernel.\nSum the kernels.\n\n\n\n\n\n\n\n\nThe kernels in this figure are not to scale."
  },
  {
    "objectID": "slides/05-distributions-slides.html#grouped-data-1",
    "href": "slides/05-distributions-slides.html#grouped-data-1",
    "title": "Lecture 05: Visualizing Distributions",
    "section": "Grouped data",
    "text": "Grouped data\nThere‚Äôs not a simple answer for how to plot multiple KDE‚Äôs, but facets are your friend."
  },
  {
    "objectID": "slides/05-distributions-slides.html#whats-it-for-3",
    "href": "slides/05-distributions-slides.html#whats-it-for-3",
    "title": "Lecture 05: Visualizing Distributions",
    "section": "What‚Äôs it for?",
    "text": "What‚Äôs it for?\nVisualize the approximate distribution of a continuous random variable without having to specify a bandwidth."
  },
  {
    "objectID": "slides/05-distributions-slides.html#procedure",
    "href": "slides/05-distributions-slides.html#procedure",
    "title": "Lecture 05: Visualizing Distributions",
    "section": "Procedure",
    "text": "Procedure\n\n\nConsider this sample:\n(0.3, 2.0, 3.4, 1.2, 2.2, 1.9).\nTo calculate its eCDF, we divide the number of observations that are less than or equal to each unique value by the total sample size.\n0.0 -> 0/6 = 0.00\n0.3 -> 1/6 = 0.17\n1.2 -> 2/6 = 0.33 \n1.9 -> 3/6 = 0.50 \n2.0 -> 4/6 = 0.67 \n2.2 -> 5/6 = 0.83\n3.4 -> 6/6 = 1.00"
  },
  {
    "objectID": "slides/05-distributions-slides.html#lookout-1",
    "href": "slides/05-distributions-slides.html#lookout-1",
    "title": "Lecture 05: Visualizing Distributions",
    "section": "‚ö†Ô∏è Lookout!",
    "text": "‚ö†Ô∏è Lookout!\nThese are a little bit harder to interpret. Gives the probability of being less than or equal to x. E.g., the probability of being 28 years old or younger is 0.5."
  },
  {
    "objectID": "slides/05-distributions-slides.html#whats-it-for-4",
    "href": "slides/05-distributions-slides.html#whats-it-for-4",
    "title": "Lecture 05: Visualizing Distributions",
    "section": "What‚Äôs it for?",
    "text": "What‚Äôs it for?\nVisualize the approximate distribution of a continuous random variable using its quartiles.\n\nUseful for plotting distributions across multiple groups."
  },
  {
    "objectID": "slides/05-distributions-slides.html#quartiles",
    "href": "slides/05-distributions-slides.html#quartiles",
    "title": "Lecture 05: Visualizing Distributions",
    "section": "Quartiles",
    "text": "Quartiles\nWhen the data are ordered from smallest to largest, the quartiles divide them into four sets of more-or-less equal size. The second quartile is the median!"
  },
  {
    "objectID": "slides/05-distributions-slides.html#a-rule-of-thumb-2",
    "href": "slides/05-distributions-slides.html#a-rule-of-thumb-2",
    "title": "Lecture 05: Visualizing Distributions",
    "section": "A rule of thumb üëç",
    "text": "A rule of thumb üëç\nSometimes easier to read when oriented horizontally."
  },
  {
    "objectID": "slides/07-evaluation-slides.html#lecture-outline",
    "href": "slides/07-evaluation-slides.html#lecture-outline",
    "title": "Lecture 07: Evaluating Linear Models",
    "section": "üìã Lecture Outline",
    "text": "üìã Lecture Outline\n\nüß™ A simple experiment\nCompeting models\nModel interpretation\n\nFormula\nPrediction\nStandard error vs prediction interval\n\nModel evaluation\n\nModel complexity\nANOVA for models\nR-Squared\nt-test for coefficients\nDiagnostic plots"
  },
  {
    "objectID": "slides/07-evaluation-slides.html#a-simple-experiment",
    "href": "slides/07-evaluation-slides.html#a-simple-experiment",
    "title": "Lecture 07: Evaluating Linear Models",
    "section": "üß™ A simple experiment",
    "text": "üß™ A simple experiment\n\n\n\n\n\n\n\n\n We take ten cars, send each down a track, have them brake at the same point, and measure the distance it takes them to stop.\nQuestion: how far do you think it will take the next car to stop?"
  },
  {
    "objectID": "slides/07-evaluation-slides.html#competing-models",
    "href": "slides/07-evaluation-slides.html#competing-models",
    "title": "Lecture 07: Evaluating Linear Models",
    "section": "Competing Models",
    "text": "Competing Models\n\n\n\n\n\n\n\n\nE[Y]: mean distance\n\n\n\n\n\n\n\n\nE[Y]: some function of speed]"
  },
  {
    "objectID": "slides/07-evaluation-slides.html#model-formula",
    "href": "slides/07-evaluation-slides.html#model-formula",
    "title": "Lecture 07: Evaluating Linear Models",
    "section": "Model Formula",
    "text": "Model Formula\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\[\n\\begin{align}\ny_i &= E[Y] + \\epsilon_i \\\\\n\\epsilon &\\sim N(0,\\sigma) \\\\\n\\sigma &= 2.91\n\\end{align}\n\\]\n\n\n\\[\n\\begin{align}\nE[Y] &\\approx \\hat\\beta X \\\\\n\\hat\\beta_0 &= -2.01 \\\\\n\\hat\\beta_1 &= 1.94\n\\end{align}\n\\]\n\n\n\\(\\hat\\beta_{0}\\) is the value of \\(Y\\) where \\(X=0\\), here the total stopping distance when the car isn‚Äôt moving, which should be zero!!!\n\\(\\hat\\beta_{1}\\) is change in \\(Y\\) relative to change in \\(X\\), here the additional distance the car will travel for each unit increase in speed."
  },
  {
    "objectID": "slides/07-evaluation-slides.html#prediction",
    "href": "slides/07-evaluation-slides.html#prediction",
    "title": "Lecture 07: Evaluating Linear Models",
    "section": "Prediction",
    "text": "Prediction\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\[\n\\begin{align}\ny_i &= E[Y] + \\epsilon_i \\\\\n\\epsilon &\\sim N(0,\\sigma) \\\\\n\\sigma &= 2.91\n\\end{align}\n\\]\n\n\n\\[\n\\begin{align}\nE[Y] &\\approx \\hat\\beta X \\\\\n\\hat\\beta_0 &= -2.01 \\\\\n\\hat\\beta_1 &= 1.94\n\\end{align}\n\\]\n\n\nQuestion: if a car is going 8 m/s when it applies the brakes, how long will it take it to stop?\n\n\\[\nE[Y] = -2.01 + 1.94 \\cdot 8 = 13.48\n\\]"
  },
  {
    "objectID": "slides/07-evaluation-slides.html#prediction-interval",
    "href": "slides/07-evaluation-slides.html#prediction-interval",
    "title": "Lecture 07: Evaluating Linear Models",
    "section": "Prediction Interval",
    "text": "Prediction Interval\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\[\n\\begin{align}\ny_i &= E[Y] + \\epsilon_i \\\\\n\\epsilon &\\sim N(0,\\sigma) \\\\\n\\sigma &= 2.91\n\\end{align}\n\\]\n\n\n\\[\n\\begin{align}\nE[Y] &\\approx \\hat\\beta X \\\\\n\\hat\\beta_0 &= -2.01 \\\\\n\\hat\\beta_1 &= 1.94\n\\end{align}\n\\]\n\n\nQuestion: But, what about \\(\\epsilon\\)?!!!\nWe can visualize this with a prediction interval, or the range within which a future outcome is expected to fall with a certain probability for some value of \\(X\\). Can generalize this outside the range of the model, but with increasing uncertainty."
  },
  {
    "objectID": "slides/07-evaluation-slides.html#confidence-interval",
    "href": "slides/07-evaluation-slides.html#confidence-interval",
    "title": "Lecture 07: Evaluating Linear Models",
    "section": "Confidence Interval",
    "text": "Confidence Interval\n\n\n\n\n\n\n\n\n\n\n\\[\n\\begin{align}\ny_i &= E[Y] + \\epsilon_i \\\\\n\\epsilon &\\sim N(0,\\sigma) \\\\\n\\sigma &= 2.91\n\\end{align}\n\\]\n\n\n\\[\n\\begin{align}\nE[Y] &\\approx \\hat\\beta X \\\\\n\\hat\\beta_0 &= -2.01 \\\\\n\\hat\\beta_1 &= 1.94\n\\end{align}\n\\]\n\n\nQuestion: But, what if we‚Äôre wrong about \\(\\beta\\)?!!!\nWe can visualize this with a confidence interval, or the range within which the average outcome is expected to fall with a certain probability for some value of \\(X\\)."
  },
  {
    "objectID": "slides/07-evaluation-slides.html#model-complexity",
    "href": "slides/07-evaluation-slides.html#model-complexity",
    "title": "Lecture 07: Evaluating Linear Models",
    "section": "Model Complexity",
    "text": "Model Complexity\n\n\n\n\n\n\n\n\n\n‚öñÔ∏è The error is smaller for the more complex model. This is a good thing, but what did it cost us? Need to weigh this against the increased complexity!"
  },
  {
    "objectID": "slides/07-evaluation-slides.html#anova-for-model",
    "href": "slides/07-evaluation-slides.html#anova-for-model",
    "title": "Lecture 07: Evaluating Linear Models",
    "section": "ANOVA for model",
    "text": "ANOVA for model\n\nProblemHypothesesStrategySum of SquaresF-statisticF-distribution\n\n\n\n\n\n\n\n\n\n\nWe have two models of our data, one simple, the other complex.\nQuestion: Does the complex (bivariate) model explain more variance than the simple (intercept) model? Does the difference arise by chance?\n\n\n\n\nThe null hypothesis:\n\n\\(H_0:\\) no difference in variance explained.\n\nThe alternate hypothesis:\n\n\\(H_1:\\) difference in variance explained.\n\n\n\nVariance Decomposition. Total variance in the dependent variable can be decomposed into the variance captured by the more complex model and the remaining (or residual) variance:\n\n\nDecompose the differences:\n\\((y_{i} - \\bar{y}) = (\\hat{y}_{i} - \\bar{y}) + (y_{i} - \\hat{y}_{i})\\)\nwhere\n\n\\((y_{i} - \\bar{y})\\) is the total error,\n\n\\((\\hat{y}_{i} - \\bar{y})\\) is the model error, and\n\\((y_{i} - \\hat{y}_{i})\\) is the residual error.\n\n\nSum and square the differences:\n\\(SS_{T} = SS_{M} + SS_{R}\\)\nwhere\n\n\\(SS_{T}\\): Total Sum of Squares\n\\(SS_{M}\\): Model Sum of Squares\n\\(SS_{R}\\): Residual Sum of Squares\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\[F = \\frac{\\text{model variance}}{\\text{residual variance}}\\]\nwhere\n\nModel variance = \\(SS_{M}/k\\) for \\(k\\) model parameters\nResidual variance = \\(SS_{R}/(n-k-1)\\) for \\(k\\) and \\(n\\) observations.\n\nQuestion: How probable is it that \\(F=\\) 19.07?\nWe can answer this by comparing the F-statistic to the F-distribution.\n\n\n\n\n\nSummary:\n\n\\(\\alpha = 0.05\\)\n\\(H_0:\\) no difference\n\\(p=\\) 0.0024\n\nTranslation: the null hypothesis is really, really unlikely. So, there must be some difference between models!"
  },
  {
    "objectID": "slides/07-evaluation-slides.html#r-squared",
    "href": "slides/07-evaluation-slides.html#r-squared",
    "title": "Lecture 07: Evaluating Linear Models",
    "section": "R-Squared",
    "text": "R-Squared\n\n\nCoefficient of Determination\n\\[R^2 = 1- \\frac{SS_{R}}{SS_{T}}\\]\n\nProportion of variance in \\(y\\) explained by the model \\(M\\).\nScale is 0 to 1. A value closer to 1 means that \\(M\\) explains more variance.\n\\(M\\) is evaluated relative to simple, intercept-only model."
  },
  {
    "objectID": "slides/07-evaluation-slides.html#t-test-for-coefficients",
    "href": "slides/07-evaluation-slides.html#t-test-for-coefficients",
    "title": "Lecture 07: Evaluating Linear Models",
    "section": "t-test for coefficients",
    "text": "t-test for coefficients\n\nProblemHypothesesStandard Errorst-statistict-test\n\n\n\n\n\n\n\n\n\n\nQuestion: Are the coefficient estimates significantly different than zero?\nTo answer this question, we need some measure of uncertainty for these estimates.\n\n\n\n\nThe null hypothesis:\n\n\\(H_0:\\) coefficient estimate is not different than zero.\n\nThe alternate hypothesis:\n\n\\(H_1:\\) coefficient estimate is different than zero.\n\n\n\n\n\n\n\n\n\n\n\nFor simple linear regression,\n\nthe standard error of the slope, \\(se(\\hat\\beta_1)\\), is the ratio of the average squared error of the model to the total squared error of the predictor.\nthe standard error of the intercept, \\(se(\\hat\\beta_0)\\), is \\(se(\\hat\\beta_1)\\) weighted by the average squared values of the predictor.\n\n\n\n\n\n\n\n\nThe t-statistic is the coefficient estimate divided by its standard error\n\\[t = \\frac{\\hat\\beta}{se(\\hat\\beta)}\\]\n\nThis can be compared to a t-distribution with \\(n-k-1\\) degrees of freedom (\\(n\\) observations and \\(k\\) independent predictors).\n\n\n\n\n\n\n\n\nSummary:\n\n\\(\\alpha = 0.05\\)\n\\(H_0\\): \\(\\beta=0\\)\np-values:\n\nintercept = 0.5263\nslope < 0.0024"
  },
  {
    "objectID": "slides/07-evaluation-slides.html#diagnostic-plots",
    "href": "slides/07-evaluation-slides.html#diagnostic-plots",
    "title": "Lecture 07: Evaluating Linear Models",
    "section": "Diagnostic Plots",
    "text": "Diagnostic Plots\n\n\n\n\n\nModel AssumptionsResidual vs FittedNormal Q-QResidual DensityCook‚Äôs Distance\n\n\n\nWeak Exogeneity: the predictor variables have fixed values and are known.\nLinearity: the relationship between the predictor variables and the response variable is linear.\nConstant Variance: the variance of the errors does not depend on the values of the predictor variables. Also known as homoscedasticity.\nIndependence of errors: the errors are uncorrelated with each other.\nNo perfect collinearity: the predictors are not linearly correlated with each other."
  }
]