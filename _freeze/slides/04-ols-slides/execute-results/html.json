{
  "hash": "46077992f27900421eb0ce20b8524d61",
  "result": {
    "markdown": "---\ntitle: \"Lecture 04: Ordinary Least Squares\"  \ndate: 'Last updated: 2023-01-30'\n---\n\n\n\n\n\n\n## üìã Lecture Outline\n\n- üß™ A simple experiment\n- Competing models\n- Model complexity\n- Bivariate statistics (covariance and correlation)\n- A general formula\n- Simple Linear Regression\n- Ordinary Least Squares (OLS)\n- Multiple Linear Regression\n- &#x1F697; Cars Model, again\n- Regression assumptions\n\n## üß™ A simple experiment\n\n:::::: {.columns}\n::: {.column}\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](04-ols-slides_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\n:::\n::: {.column}\n\n<br>\nWe take ten cars, send each down a track, have them brake at the same point, and measure the distance it takes them to stop.  \n\n__Question:__ how far do you think it will take the next car to stop?  \n:::\n::::::\n\n## Competing Models\n\n:::::: {.columns style=\"text-align:right;\"}\n::: {.column}\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](04-ols-slides_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\nE[Y]: mean distance\n\n:::\n::: {.column}\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](04-ols-slides_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\nE[Y]: some function of speed\n\n:::\n::::::\n\n## Model Complexity\n\n::: {.columns}\n::: {.column}\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](04-ols-slides_files/figure-html/unnamed-chunk-5-1.png){width=100%}\n:::\n:::\n\n\n:::\n::: {.column}\n\n<br>\n\n‚öñÔ∏è __The error is smaller__ for the more complex model. This is a good thing, __but what did it cost us__? Need to weigh this against the increased complexity! \n\n:::\n:::\n\n## Bivariate statistics\n\n\n::: {.cell}\n<style type=\"text/css\">\n\n.no-p-margins p {\n  margin: 0;\n}\n\n</style>\n:::\n\n\n::: {.no-p-margins style=\"width:80%;margin-top:1em;\"}\n\nExplore the relationship between two variables:\n\n\n::: {.cell fig.asp='0.33'}\n::: {.cell-output-display}\n![](04-ols-slides_files/figure-html/unnamed-chunk-7-1.png){width=100%}\n:::\n:::\n\n\n:::\n\n## Covariance\n\n:::::: {.columns}\n::: {.column width=\"47%\"}\n\nThe extent to which two variables vary together. \n\n$$cov(x, y) = \\frac{1}{n-1} \\sum_{i=1}^{n}  (x_i - \\bar{x})(y_i - \\bar{y})$$\n\n* Sign reflects positive or negative trend, but magnitude depends on units (e.g., $cm$ vs $km$).  \n* Variance is the covariance of a variable with itself.  \n\n:::\n::: {.column width=\"53%\"}\n\n\n::: {.cell fig.asp='0.85'}\n::: {.cell-output-display}\n![](04-ols-slides_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n\n:::\n::::::\n\n## Correlation\n\n:::::: {.columns}\n::: {.column width=\"47%\"}\n\nPearson's Correlation Coefficient\n\n$$r = \\frac{cov(x,y)}{s_{x}s_y}$$\n\n- Scales covariance (from -1 to 1) using standard deviations, $s$, thus making magnitude independent of units.  \n\n- Significance can be tested by converting to a _t_-statistic and comparing to a _t_-distribution with $df=n-2$.\n\n:::\n::: {.column width=\"53%\"}\n\n\n::: {.cell fig.asp='0.85'}\n::: {.cell-output-display}\n![](04-ols-slides_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n\n:::\n::::::\n\n## Non-linear correlation\n\n:::::: {.columns}\n::: {.column width=\"47%\"}\n\nSpearman's Rank Correlation Coefficient\n\n$$\\rho = \\frac{cov\\left(R(x),\\; R(y) \\right)}{s_{R(x)}s_{R(y)}}$$\n\n- Pearson's correlation but with ranks (R).  \n- This makes it a _robust_ estimate, less sensitive to outliers.  \n\n:::\n::: {.column width=\"53%\"}\n\n\n::: {.cell fig.asp='0.85'}\n::: {.cell-output-display}\n![](04-ols-slides_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n\n:::\n::::::\n\n## Correlation between categories\n\n:::::: {.columns}\n::: {.column width=\"47%\"}\n\nFor counts or frequencies\n\n$$\\chi^2 = \\sum \\frac{(O_{ij}-E_{ij})^2}{E_{ij}}$$\n\n- Analysis of contingency table\n- $O_{ij}$ is observed count in row $i$, column $j$ \n- $E_{ij}$ is expected count in row $i$, column $j$ \n- Significance can be tested by comparing to a $\\chi^2$-distribution with $df=k-1$ ($k$ being the number of categories).\n\n:::\n::: {.column width=\"53%\"}\n\n\n::: {.cell fig.asp='0.9'}\n::: {.cell-output-display}\n![](04-ols-slides_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n:::\n\n\n:::\n::::::\n\n## Correlation is not causation!\n\n::: {style=\"text-align:center\"}\n\n\n::: {.cell fig.asp='0.4'}\n::: {.cell-output-display}\n![](04-ols-slides_files/figure-html/unnamed-chunk-12-1.png){width=75%}\n:::\n:::\n\n\n\nAdapted from <https://www.tylervigen.com/spurious-correlations>\n\n:::\n\n## A general formula\n\n\n::: {.cell}\n<style type=\"text/css\">\n\n.gray-panel {\n  background-image: linear-gradient(to bottom right, #E1E1E1, #D5D5D5);\n}\n\n.blue-panel {\n  background-image: linear-gradient(to bottom right, #42758D, #0C556D);\n  color: white;\n}\n\n.yellow-panel {\n  background-image: linear-gradient(to bottom right, #DFC638, #D1B817);\n}\n\n.panel {\n  border: 1px solid black;\n  border-radius: 5px;\n  margin: 0.2em;\n  padding: 0.3em;\n}\n\n.panel p {\n  margin: 0 auto;\n}\n\n.flex {\n  display: flex;\n}\n\n.align-center {\n  align-items: center;\n}\n\n</style>\n:::\n\n\n\n::: {.flex .align-center}\n\n::: {.gray-panel .panel .flex .align-center style=\"width:23%;height:175px;\"}\n$$\nY = E[Y] + \\epsilon \\\\[6pt]\nE[Y] = \\beta X\n$$\n:::\n\n[{{< fa circle-arrow-right size=2xl >}}]{style=\"margin:0 0.5em;\"}\n\n::: {.yellow-panel .panel .flex .align-center style=\"width:23%;height:175px;\"}\n$$y_i = \\hat\\beta X + \\epsilon_i$$\n:::\n\n[{{< fa circle-arrow-right size=2xl >}}]{style=\"margin:0 0.5em;\"}\n\n::: {.blue-panel .panel .flex .align-center style=\"width:50%;height:175px;\"}\n$$y_i = \\hat\\beta_0 + \\hat\\beta_1 x_i + \\ldots +  \\hat\\beta_n x_i + \\epsilon_i$$\n:::\n\n:::\n\n<br>\n\n- $y_i$ is the __dependent variable__, a sample from the population $Y$  \n- $X$ is a set of __independent variables__ $x_i, \\ldots, x_n$, sometimes called the design matrix\n- $\\hat\\beta$ is a set of __coefficients of relationship__ that estimate the true relationship $\\beta$\n- $\\epsilon_i$ is the __residuals__ or errors \n\n## Simple Linear Regression\n\n:::::: {.columns}\n::: {.column}\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](04-ols-slides_files/figure-html/unnamed-chunk-14-1.png){width=672}\n:::\n:::\n\n\n\n:::\n::: {.column}\n\n<br>\n\n'Simple' means one explanatory variable (speed)  \n\n$$y_i = \\hat\\beta_0 + \\hat\\beta_1 speed_i + \\epsilon_i$$\n\n- $\\hat\\beta_0$ = -2.0107\n- $\\hat\\beta_1$ = 1.9362\n\n__Question:__ How did we get these values?\n\n:::\n::::::\n\n## Ordinary Least Squares\n\n:::::: {.columns}\n::::: {.column}\n:::: {.r-stack}\n::: {.fragment .fade-out fragment-index=0}\n\nA method for estimating the coefficients, $\\beta$, in a linear regression model by minimizing the __Residual Sum of Squares__, $SS_{R}$.  \n\n$$SS_{R} = \\sum_{i=1}^{n} (y_{i}-\\hat{y}_i)^2$$\n\nwhere $\\hat{y} \\approx E[Y]$. To minimize this, we take its derivative with respect to $\\beta$ and set it equal to zero.\n\n$$\\frac{d\\, SS_{R}}{d\\, \\beta} = 0$$\n:::\n::: {.fragment .fade-in fragment-index=0}\n\nSimple estimators for coefficients:  \n\n***\n__Slope__ [Ratio of covariance to variance]{style=\"color:#777;\"}\n$$\\beta_{1} = \\frac{cov(x, y)}{var(x)}$$ \n\n***\n__Intercept__ [Conditional difference in means]{style=\"color:#777;\"}\n$$\\beta_{0} = \\bar{y} - \\beta_1 \\bar{x}$$\n\nIf $\\beta_{1} = 0$, then $\\beta_{0} = \\bar{y}$. \n\n:::\n:::: \n:::::\n::::: {.column}\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](04-ols-slides_files/figure-html/unnamed-chunk-15-1.png){width=100%}\n:::\n:::\n\n\n:::::\n::::::\n\n## üöó Cars Model, again\n\n:::::: {.columns}\n::: {.column}\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](04-ols-slides_files/figure-html/unnamed-chunk-16-1.png){width=672}\n:::\n:::\n\n\n:::\n::: {.column}\n\n\n::: {.cell}\n\n:::\n\n\n<br>\n\n__Slope__\n\n$$\\hat\\beta_{1} = \\frac{cov(x,y)}{var(x)} = \\frac{10.426}{5.3847} = 1.9362$$\n\n***\n\n__Intercept__\n\n$$\n\\begin{align}\n\\hat\\beta_{0} &= \\bar{y} - \\hat\\beta_{1}\\bar{x} \\\\[6pt]\n              &= 10.54 - 1.9362 * 6.4821 \\\\[6pt]\n              &= -2.0107\n\\end{align}\n$$\n\n:::\n::::::\n\n## Multiple Linear Regression\n\nOLS can be extended to models containing multiple explanatory variables using matrix algebra. Then the coefficient estimator is:\n\n$$\\hat\\beta = (X^{T}X)^{-1}X^{T}y$$\n\n<br>  \n\nWhere, _if you squint a little_ \n\n- $X^{T}X$ &#x21E8; variance. \n- $X^{T}y$ &#x21E8; covariance.\n\n## Regression assumptions\n\n1. __Weak Exogeneity__: the predictor variables have fixed values and are known.  \n2. __Linearity__: the relationship between the predictor variables and the response variable is linear.  \n2. __Constant Variance__: the variance of the errors does not depend on the values of the predictor variables. Also known as _homoskedasticity_.  \n2. __Independence of errors__: the errors are uncorrelated with each other.  \n2. __No perfect collinearity__: the predictors are not linearly correlated with each other.  \n",
    "supporting": [
      "04-ols-slides_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}