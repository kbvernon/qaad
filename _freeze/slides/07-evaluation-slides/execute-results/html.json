{
  "hash": "e431cbc8c16a80449a3ce303d052a0a1",
  "result": {
    "markdown": "---\ntitle: \"Lecture 07: Evaluating Linear Models\"  \ndate: 'Last updated: 2023-02-21'\n---\n\n\n\n\n## üìã Lecture Outline\n\n- üß™ A simple experiment\n- Competing models\n- Model interpretation\n    - Formula\n    - Prediction\n    - Standard error vs prediction interval\n- Model evaluation\n    - Model complexity\n    - ANOVA for models\n    - R-Squared\n    - t-test for coefficients\n    - Diagnostic plots\n\n## üß™ A simple experiment\n\n:::::: {.columns}\n::: {.column}\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](07-evaluation-slides_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n:::\n\n\n:::\n::: {.column}\n\n<br>\nWe take ten cars, send each down a track, have them brake at the same point, and measure the distance it takes them to stop.  \n\n__Question:__ how far do you think it will take the next car to stop?  \n:::\n:::::: \n\n## Competing Models\n\n:::::: {.columns}\n::: {.column}\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](07-evaluation-slides_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\n:::{style=\"text-align:center;\"}\nE[Y]: mean distance\n:::\n\n:::\n::: {.column}\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](07-evaluation-slides_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\n:::{style=\"text-align:center;\"}\nE[Y]: some function of speed]\n:::\n\n:::\n::::::\n\n# Model Interpretation\n\n## Model Formula\n\n:::::: {.columns}\n::::: {.column}\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](07-evaluation-slides_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\n:::::\n::::: {.column}\n\n\n::: {.cell}\n<style type=\"text/css\">\n\n.blue-panel {\n  background-image: linear-gradient(to bottom right, #42758D, #0C556D);\n  color: white;\n}\n\n.panel {\n  border: 1px solid black;\n  border-radius: 5px;\n  padding: 0 0.1em;\n  height: 175px;\n  width: 45%;\n}\n\n.panel p {\n  margin: 0 auto;\n}\n\n.flex {\n  display: flex;\n  gap: 0.25em;\n}\n\n.align-center {\n  align-items: center;\n}\n\n.align-with-plot {\n  margin-top: 1.9em;\n}\n\n</style>\n:::\n\n\n:::: {.flex .align-center .align-with-plot}\n::: {.blue-panel .panel .flex .align-center}\n\n\n::: {.cell}\n\n:::\n\n\n$$\n\\begin{align}\ny_i &= E[Y] + \\epsilon_i \\\\\n\\epsilon &\\sim N(0,\\sigma) \\\\\n\\sigma &= 2.91 \n\\end{align}\n$$   \n\n:::\n::: {.blue-panel .panel .flex .align-center }\n\n$$\n\\begin{align}\nE[Y] &\\approx \\hat\\beta X \\\\\n\\hat\\beta_0 &= -2.01 \\\\\n\\hat\\beta_1 &= 1.94\n\\end{align}\n$$ \n\n:::\n::::\n\n$\\hat\\beta_{0}$ is the value of $Y$ where $X=0$, here the total stopping distance when the car isn't moving, which should be zero!!!  \n\n$\\hat\\beta_{1}$ is change in $Y$ relative to change in $X$, here the additional distance the car will travel for each unit increase in speed. \n\n:::::\n::::::\n\n## Prediction\n\n\n::: {.cell}\n<style type=\"text/css\">\n\n.relative {\n  position: relative\n}\n\n.z0 {\n  z-index: 0;\n}\n\n.z1 {\n  z-index: 1;\n}\n\n.abs-left {\n  position: absolute;\n}\n\n.w100 {\n  width: 100%;\n}\n\n</style>\n:::\n\n\n:::::: {.columns}\n::::: {.column}\n:::: {.relative}\n::: {.fragment .fade-out fragment-index=0 .abs-left .z0 .w100}\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](07-evaluation-slides_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n\n:::\n::: {.fragment .fade-in fragment-index=0 .abs-left .z1 .w100}\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](07-evaluation-slides_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n\n:::\n::::\n:::::\n::::: {.column}\n\n:::: {.flex .align-center .align-with-plot}\n::: {.blue-panel .panel .flex .align-center}\n\n$$\n\\begin{align}\ny_i &= E[Y] + \\epsilon_i \\\\\n\\epsilon &\\sim N(0,\\sigma) \\\\\n\\sigma &= 2.91 \n\\end{align}\n$$   \n\n:::\n::: {.blue-panel .panel .flex .align-center }\n\n$$\n\\begin{align}\nE[Y] &\\approx \\hat\\beta X \\\\\n\\hat\\beta_0 &= -2.01 \\\\\n\\hat\\beta_1 &= 1.94\n\\end{align}\n$$ \n\n:::\n::::\n\n__Question:__ if a car is going 8 m/s when it applies the brakes, how long will it take it to stop?\n\n::: {.fragment .fade-in fragment-index=0}\n\n$$\nE[Y] = -2.01 + 1.94 \\cdot 8 = 13.48\n$$\n\n:::\n\n:::::\n::::::\n\n## Prediction Interval\n\n:::::: {.columns}\n::::: {.column}\n:::: {.relative}\n::: {.fragment .fade-out fragment-index=0 .abs-left .z0 .w100}\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](07-evaluation-slides_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n\n:::\n::: {.fragment .fade-in fragment-index=0 .abs-left .z1 .w100}\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](07-evaluation-slides_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n:::\n\n\n:::\n::::\n:::::\n::::: {.column}\n:::: {.flex .align-center .align-with-plot}\n::: {.blue-panel .panel .flex .align-center}\n\n$$\n\\begin{align}\ny_i &= E[Y] + \\epsilon_i \\\\\n\\epsilon &\\sim N(0,\\sigma) \\\\\n\\sigma &= 2.91 \n\\end{align}\n$$   \n\n:::\n::: {.blue-panel .panel .flex .align-center }\n\n$$\n\\begin{align}\nE[Y] &\\approx \\hat\\beta X \\\\\n\\hat\\beta_0 &= -2.01 \\\\\n\\hat\\beta_1 &= 1.94\n\\end{align}\n$$ \n\n:::\n::::\n\n__Question__: But, what about $\\epsilon$?!!! \n\nWe can visualize this with a __prediction interval__, or the range within which a __future outcome__ is expected to fall with a certain probability for some value of $X$. [Can generalize this outside the range of the model, but with increasing uncertainty.]{.fragment .fade-in fragment-index=0}\n\n:::::\n::::::\n\n## Confidence Interval\n\n:::::: {.columns}\n::::: {.column}\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](07-evaluation-slides_files/figure-html/unnamed-chunk-12-1.png){width=672}\n:::\n:::\n\n\n:::::\n::::: {.column}\n:::: {.flex .align-center .align-with-plot}\n::: {.blue-panel .panel .flex .align-center}\n\n$$\n\\begin{align}\ny_i &= E[Y] + \\epsilon_i \\\\\n\\epsilon &\\sim N(0,\\sigma) \\\\\n\\sigma &= 2.91 \n\\end{align}\n$$   \n\n:::\n::: {.blue-panel .panel .flex .align-center }\n\n$$\n\\begin{align}\nE[Y] &\\approx \\hat\\beta X \\\\\n\\hat\\beta_0 &= -2.01 \\\\\n\\hat\\beta_1 &= 1.94\n\\end{align}\n$$ \n\n:::\n::::\n\n__Question__: But, what if we're wrong about $\\beta$?!!! \n\nWe can visualize this with a __confidence interval__, or the range within which the __average outcome__ is expected to fall with a certain probability for some value of $X$. \n\n:::::\n::::::\n\n# Model Evaluation\n\n## Model Complexity\n\n:::::: {.columns}\n::::: {.column}\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](07-evaluation-slides_files/figure-html/unnamed-chunk-13-1.png){width=100%}\n:::\n:::\n\n\n:::::\n::::: {.column}\n\n::: {style=\"margin-top:1.9em\"}\n‚öñÔ∏è __The error is smaller__ for the more complex model. This is a good thing, __but what did it cost us__? Need to weigh this against the increased complexity! \n:::\n\n:::::\n::::::\n\n## ANOVA for model\n\n::::::::: {.panel-tabset}\n\n### Problem\n\n:::::: {.columns}\n::::: {.column}\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](07-evaluation-slides_files/figure-html/unnamed-chunk-14-1.png){width=672}\n:::\n:::\n\n\n:::::\n::::: {.column}\n\nWe have two models of our data, one simple, the other complex.  \n\n__Question:__ Does the complex (bivariate) model explain more variance than the simple (intercept) model? Does the difference arise by chance?\n\n:::::\n::::::\n\n### Hypotheses\n\nThe __null__ hypothesis:\n\n- $H_0:$ no difference in variance explained.\n\nThe __alternate__ hypothesis:\n\n- $H_1:$ difference in variance explained.\n\n### Strategy\n\n__Variance Decomposition__. Total variance in the dependent variable can be decomposed into the variance captured by the more complex model and the remaining (or residual) variance:  \n\n:::::: {.columns}\n::::: {.column}\n\nDecompose the differences:\n\n$(y_{i} - \\bar{y}) = (\\hat{y}_{i} - \\bar{y}) + (y_{i} - \\hat{y}_{i})$\n\nwhere\n\n- $(y_{i} - \\bar{y})$ is the total error,  \n- $(\\hat{y}_{i} - \\bar{y})$ is the model error, and\n- $(y_{i} - \\hat{y}_{i})$ is the residual error. \n\n:::::\n::::: {.column}\n\nSum and square the differences:\n\n$SS_{T} = SS_{M} + SS_{R}$\n\nwhere \n\n- $SS_{T}$: Total Sum of Squares\n- $SS_{M}$: Model Sum of Squares\n- $SS_{R}$: Residual Sum of Squares \n\n:::::\n::::::\n\n### Sum of Squares\n\n\n::: {.cell fig.asp='0.34'}\n::: {.cell-output-display}\n![](07-evaluation-slides_files/figure-html/unnamed-chunk-15-1.png){width=100%}\n:::\n:::\n\n\n### F-statistic\n\n\n::: {.cell}\n\n:::\n\n\n$$F = \\frac{\\text{model variance}}{\\text{residual variance}}$$\n\nwhere\n\n- Model variance = $SS_{M}/k$ for $k$ model parameters\n\n- Residual variance = $SS_{R}/(n-k-1)$ for $k$ and $n$ observations.\n\n__Question:__ How probable is it that $F=$ 19.07?  \n\nWe can answer this by comparing the F-statistic to the F-distribution. \n\n### F-distribution\n\n:::::: {.columns}\n::::: {.column width=\"39%\"}\n\n::: {style=\"margin-top:1.9em;\"}\nSummary:\n\n- $\\alpha = 0.05$\n- $H_0:$ no difference\n- $p=$ 0.0024\n\n__Translation:__ the null hypothesis is really, really unlikely. So, there must be some difference between models!\n:::\n\n:::::\n::::: {.column width=\"60%\"}\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](07-evaluation-slides_files/figure-html/unnamed-chunk-17-1.png){width=672}\n:::\n:::\n\n\n:::::\n::::::\n\n:::::::::\n\n## R-Squared\n\n:::::: {.columns}\n::::: {.column}\n\nCoefficient of Determination\n\n$$R^2 = 1- \\frac{SS_{R}}{SS_{T}}$$  \n\n- Proportion of variance in $y$ explained by the model $M$.\n- Scale is 0 to 1. A value closer to 1 means that $M$ explains more variance.\n- $M$ is evaluated relative to simple, intercept-only model.\n\n:::::\n::::: {.column}\n\n\n::: {.cell layout-align=\"left\" fig.asp='1.2'}\n::: {.cell-output-display}\n![](07-evaluation-slides_files/figure-html/unnamed-chunk-18-1.png){fig-align='left' width=672}\n:::\n:::\n\n\n:::::\n::::::\n\n## t-test for coefficients\n\n::: {.panel-tabset}\n\n### Problem\n\n:::::: {.columns}\n::::: {.column}\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](07-evaluation-slides_files/figure-html/unnamed-chunk-19-1.png){width=672}\n:::\n:::\n\n\n:::::\n::::: {.column}\n\n__Question:__ Are the coefficient estimates significantly different than zero?\n\nTo answer this question, we need some measure of uncertainty for these estimates.\n\n:::::\n::::::\n\n### Hypotheses\n\nThe __null__ hypothesis:\n\n- $H_0:$ coefficient estimate is _not_ different than zero.\n\nThe __alternate__ hypothesis:\n\n- $H_1:$ coefficient estimate is different than zero.\n\n### Standard Errors\n\n:::::: {.columns}\n::::: {.column}\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](07-evaluation-slides_files/figure-html/unnamed-chunk-20-1.png){width=672}\n:::\n:::\n\n\n:::::\n::::: {.column}\n\nFor simple linear regression,\n\n- the __standard error of the slope__, $se(\\hat\\beta_1)$, is the ratio of the average squared error of the model to the total squared error of the predictor.\n\n- the __standard error of the intercept__, $se(\\hat\\beta_0)$, is $se(\\hat\\beta_1)$ weighted by the average squared values of the predictor.\n\n:::::\n::::::\n\n### t-statistic\n\n\n::: {.cell}\n\n:::\n\n\nThe t-statistic is the coefficient estimate divided by its standard error\n\n$$t = \\frac{\\hat\\beta}{se(\\hat\\beta)}$$ \n\n<br>\n\nThis can be compared to a t-distribution with $n-k-1$ degrees of freedom (\\\\(n\\\\) observations and \\\\(k\\\\) independent predictors). \n\n### t-test\n\n\n::: {.cell}\n\n:::\n\n\n:::::: {.columns}\n::::: {.column width=\"39%\"}\n\n::: {style=\"margin-top:1.9em;\"}\nSummary:\n\n- $\\alpha = 0.05$\n- $H_0$: $\\beta=0$\n- p-values:\n    - intercept = 0.5263\n    - slope < 0.0024\n:::\n\n:::::\n::::: {.column width=\"60%\"}\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](07-evaluation-slides_files/figure-html/unnamed-chunk-23-1.png){width=672}\n:::\n:::\n\n\n:::::\n::::::\n\n:::\n\n## Diagnostic Plots\n\n\n::: {.cell}\n<style type=\"text/css\">\n\n@media (min-width: 30em) {\n\n  .sideways .panel-tabset {\n     display: grid;\n     grid-gap: 2em;\n     grid-template-columns: 25% 75%;\n  }\n  \n  .sideways .panel-tabset-tabby {\n     border-bottom: none !important;\n     border-right: 1px solid #bbb; \n  }\n  \n  .sideways .panel-tabset [role=tab][aria-selected=true] {\n     background-color: transparent;\n     border: 1px solid #bbb !important;\n  }\n  \n  .sideways h3 {\n     margin-top: 0;\n  }\n  \n}\n\n</style>\n:::\n\n\n::::::{.sideways}\n::::: {.panel-tabset}\n\n### Model Assumptions\n\n1. __Weak Exogeneity__: the predictor variables have fixed values and are known. \n\n2. __Linearity__: the relationship between the predictor variables and the response variable is linear.  \n\n2. __Constant Variance__: the variance of the errors does not depend on the values of the predictor variables. Also known as _homoscedasticity_.  \n\n2. __Independence of errors__: the errors are uncorrelated with each other.  \n\n2. __No perfect collinearity__: the predictors are not linearly correlated with each other.  \n\n### Residual vs Fitted\n\n\n::: {.cell layout-align=\"center\" fig.asp='0.4'}\n::: {.cell-output-display}\n![](07-evaluation-slides_files/figure-html/unnamed-chunk-25-1.png){fig-align='center' width=1056}\n:::\n:::\n\n\n### Normal Q-Q\n\n\n::: {.cell layout-align=\"center\" fig.asp='1'}\n::: {.cell-output-display}\n![](07-evaluation-slides_files/figure-html/unnamed-chunk-26-1.png){fig-align='center' width=576}\n:::\n:::\n\n\n### Residual Density\n\n\n::: {.cell layout-align=\"center\" fig.asp='0.6'}\n::: {.cell-output-display}\n![](07-evaluation-slides_files/figure-html/unnamed-chunk-27-1.png){fig-align='center' width=864}\n:::\n:::\n\n\n### Cook's Distance\n\n\n::: {.cell layout-align=\"center\" fig.asp='0.6'}\n::: {.cell-output-display}\n![](07-evaluation-slides_files/figure-html/unnamed-chunk-28-1.png){fig-align='center' width=864}\n:::\n:::\n\n\n:::::\n::::::\n",
    "supporting": [
      "07-evaluation-slides_files\\figure-html"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}