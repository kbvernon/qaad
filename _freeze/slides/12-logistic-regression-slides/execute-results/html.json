{
  "hash": "2d9f4a1486b330626e00371bec0aeb60",
  "result": {
    "markdown": "---\ntitle: \"Lecture 12: Logistic Regression\"  \ndate: 'Last updated: 2023-03-28'\n---\n\n\n\n\n## ðŸ“‹ Lecture Outline\n\n- General Linear Model\n- Generalized Linear Model\n    - Distribution function\n    - Linear predictor\n    - Link function\n- Gaussian outcomes\n- Bernoulli outcomes\n- Proportional outcomes\n- Deviance\n- Information Criteria\n- ANOVE _aka_ Likelihood Ratio Test\n\n## General Linear Models\n\n$$y = E(Y) + \\epsilon$$\n$$E(Y) = \\mu = \\beta X$$\n\n- $E(Y)$ is the expected value (equal to the conditional mean, $\\mu$)  \n- $\\beta X$ is the linear predictor  \n- Error is normal, $\\epsilon \\sim N(0, \\sigma^2)$\n\n## Generalized Linear Models\n\n$$y = E(Y) + \\epsilon$$\n$$E(Y) = g(\\mu) = \\beta X$$\n\n- $g$ is the link function, makes the relationship linear \n- $\\beta X$ is the linear predictor\n- Error is exponential, $\\epsilon \\sim Exponential$\n\n## Generalized Linear Models\n\n\n::: {.cell}\n<style type=\"text/css\">\n\n@media (min-width: 30em) {\n\n  .panel-tabset {\n  \tdisplay: grid;\n  \tgrid-gap: 1em;\n  \tgrid-template-columns: 25% 75%;\n  }\n  \n  .panel-tabset-tabby {\n    border-bottom: none !important;\n  \tborder-right: 1px solid #bbb; \n  }\n  \n  .panel-tabset [role=tab][aria-selected=true] {\n    background-color: transparent;\n    border: 1px solid #bbb !important;\n  }\n  \n  h3 {\n  \tmargin-top: 0;\n  }\n  \n}\n\n</style>\n:::\n\n\n:::::: {.panel-tabset}\n\n### Exponential\n\n- A family of distributions: \n    - **Normal** - continuous and unbounded\n    - **Gamma** - continuous and non-negative\n    - **Binomial** - binary (yes/no) or proportional (0-1) \n    - **Poisson** - count data\n- Describes the distribution of the response\n- Two parameters: mean and variance\n- Variance is a function of the mean:\n\n$$Var(\\epsilon) = \\phi \\mu$$\n\nwhere $\\phi$ is a scaling parameter, assumed to be equal to 1, meaning the variance is assumed to be equal to the mean.\n\n### Linear predictor\n\n- Incorporates information about independent variables  \n- Combination of $x$ variables and associated coefficients  \n\n$$\\beta X = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\ldots + \\beta_n x_n$$\n\n- Commonly denoted with Greek letter \"eta\", as in $\\eta = \\beta X$\n\n### Link function\n\n- $g()$ modifies relationship between predictors and expected value.  \n- Makes this relationship linear\n\n\n::: {.cell}\n<style type=\"text/css\">\n\n.bob table td {\n  height: 70px;\n  vertical-align: middle;\n}\n\n</style>\n:::\n\n\n::: {.bob}\n\n| Distribution | Name     | Link                                            | Mean                                 |\n|--------------|----------|-------------------------------------------------|--------------------------------------|\n| Normal       | Identity | $\\beta X = \\mu$                                 | $\\mu = \\beta X$                      |\n| Gamma        | Inverse  | $\\beta X = \\mu^{-1}$                            | $\\mu = (\\beta X)^{-1}$               |\n| Poisson      | Log      | $\\beta X = ln\\,(\\mu)$                           | $\\mu = exp\\, (\\beta X)$              |\n| Binomial     | Logit    | $\\beta X = ln\\, \\left(\\frac{\\mu}{1-\\mu}\\right)$ | $\\mu = \\frac{1}{1+exp\\, (-\\beta X)}$ |\n\n:::\n\n::::::\n\n## Gaussian response\n\n\n::: {.cell}\n\n:::\n\n\n::::::::: {.r-stack .flt}\n:::::: {.columns .fragment .fade-out fragment-index=0 .w100}\n::::: {.column}\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](12-logistic-regression-slides_files/figure-revealjs/unnamed-chunk-4-1.png){width=576}\n:::\n:::\n\n\n:::::\n::::: {.column}\n\nAssume length is Gaussian with\n\n$Var(\\epsilon) = \\sigma^2$  \n$E(Y) = \\mu = \\beta X$  \n\n__Question__ What is the probability that we observe these data given a model with parameters $\\beta$ and $\\sigma^2$? \n\n:::::\n::::::\n\n:::::: {.columns .fragment .fade-in-then-out fragment-index=0 .w100}\n::::: {.column}\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](12-logistic-regression-slides_files/figure-revealjs/unnamed-chunk-5-1.png){width=576}\n:::\n:::\n\n\n:::::\n::::: {.column}\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](12-logistic-regression-slides_files/figure-revealjs/unnamed-chunk-6-1.png){width=576}\n:::\n:::\n\n\n:::::\n::::::\n\n:::::: {.columns .fragment .fade-in-then-out fragment-index=1 .w100}\n::::: {.column}\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](12-logistic-regression-slides_files/figure-revealjs/unnamed-chunk-7-1.png){width=576}\n:::\n:::\n\n\n:::::\n::::: {.column}\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](12-logistic-regression-slides_files/figure-revealjs/unnamed-chunk-8-1.png){width=576}\n:::\n:::\n\n\n:::::\n::::::\n\n:::::: {.columns .fragment .fade-in fragment-index=2 .w100}\n::::: {.column}\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](12-logistic-regression-slides_files/figure-revealjs/unnamed-chunk-9-1.png){width=576}\n:::\n:::\n\n\n:::::\n::::: {.column}\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](12-logistic-regression-slides_files/figure-revealjs/unnamed-chunk-10-1.png){width=576}\n:::\n:::\n\n\n:::::\n::::::\n\n:::::::::\n\n::: aside\n_Archaic dart points. Sample from Fort Hood, Texas_\n:::\n\n## Log Odds\n\n::: {.bob style=\"width:65%;\"}\n\nLocation of residential features at the Snodgrass sites\n\n|             | Inside wall                       | Outside wall                      | Total |\n|-------------|-----------------------------------|-----------------------------------|-------|\n| Count       | 38                                | 53                                | 91    |\n| Probability | &nbsp; 0.42 = $\\frac{38}{91}$     | &nbsp; 0.58 = $\\frac{53}{91}$     | 1     |\n| Odds        | &nbsp; 0.72 = $\\frac{0.42}{0.58}$ | &nbsp; 1.40 = $\\frac{0.58}{0.42}$ |       |\n| Log Odds    | -0.33 = log(0.72)                 | &nbsp; 0.33 = log(1.40)           |       |\n\n:::\n\n**Why Log Odds?**  \nBecause the distribution of odds can be highly skewed, and taking the log normalizes it (makes it more symmetric).    \n\n## Bernoulli response\n\n::::::::: {.r-stack .flt}\n:::::: {.columns .fragment .fade-out fragment-index=0 .w100}\n::::: {.column}\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](12-logistic-regression-slides_files/figure-revealjs/unnamed-chunk-11-1.png){width=576}\n:::\n:::\n\n\n:::::\n::::: {.column}\n\nLocation inside or outside of the inner wall at the Snodgrass site is a Bernoulli variable and has expectation $E(Y) = p$ where\n\n$$p = \\frac{1}{1 + exp(-\\beta X)}$$\nThis defines a [logistic curve](https://en.wikipedia.org/wiki/Logistic_function) or sigmoid, with $p$ being the probability of success. This constrains the estimate $E(Y)$ to be in the range 0 to 1.\n\n:::::\n::::::\n\n:::::: {.columns .fragment .fade-in-then-out fragment-index=0 .w100}\n::::: {.column}\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](12-logistic-regression-slides_files/figure-revealjs/unnamed-chunk-12-1.png){width=576}\n:::\n:::\n\n\n:::::\n::::: {.column}\n\nTaking the log of $p$ gives us\n\n$$log(p) = log\\left(\\frac{p}{1 - p}\\right) = \\beta X$$\n\nThis is known as the \"logit\" or [log odds](https://en.wikipedia.org/wiki/Logit). \n\n__Question__ What is the probability that we observe these data (these inside features) given a model with parameters $\\beta$? \n\n:::::\n::::::\n\n:::::: {.columns .fragment .fade-in fragment-index=1 .w100}\n::::: {.column}\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](12-logistic-regression-slides_files/figure-revealjs/unnamed-chunk-13-1.png){width=576}\n:::\n:::\n\n\n:::::\n::::: {.column}\n\nEstimated coefficients: \n\n$\\beta_0 = -8.6631$  \n$\\beta_1 = 0.0348$   \n\nFor these, the log Likelihood is\n\n$\\mathcal{l} = -28.8641$  \n\n:::::\n::::::\n:::::::::\n\n::: aside\n*Data from the Snodgrass site in Butler County, MS*\n:::\n\n## Interpretation\n\n::::::::: {.r-stack .flt}\n:::::: {.columns .fragment .fade-out fragment-index=0 .w100}\n::::: {.column}\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](12-logistic-regression-slides_files/figure-revealjs/unnamed-chunk-14-1.png){width=576}\n:::\n:::\n\n\n:::::\n::::: {.column}\n\nEstimated coefficients: \n\n$\\beta_0 = -8.6631$  \n$\\beta_1 = 0.0348$   \n\nNote that these coefficient estimates are log-odds! To get the odds, we take the exponent.   \n\n$\\beta_0 = exp(-8.6631) = 0.0002$  \n$\\beta_1 = exp(0.0348) = 1.0354$  \n\nFor a one unit increase in area, the odds of being in the inside wall increase by 1.0354.\n\n:::::\n::::::\n\n:::::: {.columns .fragment .fade-in fragment-index=0 .w100}\n::::: {.column}\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](12-logistic-regression-slides_files/figure-revealjs/unnamed-chunk-15-1.png){width=576}\n:::\n:::\n\n\n:::::\n::::: {.column}\n\nEstimated coefficients: \n\n$\\beta_0 = -8.6631$  \n$\\beta_1 = 0.0348$   \n\nTo get the probability, we can use the mean function (also known as the inverse link):\n\n$$p = \\frac{1}{1+exp(-\\beta X)}$$\n\nFor a house structure with an area of 300 square feet, the estimated probability that it occurs inside the inner wall is 0.8538.\n\n:::::\n::::::\n:::::::::\n\n::: aside\n*Data from the Snodgrass site in Butler County, MS*\n:::\n\n## Proportional response\n\n::::::::: {.r-stack .flt}\n:::::: {.columns .fragment .fade-out fragment-index=0 .w100}\n::::: {.column}\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](12-logistic-regression-slides_files/figure-revealjs/unnamed-chunk-16-1.png){width=576}\n:::\n:::\n\n\n:::::\n::::: {.column}\n\nProportion of Roman pottery is a binomial variable and has expectation $E(Y) = p$ where\n\n$$p = \\frac{1}{1 + exp(-\\beta X)}$$\nThis defines a [logistic curve](https://en.wikipedia.org/wiki/Logistic_function) or sigmoid, with $p$ being the proportion of successful Bernoulli trials. This constrains the estimate $E(Y)$ to be in the range 0 to 1.\n\n:::::\n::::::\n\n:::::: {.columns .fragment .fade-in-then-out fragment-index=0 .w100}\n::::: {.column}\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](12-logistic-regression-slides_files/figure-revealjs/unnamed-chunk-17-1.png){width=576}\n:::\n:::\n\n\n:::::\n::::: {.column}\n\nTaking the log of $p$ gives us\n\n$$log(p) = log\\left(\\frac{p}{1 - p}\\right) = \\beta X$$\n\nThis is known as the \"logit\" or [log odds](https://en.wikipedia.org/wiki/Logit). \n\n__Question__ What is the probability that we observe these data (these proportions) given a model with parameters $\\beta$? \n\n:::::\n::::::\n\n:::::: {.columns .fragment .fade-in fragment-index=1 .w100}\n::::: {.column}\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](12-logistic-regression-slides_files/figure-revealjs/unnamed-chunk-18-1.png){width=576}\n:::\n:::\n\n\n:::::\n::::: {.column}\n\nEstimated coefficients: \n\n$\\beta_0 = -1.1818$  \n$\\beta_1 = -0.0121$   \n\nFor these, the log Likelihood is\n\n$\\mathcal{l} = -4.1148$  \n\n:::::\n::::::\n:::::::::\n\n::: aside\n*Data from Fulford and Hodder (1974). A Regression Analysis of Some Late Romano-British Pottery: A\nCase Study*\n:::\n\n## Deviance\n\n- Measure of goodness of fit, so __smaller is better__\n- Gives the difference in log-Likelihood between a model $M$ and a __saturated__ model $M_S$\n    - $M_S$ has a parameter for each observation (i.e., zero degrees of freedom)\n\n$$D = -2\\,\\Big[\\,log\\,\\mathcal{L}(M_1) - log\\,\\mathcal{L}(M_S)\\,\\Big]$$\n\n- __Residual deviance__ = deviance of proposed model\n- __Null deviance__ = deviance of null model (ie, intercept-only model)\n\n## Information Criteria\n\n- A function of the deviance, so __smaller is better__\n- Two penalties: \n    - $n$ = number of observations \n    - $p$ = number of parameters\n- Akaike Information Criteria\n    - $AIC = D + 2p$\n- Bayesian Information Criteria \n    - $BIC = D + (p \\cdot log(n))$\n\n## ANOVA\n\n- Analysis of Deviance _aka_ Likelihood Ratio Test\n- For Binomial and Poisson models\n- Test statistic is the logged ratio of likelihoods between proposed, $M_1$, and null, $M_0$, models\n\n$$\n\\begin{aligned}\n\\chi^2 &= -2\\,log\\,\\frac{\\mathcal{L}(M_0)}{\\mathcal{L}(M_1)}\\\\\\\\\n&= -2\\,\\Big[\\,log\\,\\mathcal{L}(M_0) - log\\,\\mathcal{L}(M_1)\\,\\Big]\n\\end{aligned}\n$$\n\n- Compare to a $\\chi^2$ distribution with $k$ degrees of freedom - asks what the probability is of getting a value greater than the observed $\\chi^2$ value.\n- Null hypothesis is no difference in log-Likelihood.",
    "supporting": [
      "12-logistic-regression-slides_files\\figure-revealjs"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\r\n<script>\r\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\r\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\r\n  // slide changes (different for each slide format).\r\n  (function () {\r\n    // dispatch for htmlwidgets\r\n    function fireSlideEnter() {\r\n      const event = window.document.createEvent(\"Event\");\r\n      event.initEvent(\"slideenter\", true, true);\r\n      window.document.dispatchEvent(event);\r\n    }\r\n\r\n    function fireSlideChanged(previousSlide, currentSlide) {\r\n      fireSlideEnter();\r\n\r\n      // dispatch for shiny\r\n      if (window.jQuery) {\r\n        if (previousSlide) {\r\n          window.jQuery(previousSlide).trigger(\"hidden\");\r\n        }\r\n        if (currentSlide) {\r\n          window.jQuery(currentSlide).trigger(\"shown\");\r\n        }\r\n      }\r\n    }\r\n\r\n    // hookup for slidy\r\n    if (window.w3c_slidy) {\r\n      window.w3c_slidy.add_observer(function (slide_num) {\r\n        // slide_num starts at position 1\r\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\r\n      });\r\n    }\r\n\r\n  })();\r\n</script>\r\n\r\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}