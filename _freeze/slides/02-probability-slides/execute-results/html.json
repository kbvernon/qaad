{
  "hash": "d27f325213ead3dddb2b51361f592b67",
  "result": {
    "markdown": "---\ntitle: \"Lecture 02: Probability as a Model\"  \ndate: 'Last updated: 2023-02-20'\n---\n\n\n\n\n\n\n\n\n## ðŸ“‹ Lecture Outline\n\n- Why statistics?\n- ðŸ§ª A simple experiment\n- Some terminology\n- ðŸŽ° Random variables\n- ðŸŽ² Probability\n- ðŸ“Š Probability Distribution\n- Probability Distribution as a Model\n- Probability Distribution as a Function\n- Probability Mass Functions\n- Probability Density Functions\n- ðŸš— Cars Model\n- Brief Review\n- A Simple Formula\n- A Note About Complexity\n\n# Setup {visibility=\"uncounted\"}\n\n## Why statistics?\n\n::::: {.columns}\n:::: {.column width=\"60%\"}\n::: {.r-stack}\n::: {.r-stack}\n![](images/samples_and_populations-0.png)\n\n![](images/samples_and_populations-1.png)\n\n![](images/samples_and_populations-2.png)\n\n![](images/samples_and_populations-3.png)\n\n:::\n\n![](images/samples_and_populations-highlight_description.png){.fragment fragment-index=0}\n:::\n::::\n:::: {.column width=\"40%\"}\n<br>\n\n::: {.r-stack}\n\n::: {.fragment .fade-out fragment-index=0}\n\nWe want to understand something about a __population__.\n\nWe can never observe the entire population, so we draw a __sample__.  \n\nWe then use a model to __describe__ the sample.  \n\nBy comparing that model to a null model, we can __infer__ something about the population.  \n\n:::\n::: {.fragment fragment-index=0 style=\"margin:0;\"}\n\nHere, we're going to focus on statistical __description__, aka models.\n\n:::\n:::\n::::\n:::::\n\n## ðŸ§ª A simple experiment\n\n:::::: {.columns}\n:::: {.column width=\"58%\"}\n\n::: {.r-stack}\n::: {.fragment .fade-out fragment-index=0}\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](02-probability-slides_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\n:::\n::: {.fragment .fade-in fragment-index=0}\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](02-probability-slides_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\n:::\n:::\n\n::::\n\n:::: {.column width=\"42%\"}\n\n::: {style=\"padding-top:2.5rem\"}\nWe take ten cars, send each down a track, have them brake at the same point, and measure the distance it takes them to stop.  \n\n::: {.r-stack}\n::: {.fragment .fade-in-then-out fragment-index=0 style=\"margin-top:0;margin-left:0\"}\n__Question:__ how far do you think it will take the next car to stop?  \n:::\n::: {.fragment fragment-index=1 style=\"margin-top:0;margin-left:0\"}\n__Question:__ what distance is the most __probable__?  \n\nBut, how do we determine this?\n:::\n:::\n:::\n\n::::\n::::::\n\n## Some terminology\n\n::: {.r-stack}\n![](images/random_variable-coins.png) \n\n![](images/random_variable-outcome.png){.fragment} \n\n![](images/random_variable-space.png){.fragment} \n\n![](images/random_variable-variable.png){.fragment} \n:::\n\n## &#x1F3B0; Random Variables\n\nTwo types of random variable:\n\n<!---\nhttps://github.com/allisonhorst/stats-illustrations/blob/master/other-stats-artwork/continuous_discrete.png?raw=true\n--->\n\n1. __Discrete__ random variables often take only _integer_ (non-decimal) values.  \n    - Examples: number of heads in 10 tosses of a fair coin, number of victims of the Thanos snap, number of projectile points in a stratigraphic level, number of archaeological sites in a watershed.  \n\n2. __Continuous__ random variables take _real_ (decimal) values.\n    - Examples: cost in property damage of a superhero fight, kilocalories per kilogram, kilocalories per hour, ratio of isotopes  \n    - Note: for continuous random variables, the sample space is infinite!\n\n# Probability Distributions {visibility=\"uncounted\"}\n\n## &#x1F3B2; Probability\n\nLet $X$ be the number of heads in two tosses of a fair coin. What is the probability that $X=1$? \n\n:::::: {.columns}\n:::: {.column}\n::: {.r-stack}\n![](images/probability-distribution-01.png){.fragment .fade-out fragment-index=0}\n\n![](images/probability-distribution-02.png){.fragment .fade-in fragment-index=0}\n:::\n::::\n:::: {.column}\n::: {.fragment .fade-in fragment-index=1}\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](02-probability-slides_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\n:::\n::::\n::::::\n\n## Probability Distribution as a Model\n\n\n::: {.cell}\n<style type=\"text/css\">\n\n/* setting this here until I figure out how to get utility classes until reveal */\n.gray-panel {\n  background-image: linear-gradient(to bottom right, white, #f1f1f1);\n  border: 1px solid #666666;\n  border-radius: 5px;\n  margin: 1rem 0;\n  padding: 0.2em 0.5em;\n}\n\n</style>\n:::\n\n\nHas two components:  \n\n::: {.gray-panel}\n__Central-tendency__ or \"first moment\"  \n\n- Population mean ($\\mu$). Gives the expected value of an experiment, $E[X] = \\mu$.  \n- Sample mean ($\\bar{x}$). Estimate of $\\mu$ based on a sample from $X$ of size $n$.  \n:::\n::: {.gray-panel}\n__Dispersion__ or \"second moment\"   \n\n- Population variance ($\\sigma^2$). The expected value of the squared difference from the mean.  \n- Sample variance ($s^2$). Estimate of $\\sigma^2$ based on a sample from $X$ of size $n$.  \n- Standard deviation ($\\sigma$) or $s$ is the square root of the variance.  \n:::\n\n## Probability Distribution as a Function\n\nThese can be defined using precise mathematical functions:  \n\n::: {.gray-panel}\nA __probability mass function__ (PMF) for __discrete__ random variables.  \n\n- Examples: Bernoulli, Binomial, Negative Binomial, Poisson  \n- Straightforward probability interpretation.  \n:::\n::: {.gray-panel}\nA __probability density function__ (PDF) for __continuous__ random variables.  \n\n- Examples: Normal, Chi-squared, Student's t, and F  \n- Harder to interpret probability:  \n    - What is the probability that a car takes 10.317 m to stop? What about 10.31742 m?  \n    - Better to consider probability across an interval.  \n- Requires that the function integrate to one (probability is the area under the curve).  \n:::\n\n# Probability Mass Functions (PMF) {visibility=\"uncounted\"}\n\n## Bernoulli\n\n:::: {.columns}\n::: {.column}\n::: {style=\"padding-top:2.5rem\"}\n\nDf. distribution of a binary random variable (\"Bernoulli trial\") with two possible values, 1 (success) and 0 (failure), with $p$ being the probability of success. E.g., a single coin flip.\n\n$$f(x,p) = p^{x}(1-p)^{1-x}$$\n\nMean: $p$  \nVariance: $p(1-p)$\n\n:::\n:::\n::: {.column}\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](02-probability-slides_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\n:::\n::::\n\n## Binomial\n\n:::: {.columns}\n::: {.column}\n::: {style=\"padding-top:2.5rem\"}\n\nDf. distribution of a random variable whose value is equal to the number of successes in $n$ independent Bernoulli trials. E.g., number of heads in ten coin flips.\n\n$$f(x,p,n) = \\binom{n}{x}p^{x}(1-p)^{1-x}$$\n\nMean: $np$  \nVariance: $np(1-p)$\n\n:::\n:::\n::: {.column}\n::: {.r-stack}\n::: {.fragment .fade-out fragment-index=0}\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](02-probability-slides_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n\n:::\n::: {.fragment fragment-index=0}\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](02-probability-slides_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n\n:::\n:::\n:::\n::::\n\n## Poisson\n\n:::: {.columns}\n::: {.column}\n::: {style=\"padding-top:2.5rem\"}\n\nDf. distribution of a random variable whose value is equal to the number of events occurring in a fixed interval of time or space. E.g., number of orcs passing through the Black Gates in an hour.\n\n$$f(x,\\lambda) = \\frac{\\lambda^{x}e^{-\\lambda}}{x!}$$\n\nMean: $\\lambda$  \nVariance: $\\lambda$\n\n:::\n:::\n::: {.column}\n::: {.r-stack}\n::: {.fragment .fade-out fragment-index=0}\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](02-probability-slides_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n\n:::\n::: {.fragment fragment-index=0}\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](02-probability-slides_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n:::\n\n\n:::\n:::\n:::\n::::\n\n# Probability Density Functions (PDF) {visibility=\"uncounted\"}\n\n## Normal (Gaussian)\n\n:::: {.columns}\n::: {.column}\n::: {style=\"padding-top:2.5rem\"}\n\nDf. distribution of a continuous random variable that is symmetric from positive to negative infinity. E.g., the height of actors who auditioned for the role of Aragorn.\n\n$$f(x,\\mu,\\sigma) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\;exp\\left[-\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^2\\right]$$\n\nMean: $\\mu$  \nVariance: $\\sigma^2$\n\n:::\n:::\n::: {.column}\n::: {.r-stack}\n::: {.fragment .fade-out fragment-index=0}\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](02-probability-slides_files/figure-html/unnamed-chunk-12-1.png){width=672}\n:::\n:::\n\n\n:::\n::: {.fragment fragment-index=0}\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](02-probability-slides_files/figure-html/unnamed-chunk-13-1.png){width=672}\n:::\n:::\n\n\n:::\n:::\n:::\n::::\n\n# Bringing it all together {visibility=\"uncounted\"}\n\n## &#x1F697; Cars Model\n\n::::: {.columns}\n:::: {.column}\n::: {style=\"margin-top:2.5rem;\"}\n::: {.r-stack}\n::: {.fragment .fade-out fragment-index=0 style=\"margin:0;\"}\nLet's use the Normal distribution to describe the cars data.\n\n- $Y$ is stopping distance for population \n- $Y$ is normally distributed, $Y \\sim N(\\mu, \\sigma)$ \n- Experiment is a random sample of size $n$ from $Y$ with $y_1, y_2, ..., y_n$ observations.\n- Sample _statistics_ ($\\bar{y}, s$) approximate population _parameters_ ($\\mu, \\sigma$).\n:::\n::: {.fragment .fade-in-then-out fragment-index=0 style=\"margin:0;\"}\nSample statistics:\n\n- Mean ($\\bar{y}$) = 10.54 m\n:::\n::: {.fragment .fade-in-then-out fragment-index=2 style=\"margin:0;\"}\nSample statistics:\n\n- Mean ($\\bar{y}$) = 10.54 m\n\nThis is our approximate expectation\n\n- $E[Y] = \\mu \\approx \\bar{y}$\n:::\n::: {.fragment .fade-in-then-out fragment-index=3 style=\"margin:0;\"}\nSample statistics:\n\n- Mean ($\\bar{y}$) = 10.54 m\n\nBut, there's error, $\\epsilon$, in this estimate.\n\n- $\\epsilon_i = y_i - \\bar{y}$\n:::\n::: {.fragment .fade-in-then-out fragment-index=4 style=\"margin:0;\"}\nSample statistics:\n\n- Mean ($\\bar{y}$) = 10.54 m\n\nThe average squared error is the variance:\n\n- $s^2 = \\frac{1}{n-1}\\sum \\epsilon_{i}^{2}$\n:::\n::: {.fragment .fade-in-then-out fragment-index=5 style=\"margin:0;\"}\nSample statistics:\n\n- Mean ($\\bar{y}$) = 10.54 m\n- S.D. ($s$) = 5.353 m\n\nThis is our uncertainty, how big we think any given error will be.  \n:::\n::: {.fragment .fade-in-then-out fragment-index=6 style=\"margin:0;\"}\nSample statistics:\n\n- Mean ($\\bar{y}$) = 10.54 m\n- S.D. ($s$) = 5.353 m\n\nSo, here is our probability model.\n\n$$Y \\sim N(\\bar{y}, s)$$\nThis is only an estimate of $N(\\mu, \\sigma)$!\n\n:::\n::: {.fragment fragment-index=7 style=\"margin:0;\"}\nSample statistics:\n\n- Mean ($\\bar{y}$) = 10.54 m\n- S.D. ($s$) = 5.353 m\n\nWith it, we can say, for example, that the probability that a random draw from this distribution falls within one standard deviation (dashed lines) of the mean (solid line) is 68.3%.\n:::\n:::\n:::\n::::\n:::: {.column}\n::: {.r-stack}\n::: {.fragment .fade-out fragment-index=0}\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](02-probability-slides_files/figure-html/unnamed-chunk-14-1.png){width=672}\n:::\n:::\n\n\n:::\n::: {.fragment .fade-in-then-out fragment-index=0}\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](02-probability-slides_files/figure-html/unnamed-chunk-15-1.png){width=672}\n:::\n:::\n\n\n:::\n::: {.fragment .fade-in-then-out fragment-index=2}\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](02-probability-slides_files/figure-html/unnamed-chunk-16-1.png){width=672}\n:::\n:::\n\n\n:::\n::: {.fragment .fade-in-then-out fragment-index=3}\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](02-probability-slides_files/figure-html/unnamed-chunk-17-1.png){width=672}\n:::\n:::\n\n\n:::\n::: {.fragment .fade-in-then-out fragment-index=4}\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](02-probability-slides_files/figure-html/unnamed-chunk-18-1.png){width=672}\n:::\n:::\n\n\n:::\n::: {.fragment .fade-in-then-out fragment-index=5}\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](02-probability-slides_files/figure-html/unnamed-chunk-19-1.png){width=672}\n:::\n:::\n\n\n:::\n::: {.fragment .fade-in-then-out fragment-index=6}\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](02-probability-slides_files/figure-html/unnamed-chunk-20-1.png){width=672}\n:::\n:::\n\n\n:::\n::: {.fragment fragment-index=7}\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](02-probability-slides_files/figure-html/unnamed-chunk-21-1.png){width=672}\n:::\n:::\n\n\n:::\n:::\n::::\n:::::\n\n## A Simple Formula\n\n::::: {.columns}\n:::: {.column}\n::: {style=\"margin-top:2.5rem;\"}\n::: {.r-stack}\n::: {.fragment .fade-out fragment-index=0 style=\"margin:0;\"}\nThis gives us a simple formula\n\n$$y_i = \\bar{y} + \\epsilon_i$$\nwhere\n\n- $y_i$: stopping distance for car $i$, __data__\n- $\\bar{y} \\approx E[Y]$: expectation, __predictable__\n- $\\epsilon_i$: error, __unpredictable__\n:::\n::: {.fragment .fade-in-then-out fragment-index=0 style=\"margin:0;\"}\nThis gives us a simple formula\n\n$$y_i = \\bar{y} + \\epsilon_i$$\n\nIf we subtract the mean, we have a model of the errors centered on zero:\n\n$$\\epsilon_i = 0 + (y_i - \\bar{y})$$\n:::\n::: {.fragment fragment-index=1 style=\"margin:0;\"}\nThis gives us a simple formula\n\n$$y_i = \\bar{y} + \\epsilon_i$$\n\nIf we subtract the mean, we have a model of the errors centered on zero:\n\n$$\\epsilon_i = 0 + (y_i - \\bar{y})$$\n\nThis means we can construct a probability model of the errors centered on zero.\n:::\n:::\n:::\n::::\n:::: {.column}\n::: {.r-stack}\n::: {.fragment .fade-out fragment-index=0}\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](02-probability-slides_files/figure-html/unnamed-chunk-22-1.png){width=672}\n:::\n:::\n\n\n:::\n::: {.fragment fragment-index=0}\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](02-probability-slides_files/figure-html/unnamed-chunk-23-1.png){width=672}\n:::\n:::\n\n\n:::\n:::\n::::\n:::::\n\n## Probability Model of Errors \n\n\n::: {.cell fig.asp='0.4'}\n::: {.cell-output-display}\n![](02-probability-slides_files/figure-html/unnamed-chunk-24-1.png){width=1056}\n:::\n:::\n\n\n::: {style=\"text-align: center;\"}\nNote that the mean changes, but the variance stays the same.\n:::\n\n## Summary \n\nNow our simple formula is this:\n\n$$y_i = \\bar{y} + \\epsilon_i$$ \n$$\\epsilon \\sim N(0, s) $$\n\n- Again, $\\bar{y} \\approx E[Y] = \\mu$. \n- For any future outcome: \n    - The expected value is deterministic \n    - The error is stochastic  \n- Must assume that the errors are __iid__! \n    - **i**ndependent = they do not affect each other \n    - **i**dentically **d**istributed = they are from the same probability distribution \n- The distribution is now a model of the errors! \n",
    "supporting": [
      "02-probability-slides_files\\figure-html"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}