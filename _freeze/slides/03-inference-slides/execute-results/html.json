{
  "hash": "e937a1435d1748c42dba468dd2f149fa",
  "result": {
    "markdown": "---\ntitle: \"Lecture 03: Statistical Inference\"  \ndate: 'Last updated: 2023-01-24'\n---\n\n\n\n\n\n\n## ðŸ“‹ Lecture Outline\n\n- Why statistics?\n- Statistical Inference\n- Simple Example\n- Hypotheses\n- Tests\n- Rejecting the null hypothesis\n- Student's t-test\n- ANOVA\n\n## Why statistics?\n\n::::: {.columns}\n:::: {.column width=\"60%\"}\n::: {.r-stack}\n::: {.r-stack}\n![](images/samples_and_populations-0.png)\n\n![](images/samples_and_populations-1.png)\n\n![](images/samples_and_populations-2.png)\n\n![](images/samples_and_populations-3.png)\n:::\n\n![](images/samples_and_populations-highlight_inference.png){.fragment fragment-index=0}\n:::\n::::\n:::: {.column width=\"40%\"}\n<br>\n\n::: {.r-stack}\n\n::: {.fragment .fade-out fragment-index=0}\n\nWe want to understand something about a __population__.\n\nWe can never observe the entire population, so we draw a __sample__.  \n\nWe then use a model to __describe__ the sample.  \n\nBy comparing that model to a null model, we can __infer__ something about the population. \n\n:::\n::: {.fragment fragment-index=0 style=\"margin:0;\"}\n\nHere, we're going to focus on statistical __inference__.\n\n:::\n:::\n::::\n:::::\n\n## Simple Example\n\n::::: {.columns}\n:::: {.column}\n\n![](images/projectile_points.png){fig-width=\"100%\"}\n\n::::\n:::: {.column}\n\n<br>\n\nTwo samples of length (mm, n=300).  \n\n<br>\n\n__Question:__ Are these samples of the same __type__? The same __population__? \n\n::::\n:::::\n\n## Sample Distribution\n\nDo these really represent different types?  \n\n::::: {.columns}\n::: {.column}\n\n\n::: {.cell fig.asp='0.9'}\n::: {.cell-output-display}\n![](03-inference-slides_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\n:::\n::: {.column}\n\nTwo models:    \n\n- same type (same population)  \n- different types (different populations)  \n\nNote that:  \n\n- these models are mutually exclusive and  \n- the second model is more complex.  \n\n:::\n:::::\n\n## Hypotheses\n\nThe two models represent our hypotheses.\n\n::: {style=\"width:65%;margin:1em auto 0 auto;\"}\n![](images/hypothesis_null.png)\n\n![](images/hypothesis_alternate.png)\n:::\n\n## Testing Method\n\n\n::: {.cell}\n\n:::\n\n\n:::: {.columns}\n::: {.column}\n\n<br>\n\nProcedure:\n\n1. Take sample(s).\n2. Calculate test __statistic__.  \n3. Compare to test __probability__ distribution.\n4. Get __p-value__.\n5. Compare to __critical value__. \n6. Accept (or reject) null hypothesis.\n\n:::\n::: {.column}\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](03-inference-slides_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\n:::\n::::\n\n## Average of differences\n\n\n::: {.cell}\n\n:::\n\n\n:::: {.columns}\n::: {.column}\n\n<br>\n\nSuppose we take two samples from the same population, calculate the difference in their means, and repeat this 1,000 times.  \n\n__Question__: What will the average difference be between the sample means?\n\n:::\n::: {.column}\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](03-inference-slides_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n\n:::\n::::\n\n## Probability of differences\n\n:::: {.columns}\n::: {.column}\n\n<br>\n\nIf we convert these differences into a probability distribution, we can estimate the probability of any given difference.  \n\nThe __p-value__ represents _how likely it is that the difference we see arises by chance_.\n\n:::\n::: {.column}\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](03-inference-slides_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\n:::\n::::\n\n## Model of Differences\n\n\n::: {.cell}\n\n:::\n\n\n\n:::: {.columns}\n::: {.column}\n\n<br>\n\nIn classical statistics, we use a model of that distribution to estimate the probability of a given difference. Here we are using $N(0,$ 0.69$)$, where the probability of getting a difference $\\pm$ 1 mm ($\\pm2s$) or greater is 0.05.  \n\n:::\n::: {.column}\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](03-inference-slides_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n\n:::\n::::\n\n## Rejecting the Null Hypothesis\n\n:::: {.columns}\n::: {.column}\n\n<br>\n\n__Question:__ How do we decide?  \n\nDefine a __critical limit__ ($\\alpha$)  \n\n- Must be determined prior to the test!\n- If $p < \\alpha$, reject. __&larr; THIS IS THE RULE!__\n- Generally, $\\alpha = 0.05$ \n\n:::\n::: {.column}\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](03-inference-slides_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n\n:::\n::::\n\n## Why the critical limit?\n\nBecause we might be wrong! But what _kind_ of wrong?\n\n::: {style=\"width:50%;margin:0 auto;\"}\n![](images/error_table.png)\n:::\n\nWith $\\alpha=0.05$, we are saying, \"If we run this test 100 times, only 5 of those tests should result in a Type 1 Error.\"\n\n## Student's t-test\n\n::::::::: {.panel-tabset}\n\n### Problem\n\n:::::: {.columns}\n::: {.column}\n\n![](images/projectile_points-2.png)\n\n:::\n::: {.column}\n\n<br>\n\nWe have two samples of projectile points, each consisting of 300 measurements of length (mm).  \n\n__Question:__ Are these samples of the same point __type__? The same __population__?\n\n:::\n::::::\n\n### Hypotheses\n\nThe __null__ hypothesis: \n\n[$H_0: \\mu_1 = \\mu_2$]{style=\"padding-left:35px;font-size:100%\"}\n\nThe __alternate__ hypothesis: \n\n[$H_1: \\mu_1 \\neq \\mu_2$]{style=\"padding-left:35px;font-size:100%\"}\n\nThis is a __two-sided__ t-test as the difference can be positive or negative. \n\n### Difference\n\n:::::: {.columns}\n::: {.column width=\"60%\"}\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](03-inference-slides_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n:::\n\n\n:::\n::: {.column width=\"40%\"}\n\n<br>\n\n\n::: {.cell}\n\n:::\n\n\n__Question:__ Is this difference (-1.82) big enough to reject the null?\n\n:::\n::::::\n\n### t-statistic\n\nA t-statistic standardizes the difference in means using the standard error of the sample mean.  \n\n$$t = \\frac{\\bar{x}_1 - \\bar{x}_2}{s_{\\bar{x}}}$$  \n\n\n::: {.cell}\n\n:::\n\n\nFor our samples, $t =$ -3.61. This is a model of our data! \n\n__Question:__ How probable is this estimate?  \n\nWe can answer this by comparing the t-statistic to the t-distribution. \n\n### Complexity\n\nBut first, we need to evaluate how complex the t-statistic is. We do this by estimating the __degrees of freedom__, or the number of values that are free to vary after calculating a statistic. In this case, we have two samples with 300 observations each, hence:  \n\n::: {style=\"width:10%;margin:0 auto;\"}\n__df = 530__\n:::\n\nCrucially, this affects the shape of the t-distribution and, thus, determines the location of the critical value we use to evaluate the null hypothesis.\n\n### t-distribution\n\n:::::: {.columns}\n::: {.column width=\"60%\"}\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](03-inference-slides_files/figure-html/unnamed-chunk-14-1.png){width=672}\n:::\n:::\n\n\n:::\n::: {.column width=\"40%\"}\n\n<br>\n\nSummary:  \n\n- $\\alpha = 0.05$  \n- $H_{0}: \\mu_1 = \\mu_2$  \n- $p =$ 0.0003  \n\n__Translation:__ the null hypothesis is really, really unlikely. So, there must be some difference in the mean!\n\n:::\n::::::\n\n:::::::::\n\n## ANOVA\n\n\n::: {.cell}\n\n:::\n\n\n::::::::: {.panel-tabset}\n\n### Problem\n\n:::::: {.columns}\n::: {.column}\n\n![](images/projectile_points.png){width=85%}\n\n:::\n::: {.column}\n\n<br>\n\nWe have five samples of points, each consisting of 100 measurements of length (mm).  \n\n__Question:__ Are these samples of the same point __type__? The same __population__?\n\nAnalysis of Variance (ANOVA) is like a t-test but for more than two samples.\n\n:::\n::::::\n\n### Hypotheses\n\nThe __null__ hypothesis: \n\n[$H_0:$ no difference between groups]{style=\"padding-left:35px;font-size:100%\"}\n\nThe __alternate__ hypothesis: \n\n[$H_1:$ at least one group is different]{style=\"padding-left:35px;font-size:100%\"}\n\n### Strategy\n\n:::::: {.columns}\n::: {.column width=48%}\n\n__Variance Decomposition__. When group membership is known, the contribution of any value $x_{ij}$ to the variance can be split into two parts:  \n\n$$(x_{ij} - \\bar{x}) = (\\bar{x}_{j} - \\bar{x}) + (x_{ij} - \\bar{x}_{j})$$\n\nwhere\n\n- $i$ is the $i$th observation,  \n- $j$ is the $j$th group,  \n- $\\bar{x}$ is the between-group mean, and  \n- $\\bar{x_{j}}$ is the within-group mean (of group $j$).\n\n:::\n::: {.column width=52%}\n\n\n::: {.cell fig.asp='0.85'}\n::: {.cell-output-display}\n![](03-inference-slides_files/figure-html/unnamed-chunk-16-1.png){width=672}\n:::\n:::\n\n\n:::\n::::::\n\n### Sum of Squares\n\n:::::: {.columns}\n::: {.column width=48%}\n\nSum and square the differences for all $n$ observation and $m$ groups gives us:\n\n$$SS_{T} = SS_{B} + SS_{W}$$\n\nwhere \n\n- $SS_{T}$: Total Sum of Squares\n- $SS_{B}$: Between-group Sum of Squares\n- $SS_{W}$: Within-group Sum of Squares \n\n:::\n::: {.column width=52%}\n\n\n::: {.cell fig.asp='0.85'}\n::: {.cell-output-display}\n![](03-inference-slides_files/figure-html/unnamed-chunk-17-1.png){width=672}\n:::\n:::\n\n\n:::\n::::::\n\n### F-statistic\n\n\n::: {.cell}\n\n:::\n\n\nRatio of variances:\n\n$$F = \\frac{\\text{between-group variance}}{\\text{within-group variance}}$$\n\nwhere\n\n- Between-group variance = $SS_{B}/df_{B}$ and $df_{B}=m-1$.\n- Within-group variance = $SS_{W}/df_{W}$ and $df_{W}=m(n-1)$.\n\n__Question:__ Here, $F=$ 5.63. How probable is this estimate?  \n\nWe can answer this question by comparing the F-statistic to the F-distribution.  \n\n### F-distribution\n\n:::::: {.columns}\n::: {.column width=\"60%\"}\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](03-inference-slides_files/figure-html/unnamed-chunk-19-1.png){width=672}\n:::\n:::\n\n\n:::\n::: {.column width=\"40%\"}\n\n<br>\n\nSummary:  \n\n- $\\alpha = 0.05$\n- $H_{0}:$ no difference\n- $p=$ 0.003\n\n__Translation:__ the null hypothesis is really, really unlikely, so there must be some difference between groups!\n\n:::\n::::::\n\n:::::::::\n",
    "supporting": [
      "03-inference-slides_files\\figure-html"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}